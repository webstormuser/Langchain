{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Machine learning', 'summary': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.', 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}, page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\\n\\n\\n== History ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\\nAlthough the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\\nModern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on thes'),\n",
       " Document(metadata={'title': 'Attention (machine learning)', 'summary': 'Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but the later transformer design removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.', 'source': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)'}, page_content='Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but the later transformer design removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\n\\n== History ==\\n\\nAcademic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner.\\n\\n\\n=== Predecessors ===\\nSelective attention in humans had been well studied in neuroscience and cognitive psychology. In 1953, Colin Cherry studied selective attention in the context of audition, known as the cocktail party effect.\\nIn 1958, Donald Broadbent proposed the filter model of attention. Selective attention of vision was studied in the 1960s by George Sperling\\'s partial report paradigm. It was also noticed that saccade control is modulated by cognitive processes, insofar as the eye moves preferentially towards areas of high salience. As the fovea of the eye is small, the eye cannot sharply resolve the entire visual field at once. The use of saccade control allows the eye to quickly scan important features of a scene.\\nThese research developments inspired algorithms such as the Neocognitron and its variants. Meanwhile, developments in neural networks had inspired circuit models of biological visual attention. One well-cited network from 1998, for example, was inspired by the low-level primate visual system. It produced saliency maps of images using handcrafted (not learned) features, which were then used to guide a second neural network in processing patches of the image in order of reducing saliency.\\nA key aspect of attention mechanism can be written (schematically) as \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        ⟨\\n        (\\n        \\n          query\\n        \\n        \\n          )\\n          \\n            i\\n          \\n        \\n        ,\\n        (\\n        \\n          key\\n        \\n        \\n          )\\n          \\n            i\\n          \\n        \\n        ⟩\\n        (\\n        \\n          value\\n        \\n        \\n          )\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i}\\\\langle ({\\\\text{query}})_{i},({\\\\text{key}})_{i}\\\\rangle ({\\\\text{value}})_{i}}\\n  \\nwhere the angled brackets denote dot product. This shows that it involves a multiplicative operation. Multiplicative operations within artificial neural networks had been studied under the names of Group Method of Data Handling (1965) (where Kolmogorov-Gabor polynomials implement multiplicative units or \"gates\"), higher-order neural networks, multiplication units, sigma-pi units, fast weight controllers, and hyper-networks.\\n\\n\\n=== Linearized attention ===\\nJürgen Schmidhuber\\'s fast weight controller (1992) implements what was later called \"linearized attention\" or \"linear attention\" by Angelos Katharopoulos et al. (2020).  One of its two networks has  \"fast weights\" or \"dynamic links\" (1981). '),\n",
       " Document(metadata={'title': 'Neural network (machine learning)', 'summary': 'In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.', 'source': 'https://en.wikipedia.org/wiki/Neural_network_(machine_learning)'}, page_content='In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n\\n== Training ==\\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\\'s parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\\n\\n\\n== History ==\\n\\n\\n=== Early work ===\\nToday\\'s deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\\nHistorically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\\nWarren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\\nIn the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt\\'s perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester,'),\n",
       " Document(metadata={'title': 'Quantum machine learning', 'summary': 'Quantum machine learning is the integration of quantum algorithms within machine learning programs.\\nThe most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\\nBeyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\\nQuantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\\nFurthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".', 'source': 'https://en.wikipedia.org/wiki/Quantum_machine_learning'}, page_content='Quantum machine learning is the integration of quantum algorithms within machine learning programs.\\nThe most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\\nBeyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\\nQuantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\\nFurthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\\n\\n\\n== Machine learning with quantum computers ==\\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\\n\\n\\n=== Quantum associative memories and quantum pattern recognition ===\\nAssociative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.\\nTypical classical associative memories store p patterns in the \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  \\n interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.\\nUnfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, \\n  \\n    \\n      \\n        p\\n        ≤\\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle p\\\\leq O(n)}\\n  \\n.\\nQuantum associative memories'),\n",
       " Document(metadata={'title': 'Adversarial machine learning', 'summary': 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.', 'source': 'https://en.wikipedia.org/wiki/Adversarial_machine_learning'}, page_content='Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\\n\\n\\n== History ==\\nAt the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.\\nIn 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.\\nRecently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain\\'s Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches.\\nWhile adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.\\n\\n\\n=== Examples ===\\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users\\' template galleries that adapt to updated traits over time.\\nResearchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle w'),\n",
       " Document(metadata={'title': 'Active learning (machine learning)', 'summary': 'Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.\\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Active_learning_(machine_learning)'}, page_content='Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.\\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\\n\\n\\n== Definitions ==\\nLet T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.\\nDuring each iteration, i, T is broken up into three subsets\\n\\n  \\n    \\n      \\n        \\n          \\n            T\\n          \\n          \\n            K\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {T} _{K,i}}\\n  \\n: Data points where the label is known.\\n\\n  \\n    \\n      \\n        \\n          \\n            T\\n          \\n          \\n            U\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {T} _{U,i}}\\n  \\n: Data points where the label is unknown.\\n\\n  \\n    \\n      \\n        \\n          \\n            T\\n          \\n          \\n            C\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {T} _{C,i}}\\n  \\n: A subset of TU,i that is chosen to be labeled.\\nMost of the current research in active learning involves the best method to choose the data points for TC,i.\\n\\n\\n== Scenarios ==\\nPool-Based Sampling: In this approach, which is the most well known scenario, the learning algorithm attempts to evaluate the entire dataset before selecting data points (instances) for labeling. It is often initially trained on a fully labeled subset of the data using a machine-learning method such as logistic regression or SVM that yields class-membership probabilities for individual data instances. The candidate instances are those for which the prediction is most ambiguous. Instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner \"understands\" the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels. The theoretical drawback of pool-based sampling is that it is memory-intensive and is therefore limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory.\\nStream-Based Selective Sampling: Here, each consecutive unlabeled instance is examined one at a time with the machine evaluating the informati'),\n",
       " Document(metadata={'title': 'Boosting (machine learning)', 'summary': 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined as a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). A strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire answered the question in the affirmative in a paper published in 1990.This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\\nInitially, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. Algorithms that achieve this quickly became known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.', 'source': 'https://en.wikipedia.org/wiki/Boosting_(machine_learning)'}, page_content='In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined as a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). A strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire answered the question in the affirmative in a paper published in 1990.This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\\nInitially, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. Algorithms that achieve this quickly became known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\\n\\n\\n== Algorithms ==\\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners\\' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\\n\\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation), and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize.\\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.\\nThe main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\\n\\n\\n== Object categorization in computer vision ==\\n\\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\\n\\n\\n=== Problem of object categorization ===\\nObject categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There a'),\n",
       " Document(metadata={'title': 'Feature (machine learning)', 'summary': 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Feature_(machine_learning)'}, page_content='In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\\n\\n\\n== Feature types ==\\nIn feature engineering, two types of features are commonly used: numerical and categorical.\\nNumerical features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly.\\nCategorical features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features typically need to be converted to numerical features before they can be used in machine learning algorithms. This can be done using a variety of techniques, such as one-hot encoding, label encoding, and ordinal encoding.\\nThe type of feature that is used in feature engineering depends on the specific machine learning algorithm that is being used. Some machine learning algorithms, such as decision trees, can handle both numerical and categorical features. Other machine learning algorithms, such as linear regression, can only handle numerical features.\\n\\n\\n== Classification ==\\nA numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold.\\nAlgorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches.\\n\\n\\n== Examples ==\\n\\nIn character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.\\nIn speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others.\\nIn spam detection algorithms, features may include the presence or absence of certain email headers, \\nthe email structure, the language, the frequency of specific terms, the grammatical correctness of the text.\\nIn computer vision, there are a large number of possible features, such as edges and objects.\\n\\n\\n== Feature vectors ==\\n\\nIn pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression.  Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction.\\nThe vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.\\nHigher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases th'),\n",
       " Document(metadata={'title': 'Timeline of machine learning', 'summary': 'This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.', 'source': 'https://en.wikipedia.org/wiki/Timeline_of_machine_learning'}, page_content='This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.\\n\\n\\n== Overview ==\\n\\n\\n== Timeline ==\\n\\n\\n== See also ==\\nHistory of artificial intelligence\\nTimeline of artificial intelligence\\nTimeline of machine translation\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Works cited ===\\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York: BasicBooks. ISBN 0-465-02997-3.\\nMarr, Bernard (19 February 2016). \"A Short History of Machine Learning -- Every Manager Should Read\". Forbes. Archived from the original on 2022-12-05. Retrieved 2022-12-25.\\nRussell, Stuart; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach. London: Pearson Education. ISBN 0-137-90395-2.'),\n",
       " Document(metadata={'title': 'Transformer (deep learning architecture)', 'summary': 'A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).', 'source': 'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)'}, page_content='A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\\n\\n\\n== History ==\\n\\n\\n=== Predecessors ===\\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.\\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.  The linearly scaling fast weight controller (1992) learns  to compute a  weight matrix for further processing depending on the input. One of its two networks has  \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\\n\\n\\n=== Attention with seq2seq ===\\n\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see  for previous papers). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\\n(Sutskever et al, 2014)  was a 380M-parameter model for machine translation using two long short-term memory (LSTM). The architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, (Cho et al, 2014) was 130M-parameter model that used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are n'),\n",
       " Document(metadata={'title': 'Machine Learning (journal)', 'summary': 'Machine Learning  is a peer-reviewed scientific journal, published since 1986.\\nIn 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.\\nFollowing the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.', 'source': 'https://en.wikipedia.org/wiki/Machine_Learning_(journal)'}, page_content='Machine Learning  is a peer-reviewed scientific journal, published since 1986.\\nIn 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.\\nFollowing the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.\\n\\n\\n== Selected articles ==\\nJ.R. Quinlan (1986). \"Induction of Decision Trees\". Machine Learning. 1: 81–106. doi:10.1007/BF00116251.\\nNick Littlestone (1988). \"Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm\" (PDF). Machine Learning. 2 (4): 285–318. doi:10.1007/BF00116827.\\n\\nJohn R. Anderson and Michael Matessa (1992). \"Explorations of an Incremental, Bayesian Algorithm for Categorization\". Machine Learning. 9 (4): 275–308. doi:10.1007/BF00994109.\\nDavid Klahr (1994). \"Children, Adults, and Machines as Discovery Systems\". Machine Learning. 14 (3): 313–320. doi:10.1007/BF00993981.\\nThomas Dean and Dana Angluin and Kenneth Basye and Sean Engelson and Leslie Kaelbling and Evangelos Kokkevis and Oded Maron (1995). \"Inferring Finite Automata with Stochastic Output Functions and an Application to Map Learning\". Machine Learning. 18: 81–108. doi:10.1007/BF00993822.\\nLuc De Raedt and Luc Dehaspe (1997). \"Clausal Discovery\". Machine Learning. 26 (2/3): 99–146. doi:10.1023/A:1007361123060.\\nC. de la Higuera (1997). \"Characteristic Sets for Grammatical Inference\". Machine Learning. 27: 1–14.\\nRobert E. Schapire and Yoram Singer (1999). \"Improved Boosting Algorithms Using Confidence-rated Predictions\". Machine Learning. 37 (3): 297–336. doi:10.1023/A:1007614523901.\\nRobert E. Schapire and Yoram Singer (2000). \"BoosTexter: A Boosting-based System for Text Categorization\". Machine Learning. 39 (2/3): 135–168. doi:10.1023/A:1007649029923.\\nP. Rossmanith and T. Zeugmann (2001). \"Stochastic Finite Learning of the Pattern Languages\". Machine Learning. 44 (1–2): 67–91. doi:10.1023/A:1010875913047.\\nParekh, Rajesh; Honavar, Vasant (2001). \"Learning DFA from Simple Examples\". Machine Learning. 44 (1/2): 9–35. doi:10.1023/A:1010822518073.\\nAyhan Demiriz and Kristin P. Bennett and John Shawe-Taylor (2002). \"Linear Programming Boosting via Column Generation\". Machine Learning. 46: 225–254. doi:10.1023/A:1012470815092.\\nSimon Colton and Stephen Muggleton (2006). \"Mathematical Applications of Inductive Logic Programming\" (PDF). Machine Learning. 64 (1–3): 25–64. doi:10.1007/s10994-006-8259-x.\\nWill Bridewell and Pat Langley and Ljupco Todorovski and Saso Dzeroski (2008). \"Inductive Process Modeling\". Machine Learning.\\nStephen Muggleton and Alireza Tamaddoni-Nezhad (2008). \"QG/GA: a stochastic search for Progol\". Machine Learning. 70 (2–3): 121–133. doi:10.1007/s10994-007-5029-3.\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Ensemble learning', 'summary': 'In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Ensemble_learning'}, page_content='In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\\n\\n\\n== Overview ==\\nSupervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis.\\nEnsemble learning trains two or more Machine Learning algorithms to a specific classification or regression task. The algorithms within the ensemble learning model are generally referred as \"base models\", \"base learners\" or \"weak learners\" in literature. The base models can be constructed using a single modelling algorithm or several different algorithms. The idea is train a diverse collection of weak performing models to the same modelling task. As a result, the predicted or classified outcomes of each weak learner have poor predictive ability (high bias, i.e. high model errors) and among the collection of all weak learners the outcome and error values exhibit high variance. Fundamentally, an ensemble learning model trains many (at least 2) high-bias (weak) and high-variance (diverse) models to be combined into a stronger and better performing model. Essentially, it\\'s a set of algorithmic models — which would not produce satisfactory predictive results individually — that gets combined or averaged over all base models to produce a single high performing, accurate and low-variance model to fit the task as required.\\nEnsemble learning typically refers to Bagging (bootstrap-aggregating), Boosting or Stacking/Blending techniques to induce high variability among the base models. Bagging creates diversity by generating random samples from the training observations and fitting the same model to each different sample — also known as \"homogeneous parallel ensembles\". Boosting follows an iterative process by sequentially training each next base model on the up-weighted errors of the previous base model\\'s errors, producing an additive model to reduce the final model errors — also known as \"sequential ensemble learning\". Stacking or Blending consists of different base models, each trained independently (i.e. diverse/high variability) to be combined into the ensemble model — producing a \"heterogeneous parallel ensemble\". Common applications of ensemble learning include Random Forests (extension of Baggin), Boosted Tree-Models, Gradient Boosted Tree-Models and models in applications of stacking are generally more task-specific — such as combing clustering techniques with other parametric and/or non-parametric techniques. (See here for a comprehensive overview:\\nThe broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\\nEvaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning on one non-ensemble system. An ensemble system may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method.  Fast algorithms such as decision trees are commonly used in ens'),\n",
       " Document(metadata={'title': 'Hyperparameter (machine learning)', 'summary': \"In machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process. Hyperparameters can be classified as either model hyperparameters (such as the topology and size of a neural network) or algorithm hyperparameters (such as the learning rate and the batch size of an optimizer). These are named hyperparameters in contrast to parameters, which are characteristics that the model learns from the data.\\nHyperparameters are not required by every model or algorithm. Some simple algorithms such as ordinary least squares regression require none. However, the LASSO algorithm, for example, adds a regularization hyperparameter to ordinary least squares which must be set before training. Even models and algorithms without a strict requirement to define hyperparameters may not produce meaningful results if these are not carefully chosen. However, optimal values for hyperparameters are not always easy to predict. Some hyperparameters may have no meaningful effect, or one important variable may be conditional upon the value of another. Often a separate process of hyperparameter tuning is needed to find a suitable combination for the data and task. \\nAs well was improving model performance, hyperparameters can be used to by researchers introduce robustness and reproducibility into their work, especially if it uses models that incorporate random number generation.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)'}, page_content=\"In machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process. Hyperparameters can be classified as either model hyperparameters (such as the topology and size of a neural network) or algorithm hyperparameters (such as the learning rate and the batch size of an optimizer). These are named hyperparameters in contrast to parameters, which are characteristics that the model learns from the data.\\nHyperparameters are not required by every model or algorithm. Some simple algorithms such as ordinary least squares regression require none. However, the LASSO algorithm, for example, adds a regularization hyperparameter to ordinary least squares which must be set before training. Even models and algorithms without a strict requirement to define hyperparameters may not produce meaningful results if these are not carefully chosen. However, optimal values for hyperparameters are not always easy to predict. Some hyperparameters may have no meaningful effect, or one important variable may be conditional upon the value of another. Often a separate process of hyperparameter tuning is needed to find a suitable combination for the data and task. \\nAs well was improving model performance, hyperparameters can be used to by researchers introduce robustness and reproducibility into their work, especially if it uses models that incorporate random number generation.\\n\\n\\n== Considerations ==\\nThe time required to train and test a model can depend upon the choice of its hyperparameters. A hyperparameter is usually of continuous or integer type, leading to mixed-type optimization problems. The existence of some hyperparameters is conditional upon the value of others, e.g. the size of each hidden layer in a neural network can be conditional upon the number of layers.\\n\\n\\n=== Difficulty-learnable parameters ===\\nThe objective function is typically non-differentiable with respect to hyperparameters. As a result, in most instances, hyperparameters cannot be learned using gradient-based optimization methods (such as gradient descent), which are commonly employed to learn model parameters. These hyperparameters are those parameters describing a model representation that cannot be learned by common optimization methods, but nonetheless affect the loss function. An example would be the tolerance hyperparameter for errors in support vector machines.\\n\\n\\n=== Untrainable parameters ===\\nSometimes, hyperparameters cannot be learned from the training data because they aggressively increase the capacity of a model and can push the loss function to an undesired minimum (overfitting to the data), as opposed to correctly mapping the richness of the structure in the data. For example, if we treat the degree of a polynomial equation fitting a regression model as a trainable parameter, the degree would increase until the model perfectly fit the data, yielding low training error, but poor generalization performance.\\n\\n\\n=== Tunability ===\\nMost performance variation can be attributed to just a few hyperparameters. The tunability of an algorithm, hyperparameter, or interacting hyperparameters is a measure of how much performance can be gained by tuning it. For an LSTM, while the learning rate followed by the network size are its most crucial hyperparameters, batching and momentum have no significant effect on its performance.\\nAlthough some research has advocated the use of mini-batch sizes in the thousands, other work has found the best performance with mini-batch sizes between 2 and 32.\\n\\n\\n=== Robustness ===\\nAn inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance. Methods that are not robust to simple changes in hyperparameters, random seeds, or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification.\\nReinforcement learn\"),\n",
       " Document(metadata={'title': 'Support vector machine', 'summary': 'In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). \\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in the higher dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces where linear classification can be performed.  Being max-margin models, SVMs are resilient to noisy data (for example, mis-classified examples). SVMs can also be used for regression tasks, where the objective becomes \\n  \\n    \\n      \\n        ϵ\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon }\\n  \\n-sensitive.\\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \\nThe popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Support_vector_machine'}, page_content='In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). \\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in the higher dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces where linear classification can be performed.  Being max-margin models, SVMs are resilient to noisy data (for example, mis-classified examples). SVMs can also be used for regression tasks, where the objective becomes \\n  \\n    \\n      \\n        ϵ\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon }\\n  \\n-sensitive.\\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \\nThe popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.\\n\\n\\n== Motivation ==\\n\\nClassifying data is a common task in machine learning.\\nSuppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  \\n-dimensional vector (a list of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  \\n numbers), and we want to know whether we can separate such points with a \\n  \\n    \\n      \\n        (\\n        p\\n        −\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (p-1)}\\n  \\n-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.\\nMore formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier. A lower generalization error means that the implementer is less likely to experience overfitting.\\n\\nWhereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional '),\n",
       " Document(metadata={'title': 'Artificial intelligence', 'summary': 'Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}, page_content='Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning and problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and a'),\n",
       " Document(metadata={'title': 'Automated machine learning', 'summary': 'Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \\nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \\nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.', 'source': 'https://en.wikipedia.org/wiki/Automated_machine_learning'}, page_content='Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \\nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \\nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\\n\\n\\n== Comparison to the standard approach ==\\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. \\nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.\\n\\n\\n== Targets of automation ==\\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\\n\\nData preparation and ingestion (from raw data and miscellaneous formats)\\nColumn type detection; e.g., Boolean, discrete numerical, continuous numerical, or text\\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\\nTask detection; e.g., binary classification, regression, clustering, or ranking\\nFeature engineering\\nFeature selection\\nFeature extraction\\nMeta-learning and transfer learning\\nDetection and handling of skewed data and/or missing values\\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\\nHyperparameter optimization of the learning algorithm and featurization\\nNeural architecture search\\nPipeline selection under time, memory, and complexity constraints\\nSelection of evaluation metrics and validation procedures\\nProblem checking\\nLeakage detection\\nMisconfiguration detection\\nAnalysis of obtained results\\nCreating user interfaces and visualizations\\n\\n\\n== Challenges and Limitations ==\\nThere are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as \"development as a cottage industry\". This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it\\'s the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the m'),\n",
       " Document(metadata={'title': 'Tensor (machine learning)', 'summary': 'Tensor informally refers in machine learning to two different concepts that organize and represent data. Data may be organized in a multidimensional array (M-way array) that is informally referred to as a \"data tensor\";  however in the strict mathematical sense, a tensor is a multilinear mapping over a set of domain vector spaces to a range vector space.  Observations, such as images, movies, volumes, sounds, and relationships among words and concepts, stored in an M-way array (\"data tensor\") may be analyzed either by artificial neural networks or tensor methods.\\nTensor decomposition can factorize data tensors into smaller tensors. Operations on data tensors can be expressed in terms of matrix multiplication and the Kronecker product.  The computation of gradients, an important aspect of the backpropagation algorithm, can be performed using PyTorch and TensorFlow.\\nComputations are often performed on graphics processing units (GPUs) using CUDA and on dedicated hardware such as Google\\'s Tensor Processing Unit or Nvidia\\'s Tensor core. These developments have greatly accelerated neural network architectures and increased the size and complexity of models that can be trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Tensor_(machine_learning)'}, page_content='Tensor informally refers in machine learning to two different concepts that organize and represent data. Data may be organized in a multidimensional array (M-way array) that is informally referred to as a \"data tensor\";  however in the strict mathematical sense, a tensor is a multilinear mapping over a set of domain vector spaces to a range vector space.  Observations, such as images, movies, volumes, sounds, and relationships among words and concepts, stored in an M-way array (\"data tensor\") may be analyzed either by artificial neural networks or tensor methods.\\nTensor decomposition can factorize data tensors into smaller tensors. Operations on data tensors can be expressed in terms of matrix multiplication and the Kronecker product.  The computation of gradients, an important aspect of the backpropagation algorithm, can be performed using PyTorch and TensorFlow.\\nComputations are often performed on graphics processing units (GPUs) using CUDA and on dedicated hardware such as Google\\'s Tensor Processing Unit or Nvidia\\'s Tensor core. These developments have greatly accelerated neural network architectures and increased the size and complexity of models that can be trained.\\n\\n\\n== History ==\\nA tensor is by definition a multilinear map. In mathematics, this may express a multilinear relationship between sets of algebraic objects. In physics, tensor fields, considered as tensors at each point in space, are useful in expressing mechanics such as stress or elasticity. In machine learning, the exact use of tensors depends on the statistical approach being used.\\nIn 2001, the field of signal processing and statistics were making use of tensor methods. Pierre Comon surveys the early adoption of tensor methods in the fields of telecommunications, radio surveillance, chemometrics and sensor processing. Linear tensor rank methods (such as, Parafac/CANDECOMP) analyzed M-way arrays (\"data tensors\") composed of higher order statistics that were employed in blind source separation problems to compute a linear model of the data. He noted several early limitations in determining the tensor rank and efficient tensor rank decomposition.\\nIn the early 2000s, multilinear tensor methods crossed over into computer vision, computer graphics and machine learning with papers by Vasilescu  or in collaboration with Terzopoulos, such as Human Motion Signatures, TensorFaces  TensorTexures and Multilinear Projection.  Multilinear algebra, the algebra of higher-order tensors, is a suitable and transparent framework for analyzing the multifactor structure of an ensemble of observations and for addressing the difficult problem of disentangling the causal factors based on second order or higher order statistics associated with each causal factor.  \\nTensor (multilinear) factor analysis disentangles and reduces the influence of different causal factors with multilinear subspace learning.  \\nWhen treating an image or a video as a 2- or 3-way array, i.e., \"data matrix/tensor\",  tensor methods reduce spatial or time redundancies as demonstrated by Wang and Ahuja.\\nYoshua Bengio,\\nGeoff Hinton\\n and their collaborators briefly discuss the relationship between deep neural networks \\nand tensor factor analysis beyond the use of M-way arrays (\"data tensors\") as inputs.   One of the early uses of tensors for neural networks appeared in natural language processing. A single word can be expressed as a vector via Word2vec. Thus a relationship between two words can be encoded in a matrix. However, for more complex relationships such as subject-object-verb, it is necessary to build higher-dimensional networks. In 2009, the work of Sutskever introduced Bayesian Clustered Tensor Factorization to model relational concepts while reducing the parameter space. From 2014 to 2015, tensor methods become more common in convolutional neural networks (CNNs). Tensor methods organize neural network weights in a \"data tensor\", analyze and reduce the number of neural network weights.  Lebedev et al. ac'),\n",
       " Document(metadata={'title': 'Torch (machine learning)', 'summary': 'Torch is an open-source machine learning library, \\na scientific computing framework, and a scripting language based on Lua. It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created by the Idiap Research Institute at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Torch_(machine_learning)'}, page_content='Torch is an open-source machine learning library, \\na scientific computing framework, and a scripting language based on Lua. It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created by the Idiap Research Institute at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.\\n\\n\\n== torch ==\\nThe core package of Torch is torch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix–vector multiplication, matrix–matrix multiplication  and matrix product.\\nThe following exemplifies using torch via its REPL interpreter:\\n\\nThe torch package also simplifies object-oriented programming and serialization by providing various convenience functions which are used throughout its packages. The torch.class(classname, parentclass) function can be used to create object factories (classes). When the constructor is called, torch initializes and sets a Lua table with the user-defined metatable, which makes the table an object.\\nObjects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua coroutines, and Lua userdata. However, userdata can be serialized if it is wrapped by a table (or metatable) that provides  read() and write() methods.\\n\\n\\n== nn ==\\nThe nn package is used for building neural networks. It is divided into modular objects that share a common Module interface. Modules have a forward() and backward() method that allow them to feedforward and backpropagate, respectively. Modules can be joined using module composites, like Sequential, Parallel and Concat to create complex task-tailored graphs. Simpler modules like Linear, Tanh and Max make up the basic component modules. This modular interface provides first-order automatic gradient differentiation. What follows is an example use-case for building a multilayer perceptron using Modules:\\n\\nLoss functions are implemented as sub-classes of Criterion, which has a similar interface to Module. It also has forward() and backward() methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the mean squared error criterion implemented in MSECriterion and the cross-entropy criterion implemented in ClassNLLCriterion. What follows is an example of a Lua function that can be iteratively called to train \\nan mlp Module on input Tensor x, target Tensor y with a scalar learningRate:    \\n\\nIt also has StochasticGradient class for training a neural network using stochastic gradient descent, although the optim package provides much more options in this respect, like momentum and weight decay regularization.\\n\\n\\n== Other packages ==\\nMany packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. They can be installed with LuaRocks, the Lua package manager which is also included with the Torch distribution.\\n\\n\\n== Applications ==\\nTorch is used by the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build hardware implementations for data flows like those found in neural networks.\\nFacebook has released a set of extension modules as open source software.\\n\\n\\n== See also ==\\nComparison of deep learning software\\nPyTorch\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'),\n",
       " Document(metadata={'title': 'Machine learning control', 'summary': 'Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\\nwhich solves optimal control problems with methods of machine learning.\\nKey applications are complex nonlinear systems\\nfor which linear control theory methods are not applicable.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Machine_learning_control'}, page_content='Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\\nwhich solves optimal control problems with methods of machine learning.\\nKey applications are complex nonlinear systems\\nfor which linear control theory methods are not applicable.\\n\\n\\n== Types of problems and tasks ==\\nFour types of problems are commonly encountered.\\n\\nControl parameter identification: MLC translates to a parameter identification if the structure of the control law is given but the parameters are unknown. One example is the genetic algorithm for optimizing coefficients of a PID controller or discrete-time optimal control.\\nControl design as regression problem of the first kind:  MLC approximates a general nonlinear mapping from sensor signals to actuation commands, if the sensor signals and the optimal actuation command are known for every state. One example is the computation of sensor feedback from a known full state feedback. A neural network is commonly used technique for this task.\\nControl design as regression problem of the second kind: MLC may also identify arbitrary nonlinear control laws which minimize the cost function of the plant. In this case, neither a model, nor the control law structure,  nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. Genetic programming is a powerful regression technique for this purpose.\\nReinforcement learning control: The control law may be continually updated over measured performance changes (rewards) using reinforcement learning.\\nMLC comprises, for instance, neural network control, \\ngenetic algorithm based control, \\ngenetic programming control,\\nreinforcement learning control, \\nand has methodological overlaps with other data-driven control,\\nlike artificial intelligence and robot control.\\n\\n\\n== Applications ==\\nMLC has been successfully applied\\nto many nonlinear control problems,\\nexploring unknown and often unexpected actuation mechanisms.\\nExample applications include\\n\\nAttitude control of satellites.\\nBuilding thermal control.\\nFeedback turbulence control.\\nRemotely operated underwater vehicles.\\nMany more engineering MLC application are summarized in the review article of PJ Fleming & RC Purshouse (2002).\\nAs for all general nonlinear methods,\\nMLC comes with no guaranteed convergence, \\noptimality or robustness for a range of operating conditions.\\n\\n\\n== References ==\\n\\n\\n== Further reading =='),\n",
       " Document(metadata={'title': 'Leakage (machine learning)', 'summary': \"In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\\nLeakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.\", 'source': 'https://en.wikipedia.org/wiki/Leakage_(machine_learning)'}, page_content='In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model\\'s utility when run in a production environment.\\nLeakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.\\n\\n\\n== Leakage modes ==\\nLeakage can occur in many steps in the machine learning process. The leakage causes can be sub-classified into two possible sources of leakage for a model: features and training examples.\\n\\n\\n=== Feature leakage ===\\nFeature or column-wise leakage is caused by the inclusion of columns which are one of the following: a duplicate label, a proxy for the label, or the label itself. These features, known as anachronisms, will not be available when the model is used for predictions, and result in leakage if included when the model is trained.\\nFor example, including a \"MonthlySalary\" column when predicting \"YearlySalary\"; or \"MinutesLate\" when predicting \"IsLate\".\\n\\n\\n=== Training example leakage ===\\nRow-wise leakage is caused by improper sharing of information between rows of data. Types of row-wise leakage include:\\n\\nPremature featurization; leaking from premature featurization before Cross-validation/Train/Test split (must fit MinMax/ngrams/etc on only the train split, then transform the test set)\\nDuplicate rows between train/validation/test (e.g. oversampling a dataset to pad its size before splitting; e.g. different rotations/augmentations of a single image; bootstrap sampling before splitting; or duplicating rows to up sample the minority class)\\nNon-i.i.d. data\\nTime leakage (e.g. splitting a time-series dataset randomly instead of newer data in test set using a TrainTest split or rolling-origin cross validation)\\nGroup leakage—not including a grouping split column (e.g. Andrew Ng\\'s group had 100k x-rays of 30k patients, meaning ~3 images per patient. The paper used random splitting instead of ensuring that all images of a patient were in the same split. Hence the model partially memorized the patients instead of learning to recognize pneumonia in chest x-rays.)\\nA 2023 review found data leakage to be \"a widespread failure mode in machine-learning (ML)-based science\", having affected at least 294 academic publications across 17 disciplines, and causing a potential reproducibility crisis.\\n\\n\\n== Detection ==\\n\\n\\n== See also ==\\nAutoML\\nConcept drift (where the structure of the system being studied evolves over time, invalidating the model)\\nOverfitting\\nResampling (statistics)\\nSupervised learning\\nTraining, validation, and test sets\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Deep learning', 'summary': 'Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.', 'source': 'https://en.wikipedia.org/wiki/Deep_learning'}, page_content='Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\\n\\n\\n== Overview ==\\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\\nImportantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and he'),\n",
       " Document(metadata={'title': 'Transduction (machine learning)', 'summary': 'In logic, statistical inference, and supervised learning,\\ntransduction or transductive inference is reasoning from\\nobserved, specific (training) cases to specific (test) cases. In contrast,\\ninduction is reasoning from observed training cases\\nto general rules, which are then applied to the test cases. The distinction is\\nmost interesting in cases where the predictions of the transductive model are\\nnot achievable by any inductive model. Note that this is caused by transductive\\ninference on different test sets producing mutually inconsistent predictions.\\nTransduction was introduced in a computer science context by Vladimir Vapnik in the 1990s, motivated by\\nhis view that transduction is preferable to induction since, according to him, induction requires\\nsolving a more general problem (inferring a function) before solving a more\\nspecific problem (computing outputs for new cases): \"When solving a problem of\\ninterest, do not solve a more general problem as an intermediate step. Try to\\nget the answer that you really need but not a more general one.\".\\nAn example of learning which is not inductive would be in the case of binary\\nclassification, where the inputs tend to cluster in two groups. A large set of\\ntest inputs may help in finding the clusters, thus providing useful information\\nabout the classification labels. The same predictions would not be obtainable\\nfrom a model which induces a function based only on the training cases.  Some\\npeople may call this an example of the closely related semi-supervised learning, since Vapnik\\'s motivation is quite different. \\nThe most well-known example of a case-bases learning algorithm is the k-nearest neighbor algorithm, which is related to transductive learning algorithms.\\nAnother example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).\\nA third possible motivation of transduction arises through the need\\nto approximate. If exact inference is computationally prohibitive, one may at\\nleast try to make sure that the approximations are good at the test inputs. In\\nthis case, the test inputs could come from an arbitrary distribution (not\\nnecessarily related to the distribution of the training inputs), which wouldn\\'t\\nbe allowed in semi-supervised learning. An example of an algorithm falling in\\nthis category is the Bayesian Committee Machine (BCM).\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Transduction_(machine_learning)'}, page_content='In logic, statistical inference, and supervised learning,\\ntransduction or transductive inference is reasoning from\\nobserved, specific (training) cases to specific (test) cases. In contrast,\\ninduction is reasoning from observed training cases\\nto general rules, which are then applied to the test cases. The distinction is\\nmost interesting in cases where the predictions of the transductive model are\\nnot achievable by any inductive model. Note that this is caused by transductive\\ninference on different test sets producing mutually inconsistent predictions.\\nTransduction was introduced in a computer science context by Vladimir Vapnik in the 1990s, motivated by\\nhis view that transduction is preferable to induction since, according to him, induction requires\\nsolving a more general problem (inferring a function) before solving a more\\nspecific problem (computing outputs for new cases): \"When solving a problem of\\ninterest, do not solve a more general problem as an intermediate step. Try to\\nget the answer that you really need but not a more general one.\".\\nAn example of learning which is not inductive would be in the case of binary\\nclassification, where the inputs tend to cluster in two groups. A large set of\\ntest inputs may help in finding the clusters, thus providing useful information\\nabout the classification labels. The same predictions would not be obtainable\\nfrom a model which induces a function based only on the training cases.  Some\\npeople may call this an example of the closely related semi-supervised learning, since Vapnik\\'s motivation is quite different. \\nThe most well-known example of a case-bases learning algorithm is the k-nearest neighbor algorithm, which is related to transductive learning algorithms.\\nAnother example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).\\nA third possible motivation of transduction arises through the need\\nto approximate. If exact inference is computationally prohibitive, one may at\\nleast try to make sure that the approximations are good at the test inputs. In\\nthis case, the test inputs could come from an arbitrary distribution (not\\nnecessarily related to the distribution of the training inputs), which wouldn\\'t\\nbe allowed in semi-supervised learning. An example of an algorithm falling in\\nthis category is the Bayesian Committee Machine (BCM).\\n\\n\\n== Historical Context ==\\nThe mode of inference from particulars to particulars, which Vapnik came to call transduction, was already distinguished from the mode of inference from particulars to generalizations in part III of the Cambridge philosopher and logician W.E. Johnson\\'s 1924 textbook, Logic. In Johnson\\'s work, the former mode was called \\'eduction\\' and the latter was called \\'induction\\'. Bruno de Finetti developed a purely subjective form of Bayesianism in which claims about objective chances could be translated into empirically respectable claims about subjective credences with respect to observables through exchangeability properties. An early statement of this view can be found in his 1937 La Prévision: ses Lois Logiques, ses Sources Subjectives and a mature statement in his 1970 Theory of Probability. Within de Finetti\\'s subjective Bayesian framework, all inductive inference is ultimately inference from particulars to particulars. \\n\\n\\n== Example problem ==\\nThe following example problem contrasts some of the unique properties of transduction against induction.\\n\\nA collection of points is given, such that some of the points are labeled (A, B, or C), but most of the points are unlabeled (?). The goal is to predict appropriate labels for all of the unlabeled points.\\nThe inductive approach to solving this problem is to use the labeled points to train a supervised learning algorithm, and then have it predict labels for all of the unlabeled points. With this problem, however, the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model. It will certainly struggle to bui'),\n",
       " Document(metadata={'title': 'Fairness (machine learning)', 'summary': \"Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. For example gender, ethnicity, sexual orientation or disability. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumer.\", 'source': 'https://en.wikipedia.org/wiki/Fairness_(machine_learning)'}, page_content='Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. For example gender, ethnicity, sexual orientation or disability. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people\\'s lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumer.\\n\\n\\n== Context ==\\nDiscussion about fairness in machine learning is a relatively recent topic. Since 2016 there has been a sharp increase in research into the topic. This increase could be partly accounted to an influential report by ProPublica that claimed that the COMPAS software, widely used in US courts to predict recidivism, was racially biased. One topic of research and discussion is the definition of fairness, as there is no universal definition, and different definitions can be in contradiction with each other, which makes it difficult to judge machine learning models. Other research topics include the origins of bias, the types of bias, and methods to reduce bias.\\nIn recent years tech companies have made tools and manuals on how to detect and reduce bias in machine learning. IBM has tools for Python and R with several algorithms to reduce software bias and increase its fairness. Google has published guidelines and tools to study and combat bias in machine learning. Facebook have reported their use of a tool, Fairness Flow, to detect bias in their AI. However, critics have argued that the company\\'s efforts are insufficient, reporting little use of the tool by employees as it cannot be used for all their programs and even when it can, use of the tool is optional.\\nIt is important to note that the discussion about quantitative ways to test fairness and unjust discrimination in decision-making predates by several decades the rather recent debate on fairness in machine learning. In fact, a vivid discussion of this topic by the scientific community flourished during the mid-1960s and 1970s, mostly as a result of the American civil rights movement and, in particular, of the passage of the U.S. Civil Rights Act of 1964. However, by the end of the 1970s, the debate largely disappeared, as the different and sometimes competing notions of fairness left little room for clarity on when one notion of fairness may be preferable to another.\\n\\n\\n=== Language Bias ===\\nLanguage bias refers a type of statistical sampling bias tied to the language of a query that leads to \"a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository.\" Luo et al. show that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent. Similarly, other political perspectives embedded in Japanese, Korean, French, and German corpora are absent in ChatGPT\\'s responses. ChatGPT, covered itself as a multilingual ch'),\n",
       " Document(metadata={'title': 'Supervised learning', 'summary': 'Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Supervised_learning'}, page_content='Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\n\\n== Steps to follow ==\\nTo solve a given problem of supervised learning, one has to perform the following steps:\\n\\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.\\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.\\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\\n\\n\\n== Algorithm choice ==\\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\nThere are four major issues to consider in supervised learning:\\n\\n\\n=== Bias-variance tradeoff ===\\n\\nA first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n. A learning algorithm has high variance for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff bet'),\n",
       " Document(metadata={'title': 'Online machine learning', 'summary': 'In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.', 'source': 'https://en.wikipedia.org/wiki/Online_machine_learning'}, page_content='In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\\n\\n\\n== Introduction ==\\nIn the setting of supervised learning, a function of \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        →\\n        Y\\n      \\n    \\n    {\\\\displaystyle f:X\\\\to Y}\\n  \\n is to be learned, where \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n  \\n is thought of as a space of inputs and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n  \\n as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution \\n  \\n    \\n      \\n        p\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle p(x,y)}\\n  \\n on \\n  \\n    \\n      \\n        X\\n        ×\\n        Y\\n      \\n    \\n    {\\\\displaystyle X\\\\times Y}\\n  \\n. In reality, the learner never knows the true distribution \\n  \\n    \\n      \\n        p\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle p(x,y)}\\n  \\n over instances. Instead, the learner usually has access to a training set of examples \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            1\\n          \\n        \\n        )\\n        ,\\n        …\\n        ,\\n        (\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},y_{1}),\\\\ldots ,(x_{n},y_{n})}\\n  \\n. In this setting, the loss function is given as \\n  \\n    \\n      \\n        V\\n        :\\n        Y\\n        ×\\n        Y\\n        →\\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle V:Y\\\\times Y\\\\to \\\\mathbb {R} }\\n  \\n, such that \\n  \\n    \\n      \\n        V\\n        (\\n        f\\n        (\\n        x\\n        )\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle V(f(x),y)}\\n  \\n measures the difference between the predicted value \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  \\n and the true value \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n. The ideal goal is to select a function \\n  \\n    \\n      \\n        f\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f\\\\in {\\\\mathcal {H}}}\\n  \\n, where \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n  \\n is a space of functions called a hypothesis space, so that some notion of total loss is minimized. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.\\n\\n\\n== Statistical view of online learning ==\\nIn statistical learning models, the training sample \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},y_{i})}\\n  \\n are assumed to have been drawn from the true distribution \\n  \\n    \\n      \\n        p\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle p(x,y)}\\n  \\n and the objective is to minimize the expected \"risk\"\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How recursively split text by characters from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "loader = WikipediaLoader(query=\"Machine Learning\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2021-11-13', 'Title': 'Full-attention based Neural Architecture Search using Context Auto-regression', 'Authors': 'Yuan Zhou, Haiyang Wang, Shuwei Huo, Boyu Wang', 'Summary': 'Self-attention architectures have emerged as a recent advancement for\\nimproving the performance of vision tasks. Manual determination of the\\narchitecture for self-attention networks relies on the experience of experts\\nand cannot automatically adapt to various scenarios. Meanwhile, neural\\narchitecture search (NAS) has significantly advanced the automatic design of\\nneural architectures. Thus, it is appropriate to consider using NAS methods to\\ndiscover a better self-attention architecture automatically. However, it is\\nchallenging to directly use existing NAS methods to search attention networks\\nbecause of the uniform cell-based search space and the lack of long-term\\ncontent dependencies. To address this issue, we propose a full-attention based\\nNAS method. More specifically, a stage-wise search space is constructed that\\nallows various attention operations to be adopted for different layers of a\\nnetwork. To extract global features, a self-supervised search algorithm is\\nproposed that uses context auto-regression to discover the full-attention\\narchitecture. To verify the efficacy of the proposed methods, we conducted\\nextensive experiments on various learning tasks, including image\\nclassification, fine-grained image recognition, and zero-shot image retrieval.\\nThe empirical results show strong evidence that our method is capable of\\ndiscovering high-performance, full-attention architectures while guaranteeing\\nthe required search efficiency.'}, page_content='1\\nFull-attention based Neural Architecture Search\\nusing Context Auto-regression\\nYuan Zhou, Haiyang Wang, Shuwei Huo and Boyu Wang\\nAbstract—Self-attention architectures have emerged as a recent\\nadvancement for improving the performance of vision tasks.\\nManual determination of the architecture for self-attention net-\\nworks relies on the experience of experts and cannot automati-\\ncally adapt to various scenarios. Meanwhile, neural architecture\\nsearch (NAS) has signiﬁcantly advanced the automatic design of\\nneural architectures. Thus, it is appropriate to consider using\\nNAS methods to discover a better self-attention architecture\\nautomatically. However, it is challenging to directly use existing\\nNAS methods to search attention networks because of the\\nuniform cell-based search space and the lack of long-term content\\ndependencies. To address this issue, we propose a full-attention\\nbased NAS method. More speciﬁcally, a stage-wise search space\\nis constructed that allows various attention operations to be\\nadopted for different layers of a network. To extract global\\nfeatures, a self-supervised search algorithm is proposed that uses\\ncontext auto-regression to discover the full-attention architecture.\\nTo verify the efﬁcacy of the proposed methods, we conducted\\nextensive experiments on various learning tasks, including image\\nclassiﬁcation, ﬁne-grained image recognition, and zero-shot image\\nretrieval. The empirical results show strong evidence that our\\nmethod is capable of discovering high-performance, full-attention\\narchitectures while guaranteeing the required search efﬁciency.\\nIndex Terms—neural architecture search, full-attention archi-\\ntecture, context auto-regression.\\nI. INTRODUCTION\\nThe design and construction of neural network architectures\\nare critical concerns because better network architectures\\nusually lead to signiﬁcant performance improvements. How-\\never, deep neural architectures often require elaborate design\\nfor speciﬁc tasks, which indicates that designing a suitable\\narchitecture requires tremendous effort from human experts.\\nTo eliminate such extensive engineering, neural architecture\\nsearch (NAS) methods [1]–[4] have been proposed to auto-\\nmate the design of neural architectures. Many architectures\\nproduced by NAS methods have achieved higher accuracy\\nthan those manually designed for tasks, such as image clas-\\nsiﬁcation [1], [4], semantic segmentation [5], [6], and object\\ndetection [7]. NAS methods boost the model’s performance,\\nand free human experts from the tedious task of tweaking the\\narchitecture. A trend has been growing towards automatically\\ndesigning neural network architectures instead of relying on\\nhuman effort and experience.\\nRecently, self-attention network design [8]–[10] has made\\nsigniﬁcant progress. Content-based interactions and the ability\\nYuan Zhou, Haiyang Wang and Shuwei Huo are with the School of\\nElectrical and Information Engineering, Tianjin University, Tianjin 300072,\\nChina.\\nBoyu Wang is with the Department of Computer Science, University of\\nWestern Ontario, Canada.\\nto capture long-term dependencies have made self-attention a\\ncritical component in neural networks. Ramachandran et al.\\n[8] propose a pure self-attention vision model that replaces\\nevery spatial convolution with a self-attention operator. Zhao\\net al. [10] explore variations in the self-attention operator\\nand assess the effectiveness of image recognition models that\\nare based fully on self-attention. These studies show that\\nself-attention operations can be the basic building block to\\nbuild image recognition models. Although the development of\\nself-attention network designs has made signiﬁcant progress,\\nmanually designing appropriate full-attention architectures is\\nstill a challenging task that requires substantial efforts by\\nhuman experts, especially as the number of design choices\\nincreases. There is increasing interest in the automatic design\\nof neural network architectures, rather than relying on the\\nknowledge and experience of human experts. It is essential to\\nconsider using NAS methods to pursue better a self-attention\\narchitecture design.\\nMost existing NAS methods are designed to search con-\\nvolutional neural networks, but it is challenging to adopt\\nthese methods to search for full-attention networks. First,\\nthese methods [2], [4], [11] usually adopt cell-based search\\nspaces, where the cell structures in the shallow and deep\\nlayers of the network are identical. This is not suitable for\\nself-attention structures because self-attention plays different\\nroles in different stages of the network. Second, existing NAS\\nmethods [1], [12], [13] generally use classiﬁcation tasks as the\\nsupervision of architecture search and guide the optimization\\nof network structures using classiﬁcation accuracy as the eval-\\nuation criterion. The classiﬁcation task requires the model to\\npay more attention to extracting features from the local region\\nrelated to the classiﬁcation label without considering long-\\nterm dependency. Nevertheless, self-attention models focus on\\ncapturing long-term content dependencies between pixels to\\nlearn rich and broadly transferable representations. Therefore,\\nsearch methods that use classiﬁcation tasks as the supervision\\nare not suitable for searching attention structures.\\nTo address these issues, this paper proposes a novel NAS\\nmethod to design full-attention architectures automatically.\\nFirst, a stage-wise search space is established that allows the\\nsearch algorithm to choose different self-attention operations\\nfor each stage of the network more ﬂexibly. Moreover, a\\nnew self-supervised task is designed based on context auto-\\nregression which reconstructs local missing content by inte-\\ngrating it with the global feature. Using the proposed context\\nauto-regression as supervision, long-term dependencies across\\npixels can be extracted without requiring the images’ label\\ninformation.\\narXiv:2111.07139v1  [cs.CV]  13 Nov 2021\\n2\\nSpeciﬁcally, based on the designed self-supervised task,\\nwe propose a task-wise search method. The method includes\\na search phase and a ﬁne-tuning phase with different tasks\\nbeing used to supervise the architecture search in different\\nphases. In the search phase, the designed self-supervised task\\nis used as the supervision, encouraging the self-attention model\\nto capture long-term dependencies. In the ﬁne-tuning stage,\\nclassiﬁcation task is used as the supervision to search for self-\\nattention network structures.\\nThe main contributions of this paper are summarized as\\nfollows:\\n1) A full-attention based neural architecture search method\\nis proposed that automatically designs networks that use self-\\nattention operations as the primary building block.\\n2) A stage-wise search space is established in which differ-\\nent self-attention operations can be selected for each stage of\\nthe network.\\n3) A new self-supervised task based on context auto-\\nregression is designed and used to train a self-supervised\\nsearch algorithm. Compared to supervised learning-based\\nsearch, the proposed self-supervised search enables the capture\\nof long-range dependencies, thus constructing better full-\\nattention networks.\\n4) Extensive experiments demonstrate that our method can\\ndiscover high-performance full-attention networks for image\\nclassiﬁcation, ﬁne-grained image recognition and zero-shot\\nimage retrieval.\\nII. RELATED WORK\\nNeural Architecture Search. Recently, NAS which aims to\\ndesign neural networks automatically has attracted increasing\\nattention. Existing NAS approaches can be roughly divided\\ninto three categories, namely, reinforcement learning (RL)-\\nbased methods [1], [2], [11], [14], evolutionary algorithms\\n(EA)-based methods [3], [15], [16], and gradient-based meth-\\nods [4], [12], [13], [17]. RL-based methods train a recurrent\\nneural network as a controller to generate a series of ac-\\ntions to specify the CNN architecture. An alternative search\\ntechnique is to use evolutionary algorithms, that ”evolve” the\\narchitecture by mutating the optimal architectures to ﬁnd a\\nneural architecture tailored for a given task. Although these\\nworks achieved state-of-the-art results on various classiﬁcation\\ntasks, their main disadvantage was that they demand excessive\\ncomputational resources.\\nTo alleviate this issue, gradient-based NAS methods have\\nbeen proposed to speed up the searching process. In contrast\\nto treating architecture search as a black-box optimization\\nproblem, gradient-based NAS methods relax the discrete and\\nnon-differentiable architecture representations to be continu-\\nous, so that the gradient obtained during the training process\\ncan be used to optimize the architecture. Therefore, gradient-\\nbased methods successfully accelerate the architecture search\\nprocess, and usually require only a few GPU days. In addition,\\nexisting methods for CNN usually require the model to extract\\nfeatures from the local region related to the classiﬁcation\\nlabel, without considering long-term dependency. In contrast\\nto these methods, the proposed search method is enabled to\\ncapture long-range dependencies, thus constructing better full-\\nattention networks.\\nSelf-attention mechanism. Self-attention mechanisms have\\nbeen widely adopted in various tasks, such as machine trans-\\nlation [18], generative modeling [19], and visual recognition\\n[20]–[22]. Vaswani et al. [18] proposed the Transformer,\\nwhich is one of the ﬁrst attempts to apply a self-attention\\nmechanism to model long-range dependencies in machine\\ntranslation. Wang et al. [20] extended a sequential self-\\nattention network to a spacetime non-local network to capture\\nlong-range dependencies in videos. The proposed non-local\\nblock signiﬁcantly improved the video classiﬁcation accuracy\\nof CNNs. Beyond assisting CNNs to deal with long-range\\ndependencies, Ramachandran et al. [8] developed a local self-\\nattention module to limit the amount of computation and\\nleverage this module to build a fully attentional vision model.\\nIn addition, [9], [10], [23] also proposed different forms\\nof self-attention to construct pure-attention vision models.\\nIn recent work, vision-transformer-based methods [24]–[26]\\nhave attracted considerable interest. They explored a self-\\nattention architectural design that effectively learns visual\\nrepresentation.\\nDifferent from these methods for manually designing self-\\nattention models, we propose a NAS algorithm that auto-\\nmatically discovers the optimal self-attention network. Fur-\\nthermore, the discovered network is demonstrated to achieve\\nsuperior performance.\\nIII. FULL-ATTENTION BASED NEURAL ARCHITECTURE\\nSEARCH\\nIn this section, a novel NAS method is proposed to address\\nthe challenges of designing a full-attention network. First,\\na ﬂexible stage-wise search space is proposed that allows\\nvarious attention operations to be used for different stages\\nof the network. Figure 1 illustrates the search space for the\\nproposed method. Then, a continuous relaxation of the discrete\\narchitectures is developed, such that the architecture can be\\noptimized by gradient descent. Moreover, a self-supervised\\nsearch method using context auto-regression is proposed to\\ndiscover a full-attention architecture.\\nA. Search Space\\nIn this study, we propose a stage-wise search space to\\nsearch for a full-attention network. Figure 1(a) shows the\\nmacro-architecture of the search space. The searched model is\\npartitioned into a sequence of pre-deﬁned stages that gradually\\nreduce spatial resolutions of the feature maps. The precise\\narchitecture conﬁgurations are listed in Table I. The macro-\\narchitecture deﬁnes the number of layers, input dimensions,\\nand output channel number for each layer. It also speciﬁes\\nwhich layers of the network are to be searched. The ﬁrst and\\nlast layers of the network have ﬁxed operators. The remainder\\nof the macro-architecture consists of ﬁve stages, and each stage\\nhas three intermediate searchable layers.\\nTwo types of self-attention operations i.e., local multi-\\nhead self-attention [8] and non-local self-attention [20], are\\nconsidered as candidate operations in a stage. Local multi-head\\n3\\nInput\\nLocal self-attention \\nblock, 3*3, head=4\\nLocal self-attention \\nblock, 3*3, head=8\\nLocal self-attention \\nblock, 7*7, head=8\\nNon-local\\n self-attention block\\n...\\nOutput\\n Linear layer, \\nReLU\\nNon-local SA,\\n ReLU\\n Linear layer, \\n(Avg pooling)\\n＋\\nH×W×Cin\\nH×W×C\\nH×W×C\\nLocal SA_k3_h8\\nImage\\nStage 1\\nStage 2\\nStage 3\\nStage 5\\nStage 4\\nOutput\\n(a)\\n(b)\\n(c)\\nLayer1\\nLayer2\\nLayer3\\n Linear layer, \\nReLU\\nLocal SA_k_h,\\n ReLU\\n Linear layer, \\n(Avg pooling)\\n＋\\nH×W×Cin\\nH×W×C\\nH×W×C\\n(H/s)×(W/s)×Cout\\n(d)\\n(H/s)×(W/s)×Cout\\nFig. 1. Illustration of the search space. (a) The macro-architecture consists of ﬁve stages, each stage having three intermediate searchable\\nlayers. For simplicity, only the structure of the stage 2 is shown. (b) Each layer contains different candidate self-attention blocks. (c) The\\nstructure of the non-local self-attention block. (d) The structure of the local multi-head self-attention block.\\n.\\nInput shape\\nOperations\\nChannels n s\\nStem\\n32×32×3 Local SA k3 h8\\n16\\n1 1\\nStage 1\\n32×32×16 To Be Searched\\n16\\n3 1\\nStage 2\\n32×32×16 To Be Searched\\n32\\n3 2\\nStage 3\\n16×16×32 To Be Searched\\n32\\n3 1\\nStage 4\\n16×16×32 To Be Searched\\n64\\n3 2\\nStage 5\\n8×8×64\\nTo Be Searched\\n64\\n3 1\\nPooling layer\\n8×8×64\\nAvgpool\\n64\\n1 1\\nOutput\\n1×1×64\\nFC\\n10\\n1 1\\nTABLE I. Macro-architecture of the search space. ”Operations”\\ndenotes the type of operation block. ”channels? denotes the output\\nchannel number. ”n” denotes the number of intermediate searchable\\nlayers. ”s” denotes the stride of the ﬁrst layer in a stage.\\nself-attention extracts local spatial information and multiple\\nheads are used to learn multiple distinct representations of\\nthe input, analogous to group convolutions [27], [28]. The\\nsize of the local spatial extent space and the number of\\nself-attention heads are important settings in the local multi-\\nhead self-attention. A non-local self-attention operation [20]\\ncan be regarded as a global context modeling module that\\nexplicitly capturing long-range interactions among distant po-\\nsitions. In summary, the candidate operations consist of seven\\nself-attention operations, and their conﬁgurations are listed\\nin Table II. We allow the selection of different operations,\\nincluding non-local self-attention and local self-attention, with\\ndifferent spatial extents and different head numbers.\\nBased on these candidate self-attention operations, two\\ntypes of building blocks are designed as candidate blocks of\\nthe macro-architecture. Each searchable layer in the macro-\\narchitecture can choose a block. The non-local building block\\nis illustrated in Figure 1(c) and the local building block is\\nillustrated in Figure 1(d). The block structure is inspired by\\nthe ”bottleneck” building block design [29].\\nAs shown in Figure 1(c), the non-local building block\\ncontains a linear layer, followed by a non-local self-attention\\noperation, followed by another linear layer. The ﬁrst linear\\nlayer transforms the input features and reduces their channel\\ndimensionality for efﬁcient processing. The ﬁnal linear layer\\nOperations\\nSpatial Extent\\nHead\\nLocal SA k3 h4\\n3×3\\n4\\nLocal SA k3 h8\\n3×3\\n8\\nLocal SA k5 h4\\n5×5\\n4\\nLocal SA k5 h8\\n5×5\\n8\\nLocal SA k7 h4\\n7×7\\n4\\nLocal SA k7 h8\\n7×7\\n8\\nNon-local SA\\n-\\n-\\nTABLE II. Conﬁgurations of candidate operations in the search space\\nexpands the features to match the output’s dimensionality.\\nIf the stride of the layer is set to two, there is an average\\npooling operation after the ﬁnal linear layer to reduce the\\nspatial resolution. The ”ReLU” activation functions follow the\\nﬁrst linear layer and the attention operation. In addition, the\\nshortcut connections simply perform identity mapping, and\\ntheir outputs are added to the outputs of the stacked layers.\\nAs shown in Figure 1(d), the local building block has\\na similar structure, except that the intermediate non-local\\nself-attention operation is replaced by a local self-attention\\noperation with different spatial dimensions and head numbers.\\nIn summary, our overall search space contains 15 searchable\\nlayers and each layer can choose from seven candidate blocks,\\nthus producing 715 possible architectures. Finding the optimal\\nnetwork structure from such an enormous search space is a\\nnon-trivial task.\\nB. Differentiable Formulation of the Search Space\\nFor the search phase, it is infeasible to solve the search prob-\\nlem through brute-force enumeration of the search space. In\\nthis work, we use the differentiable architecture search method\\nin [4] to efﬁciently ﬁnd the optimal self-attention network. We\\ndevelop a differentiable formulation for our proposed search\\nspace. The formulation enables the architecture and its weights\\nto be jointly optimized with back-propagation, thus reduces the\\nsearch time signiﬁcantly compared to explicitly sampling and\\nevaluating different architectures.\\n4\\nNode 1\\nUpdate\\n parameters\\nUpdate\\n parameters\\nFinal \\narchitecture\\nFinal \\narchitecture\\n(a)\\n(c)\\n(d)\\nNode 0\\nNode 1\\nNode 3\\n?\\n?\\nNode 0\\nNode 3\\nNode 1\\nNode 0\\n(b)\\nList all \\noperations\\nNode 2\\n0.25\\n0.25\\n0.25\\n0.25\\n0.25\\n0.25\\n0.25\\n0.25\\nNode 1\\nNode 0\\nNode 2\\n0.62\\n0.05\\n0.12\\n0.21\\n0.04\\n0.19\\n0.09\\n0.68\\nFig. 2. Illustration of the differentiable architecture search procedure.\\n(a) The search space is represented as a directed acyclic graph.\\nThe node represents the feature map and the edge represents an\\nunknown operation. (b) All operations between each edge (shown\\nby connections with different colors) are listed. The architecture\\nparameters are shown next to each connection. (c) During the search\\nphase, the architecture parameters are constantly updated. (d) At the\\nend of the search phase, a stand-alone architecture is obtained based\\non the architecture parameters.\\n.\\nThe overall procedure for the differentiable architecture\\nsearch is shown in Figure 2. The search space is represented\\nas a directed acyclic graph of N nodes, where each node\\nrepresents a feature map. We denote the operation space as O,\\nwhere each element represents a candidate operation b(). The\\nedges fi,j represents the information ﬂow connecting node i\\nand node j, which consists of a set of operations weighted by\\nthe architecture parameters α(i,j), and is thus formulated as:\\nfi,j (xi) =\\nX\\nb∈O\\nexp\\n\\x00αb\\ni,j\\n\\x01\\nP\\nb′∈O exp\\n\\x00αb′\\ni,j\\n\\x01b (xi)\\n(1)\\nwhere the operation weights for a pair of nodes are parame-\\nterized by a vector α(i,j) of dimension |O|.\\nAfter relaxation, the architecture parameters α and the\\nweight parameters w (e.g., the weights of the self-attention\\noperations) can be optimized by applying alternating gradient\\ndescent performed on the training and validation sets. This\\noptimization procedure is deﬁned as a bilevel optimization\\nproblem:\\nmin\\nα Lvalid (α, w∗(α))\\n(2)\\ns.t. w∗(α) = arg min\\nw\\nLtrain (α, w)\\n(3)\\nwhere Lvalid is the validation loss and Ltrain is the training\\nloss. Once the training is completed, a discrete architecture can\\nbe obtained by replacing each mixed operation block with the\\nmost probable block, that is, fi,j = arg max\\nb∈O\\nαb\\ni,j.\\nC. Context Auto-regression based search method\\nCurrent NAS methods typically use classiﬁcation tasks for\\nthe supervision of architecture search. However, the classiﬁca-\\ntion task requires the model to ﬁx more attention on extracting\\nfeatures from the local area related to the classiﬁcation label,\\nwithout considering the long-term dependence. In this case,\\nL1 Loss\\nGround Truth\\nFeature extraction \\nContext reconstruction \\nInput\\nOutput\\nEncoder-decoder network\\nFig. 3. Illustration of the context auto-regression. Several regions\\non the input image are randomly masked before it was fed into an\\nencoder-decoder network. The network extracts feature and recon-\\nstructs the missing image content.\\n.\\nthe current search methods are not suitable for searching self-\\nattention structures that focus on capturing the long-term con-\\ntent dependencies between pixels. A new self-supervised task,\\ntermed context auto-regression, is designed in this study. Based\\non this task, a self-supervised search method is proposed\\nto discover optimal self-attention architecture. The proposed\\nsearch method using context auto-regression encourages the\\nmodeling of more global dependencies, which is more suitable\\nfor searching full-attention networks.\\nContext auto-regression is now proposed. As shown in Fig-\\nure 3, given an unlabeled image, several regions are randomly\\nmarked on the input image before it is fed into an encoder-\\ndecoder network. The encoder then takes the input image\\nwith missing regions and produces a feature representation\\nof the image. The decoder uses this feature representation and\\nreconstructs the missing image content. The network is trained\\nby regressing the ground truth content. L1 loss is used as\\nthe reconstruction loss to capture the overall structure of the\\nmissing region and coherence regarding its context. Instead of\\nchoosing a single mask at a ﬁxed location, several smaller,\\npossibly overlapping, random masks are added that cover up\\nto 1/4 of the image. To succeed in this task, the model has to\\nunderstand the content of the entire image, as well as produce\\na plausible hypothesis for the missing parts.\\nBased on this task, a self-supervised search method is pro-\\nposed for learning the full-attention architecture. The macro-\\narchitecture shown in Table II is used as the encoder to\\nextract the features. Then, the feature is passed through a\\nseries of up-sampling layers to produce the prediction. During\\nthe search phase, we jointly learn the architecture parameters\\nand weight parameters of the macro-architecture using context\\nauto-regression.\\nD. Fine-tuning\\nOur method consists of a search phase using context auto-\\nregression followed by a ﬁne-tuning phase. Figure 4 illustrates\\nthe overall procedure of the proposed method. In the search\\nphase, context auto-regression is used to learn the architecture\\nparameters of the search network. When the search phase is\\ncomplete, we store the architecture parameters and use them as\\n5\\nArchitecture Search Space\\nInput Layer\\nOutput Layer\\nSearch Loss\\nOptimize Architecture Parameters\\nOptimize Model Parameters\\nModel Loss\\n(a) search phase using context auto-regression\\nOutput Layer\\nTarget Dataset\\nSearch Loss\\nModel Loss\\nOptimize Architecture Parameters\\nOptimize Model Parameters\\n(b) fine-tuning phase\\nInput Layer\\nOutput Layer\\nThe searched architecture\\nobtain final \\nnetwork \\n1\\n2\\n3\\n4\\n· · · · · · \\nL-1\\nL\\nα1 \\nα2 \\nα3 \\nα4 \\nαL-1 \\nαL \\nMasked Images\\nArchitecture Search Space\\nInput Layer\\n1\\n2\\n3\\n4\\n· · · · · · \\nL-1\\nL\\nα1 \\nα2 \\nα3 \\nα4 \\nαL-1 \\nαL \\n: Candidate operations\\n:  Information flow\\n:  Best path\\n: Candidate operations\\n:  Information flow\\n:  Best path\\n1\\n2\\n3\\n4\\n· · · · · · \\nL-1\\nL\\nFig. 4. Illustration of our search procedure. The procedure consists of a search phase followed by a ﬁne-tuning phase. In the search phase,\\nwe propose a self-supervised algorithm applying context auto-regression to guide the architecture search. When ﬁne-tuning, we perform the\\nsearch on target tasks to obtain the ﬁnal architecture.\\n.\\ninitialized values for the ﬁne-tuning phase. When ﬁne-tuning,\\na differentiable architecture search is performed on the target\\ntasks. Here we take the classiﬁcation task as an example.\\nFollowing [4], the training data are partitioned into two sepa-\\nrate sets for training and validation. First-order approximation\\nis adopted while the architecture parameters α and weight\\nparameters w are alternately optimized using gradient descent.\\nSpeciﬁcally, in an iterative manner, the weight parameters are\\noptimized by descending ▽wLtrain(α, w) on the training set,\\nand the architecture parameters by descending ▽αLval(α, w)\\non the validation set. When the ﬁne-tuning procedure is\\ncompleted, we obtain the ﬁnal architecture according to the\\narchitecture parameters.\\nIV. EXPERIMENTS\\nIn this section, we perform experiments on image classi-\\nﬁcation, ﬁne-grained image recognition, and zero-shot image\\nretrieval tasks, and compare the performance of each proposed\\nmodels with other state-of-the-art models.\\nA. Datasets\\nThe experiments are ﬁrst conducted on three popular im-\\nage classiﬁcation datasets, namely CIFAR10 [30], CIFAR100,\\nand ImageNet [31]. Then a ﬁne-grained visual categorization\\ndataset, the Caltech-UCSD Birds dataset (CUB-200-2011) [32]\\nis used to test the transferability of the architectures discovered\\non CIFAR10.\\nCIFAR10 and CIFAR100 contain 50 K training and 10 K\\ntesting RGB images with a ﬁxed spatial resolution of 32×32.\\nCIFAR-10 categorizes images into 10 classes, whereas CIFAR-\\n100 has 100 classes. The ImageNet 2012 dataset consists\\nof 1.28 million training images and 50 K validation images\\nfrom 1000 different classes. CUB-200-2011 has 11,788 images\\nrepresenting 200 bird categories that are split into 5,994\\ntraining and 5,794 test images.\\nB. Implementation Details\\nThe experiments consist of three phases. First, a subset\\nof ImageNet is searched using context auto-regression. Then,\\nbased on the architecture parameters obtained in the ﬁrst\\nphase, ﬁne-tuning is performed on CIFAR-10 to obtain the\\noptimal self-attention network. Finally, the searched architec-\\nture is evaluated for multiple tasks.\\n1) Parameter Settings for searching: To reduce the search\\ntime, 100 classes are randomly chosen from the original 1000\\nclasses of ImageNet to build a training set and resize the im-\\nages to the lower resolution of 32×32. The training set is split\\ninto two equal subsets, one for tuning the network parameters\\nand the other for tuning the architecture parameters. A network\\nof 16 initial channels is trained for 20 epochs. A standard\\nSGD optimizer with a momentum of 0.9 and a weight decay\\nof 0.0003 is used to optimize the network parameters w. The\\ninitial learning rate is 0.025, which decays to zero following\\nthe cosine rule. Zero initialization is used for the architecture\\nparameter α, which implies an equal amount of attention over\\nall possible operations. The Adam [33] optimizer is used for\\nthe architecture parameters α, with a learning rate of 0.0003\\nand a weight decay of 0.001.\\n2) Parameter Settings for ﬁne-tuning: The CIFAR10 training\\nimages are randomly split into two groups, each containing\\n25 K images. One group is used for tuning the network pa-\\nrameters and the other for tuning the architecture parameters.\\nThe architecture is ﬁne-tuned for 50 epochs. A standard SGD\\noptimizer is used to optimize the network parameters w with\\nan initial learning rate of 0.025, a momentum of 0.9, and a\\nweight decay of 0.0003. The architectural parameters obtained\\nin the search process are used to initialize the architecture\\nparameter α. The Adam [33] optimizer is used for α, with a\\nlearning rate of 0.0001 and a weight decay of 0.001.\\nWe ran four times with different random seeds to obtain\\nfour different models and selected the best model based\\n6\\nModel\\nType\\nDesign method\\nSearch cost\\n(GPU days)\\nParams\\nError on\\nCIFAR-10\\nError on\\nCIFAR-100\\nSENet [34]\\nconv-based\\nmanual\\n-\\n11.2M\\n4.05%\\n-\\nWide ResNet [35]\\nconv-based\\nmanual\\n-\\n8.9M\\n4.53%\\n21.18%\\nResNeXt-29 [28]\\nconv-based\\nmanual\\n-\\n34.4M\\n3.65%\\n-\\nUPANets32 [36]\\nconv-based\\nmanual\\n-\\n5.93M\\n4.12%\\n21.22%\\nSimpleNet V2 [37]\\nconv-based\\nmanual\\n-\\n8.9M\\n4.11%\\n20.83%\\nMetaQNN [38]\\nconv-based\\nauto\\n80\\n11.2M\\n6.92%\\n27.14%\\nNAS [1]\\nconv-based\\nauto\\n16000\\n7.1M\\n4.47%\\n-\\nNet Transformation [39]\\nconv-based\\nauto\\n10\\n19.7M\\n5.70%\\n-\\nSMASH [40]\\nconv-based\\nauto\\n1.5\\n16M\\n4.03%\\n22.07%\\nHierachical NAS [15]\\nconv-based\\nauto\\n300\\n61.3M\\n3.63%\\n-\\nNSGANet [41]\\nconv-based\\nauto\\n8\\n3.3M\\n3.85%\\n20.74%\\nENAS [11]\\nconv-based\\nauto\\n0.32\\n38M\\n3.87%\\n-\\nNesT-T [26]\\nattention-based\\nmanual\\n-\\n6.2M\\n3.96%\\n21.31%\\nDeiT-B [42]\\nattention-based\\nmanual\\n-\\n85.1M\\n7.59%\\n29.51%\\nPVT-S [43]\\nattention-based\\nmanual\\n-\\n24.1M\\n7.66%\\n30.21%\\nCCT-6/3×1 [44]\\nattention-based\\nmanual\\n-\\n3.17M\\n4.71%\\n22.69%\\nSwin-B [45]\\nattention-based\\nmanual\\n-\\n86.7M\\n5.45%\\n21.55%\\nOur method\\nattention-based\\nauto\\n2\\n2.52M\\n3.44%\\n20.68%\\nTABLE III. Classiﬁcation errors on CIFAR-10 and CIFAR-100\\non its validation performance. The proposed method takes\\napproximately 2 GPU days to complete the search procedure\\non a single NVIDIA 1080Ti GPU. The discovered network is\\nshown in Figure 5(a).\\nC. Evaluation on image classiﬁcation\\nThe search model is evaluated on the standard CIFAR10\\nand CIFAR100 image classiﬁcation datasets. The network of\\n96 initial channels is trained from scratch for 500 epochs. The\\nstandard translation and ﬂipping data augmentation scheme is\\napplied to these datasets. The SGD optimizer is used with a\\nmomentum of 0.9 and a weight decay of 0.0004. The initial\\nlearning rate is 0.04, which decays to zero following the cosine\\nrule.\\nThe evaluation results and comparison with state-of-the-\\nart approaches are summarized in Table III. The proposed\\nmethod achieved test errors of 3.44% and 20.68% on CIFAR-\\n10 and CIFAR-100, respectively. As reported in Table 3, the\\nexperimental method outperforms recent strong baselines with\\nsigniﬁcantly fewer computational resources. For example, it\\nachieves superior performance compared to SOTA manually\\ndesigned conv-based models. The test errors are signiﬁcantly\\nlower than the error rates achieved by the SENet architecture\\n[34]. With fewer parameters, it also achieves performance\\ncomparable to the automatically designed convolution-based\\nmodels. In addition, it is compared with manual self-attention\\nnetworks. The full self-attention models are trained using ran-\\ndom initialization without extra pre-training and the proposed\\nmethod outperformed them by a large margin.\\nD. Transferability Validation on ImageNet\\nTo prove the transferability of our method, we transform\\nthe architecture learned on CIFAR-10 to a large and standard\\nImageNet dataset. The network of 112 initial channels is\\ntrained from scratch for 200 epochs using the cosine learning\\nrate schedule with a base learning rate of 0.04. Standard\\ndata augmentation is applied to ImageNet, including random\\nLocal_SA_k3_h8\\nImage\\nPooling, FC\\nLocal_SA_k3_h4\\nLocal_SA_k3_h4\\nNon-local SA\\nLocal_SA_k7_h4\\nLocal_SA_k5_h8\\nLocal_SA_k5_h4\\nNon-local SA\\nLocal_SA_k5_h8\\nLocal_SA_k5_h4\\nNon-local SA\\nLocal_SA_k5_h4\\nLocal_SA_k7_h4\\nLocal_SA_k3_h8\\nLocal_SA_k7_h8\\nLocal_SA_k5_h8\\nStage 1 \\nStage 2\\nStage 3\\nStage 4\\nStage 5 \\nLocal_SA_k3_h8\\nImage\\nPooling, FC\\nLocal_SA_k7_h4\\nLocal_SA_k5_h4\\nLocal_SA_k3_h8\\nLocal_SA_k3_h4\\nLocal_SA_k7_h8\\nLocal_SA_k5_h4\\nLocal_SA_k3_h8\\nLocal_SA_k3_h4\\nLocal_SA_k5_h8\\nLocal_SA_k3_h4\\nLocal_SA_k3_h4\\nLocal_SA_k7_h8\\nNon-local SA\\nLocal_SA_k3_h8\\nLocal_SA_k3_h4\\n(c) Visualization of the detailed structure discovered by \\nour method without CAR.\\n(a) Visualization of the detailed structure \\ndiscovered by our method.\\nLocal_SA_k3_h8\\nImage\\nPooling, FC\\nLocal_SA_k3_h8\\nLocal_SA_k3_h8\\nLocal_SA_k7_h8\\nNon-local SA\\nLocal_SA_k3_h4\\nLocal_SA_k7_h4\\nLocal_SA_k3_h4\\nLocal_SA_k7_h8\\nNon-local SA\\nLocal_SA_k3_h4\\nLocal_SA_k7_h4\\nLocal_SA_k3_h4\\nLocal_SA_k7_h8\\nNon-local SA\\nLocal_SA_k3_h4\\n(b) Visualization of the detailed structure discovered by \\nour method without SSS and CAR.\\nFig. 5. Visualizations of several searched architectures. The SSS\\nrefers to the stage-wise search space and CAR is the proposed search\\nalgorithm using the context auto-regression. Rounded rectangle boxes\\nare used to denote blocks for each layer. The deﬁnition of the\\noperations is described in Table II. The ﬁrst grey block ”Local\\nSA k3 h8” is ﬁxed. Different colors denote the types of the searched\\noperation; orange for non-local self-attention with green and blue for\\nlocal self-attention with different numbers of heads. Height is used\\nto denote the spatial extent of the operation.\\n.\\nresized cropping, random horizontal ﬂipping, and normaliza-\\ntion. The network parameters are optimized using an SGD\\noptimizer with a momentum of 0.9 and a weight decay of\\n3×10-5. Label smoothing regularization is used [63] with a\\ncoefﬁcient of 0.1 during training.\\nTable IV shows the quantitative results of ImageNet. On\\nImageNet, the network achieved top-1 and top-5 errors of\\n7\\nMethod\\nParams Top-1 Error Top-5 Error\\nInception-V1 [46]\\n6.6M\\n30.2%\\n10.1%\\nMobileNet-V1 [47]\\n4.2M\\n29.4%\\n10.5%\\nMobileNet-V2 [48]\\n3.4M\\n28.0%\\n9.0%\\nShufﬂeNet [49]\\n5.0M\\n29.1%\\n9.2%\\nResNet-18 [29]\\n11.7M\\n28.5%\\n-\\nCondenseNet-V2 [50] 3.6M\\n28.1%\\n9.7%\\nNASNet-C [51]\\n4.9M\\n27.5%\\n9.0%\\nDARTS [4]\\n4.9M\\n26.9%\\n9.0%\\nSNAS [52]\\n4.3M\\n27.3%\\n9.2%\\nGDAS(FRC) [12]\\n4.4M\\n27.5%\\n9.1%\\nPVTv2 [53]\\n3.4M\\n29.5%\\n-\\nT2T-ViT-7 [54]\\n4.3M\\n28.3%\\n-\\nDeiT-Ti [55]\\n5M\\n27.8%\\nOur method\\n4M\\n26.7%\\n8.4%\\nTABLE IV. Comparison with other architectures on ImageNet\\nMethod\\nBase Model Params Accuracy\\nFine-tuned VGGNet [58]\\nVGG-19\\n144M\\n77.80%\\nCoSeg(+BBox) [59]\\nVGG-19\\n144M\\n82.60%\\nB-CNN [60]\\nVGG-16\\n138M\\n84.00%\\nCBP [61]\\nVGG-16\\n138M\\n84.00%\\nFine-tuned ResNet [29]\\nResNet-50\\n25.6M\\n84.10%\\nLRBP [62]\\nVGG-16\\n138M\\n84.20%\\nFCAN [56]\\nResNet-50\\n25.6M\\n84.30%\\nKernel-Pooling [57]\\nResNet-50\\n25.6M\\n84.70%\\nOur method\\nFull-attention\\nNetwork\\n3.82M\\n85.30%\\nTABLE V. Comparison of ﬁne-grained image recognition methods\\non CUB-200-2011\\n26.7% and 8.4%, respectively. The experimental searched\\nmodels are compared with state-of-the-art models, including\\nmanually designed convolution-based models, automatically\\ndesigned convolution-based models, and manually designed\\nself-attention models. The results indicate that the experimen-\\ntal model produces performance superior to these models. This\\ndemonstrates the transfer capability of the discovered self-\\nattention architecture from a small dataset to a large dataset.\\nE. Evaluation on Fine-grained Image Recognition\\nTo investigate further the transferability of the full-attention\\nnetwork searched on CIFAR-10, the capability of the discov-\\nered model is validated on the CUB-200-2011 dataset. In the\\nexperiments, the network structure searched on CIFAR-10 is\\napplied as a feature extractor and combined with a classiﬁer\\n(e.g., fully-connected layers). For fair comparisons with other\\nmethods [56], [57], each image is resized to 448 × 448. The\\nnetwork is trained for 200 epochs. The network parameters are\\noptimized using an SGD optimizer with an initial learning rate\\nof 0.02, a momentum of 0.9, and a weight decay of 0.0005.\\nThe evaluation results summarized in Table V reﬂect a\\ndetailed comparison with existing methods. ”Params” denotes\\nthe parameters of the base model. Compared with other\\nmethods, our model achieves better performance by a large\\nmargin with fewer parameters. The results in Table Table V\\ndemonstrate the transferability of architectures searched on the\\nCIFAR-10 dataset and verify that our model is superior in\\nmore complex tasks.\\nMethod\\nBackbone\\nR@1 R@2 R@4 R@8\\nProxyNCA [54]\\nInceptionBN 49.2 61.9 67.9 72.4\\nMS [63]\\nInceptionBN 65.7\\n77\\n86.3 91.2\\nHORDE [64]\\nInceptionBN 66.8 77.4 85.1\\n91\\nXBM [65]\\nInceptionBN 65.8 75.9\\n84\\n89.9\\nMargin [66]\\nResNet-50\\n63.6 74.4 83.1\\n90\\nNormSoftMax [67]\\nResNet-50\\n65.3 76.7 85.4 91.8\\nMIC [68]\\nResNet-50\\n66.1 76.8 85.6\\n-\\nOur method\\nFull-attention\\nNetwork\\n68.4 78.5\\n86\\n91.6\\nTABLE VI. Comparison of zero-shot image retrieval methods on\\nCUB-200-2011.\\nF. Evaluation on zero-shot image retrieval\\nIn zero-shot image retrieval (ZSIR), the feature extractor is\\nrequired to learn embedding from the seen classes and then\\nto be capable of utilizing the learned knowledge to distin-\\nguish the unseen classes without any attributes or semantic\\ninformation. Therefore, because the quality of features highly\\ndepends on it, the feature extractor is critically important. In\\nthis study, our discovered full-attention model is combined\\nwith a distance metric learning method, Proxy-Neighborhood\\nComponent Analysis [54], to validate the capability of the\\nproposed model for zero-shot image retrieval. The Recall@K\\nevaluation metric is computed for a direct comparison with\\nprevious methods on the CUB-200-2011 dataset. The ﬁrst 100\\nclasses are used as training data and the remaining 100 classes\\nfor evaluation. The inputs are resized to 256 × 256 pixels and\\nthen randomly cropped to 227 × 227 pixels. The network is\\ntrained for 50 epochs. The model is optimized using Adam\\n[33] with a learning rate of 4×10-3.\\nThe evaluation results and a comparison with the state-of-\\nthe-art approaches are summarized in Table VI. From the table,\\nour method improves the performance of zero-shot image\\nretrieval by a large margin and achieves better performance\\nthan other methods. It is also notable that the full-attention\\nnetworks obtained by our method have high transferability.\\nG. Ablation Studies and Discussion\\nIn this section, we report on a series of ablation studies\\nthat validate the importance of the stage-wise search space as\\nwell as the proposed self-supervised search algorithm using\\ncontext auto-regression incorporated in our method. All the\\narchitectures of 80 initial channels are trained for 500 epochs.\\nTable VII shows the results of the ablation studies on CIFAR-\\n10. SSS refers to the stage-wise search space, and CAR is\\nthe proposed search algorithm using context auto-regression.\\nThen, we scale a baseline model with different network widths\\n(w) and depths (d) and discuss the relationship between model\\ndimensions and accuracy.\\n1) Stage-wise Search Space (SSS) design: In order to study\\nthe impact of layer diversity brought by the proposed search\\nspace, a different search space is designed. In the searched\\nmodel, the architectures of stages 1, 3, and 5 are identical\\nas are the architectures of stages 2 and 4. The detailed\\nstructure discovered on CIFAR-10 using this search space\\ndesign is shown in the Figure 5(b). As shown in Table VII, the\\n8\\n(a)\\n(b)\\n（48，5）\\n（64，5）\\n（80，5）\\n（96，5）\\n（48，8）\\n（64，8）\\n（80，8）\\nscale by depth\\nscale by width\\ncompound scaling\\nParameters (million)\\nValidation Accuracy\\n(c)\\nFig. 6. Results of ablation studies. (a) Loss curve with and without context auto-regression (CAR). (b) Accuracy curve with and without\\ncontext auto-regression (CAR). (c) Scaling up a baseline model with different network width and depth. The ﬁrst baseline network (C=48,\\nS=5) has 48 initial channels and 5 stages (each stage has three intermediate layers).\\n.\\nArchitecture\\nSSS CAR Parameters\\nError\\nOur method without\\nSSS and CAR\\n×\\n×\\n1.76M\\n4.38%\\nOur method without CAR\\n✓\\n×\\n1.74M\\n4.08%\\nOur method\\n✓\\n✓\\n1.75M\\n3.7%\\nTABLE VII. Ablation studies on Cifar-10. SSS means the stage-wise\\nsearch space. CAR means the search algorithm using the context\\nauto-regression.\\nnetwork discovered by the method with our proposed stage-\\nwise search space design exhibits high accuracy, highlighting\\nthe importance of the layer diversity throughout the network.\\n2) Search algorithm using Context Auto-Regression (CAR):\\nTable VII demonstrates the efﬁcacy of the proposed search\\nalgorithm using context auto-regression. ”Our method with-\\nout CAR” means searching on Cifar10 and the architecture\\nparameters are initialized randomly. The results show that\\nthe searched model with context auto-regression exhibits high\\naccuracy. The detailed structure discovered on CIFAR-10 by\\nthe method without context auto-regression is shown in Fig-\\nure 5(c). We further show the loss and accuracy curves during\\nthe search process in Figure 6(a) and Figure 6(b). The ﬁgure\\ndemonstrates the efﬁcacy of the proposed method. Owing to\\nthe initialized architecture parameters obtained through context\\nauto-regression, our search network exhibits fast convergence\\nand high training accuracy.\\n3) Model scaling: Figure 6(c) shows our study on scaling a\\nbaseline model with different depths and widths. The scaling\\nnetwork width is commonly used for small-size models. The\\ngeneral trend is that our models perform better as the number\\nof initial channels increases. Moreover, we attempt to repeat\\nthe structure of stage1, stage3, and stage5 once and obtain a\\ndeeper network with eight stages. When the network is narrow,\\nthe accuracy improves as the depth increases. However, the\\naccuracy gains saturate when the model is wider. The intuition\\nis that a deeper model can capture richer and more complex\\nfeatures. However, deeper networks are also more difﬁcult\\nto train due to the vanishing gradient problem. As shown in\\nFigure 6(c), when the initial width is 96 and the stage is 5,\\nthe model achieves the highest accuracy.\\nV. CONCLUSION\\nIn this article, we presented a full-attention based neural\\narchitecture search algorithm. A stage-wise search space was\\nconstructed to allow the search algorithm to ﬂexibly explore\\nvarious self-attention operations for different layers of the\\nnetwork. To extract global feature, a self-supervised search\\nmethod using context auto-regression was proposed to learn\\nfull-attention architecture representations. To verify the ef-\\nfectiveness, we applied our method to image classiﬁcation,\\nﬁne-grained image recognition and zero-shot image retrieval.\\nThe extensive experiments demonstrated that the full-attention\\nmodel discovered by our method drastically reduced the model\\nparameters while achieving excellent model accuracies.\\nREFERENCES\\n[1] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement\\nlearning,” 2017.\\n[2] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable\\narchitectures for scalable image recognition,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2018, pp. 8697–\\n8710.\\n[3] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized evolution\\nfor image classiﬁer architecture search,” in Proceedings of the aaai\\nconference on artiﬁcial intelligence, vol. 33, no. 01, 2019, pp. 4780–\\n4789.\\n[4] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture\\nsearch,” 2019.\\n[5] L.-C. Chen, M. D. Collins, Y. Zhu, G. Papandreou, B. Zoph, F. Schroff,\\nH. Adam, and J. Shlens, “Searching for efﬁcient multi-scale architectures\\nfor dense image prediction,” in Proceedings of the 32nd International\\nConference on Neural Information Processing Systems, 2018, pp. 8713–\\n8724.\\n[6] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille,\\nand L. Fei-Fei, “Auto-deeplab: Hierarchical neural architecture search\\nfor semantic image segmentation,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, 2019, pp. 82–\\n92.\\n[7] B. Yan, H. Peng, K. Wu, D. Wang, J. Fu, and H. Lu, “Lighttrack:\\nFinding lightweight neural networks for object tracking via one-shot\\narchitecture search,” in Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, 2021, pp. 15 180–15 189.\\n[8] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\\nJ. Shlens, “Stand-alone self-attention in vision models,” Advances in\\nNeural Information Processing Systems, vol. 32, 2019.\\n[9] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\\nrecognition,” in Proceedings of the IEEE/CVF International Conference\\non Computer Vision, 2019, pp. 3464–3473.\\n[10] H. Zhao, J. Jia, and V. Koltun, “Exploring self-attention for image\\nrecognition,” in Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, 2020, pp. 10 076–10 085.\\n9\\n[11] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efﬁcient neural\\narchitecture search via parameter sharing,” 2018.\\n[12] X. Dong and Y. Yang, “Searching for a robust neural architecture in four\\ngpu hours,” in Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, 2019, pp. 1761–1770.\\n[13] G. Li, G. Qian, I. C. Delgadillo, M. Muller, A. Thabet, and B. Ghanem,\\n“Sgas: Sequential greedy architecture search,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2020, pp. 1620–1630.\\n[14] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,\\nand Q. V. Le, “Mnasnet: Platform-aware neural architecture search for\\nmobile,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, 2019, pp. 2820–2828.\\n[15] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu,\\n“Hierarchical representations for efﬁcient architecture search,” 2018.\\n[16] L. Xie and A. Yuille, “Genetic cnn,” in Proceedings of the IEEE\\ninternational conference on computer vision, 2017, pp. 1379–1388.\\n[17] Y. Zhou, X. Xie, and S.-Y. Kung, “Exploiting operation importance for\\ndifferentiable neural architecture search,” IEEE Transactions on Neural\\nNetworks and Learning Systems, 2021.\\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\\nin neural information processing systems, 2017, pp. 5998–6008.\\n[19] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention\\ngenerative adversarial networks,” in International conference on machine\\nlearning.\\nPMLR, 2019, pp. 7354–7363.\\n[20] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\\nworks,” in Proceedings of the IEEE conference on computer vision and\\npattern recognition, 2018, pp. 7794–7803.\\n[21] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks for\\nobject detection,” in Proceedings of the IEEE conference on computer\\nvision and pattern recognition, 2018, pp. 3588–3597.\\n[22] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention\\naugmented convolutional networks,” in Proceedings of the IEEE/CVF\\ninternational conference on computer vision, 2019, pp. 3286–3295.\\n[23] J.-B. Cordonnier, A. Loukas, and M. Jaggi, “On the relationship between\\nself-attention and convolutional layers,” in International Conference on\\nLearning Representations, 2019.\\n[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\\n“An image is worth 16x16 words: Transformers for image recognition at\\nscale,” in International Conference on Learning Representations, 2020.\\n[25] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\\nS. Zagoruyko, “End-to-end object detection with transformers,” in\\nEuropean Conference on Computer Vision.\\nSpringer, 2020, pp. 213–\\n229.\\n[26] Z. Zhang, H. Zhang, L. Zhao, T. Chen, and T. Pﬁster, “Aggregating\\nnested transformers,” 2021.\\n[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” Advances in neural informa-\\ntion processing systems, vol. 25, pp. 1097–1105, 2012.\\n[28] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2017, pp. 1492–\\n1500.\\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 770–778.\\n[30] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\\nfrom tiny images,” Citeseer, Tech. Rep., 2009.\\n[31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large\\nscale visual recognition challenge,” International journal of computer\\nvision, vol. 115, no. 3, pp. 211–252, 2015.\\n[32] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The\\ncaltech-ucsd birds-200-2011 dataset,” 2011.\\n[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\\n2017.\\n[34] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\\nProceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2018, pp. 7132–7141.\\n[35] S. Zagoruyko and N. Komodakis, “Wide residual networks,” 2017.\\n[36] C.-H. Tseng, S.-J. Lee, J.-N. Feng, S. Mao, Y.-P. Wu, J.-Y. Shang, M.-\\nC. Tseng, and X.-J. Zeng, “Upanets: Learning from the universal pixel\\nattention networks,” 2021.\\n[37] S. H. Hasanpour, M. Rouhani, M. Fayyaz, M. Sabokrou, and E. Adeli,\\n“Towards principled design of deep convolutional networks: Introducing\\nsimpnet,” 2018.\\n[38] B. Baker, O. Gupta, N. Naik, and R. Raskar, “Designing neural network\\narchitectures using reinforcement learning,” 2017.\\n[39] H. Cai, T. Chen, W. Zhang, Y. Yu, and J. Wang, “Efﬁcient architecture\\nsearch by network transformation,” in Proceedings of the AAAI Confer-\\nence on Artiﬁcial Intelligence, vol. 32, no. 1, 2018.\\n[40] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, “Smash: One-shot\\nmodel architecture search through hypernetworks,” 2017.\\n[41] Z. Lu, I. Whalen, V. Boddeti, Y. Dhebar, K. Deb, E. Goodman, and\\nW. Banzhaf, “Nsga-net: Neural architecture search using multi-objective\\ngenetic algorithm,” in Proceedings of the Genetic and Evolutionary\\nComputation Conference, 2019, pp. 419–427.\\n[42] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\\nH. J´egou, “Training data-efﬁcient image transformers & distillation\\nthrough attention,” in International Conference on Machine Learning.\\nPMLR, 2021, pp. 10 347–10 357.\\n[43] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\\nL. Shao, “Pyramid vision transformer: A versatile backbone for dense\\nprediction without convolutions,” 2021.\\n[44] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi,\\n“Escaping the big data paradigm with compact transformers,” 2021.\\n[45] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and\\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\\nwindows,” 2021.\\n[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\\nV. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\\nin Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2015, pp. 1–9.\\n[47] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,\\nM. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convolutional neural\\nnetworks for mobile vision applications,” 2017.\\n[48] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings\\nof the IEEE conference on computer vision and pattern recognition,\\n2018, pp. 4510–4520.\\n[49] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufﬂenet: An extremely efﬁ-\\ncient convolutional neural network for mobile devices,” in Proceedings\\nof the IEEE conference on computer vision and pattern recognition,\\n2018, pp. 6848–6856.\\n[50] L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, and Q. Tian,\\n“Condensenet v2: Sparse feature reactivation for deep networks,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2021, pp. 3569–3578.\\n[51] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable\\narchitectures for scalable image recognition,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2018, pp. 8697–\\n8710.\\n[52] S. Xie, H. Zheng, C. Liu, and L. Lin, “Snas: stochastic neural architec-\\nture search,” in International Conference on Learning Representations,\\n2018.\\n[53] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\\nL. Shao, “Pvtv2: Improved baselines with pyramid vision transformer,”\\n2021.\\n[54] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and S. Singh,\\n“No fuss distance metric learning using proxies,” in Proceedings of the\\nIEEE International Conference on Computer Vision, 2017, pp. 360–368.\\n[55] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\\nH. J´egou, “Training data-efﬁcient image transformers & distillation\\nthrough attention,” in International Conference on Machine Learning.\\nPMLR, 2021, pp. 10 347–10 357.\\n[56] X. Liu, T. Xia, J. Wang, Y. Yang, F. Zhou, and Y. Lin, “Fully\\nconvolutional attention networks for ﬁne-grained recognition,” 2017.\\n[57] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie, “Kernel\\npooling for convolutional neural networks,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2017, pp. 2921–\\n2930.\\n[58] J. Fu, H. Zheng, and T. Mei, “Look closer to see better: Recurrent atten-\\ntion convolutional neural network for ﬁne-grained image recognition,”\\nin Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2017, pp. 4438–4446.\\n[59] J. Krause, H. Jin, J. Yang, and L. Fei-Fei, “Fine-grained recognition\\nwithout part annotations,” in Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, 2015, pp. 5546–5555.\\n10\\n[60] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for ﬁne-\\ngrained visual recognition,” in Proceedings of the IEEE international\\nconference on computer vision, 2015, pp. 1449–1457.\\n[61] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear\\npooling,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 317–326.\\n[62] S. Kong and C. Fowlkes, “Low-rank bilinear pooling for ﬁne-grained\\nclassiﬁcation,” in Proceedings of the IEEE conference on computer\\nvision and pattern recognition, 2017, pp. 365–374.\\n[63] X. Wang, X. Han, W. Huang, D. Dong, and M. R. Scott, “Multi-\\nsimilarity loss with general pair weighting for deep metric learning,”\\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2019, pp. 5022–5030.\\n[64] P. Jacob, D. Picard, A. Histace, and E. Klein, “Metric learning with\\nhorde: High-order regularizer for deep embeddings,” in Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision, 2019, pp.\\n6539–6548.\\n[65] X. Wang, H. Zhang, W. Huang, and M. R. Scott, “Cross-batch memory\\nfor embedding learning,” in Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2020, pp. 6388–6397.\\n[66] C.-Y. Wu, R. Manmatha, A. J. Smola, and P. Krahenbuhl, “Sampling\\nmatters in deep embedding learning,” in Proceedings of the IEEE\\nInternational Conference on Computer Vision, 2017, pp. 2840–2848.\\n[67] A. Zhai and H.-Y. Wu, “Classiﬁcation is a strong baseline for deep\\nmetric learning,” 2019.\\n[68] K. Roth, B. Brattoli, and B. Ommer, “Mic: Mining interclass character-\\nistics for improved metric learning,” in Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, 2019, pp. 8000–8009.\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs=ArxivLoader(query=\"2111.07139\",load_max_docs=4).load()\n",
    "docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Not to be confused with Artificial general intelligence. This page focuses on statistical machine'), Document(metadata={}, page_content='on statistical machine learning AI. For other topics, see Algorithmic composition, Algorithm art,'), Document(metadata={}, page_content='composition, Algorithm art, Generative art, Procedural generation.'), Document(metadata={}, page_content='Impressionistic image of figures in a futuristic opera scene'), Document(metadata={}, page_content=\"ThÃ©Ã¢tre D'opÃ©ra Spatial, an image made using generative artificial intelligence\"), Document(metadata={}, page_content='Part of a series on\\nArtificial intelligence'), Document(metadata={}, page_content='Major goals\\nApproaches\\nApplications\\nPhilosophy\\nHistory\\nGlossary\\nvte'), Document(metadata={}, page_content='Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is artificial intelligence'), Document(metadata={}, page_content='is artificial intelligence capable of generating text, images, videos, or other data using'), Document(metadata={}, page_content='videos, or other data using generative models,[2] often in response to prompts.[3][4] Generative AI'), Document(metadata={}, page_content='prompts.[3][4] Generative AI models learn the patterns and structure of their input training data'), Document(metadata={}, page_content='of their input training data and then generate new data that has similar characteristics.[5][6]'), Document(metadata={}, page_content='Improvements in transformer-based deep neural networks, particularly large language models (LLMs),'), Document(metadata={}, page_content='large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These'), Document(metadata={}, page_content='in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA,'), Document(metadata={}, page_content='Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as'), Document(metadata={}, page_content='generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators'), Document(metadata={}, page_content='text-to-video AI generators such as Sora.[7][8][9][10] Companies such as OpenAI, Anthropic,'), Document(metadata={}, page_content='such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have'), Document(metadata={}, page_content='numerous smaller firms have developed generative AI models.[3][11][12]'), Document(metadata={}, page_content='Generative AI has uses across a wide range of industries, including software development,'), Document(metadata={}, page_content='software development, healthcare, finance, entertainment, customer service,[13] sales and'), Document(metadata={}, page_content='service,[13] sales and marketing,[14] art, writing,[15] fashion,[16] and product design.[17]'), Document(metadata={}, page_content='and product design.[17] However, concerns have been raised about the potential misuse of generative'), Document(metadata={}, page_content='misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or'), Document(metadata={}, page_content='or deepfakes to deceive or manipulate people, and the mass replacement of human jobs.[18][19]'), Document(metadata={}, page_content='of human jobs.[18][19] Intellectual property law concerns also exist, around generative models that'), Document(metadata={}, page_content='around generative models that are trained on and emulate copyrighted works of art.[20]'), Document(metadata={}, page_content='History\\nMain article: History of artificial intelligence\\nEarly history'), Document(metadata={}, page_content='Since its inception, researchers in the field have raised philosophical and ethical arguments about'), Document(metadata={}, page_content='and ethical arguments about the nature of the human mind and the consequences of creating'), Document(metadata={}, page_content='the consequences of creating artificial beings with human-like intelligence; these issues have'), Document(metadata={}, page_content='these issues have previously been explored by myth, fiction and philosophy since antiquity.[21] The'), Document(metadata={}, page_content='since antiquity.[21] The concept of automated art dates back at least to the automata of ancient'), Document(metadata={}, page_content='to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of'), Document(metadata={}, page_content='such as Daedalus and Hero of Alexandria were described as having designed machines capable of'), Document(metadata={}, page_content='designed machines capable of writing text, generating sounds, and playing music.[22][23] The'), Document(metadata={}, page_content='playing music.[22][23] The tradition of creative automatons has flourished throughout history,'), Document(metadata={}, page_content=\"throughout history, exemplified by Maillardet's automaton created in the early 1800s.[24] Markov\"), Document(metadata={}, page_content='the early 1800s.[24] Markov chains have long been used to model natural languages since their'), Document(metadata={}, page_content='natural languages since their development by Russian mathematician Andrey Markov in the early 20th'), Document(metadata={}, page_content='Markov in the early 20th century. Markov published his first paper on the topic in 1906,[25][26]'), Document(metadata={}, page_content='on the topic in 1906,[25][26] and analyzed the pattern of vowels and consonants in the novel Eugeny'), Document(metadata={}, page_content='in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it'), Document(metadata={}, page_content='learned on a text corpus, it can then be used as a probabilistic text generator.[27][28]'), Document(metadata={}, page_content='Academic artificial intelligence'), Document(metadata={}, page_content='The academic discipline of artificial intelligence was established at a research workshop held at'), Document(metadata={}, page_content='a research workshop held at Dartmouth College in 1956 and has experienced several waves of'), Document(metadata={}, page_content='experienced several waves of advancement and optimism in the decades since.[29] Artificial'), Document(metadata={}, page_content='decades since.[29] Artificial Intelligence research began in the 1950s with works like Computing'), Document(metadata={}, page_content='with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research'), Document(metadata={}, page_content='Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used'), Document(metadata={}, page_content='and researchers have used artificial intelligence to create artistic works. By the early 1970s,'), Document(metadata={}, page_content='works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by'), Document(metadata={}, page_content='AI works created by AARON, the computer program Cohen created to generate paintings.[30]'), Document(metadata={}, page_content='The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer'), Document(metadata={}, page_content='the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning,'), Document(metadata={}, page_content='process planning, used to generate sequences of actions to reach a specified goal.[31][32]'), Document(metadata={}, page_content='a specified goal.[31][32] Generative AI planning systems used symbolic AI methods such as state'), Document(metadata={}, page_content='AI methods such as state space search and constraint satisfaction and were a \"relatively mature\"'), Document(metadata={}, page_content='were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action'), Document(metadata={}, page_content='to generate crisis action plans for military use,[33] process plans for manufacturing[31] and'), Document(metadata={}, page_content='for manufacturing[31] and decision plans such as in prototype autonomous spacecraft.[34]'), Document(metadata={}, page_content='Generative neural nets (2014-2019)\\nSee also: Machine learning and deep learning'), Document(metadata={}, page_content='Above: An image classifier, an example of a neural network trained with a discriminative objective.'), Document(metadata={}, page_content='a discriminative objective. Below: A text-to-image model, an example of a network trained with a'), Document(metadata={}, page_content='of a network trained with a generative objective.'), Document(metadata={}, page_content='Since its inception, the field of machine learning used both discriminative models and generative'), Document(metadata={}, page_content='models and generative models, to model and predict data. Beginning in the late 2000s, the emergence'), Document(metadata={}, page_content='the late 2000s, the emergence of deep learning drove progress and research in image classification,'), Document(metadata={}, page_content='in image classification, speech recognition, natural language processing and other tasks. Neural'), Document(metadata={}, page_content='and other tasks. Neural networks in this era were typically trained as discriminative models, due'), Document(metadata={}, page_content='as discriminative models, due to the difficulty of generative modeling.[35]'), Document(metadata={}, page_content='In 2014, advancements such as the variational autoencoder and generative adversarial network'), Document(metadata={}, page_content='adversarial network produced the first practical deep neural networks capable of learning'), Document(metadata={}, page_content='networks capable of learning generative models, as opposed to discriminative ones, for complex data'), Document(metadata={}, page_content='ones, for complex data such as images. These deep generative models were the first to output not'), Document(metadata={}, page_content='were the first to output not only class labels for images but also entire images.'), Document(metadata={}, page_content='In 2017, the Transformer network enabled advancements in generative models compared to older'), Document(metadata={}, page_content='models compared to older Long-Short Term Memory models,[36] leading to the first generative'), Document(metadata={}, page_content='to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.[37] This was'), Document(metadata={}, page_content='GPT-1, in 2018.[37] This was followed in 2019 by GPT-2 which demonstrated the ability to generalize'), Document(metadata={}, page_content='the ability to generalize unsupervised to many different tasks as a Foundation model.[38]'), Document(metadata={}, page_content='The new generative models introduced during this period allowed for large neural networks to be'), Document(metadata={}, page_content='large neural networks to be trained using unsupervised learning or semi-supervised learning, rather'), Document(metadata={}, page_content='learning, rather than the supervised learning typical of discriminative models. Unsupervised'), Document(metadata={}, page_content='models. Unsupervised learning removed the need for humans to manually label data, allowing for'), Document(metadata={}, page_content='label data, allowing for larger networks to be trained.[39]'), Document(metadata={}, page_content='Generative AI boom (2020-)\\nMain article: AI boom'), Document(metadata={}, page_content='In 2021, the release of DALL-E, a transformer-based pixel generative model, followed by Midjourney'), Document(metadata={}, page_content='model, followed by Midjourney and Stable Diffusion marked the emergence of practical high-quality'), Document(metadata={}, page_content='of practical high-quality artificial intelligence art from natural language prompts.'), Document(metadata={}, page_content='In 2022, the public release of ChatGPT popularized the use of generative AI for general-purpose'), Document(metadata={}, page_content='AI for general-purpose text-based tasks.[40]'), Document(metadata={}, page_content='In March 2023, GPT-4 was released. A team from Microsoft Research argued that \"it could reasonably'), Document(metadata={}, page_content='that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial'), Document(metadata={}, page_content='version of an artificial general intelligence (AGI) system\".[41] Other scholars have disputed that'), Document(metadata={}, page_content='scholars have disputed that GPT-4 reaches this threshold, calling generative AI \"still far from'), Document(metadata={}, page_content='generative AI \"still far from reaching the benchmark of â€˜general human intelligenceâ€™\" as of'), Document(metadata={}, page_content='human intelligenceâ€™\" as of 2023.[42] In 2023, Meta released an AI model called ImageBind which'), Document(metadata={}, page_content='model called ImageBind which combines data from text, images, video, thermal data, 3D data, audio,'), Document(metadata={}, page_content='thermal data, 3D data, audio, and motion which is expected to allow for more immersive generative'), Document(metadata={}, page_content='for more immersive generative AI content.[43][44]'), Document(metadata={}, page_content='According to a survey by SAS and Coleman Parkes Research, China is leading the world in adopting'), Document(metadata={}, page_content='leading the world in adopting generative AI, with 83% of Chinese respondents using the technology,'), Document(metadata={}, page_content='using the technology, surpassing the global average of 54% and the U.S. at 65%. A UN report'), Document(metadata={}, page_content='the U.S. at 65%. A UN report revealed China filed over 38,000 GenAI patents from 2014 to 2023, far'), Document(metadata={}, page_content='from 2014 to 2023, far exceeding the U.S.[45]'), Document(metadata={}, page_content='Modalities'), Document(metadata={}, page_content='A generative AI system is constructed by applying unsupervised machine learning (invoking for'), Document(metadata={}, page_content='learning (invoking for instance neural network architectures such as GANs, VAE, Transformer, ...)'), Document(metadata={}, page_content='GANs, VAE, Transformer, ...) or self-supervised machine learning to a data set. The capabilities of'), Document(metadata={}, page_content='data set. The capabilities of a generative AI system depend on the modality or type of the data set'), Document(metadata={}, page_content='or type of the data set used.'), Document(metadata={}, page_content='Generative AI can be either unimodal or multimodal; unimodal systems take only one type of input,'), Document(metadata={}, page_content='take only one type of input, whereas multimodal systems can take more than one type of input.[46]'), Document(metadata={}, page_content=\"than one type of input.[46] For example, one version of OpenAI's GPT-4 accepts both text and image\"), Document(metadata={}, page_content='accepts both text and image inputs.[47]')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = Path.cwd()\n",
    "\n",
    "# Construct the relative path to the file\n",
    "file_path = script_dir.parent / 'Data_Ingestion' / 'speech.txt'\n",
    "\n",
    "# Open the file using the relative path\n",
    "with file_path.open() as file:\n",
    "    speech = file.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=30)\n",
    "text_document = text_splitter.create_documents([speech])\n",
    "print(text_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Not to be confused with Artificial general intelligence. This page focuses on statistical machine'\n",
      "page_content='on statistical machine learning AI. For other topics, see Algorithmic composition, Algorithm art,'\n"
     ]
    }
   ],
   "source": [
    "print(text_document[0])\n",
    "print(text_document[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Not to be confused with Artificial general intelligence. This page focuses on statistical machine'),\n",
       " Document(metadata={}, page_content='on statistical machine learning AI. For other topics, see Algorithmic composition, Algorithm art,'),\n",
       " Document(metadata={}, page_content='composition, Algorithm art, Generative art, Procedural generation.'),\n",
       " Document(metadata={}, page_content='Impressionistic image of figures in a futuristic opera scene'),\n",
       " Document(metadata={}, page_content=\"ThÃ©Ã¢tre D'opÃ©ra Spatial, an image made using generative artificial intelligence\"),\n",
       " Document(metadata={}, page_content='Part of a series on\\nArtificial intelligence'),\n",
       " Document(metadata={}, page_content='Major goals\\nApproaches\\nApplications\\nPhilosophy\\nHistory\\nGlossary\\nvte'),\n",
       " Document(metadata={}, page_content='Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is artificial intelligence'),\n",
       " Document(metadata={}, page_content='is artificial intelligence capable of generating text, images, videos, or other data using'),\n",
       " Document(metadata={}, page_content='videos, or other data using generative models,[2] often in response to prompts.[3][4] Generative AI'),\n",
       " Document(metadata={}, page_content='prompts.[3][4] Generative AI models learn the patterns and structure of their input training data'),\n",
       " Document(metadata={}, page_content='of their input training data and then generate new data that has similar characteristics.[5][6]'),\n",
       " Document(metadata={}, page_content='Improvements in transformer-based deep neural networks, particularly large language models (LLMs),'),\n",
       " Document(metadata={}, page_content='large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These'),\n",
       " Document(metadata={}, page_content='in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA,'),\n",
       " Document(metadata={}, page_content='Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as'),\n",
       " Document(metadata={}, page_content='generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators'),\n",
       " Document(metadata={}, page_content='text-to-video AI generators such as Sora.[7][8][9][10] Companies such as OpenAI, Anthropic,'),\n",
       " Document(metadata={}, page_content='such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have'),\n",
       " Document(metadata={}, page_content='numerous smaller firms have developed generative AI models.[3][11][12]'),\n",
       " Document(metadata={}, page_content='Generative AI has uses across a wide range of industries, including software development,'),\n",
       " Document(metadata={}, page_content='software development, healthcare, finance, entertainment, customer service,[13] sales and'),\n",
       " Document(metadata={}, page_content='service,[13] sales and marketing,[14] art, writing,[15] fashion,[16] and product design.[17]'),\n",
       " Document(metadata={}, page_content='and product design.[17] However, concerns have been raised about the potential misuse of generative'),\n",
       " Document(metadata={}, page_content='misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or'),\n",
       " Document(metadata={}, page_content='or deepfakes to deceive or manipulate people, and the mass replacement of human jobs.[18][19]'),\n",
       " Document(metadata={}, page_content='of human jobs.[18][19] Intellectual property law concerns also exist, around generative models that'),\n",
       " Document(metadata={}, page_content='around generative models that are trained on and emulate copyrighted works of art.[20]'),\n",
       " Document(metadata={}, page_content='History\\nMain article: History of artificial intelligence\\nEarly history'),\n",
       " Document(metadata={}, page_content='Since its inception, researchers in the field have raised philosophical and ethical arguments about'),\n",
       " Document(metadata={}, page_content='and ethical arguments about the nature of the human mind and the consequences of creating'),\n",
       " Document(metadata={}, page_content='the consequences of creating artificial beings with human-like intelligence; these issues have'),\n",
       " Document(metadata={}, page_content='these issues have previously been explored by myth, fiction and philosophy since antiquity.[21] The'),\n",
       " Document(metadata={}, page_content='since antiquity.[21] The concept of automated art dates back at least to the automata of ancient'),\n",
       " Document(metadata={}, page_content='to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of'),\n",
       " Document(metadata={}, page_content='such as Daedalus and Hero of Alexandria were described as having designed machines capable of'),\n",
       " Document(metadata={}, page_content='designed machines capable of writing text, generating sounds, and playing music.[22][23] The'),\n",
       " Document(metadata={}, page_content='playing music.[22][23] The tradition of creative automatons has flourished throughout history,'),\n",
       " Document(metadata={}, page_content=\"throughout history, exemplified by Maillardet's automaton created in the early 1800s.[24] Markov\"),\n",
       " Document(metadata={}, page_content='the early 1800s.[24] Markov chains have long been used to model natural languages since their'),\n",
       " Document(metadata={}, page_content='natural languages since their development by Russian mathematician Andrey Markov in the early 20th'),\n",
       " Document(metadata={}, page_content='Markov in the early 20th century. Markov published his first paper on the topic in 1906,[25][26]'),\n",
       " Document(metadata={}, page_content='on the topic in 1906,[25][26] and analyzed the pattern of vowels and consonants in the novel Eugeny'),\n",
       " Document(metadata={}, page_content='in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it'),\n",
       " Document(metadata={}, page_content='learned on a text corpus, it can then be used as a probabilistic text generator.[27][28]'),\n",
       " Document(metadata={}, page_content='Academic artificial intelligence'),\n",
       " Document(metadata={}, page_content='The academic discipline of artificial intelligence was established at a research workshop held at'),\n",
       " Document(metadata={}, page_content='a research workshop held at Dartmouth College in 1956 and has experienced several waves of'),\n",
       " Document(metadata={}, page_content='experienced several waves of advancement and optimism in the decades since.[29] Artificial'),\n",
       " Document(metadata={}, page_content='decades since.[29] Artificial Intelligence research began in the 1950s with works like Computing'),\n",
       " Document(metadata={}, page_content='with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research'),\n",
       " Document(metadata={}, page_content='Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used'),\n",
       " Document(metadata={}, page_content='and researchers have used artificial intelligence to create artistic works. By the early 1970s,'),\n",
       " Document(metadata={}, page_content='works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by'),\n",
       " Document(metadata={}, page_content='AI works created by AARON, the computer program Cohen created to generate paintings.[30]'),\n",
       " Document(metadata={}, page_content='The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer'),\n",
       " Document(metadata={}, page_content='the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning,'),\n",
       " Document(metadata={}, page_content='process planning, used to generate sequences of actions to reach a specified goal.[31][32]'),\n",
       " Document(metadata={}, page_content='a specified goal.[31][32] Generative AI planning systems used symbolic AI methods such as state'),\n",
       " Document(metadata={}, page_content='AI methods such as state space search and constraint satisfaction and were a \"relatively mature\"'),\n",
       " Document(metadata={}, page_content='were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action'),\n",
       " Document(metadata={}, page_content='to generate crisis action plans for military use,[33] process plans for manufacturing[31] and'),\n",
       " Document(metadata={}, page_content='for manufacturing[31] and decision plans such as in prototype autonomous spacecraft.[34]'),\n",
       " Document(metadata={}, page_content='Generative neural nets (2014-2019)\\nSee also: Machine learning and deep learning'),\n",
       " Document(metadata={}, page_content='Above: An image classifier, an example of a neural network trained with a discriminative objective.'),\n",
       " Document(metadata={}, page_content='a discriminative objective. Below: A text-to-image model, an example of a network trained with a'),\n",
       " Document(metadata={}, page_content='of a network trained with a generative objective.'),\n",
       " Document(metadata={}, page_content='Since its inception, the field of machine learning used both discriminative models and generative'),\n",
       " Document(metadata={}, page_content='models and generative models, to model and predict data. Beginning in the late 2000s, the emergence'),\n",
       " Document(metadata={}, page_content='the late 2000s, the emergence of deep learning drove progress and research in image classification,'),\n",
       " Document(metadata={}, page_content='in image classification, speech recognition, natural language processing and other tasks. Neural'),\n",
       " Document(metadata={}, page_content='and other tasks. Neural networks in this era were typically trained as discriminative models, due'),\n",
       " Document(metadata={}, page_content='as discriminative models, due to the difficulty of generative modeling.[35]'),\n",
       " Document(metadata={}, page_content='In 2014, advancements such as the variational autoencoder and generative adversarial network'),\n",
       " Document(metadata={}, page_content='adversarial network produced the first practical deep neural networks capable of learning'),\n",
       " Document(metadata={}, page_content='networks capable of learning generative models, as opposed to discriminative ones, for complex data'),\n",
       " Document(metadata={}, page_content='ones, for complex data such as images. These deep generative models were the first to output not'),\n",
       " Document(metadata={}, page_content='were the first to output not only class labels for images but also entire images.'),\n",
       " Document(metadata={}, page_content='In 2017, the Transformer network enabled advancements in generative models compared to older'),\n",
       " Document(metadata={}, page_content='models compared to older Long-Short Term Memory models,[36] leading to the first generative'),\n",
       " Document(metadata={}, page_content='to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.[37] This was'),\n",
       " Document(metadata={}, page_content='GPT-1, in 2018.[37] This was followed in 2019 by GPT-2 which demonstrated the ability to generalize'),\n",
       " Document(metadata={}, page_content='the ability to generalize unsupervised to many different tasks as a Foundation model.[38]'),\n",
       " Document(metadata={}, page_content='The new generative models introduced during this period allowed for large neural networks to be'),\n",
       " Document(metadata={}, page_content='large neural networks to be trained using unsupervised learning or semi-supervised learning, rather'),\n",
       " Document(metadata={}, page_content='learning, rather than the supervised learning typical of discriminative models. Unsupervised'),\n",
       " Document(metadata={}, page_content='models. Unsupervised learning removed the need for humans to manually label data, allowing for'),\n",
       " Document(metadata={}, page_content='label data, allowing for larger networks to be trained.[39]'),\n",
       " Document(metadata={}, page_content='Generative AI boom (2020-)\\nMain article: AI boom'),\n",
       " Document(metadata={}, page_content='In 2021, the release of DALL-E, a transformer-based pixel generative model, followed by Midjourney'),\n",
       " Document(metadata={}, page_content='model, followed by Midjourney and Stable Diffusion marked the emergence of practical high-quality'),\n",
       " Document(metadata={}, page_content='of practical high-quality artificial intelligence art from natural language prompts.'),\n",
       " Document(metadata={}, page_content='In 2022, the public release of ChatGPT popularized the use of generative AI for general-purpose'),\n",
       " Document(metadata={}, page_content='AI for general-purpose text-based tasks.[40]'),\n",
       " Document(metadata={}, page_content='In March 2023, GPT-4 was released. A team from Microsoft Research argued that \"it could reasonably'),\n",
       " Document(metadata={}, page_content='that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial'),\n",
       " Document(metadata={}, page_content='version of an artificial general intelligence (AGI) system\".[41] Other scholars have disputed that'),\n",
       " Document(metadata={}, page_content='scholars have disputed that GPT-4 reaches this threshold, calling generative AI \"still far from'),\n",
       " Document(metadata={}, page_content='generative AI \"still far from reaching the benchmark of â€˜general human intelligenceâ€™\" as of'),\n",
       " Document(metadata={}, page_content='human intelligenceâ€™\" as of 2023.[42] In 2023, Meta released an AI model called ImageBind which'),\n",
       " Document(metadata={}, page_content='model called ImageBind which combines data from text, images, video, thermal data, 3D data, audio,'),\n",
       " Document(metadata={}, page_content='thermal data, 3D data, audio, and motion which is expected to allow for more immersive generative'),\n",
       " Document(metadata={}, page_content='for more immersive generative AI content.[43][44]'),\n",
       " Document(metadata={}, page_content='According to a survey by SAS and Coleman Parkes Research, China is leading the world in adopting'),\n",
       " Document(metadata={}, page_content='leading the world in adopting generative AI, with 83% of Chinese respondents using the technology,'),\n",
       " Document(metadata={}, page_content='using the technology, surpassing the global average of 54% and the U.S. at 65%. A UN report'),\n",
       " Document(metadata={}, page_content='the U.S. at 65%. A UN report revealed China filed over 38,000 GenAI patents from 2014 to 2023, far'),\n",
       " Document(metadata={}, page_content='from 2014 to 2023, far exceeding the U.S.[45]'),\n",
       " Document(metadata={}, page_content='Modalities'),\n",
       " Document(metadata={}, page_content='A generative AI system is constructed by applying unsupervised machine learning (invoking for'),\n",
       " Document(metadata={}, page_content='learning (invoking for instance neural network architectures such as GANs, VAE, Transformer, ...)'),\n",
       " Document(metadata={}, page_content='GANs, VAE, Transformer, ...) or self-supervised machine learning to a data set. The capabilities of'),\n",
       " Document(metadata={}, page_content='data set. The capabilities of a generative AI system depend on the modality or type of the data set'),\n",
       " Document(metadata={}, page_content='or type of the data set used.'),\n",
       " Document(metadata={}, page_content='Generative AI can be either unimodal or multimodal; unimodal systems take only one type of input,'),\n",
       " Document(metadata={}, page_content='take only one type of input, whereas multimodal systems can take more than one type of input.[46]'),\n",
       " Document(metadata={}, page_content=\"than one type of input.[46] For example, one version of OpenAI's GPT-4 accepts both text and image\"),\n",
       " Document(metadata={}, page_content='accepts both text and image inputs.[47]')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "char_splitter=CharacterTextSplitter(separator=\" \",chunk_size=500,chunk_overlap=100)\n",
    "char_splitter.split_documents(text_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1208, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Not to be confused with Artificial general intelligence. This page focuses on statistical machine learning AI. For other topics, see Algorithmic composition, Algorithm art, Generative art, Procedural generation.\n",
      "Impressionistic image of figures in a futuristic opera scene\n",
      "ThÃ©Ã¢tre D'opÃ©ra Spatial, an image made using generative artificial intelligence\n",
      "Part of a series on\n",
      "Artificial intelligence\n",
      "\n",
      "Major goals\n",
      "Approaches\n",
      "Applications\n",
      "Philosophy\n",
      "History\n",
      "Glossary\n",
      "vte\n",
      "Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]'\n",
      "page_content='Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators such as Sora.[7][8][9][10] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][11][12]'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = Path.cwd()\n",
    "\n",
    "# Construct the relative path to the file\n",
    "file_path = script_dir.parent / 'Data_Ingestion' / 'speech.txt'\n",
    "\n",
    "# Open the file using the relative path\n",
    "with file_path.open() as file:\n",
    "    speech = file.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "text_document = text_splitter.create_documents([speech])\n",
    "print(text_document[0])\n",
    "print(text_document[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split by HTMLHeaderTextLoader \n",
    "* The HTMLHeaderTextSplitter in LangChain is a specialized tool designed to split HTML documents based on specified headers. This splitter is particularly useful for processing structured HTML content while preserving the context and metadata associated with different sections of the document.\n",
    "\n",
    "* Key Features\n",
    "1. Header-Based Splitting: It splits the HTML content at specified header tags\"(e.g.,h1,h2,etc.)\".\n",
    "2. Metadata Preservation: It adds metadata for each header relevant to a given chunk, which helps in maintaining the context.\n",
    "3. Flexible Configuration: You can specify which headers to split on and whether to return each element with its associated headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Welcome to My Enhanced Website'}, page_content='About Us Services Contact'),\n",
       " Document(metadata={'Header 1': 'Welcome to My Enhanced Website', 'Header 2': 'About Us'}, page_content='Lorem ipsum dolor sit amet, consectetur adipiscing elit.'),\n",
       " Document(metadata={'Header 1': 'Welcome to My Enhanced Website', 'Header 2': 'Our Services'}, page_content='Web Development Graphic Design Digital Marketing'),\n",
       " Document(metadata={'Header 1': 'Welcome to My Enhanced Website', 'Header 2': 'Contact Us'}, page_content='Email: \\xa0 info@example.com  \\nPhone: +1234567890')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "html_code =\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <title>My Enhanced Website</title>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Welcome to My Enhanced Website</h1>\n",
    "  <nav>\n",
    "    <ul>\n",
    "      <li><a href=\"#about\">About Us</a></li>\n",
    "      <li><a href=\"#services\">Services</a></li>\n",
    "      <li><a href=\"#contact\">Contact</a></li>\n",
    "    </ul>\n",
    "  </nav>\n",
    "  <section id=\"about\">   \n",
    "\n",
    "    <h2>About Us</h2>\n",
    "    <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>   \n",
    "\n",
    "  </section>\n",
    "  <section id=\"services\">\n",
    "    <h2>Our Services</h2>\n",
    "    <ul>\n",
    "      <li>Web Development</li>\n",
    "      <li>Graphic Design</li>\n",
    "      <li>Digital Marketing</li>\n",
    "    </ul>\n",
    "  </section>\n",
    "  <section id=\"contact\">\n",
    "    <h2>Contact Us</h2>\n",
    "    <p>Email:   \n",
    " info@example.com</p>\n",
    "    <p>Phone: +1234567890</p>\n",
    "  </section>\n",
    "</body>\n",
    "</html> \"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_code)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
