{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying For Question Answering model \n",
    "* Subject- Theory of Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents from Multiple Sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Define a function to load data from URLs\n",
    "def load_data_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, features=\"lxml\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Load content from URLs\n",
    "javatpoint_url = \"https://www.javatpoint.com/automata-tutorial\"\n",
    "geeksforgeeks_url = \"https://www.geeksforgeeks.org/introduction-of-theory-of-computation/\"\n",
    "geektonight_url = \"https://www.geektonight.com/theory-of-computation-notes/\"\n",
    "\n",
    "javatpoint_content = load_data_from_url(javatpoint_url)\n",
    "geeksforgeeks_content = load_data_from_url(geeksforgeeks_url)\n",
    "geektonight_content = load_data_from_url(geektonight_url)\n",
    "\n",
    "# Load content from PDF\n",
    "pdf_path = \"theory-of-computation.pdf\"\n",
    "def load_data_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    pdf_text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        pdf_text += page.get_text()\n",
    "    return pdf_text\n",
    "\n",
    "pdf_content = load_data_from_pdf(pdf_path)\n",
    "\n",
    "# Combine content\n",
    "combined_content = \"\\n\\n\".join([javatpoint_content, geeksforgeeks_content, geektonight_content, pdf_content])\n",
    "\n",
    "# Save combined content to a text file with UTF-8 encoding\n",
    "with open(\"combined_theory_of_computation_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "      file.write(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automata Tutorial | Theory of Computation - JavatpointTutorials×PythonPython Django Numpy Pandas Tkinter Pytorch Flask OpenCVAI, ML and Data ScienceArtificial Intelligence Machine Learning Data Science Deep Learning TensorFlow Artificial Neural Network Matplotlib Python ScipyJavaJava Servlet JSP Spring Boot Spring Framework Hibernate JavaFX Java Web ServicesB.Tech and MCADBMS Data Structures Operating System Computer Network DAA Computer Organization Software Engineering Data MiningWeb TechnologyHTML CSS JavaScript Jquery Angular-8 React JS React Native Node JSSoftware TestingSoftware Testing Selenium JIRA JMeter Postman TestNG SoapUI CucumberInterview×Technical InterviewC C++ Php Java Python JavaScript TypeScriptJava InterviewJDBC Servlet Maven Jenkins Spring Spring Boot JDB Hibernate JSFWeb InterviewHTML CSS JavaScript Jquery Angular Node-JS AJAXDatabase InterviewDBMS SQL PL/SQL Oracle MySQL MongoDB Redis MariaDBCompany InterviewsIBM Adobe Microsoft Amazon TCS HCL Wipro DXC Accenture Capgemini Space X Ericsson Infosy IGate EXL IndiaMART SapientCompilerPythonJavaPhpCC++RHtmlJavascriptTypescriptSwiftHome Python Java JavaScriptHTML SQL PHP C# C++ DS Aptitude Reasoning Selenium DBMS C Andriod Interview QAutomata TutorialAutomata TutorialTheory of AutomataFinite AutomataTransition DiagramTransition TableDFAExamples of DFANFAExamples of NFAEliminating ε TransitionsConversion from NFA to DFAConversion from NFA with ε to DFAMinimization of DFARegular ExpressionRegular ExpressionExamples of Regular ExpressionConversion of RE to FAArden\\'s TheoremMoore MachineMealy MachineConversion from Mealy machine to Moore machineConversion from Moore machine to Mealy machineCFGContext-free GrammarDerivationDerivation TreeAmbiguity in GrammarUnambiguous GrammarSimplification of CFGChomsky\\'s Normal Form (CNF)Greibach Normal Form (GNF)PDAPushdown AutomataPDA AcceptanceNon-deterministic Pushdown AutomataCFG to PDA ConversionTuring MachineApplication of Different Automata | Theory of ComputationIntroduction to Computational Complexity TheoryAutomata and Game TheoryRecursive Descent Parsernext →Automata TutorialTheory of automata is a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. An automaton with a finite number of states is called a Finite automaton.In this tutorial, we are going to learn how to construct deterministic finite automata, non-deterministic finite automata, Regular expression, context-free grammar, context-free language, Push down automata, Turning machines, etc.PrerequisiteBefore learning Automata, you should have a basic understanding of string, language, alphabets, symbols.AudienceOur Automata Tutorial is designed to help beginners and professionals.ProblemsWe assure that you will not find any problem in this Automata Tutorial. But if there is any mistake, please post the problem in contact form.Next TopicTheory of Automatanext →Latest CoursesWe provides tutorials and interview questions of all technology like java tutorial, android, java frameworksContact info G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India[email\\xa0protected].Follow usLatest PostPRIVACY POLICYTutorialsJava Data Structures C Programming C++ Tutorial C# Tutorial PHP Tutorial HTML Tutorial JavaScript Tutorial jQuery Tutorial Spring TutorialInterview QuestionsTcs Intuit Wipro Adobe Infosys Amazon Accenture Cognizant Capgemini MicrosoftOnline CompilerC R C++ Php Java Html Swift Python JavaScript TypeScript© Copyright 2024 Javatpoint. All Rights Reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction of Theory of Computation - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA to DevelopmentNewly Launched!Android with KotlinGenerative AI & ChatGPTMaster Django FrameworkBecome AWS CertifiedFor Working ProfessionalsInterview 101: DSA & System DesignData Science Training ProgramJAVA Backend Development (Live)DevOps Engineering (LIVE)Software Testing & Automation (Live)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation CourseData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive Programming (Live)Full Stack Development with React & Node JS (Live)GATE Exam CoursesGATE CS & IT (Self-Paced)GATE DS & AI (Self-Paced)All CoursesTutorialsData Structures & AlgorithmsDSA for BeginnersData StructuresArraysMatrixStringsLinked ListStackQueueTreeGeneric TreeBinary TreeBinary Search TreeAVL TreeB TreeB+ TreeRed Black TreeTree Data Structure TutorialHeapHashingGraphSet Data StructureMap Data StructureAdvanced Data StructureData Structures TutorialAlgorithmsAnalysis of AlgorithmsSearching AlgorithmsLinear SearchBinary SearchSearching Algorithms TutorialSorting AlgorithmsSelection SortBubble SortInsertion SortMerge SortQuick SortHeap SortCounting SortRadix SortBucket SortSorting Algorithms TutorialGreedy AlgorithmsDynamic ProgrammingGraph AlgorithmsPattern SearchingRecursionBacktrackingDivide and ConquerMathematical AlgorithmsGeometric AlgorithmsBitwise AlgorithmsRandomized AlgorithmsBranch and BoundAlgorithms TutorialComplete DSA TutorialCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE SheetNetflix SDE SheetGoogle SDE SheetWipro Coding SheetInfosys Coding SheetTCS Coding SheetCognizant Coding SheetHCL Coding SheetDSA Cheat SheetsDSA Sheet for BeginnersSDE SheetsFAANG Coding SheetLove Babbaar SheetMass Recruiter SheetProduct-Based Coding SheetCompany-Wise Preparation SheetTop 100 DSA Interview Questions Topic-wise100 Days of CodePythonPython TutorialPython ExercisesPython List ExercisePython String ExercisePython Tuple ExercisePython Dictionary ExercisePython Set ExercisePython Excercises Topic wisePython QuizPython ProgramsAdvanced Python TutorialPython API TutorialPython Database TutorialPython JSONPython Cheat SheetPython ProjectsPython Interview QuestionsML & Data ScienceMachine LearningMachine Learning TutorialMaths for MLML Projects100 Days of Machine LearningData Science TutorialData Science PackagesPandas TutorialNumPy TutorialData VisualizationData Visualization with PythonData Visualization with RTableauPower BIData AnalysisData Analysis with PythonData Analysis with R100 Days of Data AnalyticsDeep LearningNLP TutorialOpenCV TutorialInterview QuestionsMachine Learning Interview QuestionsDeep Learning Interview QuestionsR Interview QuestionsSystem DesignSystem Design TutorialSoftware Design PatternsSystem Design RoadmapTop 10 System Design Interview QuestionsInterview CornerCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsPuzzles for InterviewsLanguagesCC++JavaPythonR TutorialC#SQLScalaPerlGo LanguageWeb DevelopmentHTMLHTML TutorialFree HTML CourseHTML Cheat SheetCSSCSS TutorialFree CSS CourseCSS Cheat SheetJavaScriptJavaScript TutorialJavaScript QuestionsJavaScript Cheat SheetDSA using JavaScriptFree JavaScript CourseJavaScript A to Z Complete GuideTypeScriptReactJSReactJS TutorialFree ReactJS CourseReactJS Cheat SheetNextJSNode.jsPHPAngularJSjQueryWeb Development Using PythonDjangoFlaskSeleniumPostmanGithubWeb Design100 Days of Web DevelopmentCS SubjectsOperating SystemDBMSComputer NetworksEngineering MathematicsComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOps And LinuxDevOps TutorialGITAWSKubernetesDockerMicrosoft Azure TutorialGoogle Cloud PlatformDevOps RoadmapDevOps Interview QuestionsLinuxLinux TutorialLinux Commands A-ZLinux Commands CheatsheetFile Permissions in LinuxLinux System AdministrationLinux Shell ScriptingLinux NetworkingLinux Interview QuestionsSchool LearningClass 8 Study MaterialClass 9 Study MaterialClass 10 Study MaterialClass 11Study MaterialClass 12 Study MaterialEnglish GrammarGfG SchoolCommerceGATEGATE Computer Science NotesLast Minute NotesGATE CS Solved PapersGATE CS Original Papers and Official KeysGATE CS 2025 SyllabusGATE DA 2025 SyllabusOther CS ExamsISROUGC NETGeeksforGeeks VideosJobsGet Hired: Apply for JobsCorporate Hiring SolutionsFiltered JobsJobs for FreshersJobs for ExperiencedAll JobsPracticePractice Coding ProblemsAll DSA ProblemsProblem of the DayCompany Wise Coding PracticeAmazonMicrosoftFlipkartExplore AllGfG SDE SheetPractice Problems Difficulty WiseBasicEasyMediumHardLanguage Wise Coding PracticeCPPJavaPythonCurated DSA ListsBeginner\\'s DSA SheetLove Babbar SheetTop 50 Array ProblemsTop 50 String ProblemsTop 50 DP ProblemsTop 50 Graph ProblemsTop 50 Tree ProblemsContestsJob-A-Thon Hiring ChallengeGfG Weekly [Rated Contest]All Contests and Events\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                                        You\\'re all caught up!!\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAptitudeEngineering MathematicsDiscrete MathematicsOperating SystemDBMSComputer NetworksDigital Logic and DesignC ProgrammingData StructuresAlgorithmsTheory of ComputationCompiler DesignComputer Org and Architecture \\n\\n\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGo PremiumShare Your ExperiencesAutomata TutorialAutomata _ IntroductionIntroduction of Theory of ComputationChomsky Hierarchy in Theory of ComputationApplications of various AutomataRegular Expression and Finite AutomataIntroduction of Finite AutomataArden\\'s Theorem in Theory of ComputationArden\\'s Theorem and Challenging Applications | Set 2L-graphs and what they represent in TOCHypothesis (language regularity) and algorithm (L-graph to NFA) in TOCRegular Expressions, Regular Grammar and Regular LanguagesHow to identify if a language is regular or notDesigning Finite Automata from Regular Expression (Set 1)Star Height of Regular Expression and Regular LanguageGenerating regular expression from Finite AutomataDesigning Deterministic Finite Automata (Set 1)Designing Deterministic Finite Automata (Set 2)DFA for Strings not ending with \"THE\"DFA of a string with at least two 0’s and at least two 1’sDFA for accepting the language L = {  anbm | n+m=even }DFA machines accepting odd number of 0’s or/and even number of 1’sDFA of a string in which 2nd symbol from RHS is \\'a\\'Union process in DFAConcatenation process in DFADFA in LEX code which accepts even number of zeros and even number of onesConversion from NFA to DFAMinimization of DFAReversing Deterministic Finite AutomataComplementation process in DFAKleene\\'s Theorem in TOC | Part-1Mealy\\xa0and\\xa0Moore\\xa0Machines in TOCDifference Between Mealy Machine and Moore MachineCFGRelationship between grammar and language in Theory of ComputationSimplifying Context Free GrammarsClosure Properties of Context Free LanguagesUnion and Intersection of Regular languages with CFLConverting Context Free Grammar to Chomsky Normal FormConverting Context Free Grammar to Greibach Normal FormPumping Lemma in Theory of ComputationCheck if the language is Context Free or NotAmbiguity in Context free Grammar and Context free LanguagesOperator grammar and precedence parser in TOCContext-sensitive Grammar (CSG) and Language (CSL)PDA (Pushdown Automata)Introduction of Pushdown AutomataPushdown Automata Acceptance by Final StateConstruct Pushdown Automata for given languagesConstruct Pushdown Automata for all length palindromeDetailed Study of PushDown AutomataNPDA for accepting the language  L = {an bm cn | m,n>=1}NPDA for accepting the language L = {an bn cm | m,n>=1}NPDA for accepting the language  L = {an bn | n>=1}NPDA for accepting the language  L = {am b(2m) | m>=1}NPDA for accepting the language  L = {am bn cp dq | m+n=p+q ; m,n,p,q>=1}Construct Pushdown automata for L = {0n1m2m3n | m,n ≥ 0}Construct Pushdown automata for L = {0n1m2(n+m) | m,n ≥ 0}NPDA for accepting the language L = {ambnc(m+n) | m,n ≥ 1}NPDA for accepting the language L = {amb(m+n)cn | m,n ≥ 1}NPDA for accepting the language L = {a2mb3m | m ≥ 1}NPDA for accepting the language L = {amb(2m+1) | m ≥ 1}NPDA for accepting the language L = {aibjckdl | i==k or j==l,i>=1,j>=1}Construct Pushdown automata for L = {a(2*m)c(4*n)dnbm | m,n ≥ 0}NPDA for L =  {0i1j2k | i==j or j==k ; i , j , k >= 1}NPDA for accepting the language L = {anb(2n) | n>=1} U {anbn | n>=1}NPDA for the language L ={w∈ {a,b}*| w contains equal no. of a\\'s and b\\'s}Turing MachineRecursive and Recursive Enumerable Languages in TOCTuring Machine in TOCTuring Machine for additionTuring machine for subtraction | Set 1Turing machine for multiplicationTuring machine for copying dataConstruct a Turing Machine for language L = {0n1n2n | n≥1}Construct a Turing Machine for language L = {wwr | w ∈ {0, 1}}Construct a Turing Machine for language L = {ww | w ∈ {0,1}}Construct Turing machine for L = {an bm a(n+m) | n,m≥1}Construct a Turing machine for L = {aibjck | i*j = k; i, j, k ≥ 1}Turing machine for 1\\'s and 2’s complementRecursive and Recursive Enumerable Languages in TOCTuring Machine for subtraction | Set 2Halting Problem in Theory of ComputationTuring Machine as ComparatorDecidabilityDecidable and Undecidable Problems in Theory of ComputationUndecidability and Reducibility in TOCComputable and non-computable problems in TOCTOC Interview preparationLast Minute Notes - Theory of ComputationTOC  Quiz and PYQ\\'s in TOCTheory of Computation - GATE CSE Previous Year QuestionsRegular languages and finite automataContext free languages and Push-down automataRecursively enumerable sets and Turing machinesUndecidabilityDSA to Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction of Theory of Computation\\n\\n\\n\\nLast Updated : \\n27 Sep, 2024\\n\\n\\n\\n\\n\\n\\nSummarize\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\nAutomata theory (also referred to as the Theory Of Computation) is a branch of Computer Science and Mathematics that studies how machines compute functions and solve problems. This field is mainly focused on mathematical structures called automata and is crucial for the purpose of studying processes occurring in discrete systems.\\nWhat is Automata Theory?\\nIn automata theory, scientists and engineers can predict the behavior of computing systems thereby improving problem-solving approaches. Originally developed to describe and explain the dynamics of systems, automata theory is the theoretical base of the formal languages theory, grammar, and computational complexity.\\nBasic Terminologies of Theory of Computation\\nNow, let’s understand the basic terminologies, which are important and frequently used in the Theory of Computation.\\xa0\\nSymbol\\nA symbol (often also called a character) is the smallest building block, which can be any alphabet, letter, or picture.\\xa0\\n\\nAlphabets (Σ)\\nAlphabets are a set of symbols, which are always finite.\\xa0\\n\\nString\\xa0\\nA string is a finite sequence of symbols from some alphabet. A string is generally denoted as w and the length of a string is denoted as |w|.\\xa0\\nEmpty string is the string with zero occurrence of symbols, represented as ε.\\nNumber of Strings (of length 2) that can be generated over the alphabet {a, b}:                     -   -                     a   a                     a   b                     b   a                     b   bLength of String |w| = 2Number of Strings = 4Conclusion:For alphabet {a, b} with length n, number of strings can be generated = 2n.\\n__mask-blockquote__index=1__\\n\\nThe Theory of Computation explores automata, languages, and complexity. If you want to dive deeper into this subject for GATE, the GATE CS Self-Paced Course covers it extensively.\\n\\nClosure Representation in TOC\\nL+: It is a Positive Closure that represents a set of all strings except Null or ε-strings.\\nL*: It is “Kleene Closure“, that represents the occurrence of certain alphabets for given language alphabets from zero to the infinite number of times. In which ε-string is also included.\\nFrom the above two statements, it can be concluded that:\\nL* = εL+\\nExample:(a) Regular expression for language accepting all combination of g\\'s over Σ={g}:                                         R = g*                               R={ε,g,gg,ggg,gggg,ggggg,...}(b) Regular Expression for language accepting all combination of g\\'s over Σ={g} : R = g+                               R={g,gg,ggg,gggg,ggggg,gggggg,...}\\nNote: Σ* is a set of all possible strings(often power set(need not be unique here or we can say multiset) of string) So this implies that language is a subset of Σ*.This is also called a “Kleene Star”.\\nKleene Star is also called a “Kleene Operator” or “Kleene Closure”. Engineers and IT professionals make use of Kleene Star to achieve all set of strings which is to be included from a given set of characters or symbols. It is one kind of Unary operator. In Kleene Star methodology all individual elements of a given string must be present but additional elements or combinations of these alphabets can be included to any extent.\\nExample:Input String: \"GFG\".Σ* = { ε,\"GFG\",\"GGFG\",\"GGFG\",\"GFGGGGGGGG\",\"GGGGGGGGFFFFFFFFFGGGGGGGG\",...}  (Kleene Star is an infinite set but if we provide any grammar rules then it can work as a finite set.Please note that we can include ε string also in given Kleene star representation.)\\nLanguage\\nA language is a set of strings, chosen from some Σ* or we can say- ‘A language is a subset of Σ* ‘. A language that can be formed over ‘ Σ ‘ can be Finite or Infinite.\\nExample of Finite Language:           L1 = { set of string of 2 }         L1 = { xy, yx, xx, yy }Example of Infinite Language:         L1 = { set of all strings starts with \\'b\\' }         L1 = { babb, baa, ba, bbb, baab, ....... }\\nConclusion\\nIt is an important branch of computation that is concerned with formal languages, and automata theory in particular. It provides a basis for other courses such as Turing machines and computational complexity that are very important in computer science.\\nIntroduction of Theory of Computation – FAQs\\nWhat is the relevance of the automata theory in computer science?\\n\\nAutomata theory is used in modeling computational problems hence enhancing the understanding and design of systems such as compilers, interpreters among others.\\n\\nwhat is the purpose of using Kleene Star in the study of formal languages?\\n\\nThe Kleene Star extends symbols from a given alphabet where one is able to create infinite strings from it or even the null string.\\n\\nIs it possible to implement automata theory into real life?\\n\\nOf course, automata theory has found its use in certain areas like compiler design, artificial intelligence, network security and natural language processing.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nabhishek1 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Article\\n\\n\\n\\nAutomata Tutorial\\n\\n\\n\\n\\nNext Article\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\n\\n\\n\\n\\n\\n\\nRead More\\n\\n\\n\\nSimilar Reads\\n\\n\\n\\nIntroduction to Computation Complex Theory\\nBroad Overview : Complexity theory, in a nutshell, a complexity word is a quite fancy word, literally, it sounds complex, but it is not an intimidating topic. What it really means is analyzing the program or we can say analyzing the efficiency of the program, figuring out whether the program is correct, figuring out whether one program is better th\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nIntroduction To Grammar in Theory of Computation\\nPrerequisite - Theory of ComputationGrammar :It is a finite set of formal rules for generating syntactically correct sentences or meaningful correct sentences.Constitute Of Grammar :Grammar is basically composed of two basic elements - Terminal Symbols - Terminal symbols are those which are the components of the sentences generated using a grammar\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nTheory of Computation - GATE CSE Previous Year Questions\\nSolving GATE Previous Year\\'s Questions (PYQs) not only clears the concepts but also helps to gain flexibility, speed, accuracy, and understanding of the level of questions generally asked in the GATE exam, and that eventually helps you to gain good marks in the examination. Previous Year Questions help a candidate practice and revise for GATE, whic\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\n\\nRelationship between grammar and language in Theory of Computation\\nA grammar is a set of production rules which are used to generate strings of a language. In this article, we have discussed how to find the language generated by a grammar and vice versa as well. Language generated by a grammar - Given a grammar G, its corresponding language L(G) represents the set of all strings generated from G. Consider the foll\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nTheory of Computation | Regular languages and finite automata | Question 2\\nWhat is the complement of the language accepted by the NFA shown below? (A) A (B) B (C) C (D) D Answer: (B) Explanation: Quiz of this QuestionPlease comment below if you find anything wrong in the above post\\n\\n\\n\\n1 min read\\n\\n\\n\\n\\nArden\\'s Theorem in Theory of Computation\\nArden\\'s theorem state that: \"If P and Q are two regular expressions over \"∑\", and if P does not contain \"∈\" , then the following equation in R given by R = Q + RP has a unique solution i.e., R = QP*.\" That means, whenever we get any equation in the form of R = Q + RP, then we can directly replace it with R = QP*. So, here we will first prove that R\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nDecidability Table in Theory of Computation\\nPrerequisite - Undecidability, Decidable and undecidable problems Identifying languages (or problems*) as decidable, undecidable or partially decidable is a very common question in GATE. With correct knowledge and ample experience, this question becomes very easy to solve. A language is undecidable if it is not decidable. An undecidable language ma\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\nAccording to Chomsky hierarchy, grammar is divided into 4 types as follows: Type 0 is known as unrestricted grammar.Type 1 is known as context-sensitive grammar.Type 2 is known as a context-free grammar.Type 3 Regular Grammar.Type 0: Unrestricted Grammar: Type-0 grammars include all formal grammar. Type 0 grammar languages are recognized by turing\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nPumping Lemma in Theory of Computation\\nThere are two Pumping Lemmas, which are defined for 1. Regular Languages, and 2. Context - Free Languages Pumping Lemma for Regular Languages For any regular language L, there exists an integer n, such that for all x ? L with |x| ? n, there exists u, v, w ? ?*, such that x = uvw, and (1) |uv| ? n (2) |v| ? 1 (3) for all i ? 0: uviw ? L In simple te\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nDecidable and Undecidable Problems in Theory of Computation\\nIn the Theory of Computation, problems can be classified into decidable and undecidable categories based on whether they can be solved using an algorithm. A decidable problem is one for which a solution can be found in a finite amount of time, meaning there exists an algorithm that can always provide a correct answer. While an undecidable problem i\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nHalting Problem in Theory of Computation\\nTo understand better the halting problem, we must know Decidability , Undecidability and Turing machine , decision problems and also a theory named as Computability theory and Computational complexity theory. Some important terms: Computability theory - The branch of theory of computation that studies which problems are computationally solvable usi\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nAutomata Theory | Set 2\\nFollowing questions have been asked in GATE CS 2012 exam. 1) What is the complement of the language accepted by the NFA shown below? Assume ∑ = {a} and ε is the empty string (A) Φ (B) ε (C) a (D) {a, ε} Answer (B) The given alphabet ∑ contains only one symbol {a} and the given NFA accepts all strings with any number of occurrences of \\'a\\'. In other\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 3\\nFollowing questions have been asked in GATE CS 2011 exam. 1) The lexical analysis for a modern language such as Java needs the power of which one of the following machine models in a necessary and sufficient sense? (A) Finite state automata (B) Deterministic pushdown automata (C) Non-deterministic pushdown automata (D) Turing machine Answer (A) Lex\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nAutomata Theory | Set 4\\nFollowing questions have been asked in GATE CS 2011 exam. 1) Let P be a regular language and Q be context-free language such that Q ⊆ P. (For example, let P be the language represented by the regular expression p*q* and Q be {pnqn|n ∈ N}). Then which of the following is ALWAYS regular? (A) P ∩ Q (B) P - Q (C) ∑* - P (D) ∑* - Q\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nAutomata Theory | Set 5\\nFollowing questions have been asked in GATE CS 2009 exam. 1) S --> aSa| bSb| a| b ;The language generated by the above grammar over the alphabet {a,b} is the set of (A) All palindromes. (B) All odd length palindromes. (C) Strings that begin and end with the same symbol (D) All even length palindromes. Answer (B) The strings accepted by language are\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 7\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider L= {(TM) | TM is the Turing machine that halts on all input and L(TM)= L\\' for some undecidable language L\\'}. Here, (TM) is the encoding of a Turing machine as a string over alphabet {0, 1} then L is: (A) decidable and recursively enumerable (B) decidable and recursive (C) decid\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 8\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Which one of the following language is Regular? (A) {wxwR | w,x ∈ (a+b)+} (B) {wxwR | w ∈ (a+b)*, x ∈ {a,b}} (C) {wwRx | w,x ∈ (a+b)+} (D) {wwR | w ∈ (a+b)*} Explanation: (A) It is correct, since this language can form regular expression which is {{ a(a + b)+a } + {b(a + b)+b}}, i.e., s\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nAutomata Theory | Set 9\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider the following two statements with respect to Countability: Statement-1: If X union of \\'Y\\' is uncountable, then both set \\'X\\' and set \\'Y\\' must be uncountable. Statement-2: The Cartesian product of two countable sets \\'X\\' and \\'Y\\' is countable. Which of the following option is true\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 10\\nThese questions for practice purpose of GATE CS Exam. Ques-1: Consider the following statements: X: For any language either a language L or its complement L\\' must be finite.Y: DFA for language which contains epsilon must have initial state as final state.Z: Non-deterministic finite automata is more powerful than deterministic finite automata. Which\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nRegular Graph in Graph Theory\\nPrerequisite: Graph Theory Basics – Set 1, Set 2 Regular Graph: A graph is called regular graph if degree of each vertex is equal. A graph is called K regular if degree of each vertex in the graph is K. Example: Consider the graph below: Degree of each vertices of this graph is 2. So, the graph is 2 Regular. Similarly, below graphs are 3 Regular an\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n5 Color Theorem in Graph Theory\\nThe graph is a data structure that is used extensively in real-life. Planar Graph: If a graph can be drawn on the plane without crossing, it is said to be planar. Coloring of a simple graph is the assignment of color to each vertex of the graph so that no two adjacent vertices are assigned the same color. Bi-Partite Graphs: A bipartite graph, also\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nMathematics | Graph Theory Basics - Set 1\\nA graph is a data structure that is defined by two components : A node or a vertex.An edge E or ordered pair is a connection between two nodes u,v that is identified by unique pair(u,v). The pair (u,v) is ordered because (u,v) is not same as (v,u) in case of directed graph.The edge may have a weight or is set to one in case of unweighted graph.Cons\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nMathematics | Graph theory practice questions\\nProblem 1 - There are 25 telephones in Geeksland. Is it possible to connect them with wires so that each telephone is connected with exactly 7 others. Solution - Let us suppose that such an arrangement is possible. This can be viewed as a graph in which telephones are represented using vertices and wires using the edges. Now we have 25 vertices in\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\nSet Theory Operations in Relational Algebra\\nRelational Algebra in DBMS These Set Theory operations are the standard mathematical operations on set. These operations are Binary operations that are, operated on 2 relations unlike PROJECT, SELECT and RENAME operations. These operations are used to merge 2 sets in various ways. The set operation is mainly categorized into the following: Union op\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nApplications of Group Theory\\nGroup theory is the branch of mathematics that includes the study of elements in a group. Group is the fundamental concept of algebraic structure like other algebraic structures like rings and fields. Group: A non-empty set G with * as operation, (G, *) is called a group if it follows the closure, associativity, identity, and inverse properties. Pr\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nQuotient Group in Group Theory\\nWe can say that \"o\" is the binary operation on set G if: G is a non-empty set & G * G = { (a,b): a, b∈ G } and o: G * G --> G. Here, aob denotes the image of ordered pair (a,b) under the function/operation o.Example - \"+\" is called a binary operation on G (any non-empty set ) if & only if: a+b ∈G; ∀ a,b ∈G and a+b give the same result ev\\n\\n\\n\\n12 min read\\n\\n\\n\\n\\nTypes of Sets in Set Theory\\nIn mathematics, a Set is a fundamental concept representing a collection of well-defined objects or elements. Sets are typically denoted by capital letters, and the individual elements within a set are listed in curly braces, separated by commas. For example, A={1,2,3,4,5} represents a set A with elements 1, 2, 3, 4, and 5. The order of elements wi\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\n\\nMathematics | Graph Theory Basics - Set 2\\nGraph theory is a basic branch of discrete mathematics that mainly focuses on the relationship between objects. These objects are called vertices and these vertices are joined by edges. Graphs are common in computer science, network analysis, and many other everyday uses because they provide a good representation of connection, relationship, and pr\\n\\n\\n\\n10 min read\\n\\n\\n\\n\\nGroup in Maths: Group Theory\\nGroup theory is one of the most important branches of abstract algebra which is concerned with the concept of the group. A group consists of a set equipped with a binary operation that satisfies four key properties: specifically, it includes property of closure, associativity, the existence of an identity element, and the existence of inverse eleme\\n\\n\\n\\n13 min read\\n\\n\\n\\n\\nMatching (Graph Theory)\\nMatching (Graph Theory): In graph theory, matching is a fundamental concept used to describe a set of edges without common vertices. Matchings are used in various applications such as network design, job assignments, and scheduling. Understanding matchings is essential for solving problems involving optimal pairings and resource allocation. Table o\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nGATE CS\\n\\n\\nTheory of Computation\\n \\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Please Login to comment...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n76k+ interested Geeks \\n\\n\\n\\nCore Computer Science Subject for Interview Preparation \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n28k+ interested Geeks \\n\\n\\n\\nGATE Computer Science & Information Technology - 2025 \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n12k+ interested Geeks \\n\\n\\n\\nCBSE Class 12 Computer Science \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nExplore More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                      Corporate & Communications Address:- A-143, 9th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305) | Registered Address:- K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalCareersIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramExploreJob-A-Thon Hiring ChallengeHack-A-ThonGfG Weekly ContestOffline Classes (Delhi/NCR)DSA in JAVA/C++Master System DesignMaster CPGeeksforGeeks VideosGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonData Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython Programming ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceAccountancyBusiness StudiesEconomicsManagementHR ManagementFinanceIncome TaxDatabasesSQLMYSQLPostgreSQLPL/SQLMongoDBPreparation CornerCompany-Wise Recruitment ProcessResume TemplatesAptitude PreparationPuzzlesCompany-Wise PreparationCompaniesCollegesCompetitive ExamsJEE AdvancedUGC NETUPSCSSC CGLSBI POSBI ClerkIBPS POIBPS ClerkMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsRecent ArticlesFree Online ToolsTyping TestImage EditorCode FormattersCode ConvertersCurrency ConverterRandom Number GeneratorRandom Password GeneratorWrite & EarnWrite an ArticleImprove an ArticlePick Topics to WriteShare your ExperiencesInternships \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\\r\\n        acknowledge that you have read and understood our\\r\\n        Cookie Policy &\\r\\n        Privacy Policy\\n\\n\\r\\n        Got It !\\r\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\nThis improvement is locked by another user right now. You can suggest the changes for now and it will be under \\'My Suggestions\\' Tab on Write.\\nYou will be notified via email once the article is available for improvement.\\r\\n                        Thank you for your valuable feedback!\\r\\n                    \\n\\nSuggest changes\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\n\\nmin 4 words, max CharLimit:2000\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n                        Can\\'t choose a topic to write? click here for suggested topics\\n                    \\n\\n\\n\\n                       Write and publish your own Article\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTheory Of Computation Notes PDF, Syllabus ✅ [2021] B Tech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHomeBest Courses\\n\\nGoogle Professional Certificates\\nHuman Resource\\n\\nHuman Resource Human Resource Management Human Resource Planning Organizational Culture Organization Development Organizational Behavior\\nLearning DealsAll Blog PostManagement\\n\\nBusiness Statistics Lean Six Sigma Management Operation Management Research Methodology Operations Research Procurement Management Production Management Supply Chain Strategic Management\\nMarketing\\n\\nEconomics Brand Management Business Business Communication Business Law Entrepreneurship Consumer Behaviour Marketing Essentials Marketing Management Sales Management Shark Tank India\\nBusiness Tech\\n\\nProject Management Business Analytics Management Information System Enterprise Resource Planning Technologies Cloud Computing\\nAbout\\n\\nAbout Us Cookie Policy DMCA Policy Disclaimer Contact Us\\nToggle website search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Category\\nClose\\n\\n\\n\\n\\n\\n\\nHome\\nBest Courses\\n\\nGoogle Professional Certificates\\n\\n\\nHuman Resource\\n\\nHuman Resource\\nHuman Resource Management\\nHuman Resource Planning\\nOrganizational Culture\\nOrganization Development\\nOrganizational Behavior\\n\\n\\nLearning Deals\\nAll Blog Post\\nManagement\\n\\nBusiness Statistics\\nLean Six Sigma\\nManagement\\nOperation Management\\nResearch Methodology\\nOperations Research\\nProcurement Management\\nProduction Management\\nSupply Chain\\nStrategic Management\\n\\n\\nMarketing\\n\\nEconomics\\nBrand Management\\nBusiness\\nBusiness Communication\\nBusiness Law\\nEntrepreneurship\\nConsumer Behaviour\\nMarketing Essentials\\nMarketing Management\\nSales Management\\nShark Tank India\\n\\n\\nBusiness Tech\\n\\nProject Management\\nBusiness Analytics\\nManagement Information System\\nEnterprise Resource Planning\\nTechnologies\\nCloud Computing\\n\\n\\nAbout\\n\\nAbout Us\\nCookie Policy\\nDMCA Policy\\nDisclaimer\\nContact Us\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\nHome>B Tech Study Material>Theory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\nPost last modified:30 March 2021\\nReading time:27 mins read\\nPost category:B Tech Study Material\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload Theory of Computation Notes PDF, syllabus for B Tech, BCA, MCA 2021. We provide a complete theory of computation pdf. Theory of Computation lecture notes includes a theory of computation notes, theory of computation book, theory of computation courses, theory of computation syllabus, theory of computation question paper, MCQ, case study, theory of computation interview questions and available in theory of computation pdf form.\\nTheory of Computation Notes\\nTheory of Computation subject is included in B Tech CSE, BCA, MCA, M Tech. So, students can able to download theory of computation notes pdf.\\n\\nTable of Content1 Theory of Computation Syllabus2 Theory of Computation PDF3 Theory of Computation Notes3.1 What is Theory of Computation?3.2 Theory of Computation Handwritten Notes4 Theory of Computation Interview Questions5 Theory of Computation Question Paper6 Theory of Computation Book\\n\\nTheory of Computation Notes can be downloaded in theory of computation pdf from the below article\\n\\nTheory of Computation Syllabus\\nA detailed theory of computation syllabus as prescribed by various Universities and colleges in India are as under. You can download the syllabus in the theory of computation pdf form.\\nUnit I\\n\\n\\n\\n\\n\\n\\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural Representations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, Proof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets Strings, Languages, Applications of Automata Theory. \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, Extending the Transition Function to Strings, The Language of a DFA \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The Languages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite Automata With Epsilon-Transitions: Uses of Î-Transitions, The Formal Notation for an Î-NFA, Epsilon-Closures, Extended Transitions and Languages for Î-NFA’s, Eliminating Î- Transitions.\\n Unit II\\nRegular Expressions and Languages: Regular Expressions: The Operators of regular Expressions, Building Regular Expressions, Precedence of Regular-Expression Operators, Precedence of Regular-Expression Operators Finite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions by Eliminating States, Converting Regular Expressions to Automata. \\nAlgebraic Laws for Regular Expressions: Properties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of the Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular Languages, Equivalence and Minimization of Automata, \\nContext-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations Using a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, Parse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and Parse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive Inferences, Applications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: Ambiguous Grammars, Removing Ambiguity.\\n\\n\\n\\n\\n\\nUnit III \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical Notation for PDA’s, Instantaneous Descriptions of a PDA, \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack to Final State, From Final State to Empty Stack Equivalence of PDA’s and CFG’s: From Grammars to Pushdown Automata, From PDA’s to Grammars \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and Deterministic PDA’s, DPDA’s and Context-Free Languages, DPDA’s and Ambiguous Grammars \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The Pumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, Decision Properties of CFL’s\\nUnit IV\\nIntroduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for Turing Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, Turing Machines and Halting Programming Techniques for Turing Machines, Extensions to the Basic Turing Machine, Restricted Turing Machines, Turing Machines and Computers\\nUNIT V\\nRecursive And Recursively Enumerable Languages: Properties of recursive and recursively enumerable languages, Universal Turing machine, The Halting problem, Undecidable problems about TMs. Context-sensitive language and linear bounded automata (LBA), Chomsky hierarchy, Decidability, Post’s correspondence problem (PCP), undecidability of PCP.\\n\\nTheory of Computation PDF\\n\\n\\n\\n\\n\\n Theory of Computation Notes PDF(How to download) Theory of Computation Notes Download  Theory of Computation Book Download Theory of Computation Syllabus Download  Theory of Computation Question Paper Download  Theory of Computation Interview Questions Download \\n\\nTheory of Computation Notes\\nWhat is Theory of Computation?\\n\\n\\n\\n\\n\\n Download PDF\\n\\n\\n\\n\\n\\n\\nTheory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory.\\nTheory of Computation Handwritten Notes\\n\\n\\n\\n Download PDF\\n\\nTheory of Computation Interview Questions\\nSome of the theory of computation interview questions are mentioned below. You can download the QnA in theory of computation pdf form.\\nWhat is TOC?What is Automata Theory in TOC?What is Regular Language in TOC?What is Grammer and Language in TOC?What is Null String in TOC?What is Grammer and Language in TOC?What is Regular Expression in TOC?What is Linear Bound Automata in TOC?What is Context-Free Language(CFL) in TOC?What is Recursive Language in TOC?What is the use of Lexical Analysis in TOC?What is Chomsky Classification of Languages in TOC?Define Kleene Star Closure in TOC?What is the Productions in TOC? Explain Production Rules.\\n\\nTheory of Computation Question Paper\\nIf you have already studied the theory of computation notes, now it’s time to move ahead and go through previous year theory of computation question paper. \\n Download PDF Fill Before Download\\nIt will help you to understand question paper pattern and type of theory of computation questions and answers asked in B Tech, BCA, MCA, M Tech theory of computation exam. You can download the syllabus in theory of computation pdf form.\\n\\nTheory of Computation Book\\nBelow is the list of theory of computation book recommended by the top university in India.\\nIntroduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, R.Motwani & J.D.Ullman (3rd Edition) – Pearson EducationTheory of Computer Science (Automata Language & Computations), by K.L.Mishra & N. Chandrashekhar, PHI\\n\\nDownload B Tech (CS) Study Material\\n\\n\\n\\nComputer Networks Notes ✅ [2020] PDF – Download \\n\\nComputer Networks Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Networks Notes) \\n\\n\\n\\nComputer Graphics Notes ✅ [2020] PDF – Download \\n\\nComputer Graphics Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Graphics Notes)\\n\\n\\n\\nOperating System Notes ✅ [2020] PDF – Download  \\n\\nOperating System Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operating System Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download\\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Compiler Design Notes)\\n\\n\\n\\nData Structures Notes ✅ [2020] PDF – Download \\n\\nData Structures Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Structures Notes)\\n\\n\\n\\nDigital Image Processing Notes ✅ [2020] PDF – Download \\n\\nDigital Image Processing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Digital Image Processing Notes) \\n\\n\\n\\nTheory of Computation Notes ✅ [2020] PDF – Download \\n\\nTheory of Computation Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Theory of Computation Notes) \\n\\n\\n\\nComputer Organization and Architecture Notes ✅ [2020] PDF – Download \\n\\nComputer Organization and Architecture Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Computer Organization and Architecture Notes) \\n\\n\\n\\nCloud Computing Notes ✅ [2020] PDF – Download \\n\\nCloud Computing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Cloud Computing Notes) \\n\\n\\n\\nData Communication and Networking Notes ✅ [2020] PDF – Download \\n\\nData Communication and Networking Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Communication and Networking Notes) \\n\\n\\n\\n Software Engineering Notes ✅ [2020] PDF – Download \\n\\nSoftware Engineering Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Software Engineering Notes) \\n\\n\\n\\nWeb Technologies Notes ✅ [2020] PDF – Download \\n\\nWeb Technologies Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Web Technologies Notes) \\n\\n\\n\\nMicroprocessor and Microcontrollers Notes ✅ [2020] PDF – Download \\n\\nMicroprocessor and Microcontrollers Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Microprocessor and Microcontrollers Notes) \\n\\n\\n\\nDesign and Analysis of Algorithm Notes ✅ [2020] PDF – Download \\n\\nDesign and Analysis of Algorithm Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Design and Analysis of Algorithm Notes) \\n\\n\\n\\nOperation Research Notes ✅ [2020] PDF – Download \\n\\nOperation Research Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operation Research Notes) \\n\\n\\n\\nDatabase Management Systems Notes ✅ [2020] PDF – Download \\n\\nDatabase Management Systems Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Database Management Systems Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download \\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Compiler Design Notes)\\n\\n\\n\\n\\n\\nIn the above article, a student can download theory of computation notes for B Tech, BCA, MCA, M Tech. Theory of Computation lecture notes and study material includes theory of computation notes, theory of computation books, theory of computation syllabus, theory of computation question paper, theory of computation case study, theory of computation interview questions, theory of computation courses in theory of computation pdf form.\\n\\nGo On, Share & Help your Friend\\n Did we miss something in B.Tech Computer Science Notes or You want something More? Come on! Tell us what you think about our post on Theory of Computation Notes | PDF, Syllabus, Book | B Tech 2020 in the comments section and Share this post with your friends.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead more articles\\n Previous PostWeb Technologies Notes | PDF, Syllabus, Book | B Tech 2021 Next PostDigital Image Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n\\nTags: B Tech, B Tech CS Books, B Tech CS Syllabus, B Tech CSE Books, B Tech CSE Notes, B Tech CSE Syllabus, B Tech Notes, B Tech Study Material, Computer Science Notes PDF, CS Notes, CSE Notes, Theory of Computation Book, Theory of Computation Course, Theory of Computation Interview Questions, Theory of Computation Notes, Theory of Computation PDF, Theory of Computation PPT, Theory of Computation Question Paper, Theory of Computation Syllabus, Theory of Computation Tutorial\\n\\n\\nYou Might Also Like\\n\\n\\n\\n\\nDigital Signal Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n \\n\\n\\nOperation Research Notes | PDF, Syllabus | MBA, B Tech 2024\\n\\n18 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDatabase Management Systems Notes | PDF | B Tech 2021\\n\\n22 March 2020\\n\\n\\n\\nAdvanced Java Programming Notes | PDF | B Tech (2024)\\n\\n23 November 2020\\n\\n\\n\\n\\n \\n\\n\\nComputer Networks Notes | PDF, Syllabus, Books | B Tech (2024)\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nSoftware Engineering Notes | PDF, Syllabus | B Tech 2021\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDesign and Analysis of Algorithm Notes PDF | B Tech (2024)\\n\\n20 March 2020\\n\\n\\n\\nDigital Communication Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nAnalog Communication PDF | Notes, Syllabus B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nBusiness Intelligence Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nHow to Download Notes on Geektonight\\n\\n27 June 2020\\n\\n\\n\\nWireless Networks Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n\\nLeave a Reply Cancel replyYou must be logged in to post a comment. \\n\\n\\n\\n\\n\\n\\nAll CategoryAll Category\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies\\nTreasury Management in Banking\\nTwitter Certification\\nUncategorized\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nWorld\\'s Best Online Courses at One Place \\n\\n\\n\\nWe’ve spent the time in finding, so you can spend your time in learning \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDigital Marketing \\n\\n\\n\\nGoogle Ads CourseFacebook Ads CourseSEO CourseInstagram Marketing CourseSEM CourseSocial Media CourseEmail Marketing CoursePinterest CourseChatbot CourseBlogging CourseContent Marketing CourseWooCommerce CourseClickbank Affiliate Marketing CourseAffiliate Marketing CourseAmazon Affiliate Marketing CourseShopify, eCommerce & Dropshipping CourseExcel Data Analysis CourseWordPress CourseGoogle Tag Manager CourseGoogle Analytics CourseDigital Marketing CourseYoutube Marketing CourseBing Ads CourseSocial Media Analytics Course \\n\\n\\n\\n\\n\\n\\n\\nBusiness \\n\\n\\n\\nProduct Strategy CourseSales CourseBrand Strategy CourseBusiness Law CourseStrategic Management CourseMarketing Analytics CourseBusiness Strategy CourseMarketing Management CourseHuman Resource CourseProduct Management CourseProduct Marketing CourseB2B Marketing CourseGrowth Hacking CoursePeople HR Analytics CourseEntrepreneurship CourseBusiness Statistics CourseProject Management CourseNegotiation CourseTime Management CourseLeadership CourseCareer Development CourseStress Management CourseAnxiety Management CourseDesign Thinking CourseEmotional Intelligence CourseTeam Building CourseBusiness Analytics CourseDigital Transformation Course \\n\\n\\n\\n\\n\\n\\n\\nPersonal Growth \\n\\n\\n\\nEnglish Grammar CourseVocabulary CourseSoft Skills CoursePublic Speaking CoursePhotography CourseBody Language CourseCommunication Skills CourseInterview Preparation CourseProductivity CourseMindfulness CourseMemory CourseSelf DisciplineSpeed ReadingAcademic WritingCopywriting CourseScientific Writing CourseNovel Writing CourseAcademic Writing CourseTravel Writer CourseCreative Writing CourseInterior Design CourseGraphic Design CourseDrawing CourseDigital Art CourseUI UX Designer Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFinance \\n\\n\\n\\nMutual Fund CourseFinancial Analysis CoursePersonal Finance CourseCost Accounting CourseAudit CourseFintech CourseValue Investing CourseTrading CourseFinancial Modeling CourseInvestment CourseProject Finance CourseStock Trading CourseFinancial & Capital Markets CourseAccounting CourseFinancial Engineering Course \\n\\n\\n\\n\\n\\n\\n\\nFinTech \\n\\n\\n\\nNFT CourseMongoDB CoursejQuery CourseBlockchain CourseCryptocurrency CourseSwift CourseAWS CourseRedux CourseGo CourseDeFi CourseSolidity CourseMetaverse CourseDjango CourseJIRA CourseConversion Rate Optimization (CRO) CourseAnalytics CourseCustomer Loyalty Course \\n\\n\\n\\n\\n\\n\\n\\nLanguage \\n\\n\\n\\nEnglish SpeakingKorean LanguageGerman LanguageSpanish LanguageFrench Language Italian Language Russian Language Japanese Language Arabic LanguageSwedish LanguageHindi LanguagePortuguese LanguageDutch LanguageLatin LanguageTurkish LanguageHungarian LanguageVietnameseAmerican AccentPronunciationSpelling Courses \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTech \\n\\n\\n\\nData Science CourseR Programming CourseBig Data CourseSQL CourseData Analytics CourseMachine Learning CoursePython CourseSQL Data Science CourseArtificial Intelligence CourseCloud Computing CourseData Warehouse CourseNLP Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment \\n\\n\\n\\nReact JS CoursesFront End Development CourseFull Stack Web Developer CourseC++ CourseData Engineering CourseHTML & CSS3 CourseMicrosoft SQL CourseMySQL CourseJava CourseJavaScript CourseTypeScript CourseBack End Development CourseDatabase CourseGraphQL Course \\n\\n\\n\\n\\n\\n\\n\\nExam Prep \\n\\n\\n\\nGRE PrepGMAT PrepMCAT PrepIELTS PrepDAT PrepPSAT PrepCFA PrepOAT PrepACT PrepLSAT PrepFRM PrepSSAT PrepCPA PrepTESOL PrepSAT PrepSSAT Prep \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython \\n\\n\\n\\nPython CourseDeep Learning Python CoursePython Data Science CoursePython for Marketing CoursePython for Finance CoursePython Pandas CoursePython Data Visualization CoursePython Machine Learning CoursePython Data Processing CoursePython Scripting CoursePython for Data Analysis CoursePython Data Structure CourseNLP Python CourseMatplotlib CourseData Cleaning CourseStatistical Modeling CourseKeras CoursePytorch CourseMachine Learning Finance Course \\n\\n\\n\\n\\n\\n\\n\\nTech \\n\\n\\n\\nSCADA CourseASP.net CourseScrum CourseSpring Boot and MVC CourseIT Support & Help Desk CourseRuby on Rails CourseKubernetes CourseDocker CourseNodeJs CourseAngular CoursePHP CourseAPI CourseAlteryx CoursePower BI CourseTableau CourseData Visualization CourseDAX CourseData Streaming CourseRegex CourseQlik Sense CoursePlotly Dash CourseData Modeling Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment \\n\\n\\n\\nAndroid CourseiOS Development CourseFlutter CourseKotlin CourseIonic CourseXamarin CourseVirtual Reality CourseMatlab CourseGit & GitHub CourseSelenium CourseShell Scripting CourseARKit CourseGame Design CourseUnity CourseUnreal Engine CourseGame Development CourseBlender CourseDreamweaver CourseVisual Studio CourseC# (C-Sharp) CourseBootstrap Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChild Care \\n\\n\\n\\nChild Nutrition CourseBaby Massage CourseChildcare & Early Education CourseBaby Sign Language CourseKids Art & Drawing CourseKids Coding CourseChild Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Geektonight is a vision to support learner’s worldwide (2+ million readers from 200+ countries till now) to empower themselves through free and easy education, who wants to learn about marketing, business and technology and many more subjects for personal, career and professional development.\\n\\n \\n\\nConnect With Us\\n\\nOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tab \\n\\n \\n\\nMoreAbout UsOpens in a new tabDisclaimerOpens in a new tabCookie PolicyOpens in a new tabPrivacy PolicyOpens in a new tabDMCA PolicyOpens in a new tab \\n\\nCategoriesCategories\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies\\nTreasury Management in Banking\\nTwitter Certification\\nUncategorized\\n\\n\\n \\n\\n\\n\\n\\n\\nCopyright 2023 Geektonight\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch this website\\n\\nType then hit enter to search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMBA NOTES \\nTHEORY OF COMPUTATION\\nBCS601T \\nTHEORY OF COMPUTATION \\nUnit – I \\n \\n(12 hours) \\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural\\nRepresentations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, \\nProof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets \\nStrings, Languages, Applications of Automata Theory. \\n \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of \\na Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, \\nExtending the Transition Function to Strings, The Language of a DFA \\n \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The \\nLanguages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite \\nAutomata With Epsilon-Transitions: Uses of \\uf0ce-Transitions, The Formal Notation for an \\n \\n\\uf0ce-NFA, Epsilon-Closures, Extended Transitions and Languages for \\uf0ce-NFA’s, Eliminating \\uf0ce-\\nTransitions. \\n \\nUnit – II \\n(12 hours) \\nRegular  Expressions\\nand  Languages:  Regular  Expressions:  The  Operators  of  regular\\nExpressions, Building \\nRegular  Expressions,  Precedence  of  Regular-Expression  Operators,\\n \\nPrecedence of Regular-Expression Operators \\n \\nFinite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting \\nDFA’s to Regular Expressions, Converting DFA’s to R egular Expressions by Eliminating States, \\nConverting Regular Expressions to Automata. \\n \\nAlgebraic Laws for Regular Expressions: \\n \\nProperties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of \\nthe Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular \\nLanguages, Equivalence and Minimization of Automata, \\n \\nUnit – III \\n(12 hours) \\nGrammar : Types of Grammar \\nContext-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations \\nUsing a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, \\n \\nParse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and \\nParse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive \\nInferences, \\n \\nApplications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: \\nAmbiguous Grammars, Removing Ambiguity From Grammars, Leftmost Derivations as a Way to \\nExpress Ambiguity, Inherent Anbiguity \\nwww.indiansbrain.com\\n \\nUnit – IV \\n \\n(12 hours) \\n \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical \\nNotation for PDA’s, Instantaneous Descriptions of a PDA, \\n \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack \\nto Final State, From Final State to Empty Stack \\n \\nEquivalence of PDA’s and CFG’s: From Grammars to Pu shdown Automata, From PDA’s to \\nGrammars \\n \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and \\nDeterministic PDA’s, DPDA’s and Context-Free La nguages, DPDA’s and Ambiguous Grammars \\n \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The \\nPumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, \\nDecision Properties of CFL’s \\n \\nUnit – V \\n \\n(12 hours) \\nIntroduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for \\nTuring Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, \\nTuring Machines and Halting \\n \\nProgramming Techniques for Turing Machines, Extensions to the Basic Turing Machine, \\nRestricted Turing Machines, Turing Machines and Computers, \\n \\nUndecidability: A Language That is Not Recursively Enumerable, Enumerating the Binary \\nStrings, Codes for Turing Machines, The Diagonalization Language \\n \\nAn Undecidable Problem That Is RE: Recursive Languages, Complements of Recursive and RE \\nlanguages, The Universal Languages, Undecidability of the Universal Language \\n \\nUndecidable Problems About Turing Machines: Reductions, Turing Machines That Accept the \\nEmpty Language. Post’s Correspondence Problem: Definition of Post’s Correspondence Problem, \\nThe “Modified” PCP, Other Undecidable Prob lems: Undecidability of Ambiguity for CFG’s \\n \\n \\n \\n \\nText Book: \\n \\n1. Introduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, \\nR.Motwani & J.D.Ullman (3rd Edition) – Pearson Education \\n \\nwww.indiansbrain.com\\n \\nUNIT - I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWhat is TOC? \\n \\nIn theoretical computer science, the theory of computation is the branch that deals with \\nwhether and how efficiently problems can be solved on a model of computation, using an \\nalgorithm. The field is divided into three major branches: automata theory, computability theory \\nand computational complexity theory. \\n \\nIn order to perform a rigorous study of computation, computer scientists work with a \\nmathematical abstraction of computers called a model of computation. There are several \\nmodels in use, but the most commonly examined is the Turing machine. \\n \\nAutomata theory \\n \\nIn theoretical computer science, automata theory is the study of abstract machines (or more \\nappropriately, abstract \\'mathematical\\' machines or systems) and the computational problems that \\ncan be solved using these machines. These abstract machines are called automata. \\n \\nThis automaton consists of \\n \\n\\uf095 states (represented in the figure by circles),\\uf020\\n\\uf095 and transitions (represented by arrows).\\uf020\\n \\nAs the automaton sees a symbol of input, it makes a transition (or jump) to another state, \\naccording to its transition function (which takes the current state and the recent symbol as \\nits inputs). \\nUses of Automata: compiler design and parsing. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIntroduction to formal proof: \\nBasic Symbols used : \\nU – Union \\n∩- Conjunction \\n \\nϵ - Empty String \\nΦ – NULL set \\n7- negation \\n \\n‘ – compliment \\n= > implies \\nwww.indiansbrain.com\\nAdditive inverse: a+(-a)=0 \\nMultiplicative inverse: a*1/a=1 \\nUniversal set U={1,2,3,4,5} \\nSubset A={1,3} \\nA’ ={2,4,5} \\nAbsorption law: AU(A ∩B) = A, A∩(AUB) = A \\n \\nDe Morgan’s Law: \\n \\n(AUB)’ =A’ ∩ B’ \\n(A∩B)’ = A’ U B’ \\nDouble compliment \\n(A’)’ =A \\n \\nA ∩ A’ = Φ \\n \\nLogic relations: \\na b = > 7a U b \\n7(a∩b)=7a U 7b \\n \\nRelations: \\n \\nLet a and b be two sets a relation R contains aXb. \\nRelations used in TOC: \\nReflexive: a = a \\nSymmetric: aRb = > bRa \\nTransition: aRb, bRc = > aRc \\n \\nIf a given relation is reflexive, symmentric and transitive then the relation is called equivalence \\nrelation. \\n \\nDeductive proof: Consists of sequence of statements whose truth lead us from some \\ninitial statement called the hypothesis or the give statement to a conclusion statement. \\n \\n \\n \\n \\nAdditional forms of proof: \\nProof of sets \\nProof by contradiction \\nProof by counter example \\n \\nDirect proof (AKA) Constructive proof: \\nIf p is true then q is true \\n \\nEg: if a and b are odd numbers then product is also an odd \\nnumber. Odd number can be represented as 2n+1 \\n \\na=2x+1, b=2y+1 \\nproduct of a X b = (2x+1) X (2y+1) \\n= 2(2xy+x+y)+1 = 2z+1 (odd number) \\nwww.indiansbrain.com\\n \\n \\n \\nProof by contrapositive: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof by Contradiction: \\n \\nH and not C implies falsehood. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBe regarded as an observation than a theorem. \\n \\n \\n \\n \\n \\n \\n \\n \\nFor any sets a,b,c if a∩b = Φ and c is a subset of b the prove that a∩c \\n=Φ Given : a∩b=Φ and c subset b \\n \\nAssume: a∩c  Φ \\n \\nThen \\n \\n \\n= > a∩b Φ = > a∩c=Φ(i.e., the assumption is wrong) \\nwww.indiansbrain.com\\nProof by mathematical Induction: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLanguages : \\n \\nThe languages we consider for our discussion is an abstraction of natural languages. That is, \\nour focus here is on formal languages that need precise and formal definitions. Programming \\nlanguages belong to this category. \\n \\nSymbols : \\n \\nSymbols are indivisible objects or entity that cannot be defined. That is, symbols are the atoms \\n \\nof the world of languages. A symbol is any single object such as begin, or do. \\n \\nAlphabets : \\n \\nAn alphabet is a finite, nonempty set of symbols. The alphabet of a language is \\nnormally denoted by \\n. When more than one alphabets are considered for discussion, \\nthen \\nsubscripts may be used (e.g.    \\netc) or sometimes other symbol like G may also be \\n \\nintroduced. \\n \\n \\n \\n \\n \\n \\n \\n \\nExample : \\n \\nStrings or Words over Alphabet : \\n \\nA string or word over an alphabet   \\nis a finite sequence of concatenated symbols of     \\n. \\n , a, 0, 1, #, \\nwww.indiansbrain.com\\nExample : 0110, 11, 001 are three strings over the binary alphabet { 0, 1 } . \\n \\naab, abcb, b, cc are four strings over the alphabet { a, b, c }. \\n \\nIt is not the case that a string over some alphabet should contain all the symbols from the alpha-\\nbet. For example, the string cc over the alphabet { a, b, c } does not contain the symbols a and b. \\nHence, it is true that a string over an alphabet is also a string over any superset of that alphabet. \\n \\nLength of a string : \\nThe number of symbols in a string w is called its length, denoted by |w|. \\n \\nExample : | 011 | = 4, |11| = 2, | b | = 1 \\n \\nConvention : We will use small case letters towards the beginning of the English alphabet \\nto denote symbols of an alphabet and small case letters towards the end to \\n \\ndenote strings over an alphabet. That is, \\n (symbols) and \\n \\nare strings. \\n \\nSome String Operations : \\nLet \\nand \\nbe two strings. The concatenation of x and y \\n \\ndenoted by xy, is the string   \\n. That is, the concatenation of x and y \\n \\ndenoted by xy is the string that has a copy of x followed by a copy of y without any intervening \\nspace between them. \\n \\nExample : Consider the string 011 over the binary alphabet. All the prefixes, suffixes and \\nsubstrings of this string are listed below. \\n \\nPrefixes:  , 0, 01, 011. \\n \\nSuffixes:  , 1, 11, 011. \\n \\nSubstrings:  , 0, 1, 01, 11, 011. \\n \\nNote that x is a prefix (suffix or substring) to x, for any string x and is a prefix (suffix or \\nsubstring) to any string. \\n \\nA string x is a proper prefix (suffix) of string y if x is a prefix (suffix) of y and x 蝤 y. \\n \\nIn the above example, all prefixes except 011 are proper prefixes. \\n \\nPowers of Strings : For any string x and integer \\n, we use \\nto denote the string formed \\nby sequentially concatenating n copies of x. We can also give an inductive \\ndefinition of \\nas follows: \\n= e, if n \\n= 0 ; otherwise \\n \\nwww.indiansbrain.com\\nExample : If x = 011, then  \\n= 011011011,  \\n= 011 and \\n \\nPowers of Alphabets : \\n \\nWe write \\n(for some integer k) to denote the set of strings of length k with symbols \\n \\nfrom \\n. In other words, \\n \\n= { w | w is a string over  \\n and | w | = k}. Hence, for any alphabet,   \\ndenotes the set \\n \\nof all strings of length zero. That is,   \\n= { e }. For the binary alphabet { 0, 1 } we have \\n \\nthe following. \\n \\n \\n \\n \\n \\n \\n \\n \\nThe set of all strings over an alphabet \\n is denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nThe set  \\n contains all the strings that can be generated by iteratively concatenating sym- \\nbols from      \\nany number of times. \\n \\nExample : If \\n= { a, b }, then \\n= {  , a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, …}. \\n \\nPlease note that if \\n, then \\n that is \\n. It may look odd that one can proceed \\nfrom the empty set to a non-empty set by iterated concatenation. But there is a reason for this \\nand we accept this convention \\n \\nThe set of all nonempty strings over an alphabet     \\nis denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nNote that  \\nis infinite. It contains no infinite strings but strings of arbitrary lengths. \\n \\nReversal : \\nFor any string  \\nthe reversal of the string is \\n. \\n \\nAn inductive definition of reversal can be given as follows: \\nwww.indiansbrain.com\\nLanguages : \\nA language over an alphabet is a set of strings over \\nthat alphabet. Therefore, a \\n \\nlanguage L is any subset of \\n. That is, any \\nis a language. \\nExample : \\n \\n \\n1. F is the empty language.  \\n \\n2.\\nis a language for any \\n. \\n \\n3. {e} is a language for any \\n. Note that, \\n. Because the language F does not \\n \\ncontain any string but {e} contains one string of length zero. \\n4. The set of all strings over { 0, 1 } containing equal number of 0\\'s and 1\\'s. \\n \\n5. The set of all strings over {a, b, c} that starts with a. \\n \\nConvention : Capital letters A, B, C, L, etc. with or without subscripts are normally used \\nto denote languages. \\n \\nSet operations on languages : Since languages are set of strings we can apply set operations to \\nlanguages. Here are some simple examples (though there is nothing new in it). \\n \\n \\nUnion : A string \\n \\n \\n \\n \\n \\n \\niff \\nor \\n \\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 0, 11, 01, 011, 111 } \\nIntersection  : \\nA   string, \\nxϵ  L1 \\n∩ L2 \\niff    x   ϵ  L1    and   x   ϵ  L2    .\\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 01 } \\n \\nComplement :  Usually, \\nis the universe that a complement is taken with respect to. \\n \\nThus for a language L, the complement is L(bar) = { \\n| \\n}. \\n \\nExample : Let L = { x | |x| is even }. Then its complement is the language { \\n| |x| is \\n \\nodd }. \\n \\nSimilarly we can define other usual set operations on languages like relative \\ncom-plement, symmetric difference, etc. \\n \\nReversal of a language : \\nThe reversal of a language L, denoted as \\n, is defined as:  \\n. \\n \\nExample : \\n \\n1.  Let L = { 0, 11, 01, 011 }. Then \\n= { 0, 11, 10, 110 }.\\nwww.indiansbrain.com\\n2.  Let L = { \\n| n is an integer }. Then  \\n=  { \\n| n is an integer }. \\n \\nLanguage concatenation : The concatenation of languages       \\nand \\nis defined as \\n \\n= { xy | \\nand \\n}. \\n \\nExample : { a, ab }{ b, ba } = { ab, aba, abb, abba }. \\n \\nNote that , \\n1. \\n  in general. \\n2. \\n \\n \\n3. \\n \\n \\nIterated concatenation of languages : Since we can concatenate two languages, we also repeat this to \\nconcatenate any number of languages. Or we can concatenate a language with itself any \\n \\nnumber of times. The operation L with itself n times. This is \\ndefined formally as follows: \\n \\n \\n \\n \\n \\n \\nExample :  Let L = { a, ab }. Then according to the definition, \\nwe have \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand so on. \\n \\n \\n \\nKleene\\'s Star operation : The Kleene star operation on a language L, denoted as is defined as \\nfollows : \\n \\n= ( Union n in N ) \\n \\n= \\n \\n \\n= { x | x is the concatenation of zero or more strings from L } \\n \\ndenotes the concatenation of \\nwww.indiansbrain.com\\nThus \\nis the set of all strings derivable by any number of concatenations of strings in \\nL. It is also useful to define \\n \\n=, i.e., all strings derivable by one or more concatenations of strings in L. That is \\n \\n= (Union n in N and n >0) \\n \\n= \\n \\nExample :  Let L = { a, ab }. Then we have, \\n \\n= \\n \\n= {e} \\n{a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\n= \\n \\n= {a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\nNote : \\nis in  \\n, for every language L, including . \\n \\nThe previously introduced definition of   \\nis an instance of Kleene star. \\n \\n \\n \\n \\n \\n(Generates) \\n(Recognizes) \\nGrammar \\nLanguage \\n  Automata \\n \\nAutomata: A algorithm or program that automatically recognizes if a particular string belongs to \\nthe language or not, by checking the grammar of the string. \\n \\nAn automata is an abstract computing device (or machine). There are different varities of such \\nabstract machines (also called models of computation) which can be defined mathematically. \\n \\nEvery Automaton fulfills the three basic requirements. \\n \\n• \\nEvery automaton consists of some essential features as in real computers. It has a mech-\\nanism for reading input. The input is assumed to be a sequence of symbols over a given \\nalphabet and is placed on an input tape(or written on an input file). The simpler automata \\ncan only read the input one symbol at a time from left to right but not change. Powerful \\nversions can both read (from left to right or right to left) and change the input. \\nwww.indiansbrain.com\\n\\uf095 The automaton can produce output of some form. If the output in response to an input \\nstring is binary (say, accept or reject), then it is called an accepter. If it produces an out-\\nput sequence in response to an input sequence, then it is called a transducer(or \\nautomaton with output).\\uf020\\n\\uf020\\n• \\nThe automaton may have a temporary storage, consisting of an unlimited number of \\ncells, each capable of holding a symbol from an alphabet ( whcih may be different from \\nthe input alphabet). The automaton can both read and change the contents of the storage \\ncells in the temporary storage. The accusing capability of this storage varies depending \\non the type of the storage. \\n \\n• \\nThe most important feature of the automaton is its control unit, which can be in any \\none of a finite number of interval states at any point. It can change state in some de-\\nfined manner determined by a transition function. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1: The figure above shows a diagrammatic representation of a generic \\nautoma-tion. \\n \\nOperation of the automation is defined as follows. \\n \\nAt any point of time the automaton is in some integral state and is reading a particular symbol \\nfrom the input tape by using the mechanism for reading input. In the next time step the automa-\\nton then moves to some other integral (or remain in the same state) as defined by the transition \\nfunction. The transition function is based on the current state, input symbol read, and the content \\nof the temporary storage. At the same time the content of the storage may be changed and the \\ninput read may be modifed. The automation may also produce some output during this transition. \\nThe internal state, input and the content of storage at any point defines the configuration of the \\nautomaton at that point. The transition from one configuration to the next ( as defined by the \\ntransition function) is called a move. Finite state machine or Finite Automation is the simplest \\ntype of abstract machine we consider. Any system that is at any point of time in one of a finite \\nnumber of interval state and moves among these states in a defined manner in response to some \\ninput, can be modeled by a finite automaton. It doesnot have any temporary storage and hence a \\nrestricted model of computation. \\nwww.indiansbrain.com\\nFinite Automata \\n \\nAutomata (singular : automation) are a particularly simple, but useful, model of compu-\\ntation. They were initially proposed as a simple model for the behavior of neurons. \\n \\nStates, Transitions and Finite-State Transition System : \\n \\n \\nLet us first give some intuitive idea about a state of a system and state transitions before \\ndescribing finite automata. \\n \\nInformally, a state of a system is an instantaneous description of that system which gives all \\nrelevant information necessary to determine how the system can evolve from that point on. \\n \\nTransitions are changes of states that can occur spontaneously or in response to inputs to the \\nstates. Though transitions usually take time, we assume that state transitions are instantaneous \\n(which is an abstraction). \\n \\nSome examples of state transition systems are: digital systems, vending machines, etc. A system \\n \\ncontaining only a finite number of states and transitions among them is \\ncalled a finite-state transition system. \\n \\nFinite-state transition systems can be modeled abstractly by a mathematical model called \\nfinite automation \\n \\nDeterministic Finite (-state) Automata \\n \\nInformally, a DFA (Deterministic Finite State Automaton) is a simple machine that reads an in-\\nput string -- one symbol at a time -- and then, after the input has been completely read, decides \\nwhether to accept or reject the input. As the symbols are read from the tape, the automaton can \\nchange its state, to reflect how it reacts to what it has seen so far. A machine for which a deter-\\nministic code can be formulated, and if there is only one unique way to formulate the code, then \\nthe machine is called deterministic finite automata. \\n \\nThus, a DFA conceptually consists of 3 parts: \\n \\n \\n \\n \\n1. A tape to hold the input string. The tape is divided into a finite number of cells. Each \\ncell holds a symbol from \\n. \\n2. A tape head for reading symbols from the tape \\n3. A control , which itself consists of 3 things: \\n \\no \\nfinite number of states that the machine is allowed to be in (zero or more states \\nare designated as accept or final states), \\n \\no a current state, initially set to a start state, \\nwww.indiansbrain.com\\no a state transition function for changing the current state. \\n \\nAn automaton processes a string on the tape by repeating the following actions until the \\ntape head has traversed the entire string: \\n \\n1. The tape head reads the current tape cell and sends the symbol s found there to \\nthe control. Then the tape head moves to the next cell. \\n \\n2. he control takes s and the current state and consults the state transition function to \\nget the next state, which becomes the new current state. \\n \\nOnce the entire string has been processed, the state in which the automation enters is examined. \\n \\nIf it is an accept state , the input string is accepted ; otherwise, the string is rejected . Summariz- \\n \\ning all the above we can formulate the following formal definition: \\n \\n \\nDeterministic Finite State Automaton : A Deterministic Finite State Automaton (DFA) is \\n \\na 5-tuple : \\n \\n\\uf095 Q is a finite set of states.\\uf020\\n• \\nis a finite set of input symbols or alphabet \\n \\n\\uf095 \\nis the “next state” transition function (which is total ). Intuitively, \\nis\\n a \\nfunction that tells which state to move to in response to an input, i.e., if M is in \\n \\nstate q and sees input a, it moves to state      \\n. \\n \\n\\uf095 \\nis the start state.\\uf020\\n• \\nis the set of accept or final states. \\n \\nAcceptance of Strings : \\n \\nA DFA accepts a string   \\nif there is a sequence of states     \\nin Q \\n \\nsuch that \\n \\n1.  \\nis the start state. \\n2.  \\nfor all \\n. \\n \\n3. \\n \\nLanguage Accepted or Recognized by a DFA : \\n \\nThe language accepted or recognized by a DFA M is the set of all strings accepted by M , and \\n \\nis denoted by \\ni.e. \\nThe  notion \\nof \\nacceptance can also be made more precise by extending the transition function \\n.\\n \\nExtended transition function : \\nwww.indiansbrain.com\\nExtend \\n(which is function on symbols) to a function on strings, i.e. . \\n \\n \\n \\nThat is, \\n is the state the automation reaches when it starts from the state q and finish \\nprocessing the string w. Formally, we can give an inductive definition as follows: \\n \\nThe language of the DFA M is the set of strings that can take the start state to one of \\nthe accepting states i.e. \\n \\n \\nL(M) = { \\n| M accepts w } \\n \\n= {\\n| \\n} \\n \\n \\nExample 1 : \\n \\n \\n \\n \\n \\n \\n \\n \\nis the start state \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is a formal description of a DFA. But it is hard to comprehend. For ex. The language of the \\nDFA is any string over { 0, 1} having at least one 1 \\n \\nWe can describe the same DFA by transition table or state transition diagram as follow-\\ning: \\n \\n \\n \\n \\nTransition Table : \\n \\n0    1 \\nwww.indiansbrain.com\\n \\n \\nIt is easy to comprehend the transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExplanation :  We cannot \\nreach find state \\nw/0 or in the i/p string. There can be any no. \\nof 0\\'s at the beginning. \\n( The self-loop at \\non label 0 indicates it ). Similarly there \\ncan be any no. of 0\\'s & 1\\'s in any order at the end of the string. \\n \\nTransition table : \\n \\nIt is basically a tabular representation of the transition function that takes two arguments (a \\nstate and a symbol) and returns a value (the “next state”). \\n \\n• \\nRows correspond to states, \\n• \\nColumns correspond to input symbols, \\n• \\nEntries correspond to next states \\n• \\nThe start state is marked with an arrow \\n• \\nThe accept states are marked with a star (*). \\n \\n \\n \\n0    1 \\n \\n \\n \\n(State) Transition diagram : \\n \\nA state transition diagram or simply a transition diagram is a directed graph which can \\nbe constructed as follows: \\n \\n1.  For each state in Q there is a node. \\n2. There is a directed edge from node q to node p labeled a iff \\n . (If there are \\nseveral input symbols that cause a transition, the edge is labeled by the list of these \\nsymbols.) \\n3. There is an arrow with no source into the start state. \\n4. Accepting states are indicated by double circle. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n5. \\n \\n6. Here is an informal description how a DFA operates. An input to a DFA can be any \\n \\nstring. \\nPut a pointer to the start state q. Read the input string w from left \\n \\nto right, one symbol at a time, moving the pointer according to the transition  \\n \\nfunction, \\n.  If the next symbol of w is a and the pointer is on state p, move the \\n \\npointer to \\n. When the end of the input string w is encountered, the pointer is on \\n \\nsome state, r. The string is said to be accepted by the DFA if \\nand \\n \\nrejected if \\n. Note that there is no formal mechanism for moving the pointer. \\n7. A language \\nis said to be regular if L = L(M) for some DFA M. \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying \\ncertain recursive rules as given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) is a RE \\n \\nii) is a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\n \\nIf \\n \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\nwww.indiansbrain.com\\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive \\nREs) by a finite no of applications of the recursive step (given in 2). \\n \\nExample : Let \\n = { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this \\nexpression by applying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n4 \\n(1+  ) \\nRule 2(iv) & Step 3 \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated \\nwith every RE). We will see later that REs are used to attribute regular languages. \\n \\n \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We \\ncan define the language L(r) associated with (or described by) a REs as follows. \\n \\n1. is the RE describing the empty language i.e. L( ) = . \\n \\n2. is a RE describing the language {\\n} i.e. L( ) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\nwww.indiansbrain.com\\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,. } {0} \\n{1} \\n \\n= {  , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nConsider the RE ab + c. The language described by the RE can be thought of either \\nL(a)L(b+c) or L (ab) L(c) as provided by the rules (of languages described by REs) \\ngiven already. But these two represents two different languages lending to \\nambiguity. To remove this ambiguity we can either \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. \\nLike other algebras mod in mathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) \\noperator. \\n \\nii) It is also important to note that concatenation & union (+) operators are \\nassociative and union operation is commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language \\nL(ab) L(c) i.e. it should be grouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For \\nexample, the language represented by the RE a(b+c) is L(a)L(b+c). \\nwww.indiansbrain.com\\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language \\nL(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all \\nstrings over {0,1} which are either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an \\neven number of 0\\'s followed by an odd number of 1\\'s i.e. \\n \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE \\nrr, \\ndenotes \\nr, and so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of \\nconsecutive 1\\'s} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and \\nwhat goes before is completely arbitrary. Considering these observations we can write \\nthe REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE \\n(0+1)*11(0+1)*+(0+1)*00(0+1)* represents the set of string over {0,1} that contains \\nthe substring 11 or 00. \\n \\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the \\nset of strings over {0,1} that contains exactly two 1\\'s. The presence of two 1\\'s in the \\nRE and any no of 0\\'s before, between and after the 1\\'s ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1\\'s. \\n \\nSolution : There must be at least two 1\\'s in the RE somewhere and what comes before, \\nbetween, and after is completely arbitrary. Hence we can write the RE as \\n(0+1)*1(0+1)*1(0+1)* . But following two REs also represent the same language, each \\nensuring presence of least two 1\\'s somewhere in the string \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\nwww.indiansbrain.com\\nL(r) = {\\n has no pair of consecutive 1\\'s} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We \\nobserver that, whenever a 1 occurs, it must be immediately followed by a 0. This \\nsubstring may be preceded & followed by any no of 0\\'s. So the final RE must be a \\nrepetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the RE is \\n(0*100*)*. But in this case the strings ending in 1 or consisting of all 0\\'s are not \\naccounted for. Taking these observations into consideration, the final RE is r = \\n(0*100*)(1+ )+0*(1+\\n). \\n \\nAlternative Solution : \\n \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as \\nr = (0+10)*(1+ ).This is a shorter expression but represents the same language. \\n \\nRegular Expression and Regular Language : \\n \\nEquivalence(of REs) with FA : \\n \\nRecall that, language that is accepted by some FAs are known as Regular language. \\nThe two concepts : REs and Regular language are essentially same i.e. (for) every \\nregular language can be developed by (there is) a RE, and for every RE there is a \\nRegular Langauge. This fact is rather suprising, because RE approach to describing \\nlanguage is fundamentally differnet from the FA approach. But REs and FA are \\nequivalent in their descriptive power. We can put this fact in the focus of the following \\nTheorem. \\n \\nTheorem : A language is regular iff some RE describes it. \\n \\nThis Theorem has two directions, and are stated & proved below as a separate lemma \\n \\n \\nRE to FA : \\n \\nREs denote regular languages : \\n \\nLemma : If L(r) is a language described by the RE r, then it is regular i.e. there is a FA \\nsuch that L(M)\\nL(r). \\n \\nProof : To prove the lemma, we apply structured index on the expression r. First, we \\n \\nshow how to construct FA for the basis elements: \\n, and for any \\n. Then we show \\nhow to combine these Finite Automata into Complex Automata that accept the Union, \\nConcatenation, Kleen Closure of the languages accepted by the original smaller \\nautomata. \\nwww.indiansbrain.com\\nUse of NFAs is helpful in the case i.e. we construct NFAs for every REs which are \\nrepresented by transition diagram only. \\n \\nBasis : \\n \\n\\uf095 Case (i) : \\n. Then \\n. Then \\nand the following NFA N \\nrecognizes L(r). Formally \\nwhere Q = {q} \\nand \\n.\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf095 Case (ii) : \\n. \\n, and the following NFA N accepts L(r). Formally\\uf020\\nwhere \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince the start state is also the accept step, and there is no any transition defined, it \\nwill accept the only string \\nand nothing else. \\n \\n\\uf095 Case (iii) : r = a for some \\n. Then L(r) = {a}, and the following NFA \\nN accepts L(r).\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFormally, \\nwhere \\nfor \\nor \\n \\n \\n \\n \\n \\nInduction : \\nwww.indiansbrain.com\\nAssume that the start of the theorem is true for REs \\nand \\n. Hence we can assume \\nthat we have automata \\nand \\nthat accepts languages denoted by REs \\nand \\n, \\n \\nrespectively i.e. \\nand \\n. The FAs are represented \\nschematically as shown below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEach has an initial state and a final state. There are four cases to consider. \\n \\n\\uf095 Case (i) : Consider the RE \\ndenoting the language \\n. We \\nconstruct FA \\n, from \\nand \\nto accept the language denoted by RE \\nas \\nfollows :\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new (initial) start state \\nand give \\n- transition to the initial state of \\n and \\n.This is the initial state of \\n. \\n \\n\\uf095 Create a final state \\nand give \\n-transition from the two final state of \\nand \\n. \\nis the only final state of \\nand final state of \\nand \\nwill be \\nordinary states in \\n.\\uf020\\n\\uf095 All the state of \\nand \\nare also state of \\n.\\uf020\\nwww.indiansbrain.com\\n\\uf095 All the moves of \\nand \\nare also moves of \\n. [ Formal Construction] \\n \\n \\nIt is easy to prove that \\n \\nProof: To show that \\nwe must show that \\n \\n= \\n \\n \\n= \\nby following transition of \\n \\n \\nStarts at initial state \\nand enters the start state of either \\nor \\nfollwoing the \\ntransition i.e. without consuming any input. WLOG, assume that, it enters the start state \\nof \\n. From this point onward it has to follow only the transition of \\nto enter the final \\n \\nstate of \\n, because this is the only way to enter the final state of M by following the e-\\ntransition.(Which is the last transition & no input is taken at hte transition). Hence the \\n \\nwhole input w is considered while traversing from the start state of \\nto the final \\nstate of \\n. Therefore \\nmust accept \\n. \\n \\nSay, \\nor \\n. \\n \\n \\nWLOG, say \\n \\nTherefore when \\nprocess the string w , it starts at the initial state and enters the final \\nstate when w consumed totally, by following its transition. Then \\nalso accepts w, by \\nstarting at state \\nand taking \\n-transition enters the start state of \\n-follows the moves \\n \\nof \\nto enter the final state of \\nconsuming input w thus takes -transition to \\n. Hence proved \\n \\n\\uf095 Case(ii) : Consider the RE \\ndenoting the language \\n. We construct \\nFA \\nfrom \\n& \\nto accept \\nas follows :\\uf020\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new start state \\nand a new final state \\n \\n1. Add \\n- transition from \\no \\nto the start state of \\n \\n \\no \\nto \\n \\n \\no final state of \\nto the start state of \\n \\n \\n2. All the states of \\nare also the states of \\n. \\nhas 2 more states than that of \\nnamely \\nand \\n. \\n3. All the moves of \\nare also included in \\n. \\n \\nBy the transition of type (b), \\ncan accept . \\nBy the transition of type (a), \\ncan enters the initial state of \\nw/o any input and then \\nfollow all kinds moves of \\nto enter the final state of \\nand then following \\n-transition \\ncan enter \\n. Hence if any \\nis accepted by \\nthen w is also accepted by \\n. By the \\ntransition of type (b), strings accepted by \\ncan be repeated by any no of times & thus \\naccepted by \\n. Hence \\naccepts \\nand any string accepted by \\nrepeated (i.e. \\n \\nconcatenated) any no of times. Hence \\n \\nCase(iv) : Let \\n=(\\n). Then the FA \\nis also the FA for (\\n), since the use of \\nparentheses does not change the language denoted by the expression \\n \\nNon-Deterministic Finite Automata \\nNondeterminism is an important abstraction in computer science. Importance of \\nnondeterminism is found in the design of algorithms. For examples, there are many \\nproblems with efficient nondeterministic solutions but no known efficient deterministic \\nsolutions. ( Travelling salesman, Hamiltonean cycle, clique, etc). Behaviour of a process \\nis in a distributed system is also a good example of nondeterministic situation. Because \\nwww.indiansbrain.com\\nthe behaviour of a process might depend on some messages from other processes \\nthat might arrive at arbitrary times with arbitrary contents. \\nIt is easy to construct and comprehend an NFA than DFA for a given regular \\nlanguage. The concept of NFA can also be used in proving many theorems and \\nresults. Hence, it plays an important role in this subject. \\nIn the context of FA nondeterminism can be incorporated naturally. That is, an NFA is \\ndefined in the same way as the DFA but with the following two exceptions: \\n\\uf095 multiple next state.\\uf020\\n\\uf020\\n\\uf095 \\n- transitions.\\uf020\\n \\nMultiple Next State : \\n \\n\\uf095 In contrast to a DFA, the next state is not necessarily uniquely determined by the \\ncurrent state and input symbol in case of an NFA. (Recall that, in a DFA there is \\nexactly one start state and exactly one transition out of every state for each \\nsymbol in \\n).\\uf020\\n\\uf095 \\nThis means that - in a state q and with input symbol a - there could be one, more \\nthan one or zero next state to go, i.e. the value of \\nis a subset of Q. Thus\\uf020\\n \\n= \\nwhich means that any one of \\ncould be the next \\nstate. \\n \\n\\uf095 The zero next state case is a special one giving \\n=\\n, which means that \\nthere is no next state on input symbol when the automata is in state q. In such \\na case, we may think that the automata \"hangs\" and the input will be rejected.\\uf020\\n \\n- transitions : \\n \\nIn an -transition, the tape head doesn\\'t do anything- it doesnot read and it doesnot move. \\nHowever, the state of the automata can be changed - that is can go to zero, one \\n \\nor more states. This is written formally as \\nimplying that the next \\nstate could by any one of \\nw/o consuming the next input symbol. \\n \\n \\n \\n \\nAcceptance : \\n \\nInformally, an NFA is said to accept its input \\nif it is possible to start in some start state \\nand process \\n, moving according to the transition rules and making choices along the way \\nwhenever the next state is not uniquely defined, such that when \\nis completely processed \\n(i.e. end of \\nis reached), the automata is in an accept state. There may be several \\npossible paths through the automation in response to an input \\nsince the start state is not \\ndetermined and there are choices along the way because of multiple next states. Some of \\nthese paths may lead to accpet states while others may not. The \\nwww.indiansbrain.com\\nautomation is said to accept \\nif at least one computation path on input \\nstarting from at \\nleast one start state leads to an accept state- otherwise, the automation rejects input \\n. \\nAlternatively, we can say that, \\nis accepted iff there exists a path with label \\nfrom some \\nstart state to some accept state. Since there is no mechanism for determining which state \\nto start in or which of the possible next moves to take (including the \\n- transitions) in \\nresponse to an input symbol we can think that the automation is having some \"guessing\" \\npower to chose the correct one in case the input is accepted \\n \\nExample 1 : Consider the language L = {\\n {0, 1}* | The 3rd symbol from the right \\nis 1}. The following four-state automation accepts L. \\n \\nThe m/c is not deterministic since there are two transitions from state \\non input 1 \\nand no transition (zero transition) from \\non both 0 & 1. \\n \\nFor any string \\nwhose 3rd symbol from the right is a 1, there exists a sequence of legal \\ntransitions leading from the start state q, to the accept state \\n. But for any string \\nwhere 3rd symbol from the right is 0, there is no possible sequence of legal \\n \\ntranisitons leading from \\nand \\n. Hence m/c accepts L. How does it accept any string \\nL? \\n \\nFormal definition of NFA : \\n \\nFormally, an NFA is a quituple \\nwhere Q, \\n, \\n, and F bear \\nthe same meaning as for a DFA, but \\n, the transition function is redefined as follows: \\n \\n \\n \\n \\nwhere P(Q) is the power set of Q i.e. \\n. \\n \\nThe Langauge of an NFA : \\n \\nFrom the discussion of the acceptance by an NFA, we can give the formal definition of a \\nlanguage accepted by an NFA as follows : \\n \\nIf \\nis an NFA, then the langauge accepted by N is writtten as L(N) \\nis given by \\n. \\n \\nThat is, L(N) is the set of all strings w in \\nsuch that \\ncontains at least \\none accepting state. \\nwww.indiansbrain.com\\nRemoving ϵ-transition: \\n \\n- transitions do not increase the power of an NFA . That is, any \\n- NFA ( NFA with \\ntransition), we can always construct an equivalent NFA without \\n-transitions. The \\n \\nequivalent NFA must keep track where the \\nNFA goes at every step during \\ncomputation. This can be done by adding extra transitions for removal of every \\n- \\ntransitions from the - NFA as follows. \\n \\nIf we removed the \\n- transition \\nfrom the - NFA , then we need to moves \\n \\nfrom state p to all the state on input symbol \\nwhich are reachable from state q \\n(in the - NFA ) on same input symbol q. This will allow the modified NFA to move \\nfrom state p to all states on some input symbols which were possible in case of \\n-NFA \\non the same input symbol. This process is stated formally in the following theories. \\n \\nTheorem if L is accepted by an - NFA N , then there is some \\nequivalent \\nwithout transitions accepting the same language L \\nProof: \\n \\nLet \\nbe the given \\nwith \\n \\n \\nWe construct \\n \\nWhere, \\nfor all \\nand \\nand \\n \\n \\n \\n \\n \\nOther elements of N\\' and N \\n \\nWe can show that \\ni.e. N\\' and N are equivalent. \\n \\nWe need to prove that \\n \\n \\n i.e. \\n \\n \\n \\n \\n \\nWe will show something more, that is, \\nwww.indiansbrain.com\\nWe will show something more, that is, \\n \\n \\nBasis : \\n, then \\n \\n \\nBut \\nby definition of \\n. \\n \\nInduction hypothesis Let the statement hold for all \\nwith \\n. \\n \\n \\nBy definition of extension of \\n \\n \\nBy inductions hypothesis. \\n \\nAssuming that \\n \\n \\n \\n \\nBy definition of \\n \\n \\nSince \\n \\n \\n \\nTo complete the proof we consider the case \\n \\nWhen \\ni.e. \\nthen \\nwww.indiansbrain.com\\n \\nand by the construction of \\nwherever \\nconstrains a state in F. \\n \\nIf \\n(and thus \\nis not in F ), then \\nwith \\nleads to an accepting state in N\\' iff it \\nlead to an accepting state in N ( by the construction of N\\' and N ). \\n \\nAlso, if (\\n , thus w is accepted by N\\' iff w is accepted by N (iff \\n) \\n \\nIf \\n(and, thus in M we load \\nin F ), thus is accepted by both N\\' and N . \\n \\nLet \\n. If w cannot lead to \\nin N , then \\n. (Since can add transitions to get an accept \\nstate). So there is no harm in making \\nan accept state in N\\'. \\n \\nEx: Consider the following NFA with \\n- transition. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition Diagram \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition diagram for \\n\\' for the equivalent NFA without - moves \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n          1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince \\nthe start state q0 must be final state in the equivalent NFA . \\n \\nSince \\nand \\nand \\nwe add moves \\nand \\nin the equivalent NFA . Other moves are also constructed accordingly. \\n \\n-closures: \\n \\nThe concept used in the above construction can be made more formal by defining the \\n-closure for a state (or a set of states). The idea of \\n-closure is that, when moving \\n \\nfrom a state p to a state q (or from a set of states Si to a set of states Sj ) an input \\n, we need to take account of all \\n-moves that could be made after the transition. \\nFormally, for a given state q, \\n \\n \\n-closures: \\n \\nSimilarly, for a given set \\n \\n \\n-closures: \\n \\n \\n \\nSo, in the construction of equivalent NFA N\\' without -transition from any NFA with \\n \\nmoves. the first rule can now be written as \\nwww.indiansbrain.com\\nEquivalence of NFA and DFA \\n \\nIt is worth noting that a DFA is a special type of NFA and hence the class of languages \\naccepted by DFA s is a subset of the class of languages accepted by NFA s. \\nSurprisingly, these two classes are in fact equal. NFA s appeared to have more power \\nthan DFA s because of generality enjoyed in terms of \\n-transition and multiple next \\nstates. But they are no more powerful than DFA s in terms of the languages they \\naccept. \\n \\nConverting DFA to NFA \\n \\n \\n \\n \\nTheorem: Every DFA has as equivalent NFA \\n \\nProof: A DFA is just a special type of an NFA . In a DFA , the transition functions is \\ndefined from \\nwhereas in case of an NFA it is defined from \\nand \\nbe a DFA . We construct an equivalent NFA \\nas follows. \\n \\n \\n \\n \\ni. e \\n \\nIf \\nand \\n \\nAll other elements of N are as in D. \\n \\nIf \\nthen there is a sequence of states \\nsuch that \\n \\n \\n \\nThen it is clear from the above construction of N that there is a sequence of states (in N) \\nsuch that \\nand \\nand hence \\n \\n \\nSimilarly we can show the converse. \\n \\nHence , \\n \\n \\nGiven any NFA we need to construct as equivalent DFA i.e. the DFA need to simulate \\nthe behaviour of the NFA . For this, the DFA have to keep track of all the states where \\nthe NFA could be in at every step during processing a given input string. \\nwww.indiansbrain.com\\nThere are \\npossible subsets of states for any NFA with n states. Every subset \\ncorresponds to one of the possibilities that the equivalent DFA must keep track of. Thus, \\n \\nthe equivalent DFA will have \\nstates. \\n \\nThe formal constructions of an equivalent DFA for any NFA is given below. We \\nfirst consider an NFA without \\ntransitions and then we incorporate the affects of \\ntransitions later. \\n \\nFormal construction of an equivalent DFA for a given NFA without transitions. \\n \\nGiven an \\nwithout - moves, we construct an equivalent DFA \\n \\n \\nas follows \\n \\ni.e. \\n \\n \\n \\n \\n \\n(i.e. every subset of Q which as an element in F is considered as a final stat\\nin DFA D ) \\n \\n \\n \\n \\nfor all \\nand \\n \\n \\nwhere \\n \\n \\nThat is, \\n \\n \\nTo show that this construction works we need to show that L(D)=L(N) i.e. \\n \\n \\n \\n \\n \\n \\nOr,\\n \\n \\nWe will prove the following which is a stranger statement thus required. \\nwww.indiansbrain.com\\n \\n \\nProof : We will show by inductions on \\n \\n \\nBasis If \\n=0, then w =  \\n \\nSo, \\nby definition. \\n \\nInductions hypothesis : Assume inductively that the statement holds \\nof \\nlength less than or equal to n. \\n \\nInductive step \\n \\nLet \\n, then \\nwith \\n \\n \\nNow, \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNow, given any NFA with -transition, we can first construct an equivalent NFA without \\n-transition and then use the above construction process to construct an equivalent \\nDFA , thus, proving the equivalence of NFA s and DFA s.. \\n \\nIt is also possible to construct an equivalent DFA directly from any given NFA with \\n- transition by integrating the concept of \\n-closure in the above construction. \\n \\nRecall that, for any \\n \\n \\n- closure : \\nwww.indiansbrain.com\\nIn the equivalent DFA , at every step, we need to modify the transition functions \\nto \\nkeep track of all the states where the NFA can go on \\n-transitions. This is done by \\nreplacing \\nby \\n-closure \\n, i.e. we now compute \\nat every step as \\nfollows: \\n \\n \\n \\nBesides this the initial state of the DFA D has to be modified to keep track of all the \\nstates that can be reached from the initial state of NFA on zero or more -transitions. \\nThis can be done by changing the initial state \\nto -closure (\\n ) . \\n \\nIt is clear that, at every step in the processing of an input string by the DFA D , it enters \\na state that corresponds to the subset of states that the NFA N could be in at that \\nparticular point. This has been proved in the constructions of an equivalent NFA for any \\n-NFA \\nIf the number of states in the NFA is n , then there are \\nstates in the DFA . That is, \\neach state in the DFA is a subset of state of the NFA . \\n \\nBut, it is important to note that most of these \\nstates are inaccessible from the start state \\nand hence can be removed from the DFA without changing the accepted language. Thus, \\nin fact, the number of states in the equivalent DFA would be much less \\n \\nthan \\n. \\nExample : Consider the NFA given below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n{\\n} \\n \\n \\n \\n \\nSince there are 3 states in the NFA \\nwww.indiansbrain.com\\nThere will be \\nstates (representing all possible subset of states) in the \\nequivalent DFA . The transition table of the DFA constructed by using the subset \\nconstructions process is produced here. \\n \\n0 \\n \\n1 The start state of the DFA is   - closures \\n \\n \\n \\n \\n The final states are all those subsets that contains \\n(since \\nin the NFA). \\n \\n{   } \\nLet us compute one entry, \\n \\n \\n \\n \\n \\n \\n \\n \\n Similarly, all other transitions can be computed \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCorresponding Transition fig. for DFA.Note that states \\n \\nare not accessible and hence can be removed. \\nThis gives us the following simplified DFA with only 3 states. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is interesting to note that we can avoid encountering all those inaccessible \\nor unnecessary states in the equivalent DFA by performing the following two \\nsteps inductively. \\n \\n1. If \\nis the start state of the NFA, then make \\n- closure ( \\n) the start state of the \\nequivalent DFA . This is definitely the only accessible state. \\n \\n2. If we have already computed a set \\nof states which are accessible. Then \\n. \\ncompute \\nbecause these set of states will also be accessible. \\n \\nFollowing these steps in the above example, we get the transition table given below \\nwww.indiansbrain.com\\nUNIT-II \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying certain recursive rules \\nas given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) \\nis a RE \\n \\nii) \\nis a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\nIf \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\n \\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive REs) by a finite no of \\napplications of the recursive step (given in 2). \\n \\nExample : Let \\n= { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this expression by \\napplying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n \\n \\nwww.indiansbrain.com\\n4 \\n(1+   ) \\nRule 2(iv) & Step 3 \\n \\n \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated with every RE). \\nWe will see later that REs are used to attribute regular languages. \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We can define the \\nlanguage L(r) associated with (or described by) a REs as follows. \\n \\n1. \\nis the RE describing the empty language i.e. L(\\n) = \\n. \\n \\n2. \\nis a RE describing the language {\\n} i.e. L(\\n) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\n \\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,........} {0} \\n{1} \\n \\n= {\\n , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nwww.indiansbrain.com\\nConsider the RE ab + c. The language described by the RE can be thought of either L(a)L(b+c) or \\n \\nL(ab)\\nL(c) as provided by the rules (of languages described by REs) given already. But these two \\nrepresents two different languages lending to ambiguity. To remove this ambiguity we can either \\n \\n \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. Like other algebras mod in \\nmathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) operator. \\n \\nii) It is also important to note that concatenation & union (+) operators are associative and union operation \\nis commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language L(ab) \\nL(c) i.e. it should be \\ngrouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For example, the \\nlanguage represented by the RE a(b+c) is L(a)L(b+c). \\n \\n \\n \\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language L(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all strings over {0,1} which \\nare either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an even number of 0\\'s \\n \\nfollowed by an odd number of 1\\'s i.e. \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE rr, \\ndenotes \\nr, \\nand so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of consecutive 1\\'s} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and what goes before is \\ncompletely arbitrary. Considering these observations we can write the REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE (0+1)*11(0+1)*+(0+1)*00(0+1)* \\nrepresents the set of string over {0,1} that contains the substring 11 or 00. \\nwww.indiansbrain.com\\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the set of strings over {0,1} \\nthat contains exactly two 1\\'s. The presence of two 1\\'s in the RE and any no of 0\\'s before, between and after the \\n1\\'s ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1\\'s. \\n \\nSolution : There must be at least two 1\\'s in the RE somewhere and what comes before, between, and after is \\ncompletely arbitrary. Hence we can write the RE as (0+1)*1(0+1)*1(0+1)*. But following two REs also \\nrepresent the same language, each ensuring presence of least two 1\\'s somewhere in the string \\n \\n \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\n \\nL(r) = {\\n has no pair of consecutive 1\\'s} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We observer that, whenever \\na 1 occurs, it must be immediately followed by a 0. This substring may be preceded & followed by any no of \\n0\\'s. So the final RE must be a repetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the \\nRE is (0*100*)*. But in this case the strings ending in 1 or consisting of all 0\\'s are not accounted for. Taking \\nthese observations into consideration, the final RE is  r = (0*100*)(1+ \\n)+0*(1+\\n). \\n \\nAlternative Solution : \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as r = (0+10)*(1+\\n).This \\nis a shorter expression but represents the same language. \\n \\nRegular Expression: \\n \\nFA to regular expressions: \\n \\nFA to RE (REs for Regular Languages) : \\n \\nLemma : If a language is regular, then there is a RE to describe it. i.e. if L = L(M) for some DFA M, then there is a \\nRE r such that L = L(r). \\n \\nProof : We need to construct a RE r such that \\n. Since M is a DFA, it has a finite no \\nof states. Let the set of states of M is Q = {1, 2, 3,..., n} for some integer n. [ Note : if the n states of M were \\ndenoted by some other symbols, we can always rename those to indicate as 1, 2, 3,..., n ]. The required RE is \\nconstructed inductively. \\n \\nNotations : \\nis a RE denoting the language which is the set of all strings w such that w is the label of a \\npath from state i to state j \\nin M, and that path has no intermediate state whose number is \\ngreater then k. ( i & j (begining and end pts) are not considered to be \"intermediate\" so i and /or j can be \\nwww.indiansbrain.com\\ngreater than k ) \\n \\nWe now construct \\ninductively, for all i, j \\nQ starting at k = 0 and finally reaching k = n. \\n \\nBasis : k = 0, \\ni.e. the paths must not have any intermediate state ( since all states are numbered 1 or \\nabove). There are only two possible paths meeting the above condition : \\n \\n1. A direct transition from state i to state j. \\no \\n= a if then is a transition from state i to state j on symbol the single symbol a. \\n \\no \\n= \\nif there are multiple transitions from state i to state j on symbols \\n \\n. \\no \\n= f if there is no transition at all from state i to state j. \\n \\n2. All paths consisting of only one node i.e. when i = j. This gives the path of length 0 (i.e. the RE \\ndenoting the string \\n) and all self loops. By simply adding Î to various cases above we get \\nthe corresponding REs i.e. \\no \\n= \\n+ a if there is a self loop on symbol a in state i . \\n \\no \\n= \\n+ \\nif there are self loops in state i as multiple symbols \\n \\n. \\n \\no \\n= \\nif there is no self loop on state i. \\n \\nInduction : \\n \\nAssume that there exists a path from state i to state j such that there is no intermediate state whose number is \\n \\ngreater than k. The corresponding Re for the label of the path is \\n. There are only two possible cases : \\n \\n1. The path dose not go through the state k at all i.e. number of all the intermediate states are less \\nthan k. So, the label of the path from state i to state j is tha language described by the RE \\n. \\n \\n2. The path goes through the state k at least once. The path may go from i to j and k may appear more \\nthan once. We can break the into pieces as shown in the figure 7. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7 \\n \\n1. The first part from the state i to the state k which is the first recurence. In this path, all \\nintermediate states are less than k and it starts at iand ends at k. So the RE \\ndenotes the \\nlanguage of the label of path. \\n \\n2. The last part from the last occurence of the state k in the path to state j. In this path also, no \\nintermediate state is numbered greater than k. Hence the RE \\ndenoting the language of the \\nlabel of the path. \\n \\n3. In the middle, for the first occurence of k to the last occurence of k , represents a loop which may be \\ntaken zero times, once or any no of times. And all states between two consecutive k\\'s are \\nnumbered less than k. \\n \\nHence the label of the path of the part is denoted by the RE \\n.The label of the path from state i to state \\nj is the concatenation of these 3 parts which is \\n \\n \\n \\nSince either case 1 or case 2 may happen the labels of all paths from state i to j is denoted by the following RE \\n \\n \\n \\n \\nWe can construct \\nfor all i, j \\n{1,2,..., n} in increasing order of k starting with the basis k = 0 upto k = n since \\ndepends only on expressions with a small superscript (and hence will be available). WLOG, assume \\n \\nthat state 1 is the start state and \\nare the m final states where ji \\n{1, 2, ... , n }, \\nand \\n \\n. According to the convention used, the language of the automatacan be denoted by the RE \\nwww.indiansbrain.com\\n \\n \\nSince \\nis the set of all strings that starts at start state 1 and finishes at final state \\nfollowing the transition of \\nthe FA with any value of the intermediate state (1, 2, ... , n) and hence accepted by the automata. \\n \\nRegular Grammar: \\n \\nA grammar \\nis right-linear if each production has one of the following three forms: \\n \\n\\uf095 \\nA\\ncB ,\\uf020\\n\\uf020\\n\\uf095 \\nA\\nc,\\uf020\\n\\uf095 \\nA\\n\\uf020\\n \\nWhere A, B \\n( with A = B allowed) and \\n. A grammar G is left-linear if each production has once of \\nthe following three forms. \\n \\nA\\nBc , A\\nc, A\\n \\n \\nA right or left-linear grammar is called a regular grammar. \\n \\nRegular grammar and Finite Automata are equivalent as stated in the following theorem. \\n \\nTheorem : A language L is regular iff it has a regular grammar. We use the following two lemmas to prove the \\nabove theorem. \\n \\nLemma 1 : If L is a regular language, then L is generated by some right-linear grammar. \\n \\nProof : Let \\nbe a DFA that accepts L. \\n \\nLet \\nand \\n. \\n \\nWe construct the right-linear grammar \\nby letting \\n \\nN = Q , \\nand \\n \\n[ Note: If \\n, then \\n] \\n \\nLet \\n. For M to accept w, there must be a sequence of states \\nsuch that \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\nBy construction, the grammar G will have one production for each of the above transitions. Therefore, we have \\nthe corresponding derivation. \\n \\n \\n \\n \\nHence w \\nL(g). \\n \\nConversely, if \\n, then the derivation of w in G must have the form as given above. \\nBut, then the construction of G from M implies that \\n \\n, where \\n, completing the proof. \\n \\nLemma 2 : Let \\nbe a right-linear grammar. Then L(G) is a regular \\nlanguage. Proof: To prove it, we construct a FA M from G to accept the same language. \\n \\nis constructed as follows: \\n \\n( \\nis a special sumbol not in N ) \\n \\n, \\n \\nFor any \\nand \\nand \\nis defined as \\n \\n \\nif \\n \\nand \\n, if \\n. \\nWe now show that this construction works. \\n \\nLet \\n. Then there is a derivation of w in G of the form \\nwww.indiansbrain.com\\n \\n \\nBy contradiction of M, there must be a sequence of transitions \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimplying that \\ni.e. w is accepted by M. \\n \\nConversely, if \\nis accepted by M, then because \\nis the only accepting state of M, the transitions \\ncausing w to be accepted by M will be of the form given above. These transitions corresponds to a \\n \\nderivationof w in the grammar G. Hence \\n, completing the proof of the lemma. \\n \\nGiven any left-linear grammar G with production of the form \\n, we can construct from it a \\nright-linear grammar \\nby replacing every production of G of the form \\nwith \\n \\n \\nIt is easy to prove that \\n. Since \\nis right-linear, \\nis regular. But then so are \\ni.e. \\nbecause regular languages are closed under reversal. \\n \\nPutting the two lemmas and the discussions in the above paragraph together we get the proof of the theorem- \\n \\nA language L is regular iff it has a regular grammar \\n \\nExample : Consider the grammar \\n \\n \\n \\nIt is easy to see that G generates the language denoted by the regular expression \\n(01)*0. The construction of lemma 2 for this grammar produces the follwoing FA. \\n \\nThis FA accepts exactly (01)*1. \\n \\nDecisions Algorithms for CFL \\n \\nIn this section, we examine some questions about CFLs we can answer. A CFL may be represented using a \\nCFG or PDA. But an algorithm that uses one representation can be made to work for the others, since we can \\nconstruct one from the other. \\nwww.indiansbrain.com\\nTesting Emptiness : \\n \\nTheorem : There are algorithms to test emptiness of a CFL. \\n \\nProof : Given any CFL L, there is a CFG G to generate it. We can determine, using the construction described \\n \\nin the context of elimination of useless symbols, whether the start symbol is useless. If so, then \\n; \\notherwise not. \\n \\nTesting Membership : \\n \\nGiven a CFL L and a string x, the membership, problem is to determine whether \\n? \\n \\nGiven a PDA P for L, simulating the PDA on input string x doesnot quite work, because the PDA can grow \\nits stack indefinitely on \\ninput, and the process may never terminate, even if the PDA is deterministic. \\n \\nSo, we assume that a CFG \\nis given such that L = L(G). \\n \\nLet us first present a simple but inefficient algorithm. \\n \\nConvert G to \\nin CNF generating \\n. If the input string \\n, then we need to \\n \\ndetermine whether \\nand it can easily be done using the technique given in the context of elimination of \\n \\n-production. If , \\nthen \\niff \\n. Consider a derivation under a grammar in CNF. At \\nevery step, a production in CNF in used, and hence it adds exactly one terminal symbol to the sentential form. \\n \\nHence, if the length of the input string x is n, then it takes exactly n steps to derive x ( provided x is in \\n). \\n \\nLet the maximum number of productions for any nonterminal in \\nis K. So at every step in derivation, there are \\natmost k choices. We may try out all these choices, systematically., to derive the string x in \\n. Since \\n \\nthere are atmost \\ni.e. \\nchoices. This algorithms is of exponential time complexity. We now present an \\nefficient (polynomial time) membership algorithm. \\n \\nPumping Lemma: \\n \\nLimitations of Finite Automata and Non regular Languages : \\n \\nThe class of languages recognized by FA s is strictly the regular set. There are certain languages which are \\nnon regular i.e. cannot be recognized by any FA \\n \\nConsider the language \\n \\n \\nIn order to accept is language, we find that, an automaton seems to need to remember when passing the \\ncenter point between a\\'s and b\\'s how many a\\'s it has seen so far. Because it would have to compare that \\nwith the number of b\\'s to either accept (when the two numbers are same) or reject (when they are not same) \\nthe input string. \\nwww.indiansbrain.com\\nBut the number of a\\'s is not limited and may be much larger than the number of states since the string may \\nbe arbitrarily long. So, the amount of information the automaton need to remember is unbounded. \\n \\nA finite automaton cannot remember this with only finite memory (i.e. finite number of states). The fact that \\nFA s have finite memory imposes some limitations on the structure of the languages recognized. Inductively, we \\ncan say that a language is regular only if in processing any string in this language, the information that has to \\nbe remembered at any point is strictly limited. The argument given above to show that \\nis non regular is \\ninformal. We now present a formal method for showing that certain languages such as \\nare non regular \\n \\nProperties of CFL’s \\n \\nClosure properties of CFL: \\n \\nWe consider some important closure properties of CFLs. \\n \\nTheorem : If \\nand \\nare CFLs then so is \\n \\n \\nProof : Let \\nand \\nbe CFGs generating. Without loss of generality, we \\ncan assume that \\n. Let \\nis a nonterminal not in \\nor \\n. We construct the grammar \\n \\nfrom \\nand \\n, where \\n \\n, \\n \\n \\n \\n \\n \\n \\n \\n \\nWe now show that \\n \\n \\nThus proving the theorem. \\n \\nLet \\n. Then \\n. All productions applied in their derivation are also in \\n. Hence \\ni.e. \\n \\n \\n \\nSimilarly, if \\n, then \\n \\n \\nThus \\n. \\nwww.indiansbrain.com\\nConversely, let \\n. Then \\nand the first step in this derivation must be either \\nor \\n. Considering the former case, we have \\n \\n \\nSince \\nand \\nare disjoint, the derivation \\nmust use the productions of \\nonly ( which are also in \\n \\n) Since \\nis the start symbol of \\n. Hence, \\ngiving \\n. \\n \\nUsing similar reasoning, in the latter case, we get \\n. Thus \\n. \\n \\nSo, \\n, as claimed \\n \\n \\nTheorem : If \\nand \\nare CFLs, then so is \\n. \\n \\nProof : Let \\nand \\nbe the CFGs generating \\nand \\nrespectively. \\nAgain, we assume that \\nand \\nare disjoint, and \\nis a nonterminal not in \\nor \\n. we construct the CFG \\nfrom \\nand \\n, where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe claim that \\n \\n \\n \\nTo prove it, we first assume that \\nand \\n. Then \\nand \\n. We can derive the string xy \\nin \\nas shown below. \\n \\n \\n \\n \\n \\nsince \\nand \\n. Hence \\n. \\nwww.indiansbrain.com\\nFor the converse, let \\n. Then the derivation of w in \\nwill be of the form \\n \\ni.e. the first step in the derivation must see the rule \\n. Again, since \\nand \\nare \\ndisjoint and \\nand \\n, some string x will be generated from \\nusing productions in \\n( which \\nare also in \\n) and such that \\n. \\n \\nThus \\n \\nHence \\nand \\n. \\n \\nThis means that w can be divided into two parts x, y such that \\nand \\n. Thus \\n.This \\n \\ncompletes the proof \\nTheorem : If L is a CFL, then so is \\n. \\nProof : Let \\nbe the CFG generating L. Let us construct the CFG \\n \\n \\nwhere \\n. \\n \\nWe now prove that \\n, which prove the theorem. \\n \\ncan generate \\nin one step by using the production \\nsince \\n, \\n Let \\nfor any n >1 we can \\nwrite \\nwhere \\nfor \\n \\n \\n \\n \\nusing following steps. \\n \\n \\n \\n \\n \\nFirst (n-1)-steps uses the production S\\nSS producing the sentential form of n numbers of S \\'s. The \\nnonterminal S in the i-th position then generates \\nusing production in P ( which are also in \\n) \\n \\nIt is also easy to see that G can generate the empty string, any string in L and any string \\nfor n >1 \\nand none other. \\n \\nHence \\n \\n \\nTheorem : CFLs are not closed under intersection \\n \\nProof : We prove it by giving a counter example. Consider the language \\n.The following \\nCFG generates L1 and hence a CFL \\n \\ncan generate any string in L. \\n. w can be generated by \\n \\nfrom G \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\nThe nonterminal X generates strings of the form \\nand C generates strings of the form \\n, \\n. These are the only types of strings generated by X and C. Hence, S generates \\n. \\n \\nUsing similar reasoning, it can be shown that the following grammar \\nand hence it is \\nalso a CFL. \\n \\n \\n \\n \\n \\n \\n \\nBut, \\nand is already shown to be not context-free. \\n \\nHence proof. \\n \\nTheorem : A CFL\\'s are not closed under complementations \\n \\nProof : Assume, for contradiction, that CFL\\'s are closed under complementation. SInce, CFL\\'s are also closed \\nunder union, the language \\n, where \\nand \\nare CFL\\'s must be CFL. But by DeMorgan\\'s law \\n \\n \\n \\n \\nThis contradicts the already proved fact that CFL\\'s are not closed under intersection. \\nBut it can be shown that the CFL\\'s are closed under intersection with a regular set. \\n \\nTheorem : If L is a CFL and R is a regular language, then \\nis a CFL. \\n \\nProof : Let \\nbe a PDA for L and let \\nbe a DFA for \\nR. We construct a PDA M from P and D as follows \\n \\n \\nwhere \\nis defined as \\n \\ncontains \\niff \\nwww.indiansbrain.com\\nand \\ncontains \\n \\n \\nThe idea is that M simulates the moves of P and D parallely on input w, and accepts w iff both P and \\nD accepts. That means, we want to show that \\n \\n \\n \\n \\nWe apply induction on n, the number of moves, to show that \\n \\niff \\n \\nand \\n \\nBasic Case is n=0. Hence \\n, \\nand \\n. For this case it is trivially true \\n \\nInductive hypothesis : Assume that the statement is true for n -1. \\n \\nInductive Step : Let w = xa and \\n \\n \\n \\nLet \\n \\nBy inductive hypothesis, \\nand \\n \\n \\nFrom the definition of \\nand considering the n-th move of the PDA M above, we have \\n \\nand \\n \\n \\nHence \\nand \\n \\n \\nIf \\nand \\n, then \\nand we got that if M accepts w, then both P and D accepts it. \\n \\nWe can show that converse, in a similar way. Hence \\nis a CFL ( since it is accepted by a PDA M ) \\nThis property is useful in showing that certain languages are not context-free. \\n \\nExample : Consider the language \\n \\n \\nIntersecting L with the regular set \\n, we get \\nwww.indiansbrain.com\\n \\n \\n \\nWhich is already known to be not context-free. Hence L is not context-free \\nTheorem : CFL\\'s are closed under reversal. That is if L is a CFL, then so is \\n \\n \\nProof : Let the CFG \\ngenerates L. We construct a CFG \\nwhere \\n \\n. We now show that \\n, thus proving the theorem. \\nWe need to prove that \\niff \\n. \\n \\nThe proof is by induction on n, the number of steps taken by the derivation. We assume, for simplicity (and \\nof course without loss of generality), that G and hence \\nare in CNF. \\n \\nThe basis is n=1 in which case it is trivial. Because \\nmust be either \\nor BC with \\n. \\n \\nHence \\niff \\n \\n \\nAssume that it is true for (n-1)-steps. Let \\n. Then the first step must apply a rule of the \\nform \\nand it gives \\n \\nwhere \\nand \\n \\n \\nBy constructing of G\\', \\n \\nHence \\n \\n \\nThe converse case is exactly similar \\nSubstitution : \\n \\n, let \\nbe a language (over any alphabet). This defines a function S, called substitution, on \\nwhich is \\n \\ndenoted as \\n- for all \\n \\n \\nThis definition of substitution can be extended further to apply strings and langauge as well. \\nIf \\n, where \\n, is a string in \\n, then \\n \\n. \\nSimilarly, for any language L, \\n \\nThe following theorem shows that CFLs are closed under substitution. \\n \\nThereom : Let \\nis a CFL, and s is a substitution on \\nsuch that \\nis a CFL for all \\n, \\nthus s(L) is a CFL \\n \\nProof : Let L = L(G) for a CFG \\nand for every \\n, \\nfor some \\n. Without loss of generality, assume that the sets of nonterminals N and \\n\\'s \\nare disjoint. \\nwww.indiansbrain.com\\nNow, we construct a grammar \\n, generating s(L), from G and \\n\\'s as follows : \\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095 \\nconsists of\\uf020\\n\\uf020\\n1. \\nand \\n \\n2. The production of P but with each terminal a in the right hand side of a production replaced \\nby \\neverywhere. \\nWe now want to prove that this construction works i.e. \\niff \\n. \\n \\nIf Part : Let \\nthen according to the definition there is some string \\nand \\nfor \\nsuch that \\n \\nWe will show that \\n. \\n \\nFrom the construction of \\n, we find that, there is a derivation \\ncorresponding to the string \\n \\n(since \\ncontains all productions of G but every ai replaced with \\nin the RHS of any \\nproduction). \\n \\nEvery \\nis the start symbol of \\nand all productions of \\nare also included in \\n. \\nHence \\n \\n \\n \\n \\n \\n \\n \\n \\nTherefore, \\n \\n \\n(Only-if Part) Let \\n. Then there must be a derivative as follows : \\n \\n(using the production of G include in \\nas modified by (step 2) of the construction of \\n.) \\n \\nEach \\n(\\n) can only generate a string \\n, since each \\n\\'s and N are disjoin. \\nTherefore, we get \\n \\n \\nsince \\n \\nwww.indiansbrain.com\\nsince \\n \\n \\n \\n \\n \\nThe string \\nis formed by substituting strings \\nfor each \\nand hence \\n. \\n \\nTheorem : CFL\\'s are closed under homomorphism \\n \\nProof : Let \\nbe a CFL, and h is a homomorphism on \\ni.e \\nfor some alphabets \\n. consider the \\nfollowing substitution S:Replace each symbol \\nby the language consisting of the only string h(a), i.e. \\n \\nfor all \\n. Then, it is clear that, h(L) = s(L). Hence, CFL\\'s being closed under \\nsubstitution must also be closed under homomorphism. \\nwww.indiansbrain.com\\n \\n \\nUNIT- III \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nGrammar \\n \\nA grammar is a mechanism used for describing languages. This is one of the most simple but yet powerful \\nmechanism. There are other notions to do the same, of course. \\n \\nIn everyday language, like English, we have a set of symbols (alphabet), a set of words constructed from \\nthese symbols, and a set of rules using which we can group the words to construct meaningful sentences. The \\ngrammar for English tells us what are the words in it and the rules to construct sentences. It also tells us \\nwhether a particular sentence is well-formed (as per the grammar) or not. But even if one follows the rules of \\nthe english grammar it may lead to some sentences which are not meaningful at all, because of impreciseness \\nand ambiguities involved in the language. In english grammar we use many other higher level constructs like \\nnoun-phrase, verb-phrase, article, noun, predicate, verb etc. A typical rule can be defined as \\n \\n< sentence >\\n< noun-phrase > < predicate > \\n \\nmeaning that \"a sentence can be constructed using a \\'noun-phrase\\' followed by a predicate\". \\n \\nSome more rules are as follows: \\n \\n< noun-phrase >\\n< article >< noun > \\n \\n< predicate > \\n< verb > \\n \\nwith similar kind of interpretation given above. \\n \\nIf we take {a, an, the} to be <article>; cow, bird, boy, Ram, pen to be examples of <noun>; and eats, runs, \\nswims, walks, are associated with <verb>, then we can construct the sentence- a cow runs, the boy eats, an \\npen walks- using the above rules. Even though all sentences are well-formed, the last one is not meaningful. \\nWe observe that we start with the higher level construct <sentence> and then reduce it to <noun-phrase>, \\n<article>, <noun>, <verb> successively, eventually leading to a group of words associated with these \\nconstructs. \\n \\nThese concepts are generalized in formal language leading to formal grammars. The word \\'formal\\' here refers \\nto the fact that the specified rules for the language are explicitly stated in terms of what strings or symbols can \\noccur. There can be no ambiguity in it. \\n \\nFormal definitions of a Grammar \\nwww.indiansbrain.com\\nA grammar G is defined as a quadruple. \\n \\n \\n \\n \\nN is a non-empty finite set of non-terminals or variables, \\n \\nis a non-empty finite set of terminal symbols such that \\n \\n \\n, is a special non-terminal (or variable) called the start symbol, and \\nis a \\nfinite set of production rules. \\n \\nThe binary relation defined by the set of production rules is denoted by \\n, i.e. \\niff \\n. \\n \\nIn other words, P is a finite set of production rules of the form \\n, where \\nand \\n \\n \\n \\nProduction rules: \\n \\nThe production rules specify how the grammar transforms one string to another. Given a string \\n, we say that \\nthe production rule \\nis applicable to this string, since it is possible to use the rule \\nto rewrite the \\n(in \\n) to \\nobtaining a new string \\n. We say that \\nderives \\nand is denoted as \\n \\n \\n \\n \\nSuccessive strings are dervied by applying the productions rules of the grammar in any arbitrary order. \\nA particular rule can be used if it is applicable, and it can be applied as many times as described. \\n \\nWe write \\nif the string \\ncan be derived from the string \\nin zero or more steps; \\nif \\ncan be \\nderived from \\nin one or more steps. \\n \\nBy applying the production rules in arbitrary order, any given grammar can generate many strings of terminal \\nsymbols starting with the special start symbol, S, of the grammar. The set of all such terminal strings is \\ncalled the language generated (or defined) by the grammar. \\n \\nFormaly, for a given grammar \\nthe language generated by G is \\n \\n \\n \\n \\n \\n \\nThat is \\niff \\n. \\nwww.indiansbrain.com\\nIf \\n, we must have for some \\n, \\n, denoted as a \\nderivation sequence of w, The strings \\n \\nare denoted as sentential forms of the \\nderivation.  \\n \\n \\nExample : Consider the grammar \\n \\n, where N = {S},={a, b} and P is the set of the following \\nproduction rules \\n \\n \\n \\n \\n{ S \\nab, SaSb} \\n \\nSome terminal strings generated by this grammar together with their derivation is given below. \\n \\nS \\nab \\n \\nS \\naSb\\naabb \\n \\nS \\naSb\\naaSbb\\naaabbb \\n \\nIt is easy to prove that the language generated by this grammar is \\n \\n \\n \\n \\nBy using the first production, it generates the string ab ( for i =1 ). \\n \\nTo generate any other string, it needs to start with the production S\\naSb and then the non-terminal S in the RHS can be \\nreplaced either by ab (in which we get the string aabb) or the same production S\\naSb can be used one or more \\ntimes. Every time it adds an \\'a\\' to the left and a \\'b\\' to the right of S, thus giving the sentential \\n \\nform \\n. When the non-terminal is replaced by ab (which is then only possibility for generating \\na terminal string) we get a terminal string of the form \\n. \\n \\nThere is no general rule for finding a grammar for a given language. For many languages we can devise \\ngrammars and there are many languages for which we cannot find any grammar. \\n \\nExample: Find a grammar for the language \\n. \\n \\nIt is possible to find a grammar for L by modifying the previous grammar since we need to generate an extra b \\nat the end of the string \\n. We can do this by adding a production S\\nBb where the non-terminal B \\ngenerates \\nas given in the previous example. \\n \\nUsing the above concept we devise the follwoing grammar for L. \\n \\n \\nwhere, N = { S, B }, P = { S\\nBb, B\\nab, B\\naBb } \\n \\nParse Trees: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\nConstruction of a Parse tree: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nYield of a Parse tree: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAmbiguity in languages and grammars: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\nUNIT-IV \\n \\n \\n \\n \\n \\n \\nPush down automata: \\n \\nRegular language can be charaterized as the language accepted by finite automata. Similarly, we can \\ncharacterize the context-free language as the langauge accepted by a class of machines called \\n\"Pushdown Automata\" (PDA). A pushdown automation is an extension of the NFA. \\n \\nIt is observed that FA have limited capability. (in the sense that the class of languages accepted or characterized by \\nthem is small). This is due to the \"finite memory\" (number of states) and \"no external memory\" involved with them. A \\nPDA is simply an NFA augmented with an \"external stack memory\". The addition of a stack provides the PDA with a \\nlast-in, first-out memory management cpapability. This \"Stack\" or \"pushdown store\" can be used to record a \\npotentially unbounded information. It is due to this memory management capability with the help of the stack that a \\nPDA can overcome the memory limitations that prevents a FA to \\n \\naccept many interesting languages like \\n. Although, a PDA can store an unbounded amount of \\ninformation on the stack, its access to the information on the stack is limited. It can push an element onto the \\ntop of the stack and pop off an element from the top of the stack. To read down into the stack the top elements \\nmust be popped off and are lost. Due to this limited access to the information on the stack, a PDA still has \\nsome limitations and cannot accept some other interesting languages. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAs shown in figure, a PDA has three components: an input tape with read only head, a finite control and \\na pushdown store. \\n \\nThe input head is read-only and may only move from left to right, one symbol (or cell) at a time. In each step, the \\nPDA pops the top symbol off the stack; based on this symbol, the input symbol it is currently reading, and \\nwww.indiansbrain.com\\nits present state, it can push a sequence of symbols onto the stack, move its read-only head one cell \\n(or symbol) to the right, and enter a new state, as defined by the transition rules of the PDA. \\n \\nPDA are nondeterministic, by default. That is, \\n- transitions are also allowed in which the PDA can pop and \\npush, and change state without reading the next input symbol or moving its read-only head. Besides this, \\nthere may be multiple options for possible next moves. \\n \\nFormal Definitions : Formally, a PDA M is a 7-tuple M =\\n \\nwhere, \\n \\n\\uf095 \\nis a finite set of states,\\uf020\\n\\uf095 \\nis a finite set of input symbols (input alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a finite set of stack symbols (stack alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a transition function from \\nto subset of \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the start state\\uf020\\n\\uf095 \\n, is the initial stack symbol, and\\uf020\\n\\uf020\\n\\uf095 \\n, is the final or accept states.\\uf020\\n \\nExplanation of the transition function, \\n: \\n \\nIf, for any \\n, \\n. This means intitutively that whenever \\nthe PDA is in state q reading input symbol a and z on top of the stack, it can nondeterministically for any i, \\n \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack (where \\n) (The usual convention is that if \\n, \\nthen \\nwill be at the top and \\nat the bottom.)\\uf020\\n\\uf095 \\nmove read head right one cell past the current symbol a.\\uf020\\n \\nIf a = \\n, then \\nmeans intitutively that whenver the PDA is in state \\nq with z on the top of the stack regardless of the current input symbol, it can nondeterministically for any \\n \\ni, \\n, \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack, and\\uf020\\n\\uf095 \\nleave its read-only head where it is.\\uf020\\nwww.indiansbrain.com\\nState transition diagram : A PDA can also be depicted by a state transition diagram. The labels on the arcs \\nindicate both the input and the stack operation. The transition \\n \\nfor \\nand \\nis depicted by \\n \\n \\n \\n \\n \\n \\n \\nFinal states are indicated by double circles and the start state is indicated by an arrow to it from nowhere. \\n \\n \\nConfiguration or Instantaneous Description (ID) : \\n \\nA configuration or an instantaneous description (ID) of PDA at any moment during its computation is an \\nelement of \\ndescribing the current state, the portion of the input remaining to be read (i.e. \\nunder and to the right of the read head), and the current stack contents. Only these three elements \\ncan affect the computation from that point on and, hence, are parts of the ID. \\n \\nThe start or inital configuartion (or ID) on input \\nis \\n. That is, the PDA always starts in its \\nstart state, \\nwith its read head pointing to the leftmost input symbol and the stack containing only \\nthe start/initial stack symbol, \\n. \\n \\nThe \"next move relation\" one figure describes how the PDA can move from one configuration to \\nanother in one step. \\n \\nFormally, \\n \\n \\n \\n \\niff \\n\\'a\\' may be \\nor an input symbol. \\n \\nLet I, J, K be IDs of a PDA. We define we write I\\nK, if ID I can become K after exactly i moves. The \\nrelations \\nand \\ndefine as follows \\n \\nI \\nK \\n \\nI \\nJ if \\nsuch that I \\nK and K\\n J \\n \\nI \\nJ if \\nsuch that I \\nJ. \\nwww.indiansbrain.com\\nThat is, \\nis the reflexive, transitive closure of \\n. We say that I \\nJ if the ID J follows from the ID I in \\nzero or more moves. \\n \\n( Note : subscript M can be dropped when the particular PDA M is understood. ) \\n \\nLanguage accepted by a PDA M \\n \\nThere are two alternative definiton of acceptance as given below. \\n \\n1. Acceptance by final state : \\n \\nConsider the PDA \\n. Informally, the PDA M is said to accept its input \\nby \\nfinal state if it enters any final state in zero or more moves after reading its entire input, starting in the start \\nconfiguration on input \\n. \\n \\nFormally, we define L(M), the language accepted by final state to be \\n \\n{ \\n| \\nfor some \\nand \\n} \\n \\n \\n \\n \\n2. Acceptance by empty stack (or Null stack) : The PDA M accepts its input \\nby empty stack if starting in the \\n \\nstart configuration on input \\n, it ever empties the stack w/o pushing anything back on after reading the \\nentire input. Formally, we define N(M), the language accepted by empty stack, to be \\n \\n{ \\n| \\nfor some \\n} \\n \\nNote that the set of final states, F is irrelevant in this case and we usually let the F to be the empty set i.e. F = \\nQ . \\n \\nExample 1 : Here is a PDA that accepts the language \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n, and \\nconsists of the following transitions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe PDA can also be described by the adjacent transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever the PDA M sees an input a in the start state \\nwith the start symbol z on the top of the stack \\nit pushes a onto the stack and changes state to \\n. (to remember that it has seen the first \\'a\\'). On state \\nif it \\nsees anymore a, it simply pushes it onto the stack. Note that when M is on state \\n, the symbol on the \\n \\ntop of the stack can only be a. On state \\nif it sees the first b with a on the top of the stack, then it needs to \\nstart comparison of numbers of a\\'s and b\\'s, since all the a\\'s at the begining of the input have already been \\npushed onto the stack. It start this process by popping off the a from the top of the stack and enters in state q3 \\n \\n(to remember that the comparison process has begun). On state \\n, it expects only b\\'s in the input (if it sees \\nany more a in the input thus the input will not be in the proper form of anbn). Hence there is no more on input a \\nwhen it is in state \\n. On state \\nit pops off an a from the top of the stack for every b in the input. When it sees \\nthe last b on state q3 (i.e. when the input is exaushted), then the last a from the stack will be popped off and the \\nstart symbol z is exposed. This is the only possible case when the input (i.e. on \\n-input ) the PDA M \\n \\nwill move to state \\nwhich is an accept state. \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\nLet the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) \\n \\n( using transition 2 ) \\n \\n( using transition 3 ) \\nwww.indiansbrain.com\\n( using transition 4 ), \\n( using transition 5 ) , \\nis final state. Hence , accept. So \\nthe string aabb is rightly accepted by M \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nExample 2 : We give an example of a PDA M that accepts the set of balanced strings of parentheses [] by \\nempty stack. \\nThe PDA M is given below. \\n \\n \\nwhere \\nis defined as \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever it sees a [, it will push the ] onto the stack. (first two transitions), and whenever it sees a ] \\nand the top of the stack symbol is [, it will pop the symbol [ off the stack. (The third transition). The fourth \\ntransition is used when the input is exhausted in order to pop z off the stack ( to empty the stack) and accept. \\nNote that there is only one state and no final state. The following is a sequence of configurations leading to the \\nacceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn\\'t matter which one we use, since \\nwww.indiansbrain.com\\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M\\' from M and the proof of equivalence of M & M\\' \\nare given below. \\n \\nThere are two cases to be considered. \\n \\nCASE I : PDA M accepts by final state, Let \\nLet qf be a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and M\\' are equivalent i.e. \\n \\nL(M) = L(\\n) \\n \\nLet \\nL(M) . Then \\nfor some \\nand \\n \\n \\nThen \\n \\n \\nThus \\naccepts \\n \\n \\nConversely, let \\naccepts \\ni.e. \\nL(\\n), then \\nfor \\ninherits all other moves except the last one from M. Hence \\nfor some \\n \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE II : PDA M accepts by empty stack. \\n \\nWe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand X\\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\n \\nand \\nwww.indiansbrain.com\\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M\\'s stack. From this point onward \\nwill simulate every move of M \\nsince all the transitions of M are also in \\n \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point, \\nwill enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. We will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. Then \\n \\nfor some \\n. But then \\n \\n \\n( by transition rule 1) \\n \\n( Since \\nincludes all the moves of M ) \\n \\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n. Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some \\n \\n \\nEvery move in the sequence, \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n \\n \\nEquivalence of PDA’s and CFG’s: \\nWe will now show that pushdown automata and context-free grammars are equivalent in expressive power, \\nthat is, the language accepted by PDAs are exactly the context-free languages. To show this, we have to \\nprove each of the following: \\n \\ni) \\nGiven any arbitrary CFG G there exists some PDA M that accepts exactly the same \\nlanguage generated by G. \\n \\nii) \\nGiven any arbitrary PDA M there exists a CFG G that generates exactly the same \\nlanguage accpeted by M. \\n \\n(i) CFA to PDA \\n \\nWe will first prove that the first part i.e. we want to show to convert a given CFG to an equivalent PDA. \\nwww.indiansbrain.com\\nLet the given CFG is \\n. Without loss of generality we can assume that G is in \\nGreibach Normal Form i.e. all productions of G are of the form . \\n \\n where \\nand \\n. \\n \\nFrom the given CFG G we now construct an equivalent PDA M that accepts by empty stack. Note that there \\nis only one state in M. Let \\n \\n, where \\n \\n\\uf095 \\nq is the only state\\uf020\\n\\uf020\\n\\uf095 \\nis the input alphabet,\\uf020\\n\\uf095 \\nN is the stack alphabet ,\\uf020\\n\\uf020\\n\\uf095 \\nq is the start state.\\uf020\\n\\uf095 \\nS is the start/initial stack symbol, and \\n, the transition relation is defined as follows\\uf020\\n \\nFor each production \\n, \\n. We now want to show \\nthat M and G are equivalent i.e. L(G)=N(M). i.e. for any \\n. \\niff \\n. \\n \\nIf \\n, then by definition of L(G), there must be a leftmost derivation starting with S and deriving w. \\n \\ni.e. \\n \\n \\nAgain if \\n, then one sysmbol. Therefore we need to show that for any \\n. \\n \\niff \\n. \\n \\nBut we will prove a more general result as given in the following lemma. Replacing A by S (the start \\nsymbol) and \\nby \\ngives the required proof. \\n \\nLemma For any \\n, \\nand \\n, \\nvia a leftmost derivative iff \\n. \\n \\nProof : The proof is by induction on n. \\n \\nBasis : n = 0 \\nwww.indiansbrain.com\\n \\niff \\ni.e. \\nand \\n \\n \\n \\niff \\n \\niff \\n \\n \\nInduction Step : \\n \\nFirst, assume that \\nvia a leftmost derivation. Let the last production applied in their derivation is \\nfor some \\nand \\n. \\n \\nThen, for some \\n, \\n \\n \\n \\n \\n \\n \\nwhere \\nand \\n \\n \\nNow by the indirection hypothesis, we get, \\n \\n.............................................................................(1) \\nAgain by the construction of M, we get \\n \\n \\nso, from (1), we get \\n \\n \\n \\n \\nsince \\nand \\n, we get \\n \\n \\nThat is, if \\n, then \\n. Conversely, assume that \\nand let \\nwww.indiansbrain.com\\nbe the transition used in the last move. Then for some \\n, \\nand \\n \\n \\n \\nwhere \\nand \\n. \\n \\nNow, by the induction hypothesis, we get \\n \\nvia a leftmost derivation. \\n \\nAgain, by the construction of M, \\nmust be a production of G. [ Since \\n]. Applying the production to the sentential form \\nwe get \\n \\n \\n \\n \\n \\ni.e. \\n \\n \\nvia a leftmost derivation. \\n \\nHence the proof. \\n \\nExample : Consider the CFG G in GNF \\n \\nS\\naAB \\n \\nA\\na / aA \\nB\\na / bB \\n \\nThe one state PDA M equivalent to G is shown below. For convenience, a production of G and \\nthe corresponding transition in M are marked by the same encircled number. \\n \\n(1) S\\naAB \\n \\n(2) A \\na \\n \\n(3) A\\naA \\n \\n(4) B \\na \\n \\n(5) B \\nbB \\n \\n \\n. We have used the same construction discussed earlier \\n \\nSome Useful Explanations : \\nConsider the moves of M on input aaaba leading to acceptance of the string. \\n \\nSteps \\nwww.indiansbrain.com\\n \\n1. (q, aaaba, s) \\n( q, aaba, AB ) \\n2. \\n( q, aba, AB ) \\n3. \\n( q, ba, B ) \\n4. \\n( q, a, B ) \\n5. \\n( q,   ,   )    Accept by empty stack. \\n \\nNote : encircled numbers here shows the transitions rule applied at every step. \\n \\nNow consider the derivation of the same string under grammar G. Once again, the production used at \\nevery step is shown with encircled number. \\n \\nS \\naAB \\naaAB \\naaaB \\naaabB \\naaaba \\n \\nSteps \\n1 \\n2 \\n3 \\n4 \\n5\\n \\nObservations: \\n\\uf095 \\nThere is an one-to-one correspondence of the sequence of moves of the PDA M and the derivation\\uf020\\n\\uf020\\nsequence under the CFG G for the same input string in the sense that - number of steps in both \\nthe cases are same and transition rule corresponding to the same production is used at every step \\n(as shown by encircled number). \\n\\uf020\\n\\uf095 \\nconsidering the moves of the PDA and derivation under G together, it is also observed that at \\nevery step the input read so far and the stack content together is exactly identical to the \\ncorresponding sentential form i.e.\\uf020\\n\\uf020\\n<what is Read><stack> = <sentential form> \\n \\nSay, at step 2, Read so far = \\na stack = AB \\nSentential form = aAB From this property we claim that \\niff \\n. If the claim is \\n \\ntrue, then apply with \\nand we get \\niff \\n or \\niff \\n( \\nby definition ) \\n \\nThus N(M) = L(G) as desired. Note that we have already proved a more general version of the \\nclaim PDA and CFG: \\nWe now want to show that for every PDA M that accpets by empty stack, there is a CFG G such that L(G) = \\nN(M) \\n \\nwe first see whether the \"reverse of the construction\" that was used in part (i) can be used here to construct \\nan equivalent CFG from any PDA M. \\n \\nIt can be show that this reverse construction works only for single state PDAs. \\nwww.indiansbrain.com\\n\\uf095 \\nThat is, for every one-state PDA M there is CFG G such that L(G) = N(M). For every move of the \\nPDA M \\nwe introduce a production \\nin the \\ngrammar \\nwhere N = T and \\n.\\uf020\\n \\nwe can now apply the proof in part (i) in the reverse direction to show that L(G) = N(M). \\n \\nBut the reverse construction does not work for PDAs with more than one state. For example, consider the PDA \\nM produced here to accept the langauge \\n \\n \\n \\n \\n \\nNow let us construct CFG \\nusing the \"reverse\" construction. \\n \\n( Note \\n). \\n \\nTransitions in M \\nCorresponding Production in G \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe can drive strings like aabaa which is in the language. \\n \\n \\n \\n \\nBut under this grammar we can also derive some strings which are not in the language. e.g \\n \\n \\n \\n \\nand \\n. But \\n \\n \\nTherefore, to complete the proof of part (ii) we need to prove the following claim also. \\n \\nClaim: For every PDA M there is some one-state PDA \\nsuch that \\n. \\n \\nIt is quite possible to prove the above claim. But here we will adopt a different approach. We start with \\nany arbitrary PDA M that accepts by empty stack and directly construct an equivalent CFG G. \\nwww.indiansbrain.com\\nPDA to CFG \\n \\nWe want to construct a CFG G to simulate any arbitrary PDA M with one or more states. Without loss \\nof generality we can assume that the PDA M accepts by empty stack. \\n \\nThe idea is to use nonterminal of the form <PAq> whenever PDA M in state P with A on top of the stack goes \\n \\nto state \\n. That is, for example, for a given transition of the PDA corresponding production in the grammar as \\nshown below, \\nAnd, we would like to show, in general, that \\niff the PDA M, when started from state P with A on \\n \\nthe top of the stack will finish processing \\n, arrive at state q and remove A from the stack. \\n \\nwe are now ready to give the construction of an equivalent CFG G from a given PDA M. we need to introduce \\ntwo kinds of producitons in the grammar as given below. The reason for introduction of the first kind of \\nproduction will be justified at a later point. Introduction of the second type of production has been justified in \\nthe above discussion. \\n \\nLet \\nbe a PDA. We construct from M a equivalent CFG \\n \\n \\nWhere \\n \\n\\uf095 \\nN is the set of nonterminals of the form <PAq> for \\nand \\nand P contains the follwoing\\uf020\\n \\ntwo kind of production \\n \\n1.  \\n \\n \\n2. If \\n, then for every choice of the sequence \\n,\\n \\n, \\n. \\n \\n \\nInclude the follwoing production \\n \\n \\n \\n \\nIf n = 0, then the production is \\n.For the whole exercise to be meaningful we want \\nmeans there is a sequence of transitions ( for PDA M ), starting in state q, ending in \\n, \\n \\nduring which the PDA M consumes the input string \\nand removes A from the stack (and, of course, all \\nother symbols pushed onto stack in A\\'s place, and so on.) \\n \\nThat is we want to claim that \\n \\niff \\n \\n \\nIf this claim is true, then let \\nto get \\niff \\nfor some \\n \\n. But for all \\nwe have \\nas production in G. Therefore, \\nwww.indiansbrain.com\\niff \\ni.e. \\niff PDA M accepts w by empty stack or L(G) = N(M) \\n \\nNow, to show that the above construction of CFG G from any PDA M works, we need to prove the \\nproposed claim. \\n \\nNote: At this point, the justification for introduction of the first type of production (of the form \\n) in \\nthe CFG G, is quite clear. This helps use deriving a string from the start symbol of the grammar. \\n \\nProof : Of the claim \\niff \\nfor some \\n, \\nand \\n \\n \\nThe proof is by induction on the number of steps in a derivation of G (which of course is equal to the number \\nof moves taken by M). Let the number of steps taken is n. \\n \\nThe proof consists of two parts: \\' if \\' part and \\' only if \\' part. First, consider the \\' if \\' part \\n \\nIf \\nthen \\n. \\n \\nBasis is n =1 \\n \\nThen \\n. In this case, it is clear that \\n. Hence, by construction \\nis a production of G. \\n \\nThen \\n \\nInductive Hypothesis : \\n \\n \\n \\n \\n \\nInductive Step : \\n \\n \\nFor n >1, let w = ax for some \\nand \\nconsider the first move of the PDA M which uses \\nthe general transition \\n= \\n \\n. Now M must remove \\nfrom stack \\nwhile consuming x in the remaining n-1 moves. \\n \\nLet \\n, where \\nis the prefix of x that M has consumed when \\nfirst appears at top \\nof the stack. Then there must exist a sequence of states in M (as per construction) \\n(with \\n \\n), such that \\nwww.indiansbrain.com\\n \\n [ This step implies \\n] \\n [ This step implies \\n] \\n \\n... \\n \\n \\n=\\n \\n \\n[ Note: Each step takes less than or equal to n -1 moves because the total number of moves required \\nassumed to be n-1.] \\n \\nThat is, in general \\n \\n \\n, \\n. \\n \\nSo, applying inductive hypothesis we get \\n \\n, \\n. But corresponding to the original move \\n \\nin M we have added the following production in G. \\n \\nWe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) , \\n( using transition 2 ) \\n \\n( using transition 3 ), \\n( using transition 4 ) \\n \\n( using transition 5 ) , \\nis final state. Hence, accept. \\n \\nSo the string aabb is rightly accepted by M. \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\nwww.indiansbrain.com\\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nThe following is a sequence of configurations leading to the acceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn\\'t matter which one we use, since \\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\n \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M\\' from M and the proof of equivalence of M & M\\' \\nare given below \\n \\nThere are two cases to be considered. \\n \\nCASE 1 : PDA M accepts by final state, Let \\n. Let \\nbe a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and \\nare equivalent i.e. \\n \\n \\n. \\n \\nLet \\n. Then \\nfor some \\nand \\n \\n \\nThen \\n. \\n \\nThus \\naccepts \\n. \\nwww.indiansbrain.com\\nConversely, let \\naccepts \\ni.e. \\n, then \\nfor some \\n. \\ninherits all other moves except the last one from M. Hence \\nfor some \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE 2 : PDA M accepts by empty stack. \\n \\nwe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand \\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\nand \\n \\n \\n \\n \\n \\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M\\'s stack. From this point onward M\\' will simulate every move of M \\n \\nsince all the transitions of M are also in \\n. \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point\\n, will enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. we will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. \\n \\nThen \\n \\nfor some \\n. But then, \\n \\n \\n( by transition rule 1 ) \\n \\n( since \\ninclude all the moves of M ) \\nwww.indiansbrain.com\\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n.Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some Q . \\n \\nEvery move in the sequence \\n \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n. \\n \\nDeterministic PDA: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRegular Languages and DPDA’s The DPDA’s accepts a class of languages that is in between the regular \\nlanguages and CFL’s. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDeterministic Pushdown Automata (DPDA) and Deterministic Context-free Languages (DCFLs) \\n \\nPushdown automata that we have already defined and discussed are nondeterministic by default, that is , there may be two or \\nmore moves involving the same combinations of state, input symbol, and top of the stock, and again, for some state and \\ntop of the stock the machine may either read and input symbol or make an \\n- transition (without consuming any input). \\n \\nIn deterministic PDA , there is never a choice of move in any situation. This is handled by preventing the above mentioned two \\ncases as described in the definition below. \\n \\nDefnition : Let \\nbe a PDA . Then M is deterministic if and only if both the following conditions are \\nsatisfied. \\n \\n1. \\nhas at most one element for any \\nand \\n(this condition prevents multiple choice f \\nany combination of \\n) \\n2. \\nIf \\nand \\nfor every \\n \\n \\n(This condition prevents the possibility of a choice between a move with or without an input symbol). \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEmpty Production Removal \\n \\nThe productions of context-free grammars can be coerced into a variety of forms without \\naffecting the expressive power of the grammars. If the empty string does not belong to a language, \\nthen there is a way to eliminate the productions of the form A→ λ from the grammar. \\n \\nIf the empty string belongs to a language, then we can eliminate λ from all productions \\n \\nsave for the single production S → λ. In this case we can also eliminate any occurrences of S \\nfrom the right-hand side of productions. \\n \\nProcedure to find CFG with out empty Productions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUnit production removal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLeft Recursion Removal \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNORMAL FORMS \\n \\nTwo kinds of normal forms viz., Chomsky Normal Form and Greibach Normal Form (GNF) \\nare considered here. \\n \\nChomsky Normal Form (CNF) \\n \\nAny context-free language L without any λ-production is generated by a grammar is \\nwhich productions are of the form A → BC or A→ a, where A, B ∈VN , and a ∈ V Τ. \\n \\nProcedure to find Equivalent Grammar in CNF \\n \\n(i) Eliminate the unit productions, and λ-productions if any, \\n \\n(ii) Eliminate the terminals on the right hand side of length two or more. \\n \\n(iii) Restrict the number of variables on the right hand side of productions to two. \\nProof: \\n \\nFor Step (i): Apply the following theorem: “Every context free language can be generated by \\na grammar with no useless symbols and no unit productions”. \\n \\nAt the end of this step the RHS of any production has a single terminal or two or more symbols. \\n \\nLet us assume the equivalent resulting grammar as G = (VN ,VT ,P ,S ). \\nFor Step (ii): Consider any production of the form \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExample \\n \\nObtain a grammar in Chomsky Normal Form (CNF) equivalent to the grammar G \\nwith productions P given \\n \\n \\n \\n \\n \\n \\nSolution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\nPumping Lemma for CFG \\nA “Pumping Lemma” is a theorem used to show that, if certain strings belong to a \\n \\nlanguage, then certain other strings must also belong to the language. Let us discuss a Pumping \\nLemma for CFL. We will show that , if L is a context-free language, then strings of L that are at \\nleast ‘m’ symbols long can be “pumped” to produce additional strings in L. The value of ‘m’ \\ndepends on the particular language. Let L be an infinite context-free language. Then there is some \\npositive integer ‘m’ such that, if S is a string of L of Length at least ‘m’, then \\n \\n(i) S = uvwxy (for some u, v, w, x, y) \\n \\n(ii) | vwx| \\uf064 m \\n(iii) | vx| \\uf0651 \\n(iv) uv iwx i y∈L. \\n \\nfor all non-negative values of i. \\nIt should be understood that \\n \\n(i) If S is sufficiently long string, then there are two substrings, v and x, somewhere in \\nS. There is stuff (u) before v, stuff (w) between v and x, and stuff (y), after x. \\n \\n(ii) The stuff between v and x won’t be too long, because | vwx | can’t be larger than m. \\n(iii) Substrings v and x won’t both be empty, though either one could be. \\n \\n(iv) If we duplicate substring v, some number (i) of times, and duplicate x the same \\nnumber of times, the resultant string will also be in L. \\n \\nDefinitions \\nA variable is useful if it occurs in the derivation of some string. This requires that \\n \\n(a) the variable occurs in some sentential form (you can get to the variable if you start from S), and \\n \\n(b) a string of terminals can be derived from the sentential form (the variable is not a “dead end”). \\nA variable is “recursive” if it can generate a string containing itself. For example, variable A is \\nrecursive if \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof of Pumping Lemma \\n \\n(a) Suppose we have a CFL given by L. Then there is some context-free Grammar G that \\ngenerates L. Suppose \\n(i) L is infinite, hence there is no proper upper bound on the length of strings belonging to L. \\n \\n(ii) L does not contain l. \\n(iii) G has no productions or l-productions. \\nwww.indiansbrain.com\\nThere are only a finite number of variables in a grammar and the productions for each \\n \\nvariable have finite lengths. The only way that a grammar can generate arbitrarily long strings is if \\none or more variables is both useful and recursive. Suppose no variable is recursive. Since the start \\nsymbol is non recursive, it must be defined only in terms of terminals and other variables. Then \\nsince those variables are non recursive, they have to be defined in terms of terminals and still other \\nvariables and so on. \\n \\nAfter a while we run out of “other variables” while the generated string is still finite. Therefore \\nthere is an upper bond on the length of the string which can be generated from the start \\nsymbol. This contradicts our statement that the language is finite. \\nHence, our assumption that no variable is recursive must be incorrect. \\n \\n(b) Let us consider a string X belonging to L. If X is sufficiently long, then the derivation of X \\nmust have involved recursive use of some variable A. Since A was used in the derivation, the \\nderivation should have started as \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUsage of Pumping Lemma \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHence our original assumption, that L is context free should be false. Hence the language L is not \\ncon text-free. \\n \\nExample \\n \\nCheck whether the language given by L \\uf03d {a mbmcn : m \\uf064 n \\uf064 2m} is a CFL or not. \\nSolution \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nClosure properties of CFL – Substitution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nApplications of substitution theorem \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReversal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInverse Homomorphism: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUNIT-V \\nTuring machine: \\n \\nInformal Definition: \\n \\nWe consider here a basic model of TM which is deterministic and have one-tape. There are many variations, \\nall are equally powerfull. \\n \\nThe basic model of TM has a finite set of states, a semi-infinite tape that has a leftmost cell but is infinite to \\nthe right and a tape head that can move left and right over the tape, reading and writing symbols. \\n \\nFor any input w with |w|=n, initially it is written on the n leftmost (continguous) tape cells. The infinitely many \\ncells to the right of the input all contain a blank symbol, B whcih is a special tape symbol that is not an input \\nsymbol. The machine starts in its start state with its head scanning the leftmost symbol of the input w. De-\\npending upon the symbol scanned by the tape head and the current state the machine makes a move which \\nconsists of the following: \\n \\n\\uf095 \\nwrites a new symbol on that tape cell,  \\uf095\\uf020\\n\\uf020\\nmoves its head one cell either to the left or to the right and \\n\\uf095 \\n(possibly) enters a new state.\\uf020\\n \\nThe action it takes in each step is determined by a transition functions. The machine continues computing (i.e. \\nmaking moves) until \\n \\n\\uf095 \\nit decides to \"accept\" its input by entering a special state called accept or final state or\\uf020\\n\\uf095 \\nhalts without accepting i.e. rejecting the input when there is no move defined.\\uf020\\n \\nOn some inputs the TM many keep on computing forever without ever accepting or rejecting the input, in \\nwhich case it is said to \"loop\" on that input \\n \\nFormal Definition : \\n \\nFormally, a deterministic turing machine (DTM) is a 7-tuple \\n, where \\n \\n\\uf095 \\nQ is a finite nonempty set of states.\\uf020\\n\\uf095 \\nis a finite non-empty set of tape symbols, callled the tape alphabet of M.\\uf020\\n\\uf020\\n\\uf095 \\nis a finite non-empty set of input symbols, called the input alphabet of M.\\uf020\\n\\uf095 \\nis the transition function of M,\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nis the initial or start state.\\uf020\\n\\uf095 \\nis the blank symbol\\uf020\\n\\uf020\\n\\uf095 \\nis the set of final state.\\uf020\\n \\nSo, given the current state and tape symbol being read, the transition function describes the next state, symbol \\nto be written on the tape, and the direction in which to move the tape head ( L and R denote left and right, \\nrespectively ). \\n \\nTransition function :\\n \\n \\n\\uf095 \\nThe heart of the TM is the transition function, \\nbecause it tells us how the machine gets one step \\nto the next.\\uf020\\n\\uf020\\n\\uf095 \\nwhen the machine is in a certain state q\\nQ and the head is currently scanning the tape symbol \\n, and if \\n, then the machine\\uf020\\n \\n1. replaces the symbol X by Y on the tape \\n \\n2. goes to state p, and \\n3. the tape head moves one cell ( i.e. one tape symbol ) to the left ( or right ) if D is L ( or R ). \\n \\nThe ID (instantaneous description) of a TM capture what is going out at any moment i.e. it contains all the \\ninformation to exactly capture the \"current state of the computations\". \\n \\nIt contains the following: \\n \\n\\uf095 \\nThe current state, q\\uf020\\n\\uf095 \\nThe position of the tape head,\\uf020\\n\\uf020\\n\\uf095 \\nThe constants of the tape up to the rightmost nonblank symbol or the symbol to the left of the head, \\nwhichever is rightmost.\\uf020\\n \\nNote that, although there is no limit on how far right the head may move and write nonblank symbols on the \\ntape, at any finite \\n \\ntime, the TM has visited only a finite prefix of the infinite tape. \\n \\nAn ID (or configuration) of a TM M is denoted by \\nwhere \\nand \\n \\n\\uf095 \\nis the tape contents to the left of the head\\uf020\\n\\uf095 \\nq is the current state.\\uf020\\n\\uf020\\n\\uf095 \\nis the tape contents at or to the right of the tape head\\uf020\\n \\nThat is, the tape head is currently scanning the leftmost tape symbol of \\n. ( Note that if \\n, then the \\ntape head is scanning a blank symbol) \\n \\nIf \\nis the start state and w is the input to a TM M then the starting or initial configuration of M is onviously \\ndenoted by \\n \\nwww.indiansbrain.com\\nMoves of Turing Machines \\n \\nTo indicate one move we use the symbol \\n. Similarly, zero, one, or more moves will be represented by \\n. A \\nmove of a TM \\n \\nM is defined as follows. \\n \\nLet \\nbe an ID of M where \\n, \\nand \\n. \\n \\nLet there exists a transition \\nof M. \\n \\nThen we write \\nmeaning that ID \\nyields \\n \\n \\n\\uf095 \\nAlternatively \\n, \\nif \\nis \\na \\ntransition \\nof \\nM, \\nthen \\nwe \\nwrite \\nwhich means that the ID \\nyields \\n\\uf020\\n\\uf020\\n\\uf095 \\nIn other words, when two IDs are related by the relation \\n, we say that the first one yields the \\nsecond ( or the second is the result of the first) by one move.\\uf020\\n\\uf095 \\nIf IDj results from IDi by zero, one or more (finite) moves then we write \\n( If the TM M is understand, \\nthen the subscript M can be dropped from \\nor \\n)\\uf020\\n \\nSpecial Boundary Cases \\n \\n\\uf095 \\nLet \\nbe an ID and \\nbe an transition of M. Then \\n. That is, the head is not \\nallowed to fall off the left end of the tape.\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure (Note that \\nis equivalent to \\n)\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n \\nThe language accepted by a TM \\n, denoted as L(M) is \\n \\nL(M) = { w | \\nand figure for some p\\nF and \\n} \\n \\nIn other words the TM M accepts a string \\nthat cause M to enter a final or accepting state when started \\nin its initial ID (i.e. \\n). That is a TM M accepts the string \\nif a sequence of IDs, \\n \\nexists such that \\n \\n\\uf095 \\nis the initial or starting ID of M\\uf020\\n\\uf095 \\n; \\n\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nThe representation of IDk contains an accepting state.\\uf020\\n \\nThe set of strings that M accepts is the language of M, denoted L(M), as defined \\nabove More about configuration and acceptance \\n \\n\\uf095 \\nAn ID \\nof M is called an accepting (or final) ID if \\n\\uf020\\n\\uf020\\n\\uf095 \\nAn ID \\nis called a blocking (or halting) ID if \\nis undefined i.e. the TM has no move at this \\npoint.\\uf020\\n\\uf095 \\nis called reactable from \\nif \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the initial (or starting) ID if \\nis the input to the TM and \\nis the initial (or start) state \\nof M.\\uf020\\n \\nOn any input string \\n \\n \\neither \\n \\n\\uf095 \\nM halts on w if there exists a blocking (configuration) ID, \\nsuch that \\n\\uf020\\n \\nThere are two cases to be considered \\n \\n\\uf095 \\nM accepts w if I is an accepting ID. The set of all \\naccepted by M is denoted as L(M) as\\uf020\\nalready defined \\n \\n\\uf095 \\nM rejects w if \\nis a blocking configuration. Denote by reject (M), the set of all \\nrejected by M.\\uf020\\n \\nor \\n \\n\\uf095 \\nM loops on w if it does not halt on w.\\uf020\\n \\nLet loop(M) be the set of all \\non which M loops for. \\n \\nIt is quite clear that \\n \\n \\n \\n \\nThat is, we assume that a TM M halts \\n \\n\\uf095 \\nWhen it enters an accepting \\nor\\uf020\\n\\uf095 \\nWhen it enters a blocking \\ni.e. when there is no next move.\\uf020\\n \\nHowever, on some input string, , \\n, it is possible that the TM M loops for ever i.e. it never halts \\nwww.indiansbrain.com\\n \\nThe Halting Problem \\n \\nThe input to a Turing machine is a string. Turing machines themselves can be written as strings. \\nSince these strings can be used as input to other Turing machines. A “Universal Turing \\nmachine” is one whose input consists of a description M of some arbitrary Turing machine, and \\nsome input w to which machine M is to be applied, we write this combined input as M + w. \\nThis produces the same output that would be produced by M. This is written as \\n \\nUniversal Turing Machine (M + w) = M (w). \\n \\nAs a Turing machine can be represented as a string, it is fully possible to supply a Turing \\n \\nmachine as input to itself, for example M (M). This is not even a particularly bizarre thing to do for \\nexample, suppose you have written a C pretty printer in C, then used the Pretty printer on itself. \\nAnother common usage is Bootstrapping—where some convenient languages used to write a \\nminimal compiler for some new language L, then used this minimal compiler for L to write a new, \\nimproved compiler for language L. Each time a new feature is added to language L, you can \\nrecompile and use this new feature in the next version of the compiler. Turing machines sometimes \\nhalt, and sometimes they enter an infinite loop. \\n \\nA Turing machine might halt for one input string, but go into an infinite loop when given \\nsome other string. The halting problem asks: “It is possible to tell, in general, whether a given \\n \\nmachine will halt for some given input?” If it is possible, then there is an effective procedure to look \\nat a Turing machine and its input and determine whether the machine will halt with that input. If \\nthere is an effective procedure, then we can build a Turing machine to implement it. Suppose we \\nhave a Turing machine “WillHalt” which, given an input string M + w, will halt and accept the string \\nif Turing machine M halts on input w and will halt and reject the string if Turing machine M does \\nnot halt on input w. When viewed as a Boolean function, “WillHalt (M, w)” halts and returns \\n“TRUE” in the first case, and (halts and) returns “FALSE” in the second. \\n \\nTheorem \\nTuring Machine “WillHalt (M, w)” does not exist. \\n \\nProof: This theorem is proved by contradiction. Suppose we could build a machine “WillHalt”. \\nThen we can certainly build a second machine, “LoopIfHalts”, that will go into an infinite loop \\nif and only if “WillHalt” accepts its input: \\n Function LoopIfHalts (M, \\nw): if WillHalt (M, w) then \\nwhile true do { } \\nelse \\nreturn false; \\n \\nWe will also define a machine “LoopIfHaltOnItSelf” that, for any given input M, representing a \\nTuring machine, will determine what will happen if M is applied to itself, and loops if M will halt \\nin this case. \\n Function LoopIfHaltsOnItself (M): \\nreturn LoopIfHalts (M, M): \\n \\nFinally, we ask what happens if we try: \\nFunc tion Impos sible: \\nreturn LoopIfHaltsOnItself (LoopIfHaltsOnItself): \\n \\nThis machine, when applied to itself, goes into an infinite loop if and only if it halts \\nwhen applied to itself. This is impossible. Hence the theorem is proved. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nImplications of Halting Problem \\n \\nProgramming \\nThe Theorem of “Halting Problem” does not say that we can never determine whether or not \\n \\na given program halts on a given input. Most of the times, for practical reasons, we could \\neliminate infinite loops from programs. Sometimes a “meta-program” is used to check another \\nprogram for potential infinite loops, and get this meta-program to work most of the time. \\n \\nThe theorem says that we cannot ever write such a meta-program and have it work all of the \\ntime. This result is also used to demonstrate that certain other programs are also impossible. \\nThe basic outline is as follows: \\n \\n(i) If we could solve a problem X, we could solve the Halting problem \\n(ii) We cannot solve the Halting Problem \\n(iii) Therefore, we cannot solve problem X \\n \\n \\n \\n \\n \\n \\nA Turing machine can be \"programmed,\" in much the same manner as a computer is \\n \\nprogrammed. When one specifies the function which we usually call δ for a Tm, he is really \\nwriting a program for the Tm. \\n \\n1. Storage in finite Control \\n \\nThe finite control can be used to hold a finite amount of information. To do so, the state is written \\nas a pair of elements, one exercising control and the other storing a symbol. It should be \\n \\nemphasized that this arrangement is for conceptual purposes only. No modification in the definition \\nof the Turing machine has been made. \\n \\nExample \\nConsider the Turing machine \\nSolution \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2. Multiple Tracks \\n \\nWe can imagine that the tape of the Turing machine is divided into k tracks, for any finite k. This \\narrangement is shown in Fig., with k = 3. What is actually done is that the symbols on the tape \\nare considered as k-tuples. One component for each track. \\n \\nExample \\nThe tape in Fig. can be imagined to be that of a Turing machine which takes a binary input \\n \\ngreater than 2, written on the first track, and determines if it is a prime. The input is surrounded by \\n¢ and $ on the first track. \\n \\nThus, the allowable input symbols are [¢, B, B], [0, B, B ], [1, B, B ], and [$, B, B]. These \\n \\nsymbols can be identified with ¢, 0, 1, and $, respectively, when viewed as input symbols. The blank \\nwww.indiansbrain.com\\nsymbol can be represented by [B, B, B ] \\n \\nTo test if its input is a prime, the Tm first writes the number two in binary on the second track \\n \\nand copies the first track onto the third track. Then, the second track is subtracted, as many times \\nas possible, from the third track, effectively dividing the third track by the second and leaving the \\nremainder. If the remainder is zero, the number on the first track is not a prime. If the remainder is \\nnonzero, increase the number on the second track by one. \\n \\nIf now the second track equals the first, the number on the first track is a prime, because it cannot \\nbe divided by any number between one and itself. If the second is less than the first, the whole \\noperation is repeated for the new number on the second track. In Fig., the Tm is testing to determine \\nif 47 is a prime. The Tm is dividing by 5; already 5 has been subtracted twice, so 37 appears on the \\nthird track. \\n \\n3. Subroutines \\nwww.indiansbrain.com\\n \\nUNDECIDABILITY \\n \\n \\nDesign a Turing machine to add two given integers. \\n \\nSolution: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSome unsolvable Problems are as follows: \\n(i) Does a given Turing machine M halts on all input? \\n(ii) Does Turing machine M halt for any input? \\n(iii) Is the language L(M) finite? \\n \\n(iv) Does L(M) contain a string of length k, for some given k? \\n \\n(v) Do two Turing machines M1 and M2 accept the same language? \\n \\nIt is very obvious that if there is no algorithm that decides, for an arbitrary given Turing machine \\nM and input string w, whether or not M accepts w. These problems for which no algorithms exist \\nare called “UNDECIDABLE” or “UNSOLVABLE”. \\n \\nCode for Turing Machine: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\nDiagonalization language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis table represents language acceptable by Turing machine \\nwww.indiansbrain.com\\nProof that Ld is not recursively enumerable: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRecursive Languages: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUniversal \\n \\nLanguage: \\nwww.indiansbrain.com\\n \\nUndecidability of Universal Language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProblem -Reduction : \\nIf P1 reduced to P2, \\n \\nThen P2 is at least as hard as P1. \\nTheorem: If P1 reduces to P2 then, \\n\\uf095 If P1 is undecidable the so is P2.\\uf020\\n\\uf095 If P1 is Non-RE then so is P2.\\uf020\\nwww.indiansbrain.com\\nPost\\'s Correspondence Problem (PCP) \\n \\nA post correspondence system consists of a finite set of ordered pairs \\nwhere \\nfor some alphabet \\n. \\n \\nAny sequence of numbers \\n \\n \\nis called a solution to a Post Correspondence System. \\n \\nThe Post\\'s Correspondence Problem is the problem of determining whether \\na Post Correspondence system has a solutions. \\n \\nExample 1 : Consider the post correspondence system \\n \\n The list 1,2,1,3 is a solution to it. \\n \\nBecause \\n \\n \\n \\n \\n \\n \\n \\ni \\n             xi                                 yi \\n \\n1 \\n \\n2 \\n \\n3 \\n \\n \\n(A post correspondence system is also denoted as an instance of the \\nPCP) Example 2 : The following PCP instance has no solution \\ni \\n          xi                          yi \\n \\n1 \\n \\n2 \\n \\nThis can be proved as follows. \\ncannot be chosen at the start, since than the LHS and RHS would \\ndiffer in the first symbol ( \\nin LHS and \\nin RHS). So, we must start with \\n. The next pair must be \\nso that the 3 rd symbol in the RHS becomes identical to that of the LHS, which is a . After this step, \\nLHS and RHS are not matching. If \\nis selected next, then would be mismatched in the 7 th symbol \\nwww.indiansbrain.com\\n( \\nin LHS and \\nin RHS). If \\nis selected, instead, there will not be any choice to match the both side \\nin the next step. \\n \\nExample3 : The list 1,3,2,3 is a solution to the following PCP instance. \\n \\n \\ni \\n   \\nx\\ni \\n   \\ny\\ni \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n1 \\n \\n1 \\n \\n101 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n2 \\n \\n10 \\n \\n00 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n3 \\n011 \\n11 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\nThe following properties can easily be proved. \\n \\nProposition The Post Correspondence System \\n \\n has solutions if and only if \\n \\n \\n \\n \\n \\n \\nCorollary : PCP over one-letter alphabet is decidable. \\n \\nProposition Any PCP instance over an alphabet \\nwith \\nis equivalent to a PCP instance over \\nan alphabet \\nwith \\n \\n \\nProof : Let \\n \\n \\nConsider \\nWe can now encode every \\nas \\nany PCP instance over \\nwill now \\nhave only two symbols, 0 and 1 and, hence, is equivalent to a PCP instance over \\n \\n \\nTheorem : PCP is undecidable. That is, there is no algorithm that determines whether an arbitrary Post \\nCorrespondence System has a solution. \\n \\nProof: The halting problem of turning machine can be reduced to PCP to show the undecidability of PCP. Since \\nhalting problem of TM is undecidable (already proved), This reduction shows that PCP is also undecidable. \\nThe proof is little bit lengthy and left as an exercise. \\n \\nSome undecidable problem in context-free languages \\n \\nWe can use the undecidability of PCP to show that many problem concerning the context-free languages \\nare undecidable. To prove this we reduce the PCP to each of these problem. The following discussion \\nmakes it clear how PCP can be used to serve this purpose. \\nwww.indiansbrain.com\\nLet \\nbe a Post Correspondence System over the alphabet \\n. We \\nconstruct two CFG\\'s Gx and Gy from the ordered pairs x,y respectively as follows. \\n \\n and \\n \\n where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\n \\nit is clear that the grammar \\ngenerates the strings that can appear in the LHS of a sequence while solving the \\nPCP followed by a sequence of numbers. The sequence of number at the end records the sequence of \\n \\nstrings from the PCP instance (in reverse order) that generates the string. Similarly, \\ngenerates the \\nstrings that can be obtained from the RHS of a sequence and the corresponding sequence of numbers (in \\nreverse order). \\n \\nNow, if the Post Correspondence System has a solution, then there must be a sequence \\n \\n \\n \\n \\n \\n \\n \\n \\nAccording to the construction of \\nand \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn this case \\nwww.indiansbrain.com\\n \\n \\nHence , \\nand \\nimplying \\n \\n \\n \\n \\n \\nConversely, let \\n \\n \\nHence, w must be in the form w1w2 where \\nand w2 in a sequence \\n(since, only that kind \\nof strings can be generated by each of \\nand \\n). \\n \\nNow, the string \\nis a solution to the Post Correspondence System. \\n \\nIt is interesting to note that we have here reduced PCP to the language of pairs of CFG,s whose intersection is \\nnonempty. The following result is a direct conclusion of the above. \\n \\nTheorem : Given any two CFG\\'s G1 and G2 the question \"Is \\n\" is undecidable. \\n \\nProof: Assume for contradiction that there exists an algorithm A to decide this question. This would imply \\nthat PCP is decidable as shown below. \\n \\nFor any Post Correspondence System, P construct grammars \\nand \\nby using the constructions \\nelaborated already. We can now use the algorithm A to decide whether and \\nThus, PCP is decidable, a contradiction. So, such an algorithm does not exist. \\n \\nIf \\nand \\nare CFG\\'s constructed from any arbitrary Post Correspondence System, than it is not difficult to \\n \\nshow that \\nand \\nare also context-free, even though the class of context-free languages are \\nnot closed under complementation. \\n \\nand their complements can be used in various ways to show that many other questions \\nrelated to CFL\\'s are undecidable. We prove here some of those. \\n \\nTheorem : Foe any two arbitrary CFG\\'s \\nthe following questions are undecidable \\n \\ni. \\nIs \\n \\n \\nii. \\nIs \\n \\nwww.indiansbrain.com\\niii. Is \\n \\nProof : \\ni. \\nIf \\nthen, \\n \\n \\nHence, it suffice to show that the question “Is \\n\" is undecidable. \\n \\nSince, \\nand \\nare CFl\\'s and CFL\\'s are closed under union, \\nis also context-\\nfree. By DeMorgan\\'s theorem, \\n \\n \\nIf there is an algorithm to decide whether \\nwe can use it to decide whether \\nor not. But this problem has already been proved to be undecidable. \\n \\n \\nHence there is no such algorithm to decide or not. \\nii. \\n \\nLet P be any arbitrary Post correspondence system and \\nand \\nare CFg\\'s constructed from the pairs of \\nstrings. \\n \\nmust be a CFL and let G1generates L1. That is, \\n \\n \\n \\n \\n \\nby De Morgan\\'s theorem, as shown already, any string, \\nrepresents a solution to the \\nPCP. Hence, \\ncontains all but those strings representing the solution to the PCP. \\n \\nLet \\nfor same CFG G2. \\n \\nIt is now obvious that \\nif and only if the PCP has no solutions, which is already proved to be \\nundecidable. Hence, the question “Is \\n?\" is undecidable. \\n \\niii. \\nwww.indiansbrain.com\\nLet \\nbe a CFG generating the language \\nand G2 be a CFG generating \\nwhere \\nand \\nare CFG.s constructed from same arbitrary instance of PCP. \\n \\niff \\n \\n \\ni.e. iff the PCP instance has no solutions as discussed in part (ii). \\n \\nHence the proof. \\n \\nTheorem : It is undecidable whether an arbitrary CFG is ambiguous. \\n \\nProof : Consider an arbitrary instance of PCP and construct the CFG\\'s \\nand \\nfrom the ordered pairs \\nof strings. \\n \\nWe construct a new grammar G from \\nand \\nas follows. \\n \\n where \\n \\n \\n \\n \\n \\nis same as that of \\nand \\n. \\n \\n \\n \\n \\n \\nThis constructions gives a reduction of PCP to the -------- of whether a CFG is ambiguous, thus leading to \\nthe undecidability of the given problem. That is, we will now show that the PCP has a solution if and only if G \\nis ambiguous. (where G is constructed from an arbitrary instance of PCP). \\n \\nOnly if Assume that \\nis a solution sequence to this instance of PCP. \\n \\nConsider the following two derivation in \\n. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBut , \\n \\n \\n \\n \\nis a solution to the PCP. Hence the same string of terminals \\nhas two derivations. Both these \\nderivations are, clearly, leftmost. Hence G is ambiguous. \\n \\nIf It is important to note that any string of terminals cannot have more than one derivation in \\nand \\n \\nBecause, every terminal string which are derivable under these grammars ends with a sequence of integers \\nThis sequence uniquely determines which productions must be used at every step of the derivation. \\n \\nHence, if a terminal string, \\n, has two leftmost derivations, then one of them must begin with \\nthe step. \\n \\nthen continues with derivations under \\n \\n \\nIn both derivations the resulting string must end with a sequence \\nfor same \\nThe reverse \\nof this sequence must be a solution to the PCP, because the string that precede in one case is \\n \\nand \\nin the other case. Since the string derived in both cases are identical, the \\n \\nsequence \\n \\nmust be a solution to the PCP. \\n \\nHence the proof \\nwww.indiansbrain.com\\nClass p-problem solvable in polynomial time: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNon deterministic polynomial time: \\n \\nA nondeterministic TM that never makes more than p(n) moves in any sequence of choices for \\nsome polynomial p is said to be non polynomial time NTM. \\n\\uf095 NP is the set of languags that are accepted by polynomial time NTM’s\\uf020\\n\\uf020\\n\\uf095 Many problems are in NP but appear not to be in p.\\uf020\\n\\uf095 One of the great mathematical questions of our age: is there anything in NP that is not in p?\\uf020\\nNP-complete problems: \\n\\uf020\\nIf We cannot resolve the “p=np question, we can at least demonstrate that certain problems in NP \\nare the hardest , in the sense that if any one of them were in P , then P=NP. \\n\\uf020\\n\\uf095 These are called NP-complete.\\uf020\\n\\uf020\\n\\uf095 Intellectual leverage: Each NP-complete problem’s apparent difficulty reinforces the belief \\nthat they are all hard.\\uf020\\n \\nMethods for proving NP-Complete problems: \\n \\n\\uf095 Polynomial time reduction (PTR): Take time that is some polynomial in the input size to \\nconvert instances of one problem to instances of another.\\uf020\\n\\uf020\\n\\uf095 If P1 PTR to P2 and P2 is in P1 the so is P1.\\uf020\\n\\uf095 Start by showing every problem in NP has a PTR to Satisfiability of Boolean formula.\\uf020\\n\\uf020\\n\\uf095 Then, more problems can be proven NP complete by showing that SAT PTRs to them \\ndirectly or indirectly.\\uf020\\nwww.indiansbrain.com\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Combined Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Ensure the TextLoader uses UTF-8 encoding\n",
    "loader = TextLoader(\"combined_theory_of_computation_content.txt\", encoding=\"utf-8\")\n",
    "loaded_data = loader.load()\n",
    "print(\"Loaded data successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'combined_theory_of_computation_content.txt'}, page_content='Automata Tutorial | Theory of Computation - JavatpointTutorials×PythonPython Django Numpy Pandas Tkinter Pytorch Flask OpenCVAI, ML and Data ScienceArtificial Intelligence Machine Learning Data Science Deep Learning TensorFlow Artificial Neural Network Matplotlib Python ScipyJavaJava Servlet JSP Spring Boot Spring Framework Hibernate JavaFX Java Web ServicesB.Tech and MCADBMS Data Structures Operating System Computer Network DAA Computer Organization Software Engineering Data MiningWeb TechnologyHTML CSS JavaScript Jquery Angular-8 React JS React Native Node JSSoftware TestingSoftware Testing Selenium JIRA JMeter Postman TestNG SoapUI CucumberInterview×Technical InterviewC C++ Php Java Python JavaScript TypeScriptJava InterviewJDBC Servlet Maven Jenkins Spring Spring Boot JDB Hibernate JSFWeb InterviewHTML CSS JavaScript Jquery Angular Node-JS AJAXDatabase InterviewDBMS SQL PL/SQL Oracle MySQL MongoDB Redis MariaDBCompany InterviewsIBM Adobe Microsoft Amazon TCS HCL Wipro DXC Accenture Capgemini Space X Ericsson Infosy IGate EXL IndiaMART SapientCompilerPythonJavaPhpCC++RHtmlJavascriptTypescriptSwiftHome Python Java JavaScriptHTML SQL PHP C# C++ DS Aptitude Reasoning Selenium DBMS C Andriod Interview QAutomata TutorialAutomata TutorialTheory of AutomataFinite AutomataTransition DiagramTransition TableDFAExamples of DFANFAExamples of NFAEliminating ε TransitionsConversion from NFA to DFAConversion from NFA with ε to DFAMinimization of DFARegular ExpressionRegular ExpressionExamples of Regular ExpressionConversion of RE to FAArden\\'s TheoremMoore MachineMealy MachineConversion from Mealy machine to Moore machineConversion from Moore machine to Mealy machineCFGContext-free GrammarDerivationDerivation TreeAmbiguity in GrammarUnambiguous GrammarSimplification of CFGChomsky\\'s Normal Form (CNF)Greibach Normal Form (GNF)PDAPushdown AutomataPDA AcceptanceNon-deterministic Pushdown AutomataCFG to PDA ConversionTuring MachineApplication of Different Automata | Theory of ComputationIntroduction to Computational Complexity TheoryAutomata and Game TheoryRecursive Descent Parsernext →Automata TutorialTheory of automata is a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. An automaton with a finite number of states is called a Finite automaton.In this tutorial, we are going to learn how to construct deterministic finite automata, non-deterministic finite automata, Regular expression, context-free grammar, context-free language, Push down automata, Turning machines, etc.PrerequisiteBefore learning Automata, you should have a basic understanding of string, language, alphabets, symbols.AudienceOur Automata Tutorial is designed to help beginners and professionals.ProblemsWe assure that you will not find any problem in this Automata Tutorial. But if there is any mistake, please post the problem in contact form.Next TopicTheory of Automatanext →Latest CoursesWe provides tutorials and interview questions of all technology like java tutorial, android, java frameworksContact info G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India[email\\xa0protected].Follow usLatest PostPRIVACY POLICYTutorialsJava Data Structures C Programming C++ Tutorial C# Tutorial PHP Tutorial HTML Tutorial JavaScript Tutorial jQuery Tutorial Spring TutorialInterview QuestionsTcs Intuit Wipro Adobe Infosys Amazon Accenture Cognizant Capgemini MicrosoftOnline CompilerC R C++ Php Java Html Swift Python JavaScript TypeScript© Copyright 2024 Javatpoint. All Rights Reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction of Theory of Computation - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA to DevelopmentNewly Launched!Android with KotlinGenerative AI & ChatGPTMaster Django FrameworkBecome AWS CertifiedFor Working ProfessionalsInterview 101: DSA & System DesignData Science Training ProgramJAVA Backend Development (Live)DevOps Engineering (LIVE)Software Testing & Automation (Live)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation CourseData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive Programming (Live)Full Stack Development with React & Node JS (Live)GATE Exam CoursesGATE CS & IT (Self-Paced)GATE DS & AI (Self-Paced)All CoursesTutorialsData Structures & AlgorithmsDSA for BeginnersData StructuresArraysMatrixStringsLinked ListStackQueueTreeGeneric TreeBinary TreeBinary Search TreeAVL TreeB TreeB+ TreeRed Black TreeTree Data Structure TutorialHeapHashingGraphSet Data StructureMap Data StructureAdvanced Data StructureData Structures TutorialAlgorithmsAnalysis of AlgorithmsSearching AlgorithmsLinear SearchBinary SearchSearching Algorithms TutorialSorting AlgorithmsSelection SortBubble SortInsertion SortMerge SortQuick SortHeap SortCounting SortRadix SortBucket SortSorting Algorithms TutorialGreedy AlgorithmsDynamic ProgrammingGraph AlgorithmsPattern SearchingRecursionBacktrackingDivide and ConquerMathematical AlgorithmsGeometric AlgorithmsBitwise AlgorithmsRandomized AlgorithmsBranch and BoundAlgorithms TutorialComplete DSA TutorialCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE SheetNetflix SDE SheetGoogle SDE SheetWipro Coding SheetInfosys Coding SheetTCS Coding SheetCognizant Coding SheetHCL Coding SheetDSA Cheat SheetsDSA Sheet for BeginnersSDE SheetsFAANG Coding SheetLove Babbaar SheetMass Recruiter SheetProduct-Based Coding SheetCompany-Wise Preparation SheetTop 100 DSA Interview Questions Topic-wise100 Days of CodePythonPython TutorialPython ExercisesPython List ExercisePython String ExercisePython Tuple ExercisePython Dictionary ExercisePython Set ExercisePython Excercises Topic wisePython QuizPython ProgramsAdvanced Python TutorialPython API TutorialPython Database TutorialPython JSONPython Cheat SheetPython ProjectsPython Interview QuestionsML & Data ScienceMachine LearningMachine Learning TutorialMaths for MLML Projects100 Days of Machine LearningData Science TutorialData Science PackagesPandas TutorialNumPy TutorialData VisualizationData Visualization with PythonData Visualization with RTableauPower BIData AnalysisData Analysis with PythonData Analysis with R100 Days of Data AnalyticsDeep LearningNLP TutorialOpenCV TutorialInterview QuestionsMachine Learning Interview QuestionsDeep Learning Interview QuestionsR Interview QuestionsSystem DesignSystem Design TutorialSoftware Design PatternsSystem Design RoadmapTop 10 System Design Interview QuestionsInterview CornerCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsPuzzles for InterviewsLanguagesCC++JavaPythonR TutorialC#SQLScalaPerlGo LanguageWeb DevelopmentHTMLHTML TutorialFree HTML CourseHTML Cheat SheetCSSCSS TutorialFree CSS CourseCSS Cheat SheetJavaScriptJavaScript TutorialJavaScript QuestionsJavaScript Cheat SheetDSA using JavaScriptFree JavaScript CourseJavaScript A to Z Complete GuideTypeScriptReactJSReactJS TutorialFree ReactJS CourseReactJS Cheat SheetNextJSNode.jsPHPAngularJSjQueryWeb Development Using PythonDjangoFlaskSeleniumPostmanGithubWeb Design100 Days of Web DevelopmentCS SubjectsOperating SystemDBMSComputer NetworksEngineering MathematicsComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOps And LinuxDevOps TutorialGITAWSKubernetesDockerMicrosoft Azure TutorialGoogle Cloud PlatformDevOps RoadmapDevOps Interview QuestionsLinuxLinux TutorialLinux Commands A-ZLinux Commands CheatsheetFile Permissions in LinuxLinux System AdministrationLinux Shell ScriptingLinux NetworkingLinux Interview QuestionsSchool LearningClass 8 Study MaterialClass 9 Study MaterialClass 10 Study MaterialClass 11Study MaterialClass 12 Study MaterialEnglish GrammarGfG SchoolCommerceGATEGATE Computer Science NotesLast Minute NotesGATE CS Solved PapersGATE CS Original Papers and Official KeysGATE CS 2025 SyllabusGATE DA 2025 SyllabusOther CS ExamsISROUGC NETGeeksforGeeks VideosJobsGet Hired: Apply for JobsCorporate Hiring SolutionsFiltered JobsJobs for FreshersJobs for ExperiencedAll JobsPracticePractice Coding ProblemsAll DSA ProblemsProblem of the DayCompany Wise Coding PracticeAmazonMicrosoftFlipkartExplore AllGfG SDE SheetPractice Problems Difficulty WiseBasicEasyMediumHardLanguage Wise Coding PracticeCPPJavaPythonCurated DSA ListsBeginner\\'s DSA SheetLove Babbar SheetTop 50 Array ProblemsTop 50 String ProblemsTop 50 DP ProblemsTop 50 Graph ProblemsTop 50 Tree ProblemsContestsJob-A-Thon Hiring ChallengeGfG Weekly [Rated Contest]All Contests and Events\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\n\\n                                        You\\'re all caught up!!\\n\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAptitudeEngineering MathematicsDiscrete MathematicsOperating SystemDBMSComputer NetworksDigital Logic and DesignC ProgrammingData StructuresAlgorithmsTheory of ComputationCompiler DesignComputer Org and Architecture \\n\\n\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGo PremiumShare Your ExperiencesAutomata TutorialAutomata _ IntroductionIntroduction of Theory of ComputationChomsky Hierarchy in Theory of ComputationApplications of various AutomataRegular Expression and Finite AutomataIntroduction of Finite AutomataArden\\'s Theorem in Theory of ComputationArden\\'s Theorem and Challenging Applications | Set 2L-graphs and what they represent in TOCHypothesis (language regularity) and algorithm (L-graph to NFA) in TOCRegular Expressions, Regular Grammar and Regular LanguagesHow to identify if a language is regular or notDesigning Finite Automata from Regular Expression (Set 1)Star Height of Regular Expression and Regular LanguageGenerating regular expression from Finite AutomataDesigning Deterministic Finite Automata (Set 1)Designing Deterministic Finite Automata (Set 2)DFA for Strings not ending with \"THE\"DFA of a string with at least two 0’s and at least two 1’sDFA for accepting the language L = {  anbm | n+m=even }DFA machines accepting odd number of 0’s or/and even number of 1’sDFA of a string in which 2nd symbol from RHS is \\'a\\'Union process in DFAConcatenation process in DFADFA in LEX code which accepts even number of zeros and even number of onesConversion from NFA to DFAMinimization of DFAReversing Deterministic Finite AutomataComplementation process in DFAKleene\\'s Theorem in TOC | Part-1Mealy\\xa0and\\xa0Moore\\xa0Machines in TOCDifference Between Mealy Machine and Moore MachineCFGRelationship between grammar and language in Theory of ComputationSimplifying Context Free GrammarsClosure Properties of Context Free LanguagesUnion and Intersection of Regular languages with CFLConverting Context Free Grammar to Chomsky Normal FormConverting Context Free Grammar to Greibach Normal FormPumping Lemma in Theory of ComputationCheck if the language is Context Free or NotAmbiguity in Context free Grammar and Context free LanguagesOperator grammar and precedence parser in TOCContext-sensitive Grammar (CSG) and Language (CSL)PDA (Pushdown Automata)Introduction of Pushdown AutomataPushdown Automata Acceptance by Final StateConstruct Pushdown Automata for given languagesConstruct Pushdown Automata for all length palindromeDetailed Study of PushDown AutomataNPDA for accepting the language  L = {an bm cn | m,n>=1}NPDA for accepting the language L = {an bn cm | m,n>=1}NPDA for accepting the language  L = {an bn | n>=1}NPDA for accepting the language  L = {am b(2m) | m>=1}NPDA for accepting the language  L = {am bn cp dq | m+n=p+q ; m,n,p,q>=1}Construct Pushdown automata for L = {0n1m2m3n | m,n ≥ 0}Construct Pushdown automata for L = {0n1m2(n+m) | m,n ≥ 0}NPDA for accepting the language L = {ambnc(m+n) | m,n ≥ 1}NPDA for accepting the language L = {amb(m+n)cn | m,n ≥ 1}NPDA for accepting the language L = {a2mb3m | m ≥ 1}NPDA for accepting the language L = {amb(2m+1) | m ≥ 1}NPDA for accepting the language L = {aibjckdl | i==k or j==l,i>=1,j>=1}Construct Pushdown automata for L = {a(2*m)c(4*n)dnbm | m,n ≥ 0}NPDA for L =  {0i1j2k | i==j or j==k ; i , j , k >= 1}NPDA for accepting the language L = {anb(2n) | n>=1} U {anbn | n>=1}NPDA for the language L ={w∈ {a,b}*| w contains equal no. of a\\'s and b\\'s}Turing MachineRecursive and Recursive Enumerable Languages in TOCTuring Machine in TOCTuring Machine for additionTuring machine for subtraction | Set 1Turing machine for multiplicationTuring machine for copying dataConstruct a Turing Machine for language L = {0n1n2n | n≥1}Construct a Turing Machine for language L = {wwr | w ∈ {0, 1}}Construct a Turing Machine for language L = {ww | w ∈ {0,1}}Construct Turing machine for L = {an bm a(n+m) | n,m≥1}Construct a Turing machine for L = {aibjck | i*j = k; i, j, k ≥ 1}Turing machine for 1\\'s and 2’s complementRecursive and Recursive Enumerable Languages in TOCTuring Machine for subtraction | Set 2Halting Problem in Theory of ComputationTuring Machine as ComparatorDecidabilityDecidable and Undecidable Problems in Theory of ComputationUndecidability and Reducibility in TOCComputable and non-computable problems in TOCTOC Interview preparationLast Minute Notes - Theory of ComputationTOC  Quiz and PYQ\\'s in TOCTheory of Computation - GATE CSE Previous Year QuestionsRegular languages and finite automataContext free languages and Push-down automataRecursively enumerable sets and Turing machinesUndecidabilityDSA to Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction of Theory of Computation\\n\\n\\n\\nLast Updated : \\n27 Sep, 2024\\n\\n\\n\\n\\n\\n\\nSummarize\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\nAutomata theory (also referred to as the Theory Of Computation) is a branch of Computer Science and Mathematics that studies how machines compute functions and solve problems. This field is mainly focused on mathematical structures called automata and is crucial for the purpose of studying processes occurring in discrete systems.\\nWhat is Automata Theory?\\nIn automata theory, scientists and engineers can predict the behavior of computing systems thereby improving problem-solving approaches. Originally developed to describe and explain the dynamics of systems, automata theory is the theoretical base of the formal languages theory, grammar, and computational complexity.\\nBasic Terminologies of Theory of Computation\\nNow, let’s understand the basic terminologies, which are important and frequently used in the Theory of Computation.\\xa0\\nSymbol\\nA symbol (often also called a character) is the smallest building block, which can be any alphabet, letter, or picture.\\xa0\\n\\nAlphabets (Σ)\\nAlphabets are a set of symbols, which are always finite.\\xa0\\n\\nString\\xa0\\nA string is a finite sequence of symbols from some alphabet. A string is generally denoted as w and the length of a string is denoted as |w|.\\xa0\\nEmpty string is the string with zero occurrence of symbols, represented as ε.\\nNumber of Strings (of length 2) that can be generated over the alphabet {a, b}:                     -   -                     a   a                     a   b                     b   a                     b   bLength of String |w| = 2Number of Strings = 4Conclusion:For alphabet {a, b} with length n, number of strings can be generated = 2n.\\n__mask-blockquote__index=1__\\n\\nThe Theory of Computation explores automata, languages, and complexity. If you want to dive deeper into this subject for GATE, the GATE CS Self-Paced Course covers it extensively.\\n\\nClosure Representation in TOC\\nL+: It is a Positive Closure that represents a set of all strings except Null or ε-strings.\\nL*: It is “Kleene Closure“, that represents the occurrence of certain alphabets for given language alphabets from zero to the infinite number of times. In which ε-string is also included.\\nFrom the above two statements, it can be concluded that:\\nL* = εL+\\nExample:(a) Regular expression for language accepting all combination of g\\'s over Σ={g}:                                         R = g*                               R={ε,g,gg,ggg,gggg,ggggg,...}(b) Regular Expression for language accepting all combination of g\\'s over Σ={g} : R = g+                               R={g,gg,ggg,gggg,ggggg,gggggg,...}\\nNote: Σ* is a set of all possible strings(often power set(need not be unique here or we can say multiset) of string) So this implies that language is a subset of Σ*.This is also called a “Kleene Star”.\\nKleene Star is also called a “Kleene Operator” or “Kleene Closure”. Engineers and IT professionals make use of Kleene Star to achieve all set of strings which is to be included from a given set of characters or symbols. It is one kind of Unary operator. In Kleene Star methodology all individual elements of a given string must be present but additional elements or combinations of these alphabets can be included to any extent.\\nExample:Input String: \"GFG\".Σ* = { ε,\"GFG\",\"GGFG\",\"GGFG\",\"GFGGGGGGGG\",\"GGGGGGGGFFFFFFFFFGGGGGGGG\",...}  (Kleene Star is an infinite set but if we provide any grammar rules then it can work as a finite set.Please note that we can include ε string also in given Kleene star representation.)\\nLanguage\\nA language is a set of strings, chosen from some Σ* or we can say- ‘A language is a subset of Σ* ‘. A language that can be formed over ‘ Σ ‘ can be Finite or Infinite.\\nExample of Finite Language:           L1 = { set of string of 2 }         L1 = { xy, yx, xx, yy }Example of Infinite Language:         L1 = { set of all strings starts with \\'b\\' }         L1 = { babb, baa, ba, bbb, baab, ....... }\\nConclusion\\nIt is an important branch of computation that is concerned with formal languages, and automata theory in particular. It provides a basis for other courses such as Turing machines and computational complexity that are very important in computer science.\\nIntroduction of Theory of Computation – FAQs\\nWhat is the relevance of the automata theory in computer science?\\n\\nAutomata theory is used in modeling computational problems hence enhancing the understanding and design of systems such as compilers, interpreters among others.\\n\\nwhat is the purpose of using Kleene Star in the study of formal languages?\\n\\nThe Kleene Star extends symbols from a given alphabet where one is able to create infinite strings from it or even the null string.\\n\\nIs it possible to implement automata theory into real life?\\n\\nOf course, automata theory has found its use in certain areas like compiler design, artificial intelligence, network security and natural language processing.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nabhishek1 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Article\\n\\n\\n\\nAutomata Tutorial\\n\\n\\n\\n\\nNext Article\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\n\\n\\n\\n\\n\\n\\nRead More\\n\\n\\n\\nSimilar Reads\\n\\n\\n\\nIntroduction to Computation Complex Theory\\nBroad Overview : Complexity theory, in a nutshell, a complexity word is a quite fancy word, literally, it sounds complex, but it is not an intimidating topic. What it really means is analyzing the program or we can say analyzing the efficiency of the program, figuring out whether the program is correct, figuring out whether one program is better th\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nIntroduction To Grammar in Theory of Computation\\nPrerequisite - Theory of ComputationGrammar :It is a finite set of formal rules for generating syntactically correct sentences or meaningful correct sentences.Constitute Of Grammar :Grammar is basically composed of two basic elements - Terminal Symbols - Terminal symbols are those which are the components of the sentences generated using a grammar\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nTheory of Computation - GATE CSE Previous Year Questions\\nSolving GATE Previous Year\\'s Questions (PYQs) not only clears the concepts but also helps to gain flexibility, speed, accuracy, and understanding of the level of questions generally asked in the GATE exam, and that eventually helps you to gain good marks in the examination. Previous Year Questions help a candidate practice and revise for GATE, whic\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\n\\nRelationship between grammar and language in Theory of Computation\\nA grammar is a set of production rules which are used to generate strings of a language. In this article, we have discussed how to find the language generated by a grammar and vice versa as well. Language generated by a grammar - Given a grammar G, its corresponding language L(G) represents the set of all strings generated from G. Consider the foll\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nTheory of Computation | Regular languages and finite automata | Question 2\\nWhat is the complement of the language accepted by the NFA shown below? (A) A (B) B (C) C (D) D Answer: (B) Explanation: Quiz of this QuestionPlease comment below if you find anything wrong in the above post\\n\\n\\n\\n1 min read\\n\\n\\n\\n\\nArden\\'s Theorem in Theory of Computation\\nArden\\'s theorem state that: \"If P and Q are two regular expressions over \"∑\", and if P does not contain \"∈\" , then the following equation in R given by R = Q + RP has a unique solution i.e., R = QP*.\" That means, whenever we get any equation in the form of R = Q + RP, then we can directly replace it with R = QP*. So, here we will first prove that R\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nDecidability Table in Theory of Computation\\nPrerequisite - Undecidability, Decidable and undecidable problems Identifying languages (or problems*) as decidable, undecidable or partially decidable is a very common question in GATE. With correct knowledge and ample experience, this question becomes very easy to solve. A language is undecidable if it is not decidable. An undecidable language ma\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\nAccording to Chomsky hierarchy, grammar is divided into 4 types as follows: Type 0 is known as unrestricted grammar.Type 1 is known as context-sensitive grammar.Type 2 is known as a context-free grammar.Type 3 Regular Grammar.Type 0: Unrestricted Grammar: Type-0 grammars include all formal grammar. Type 0 grammar languages are recognized by turing\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nPumping Lemma in Theory of Computation\\nThere are two Pumping Lemmas, which are defined for 1. Regular Languages, and 2. Context - Free Languages Pumping Lemma for Regular Languages For any regular language L, there exists an integer n, such that for all x ? L with |x| ? n, there exists u, v, w ? ?*, such that x = uvw, and (1) |uv| ? n (2) |v| ? 1 (3) for all i ? 0: uviw ? L In simple te\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nDecidable and Undecidable Problems in Theory of Computation\\nIn the Theory of Computation, problems can be classified into decidable and undecidable categories based on whether they can be solved using an algorithm. A decidable problem is one for which a solution can be found in a finite amount of time, meaning there exists an algorithm that can always provide a correct answer. While an undecidable problem i\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nHalting Problem in Theory of Computation\\nTo understand better the halting problem, we must know Decidability , Undecidability and Turing machine , decision problems and also a theory named as Computability theory and Computational complexity theory. Some important terms: Computability theory - The branch of theory of computation that studies which problems are computationally solvable usi\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nAutomata Theory | Set 2\\nFollowing questions have been asked in GATE CS 2012 exam. 1) What is the complement of the language accepted by the NFA shown below? Assume ∑ = {a} and ε is the empty string (A) Φ (B) ε (C) a (D) {a, ε} Answer (B) The given alphabet ∑ contains only one symbol {a} and the given NFA accepts all strings with any number of occurrences of \\'a\\'. In other\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 3\\nFollowing questions have been asked in GATE CS 2011 exam. 1) The lexical analysis for a modern language such as Java needs the power of which one of the following machine models in a necessary and sufficient sense? (A) Finite state automata (B) Deterministic pushdown automata (C) Non-deterministic pushdown automata (D) Turing machine Answer (A) Lex\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nAutomata Theory | Set 4\\nFollowing questions have been asked in GATE CS 2011 exam. 1) Let P be a regular language and Q be context-free language such that Q ⊆ P. (For example, let P be the language represented by the regular expression p*q* and Q be {pnqn|n ∈ N}). Then which of the following is ALWAYS regular? (A) P ∩ Q (B) P - Q (C) ∑* - P (D) ∑* - Q\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nAutomata Theory | Set 5\\nFollowing questions have been asked in GATE CS 2009 exam. 1) S --> aSa| bSb| a| b ;The language generated by the above grammar over the alphabet {a,b} is the set of (A) All palindromes. (B) All odd length palindromes. (C) Strings that begin and end with the same symbol (D) All even length palindromes. Answer (B) The strings accepted by language are\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 7\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider L= {(TM) | TM is the Turing machine that halts on all input and L(TM)= L\\' for some undecidable language L\\'}. Here, (TM) is the encoding of a Turing machine as a string over alphabet {0, 1} then L is: (A) decidable and recursively enumerable (B) decidable and recursive (C) decid\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 8\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Which one of the following language is Regular? (A) {wxwR | w,x ∈ (a+b)+} (B) {wxwR | w ∈ (a+b)*, x ∈ {a,b}} (C) {wwRx | w,x ∈ (a+b)+} (D) {wwR | w ∈ (a+b)*} Explanation: (A) It is correct, since this language can form regular expression which is {{ a(a + b)+a } + {b(a + b)+b}}, i.e., s\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nAutomata Theory | Set 9\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider the following two statements with respect to Countability: Statement-1: If X union of \\'Y\\' is uncountable, then both set \\'X\\' and set \\'Y\\' must be uncountable. Statement-2: The Cartesian product of two countable sets \\'X\\' and \\'Y\\' is countable. Which of the following option is true\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 10\\nThese questions for practice purpose of GATE CS Exam. Ques-1: Consider the following statements: X: For any language either a language L or its complement L\\' must be finite.Y: DFA for language which contains epsilon must have initial state as final state.Z: Non-deterministic finite automata is more powerful than deterministic finite automata. Which\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nRegular Graph in Graph Theory\\nPrerequisite: Graph Theory Basics – Set 1, Set 2 Regular Graph: A graph is called regular graph if degree of each vertex is equal. A graph is called K regular if degree of each vertex in the graph is K. Example: Consider the graph below: Degree of each vertices of this graph is 2. So, the graph is 2 Regular. Similarly, below graphs are 3 Regular an\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n5 Color Theorem in Graph Theory\\nThe graph is a data structure that is used extensively in real-life. Planar Graph: If a graph can be drawn on the plane without crossing, it is said to be planar. Coloring of a simple graph is the assignment of color to each vertex of the graph so that no two adjacent vertices are assigned the same color. Bi-Partite Graphs: A bipartite graph, also\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nMathematics | Graph Theory Basics - Set 1\\nA graph is a data structure that is defined by two components : A node or a vertex.An edge E or ordered pair is a connection between two nodes u,v that is identified by unique pair(u,v). The pair (u,v) is ordered because (u,v) is not same as (v,u) in case of directed graph.The edge may have a weight or is set to one in case of unweighted graph.Cons\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nMathematics | Graph theory practice questions\\nProblem 1 - There are 25 telephones in Geeksland. Is it possible to connect them with wires so that each telephone is connected with exactly 7 others. Solution - Let us suppose that such an arrangement is possible. This can be viewed as a graph in which telephones are represented using vertices and wires using the edges. Now we have 25 vertices in\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\nSet Theory Operations in Relational Algebra\\nRelational Algebra in DBMS These Set Theory operations are the standard mathematical operations on set. These operations are Binary operations that are, operated on 2 relations unlike PROJECT, SELECT and RENAME operations. These operations are used to merge 2 sets in various ways. The set operation is mainly categorized into the following: Union op\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nApplications of Group Theory\\nGroup theory is the branch of mathematics that includes the study of elements in a group. Group is the fundamental concept of algebraic structure like other algebraic structures like rings and fields. Group: A non-empty set G with * as operation, (G, *) is called a group if it follows the closure, associativity, identity, and inverse properties. Pr\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nQuotient Group in Group Theory\\nWe can say that \"o\" is the binary operation on set G if: G is a non-empty set & G * G = { (a,b): a, b∈ G } and o: G * G --> G. Here, aob denotes the image of ordered pair (a,b) under the function/operation o.Example - \"+\" is called a binary operation on G (any non-empty set ) if & only if: a+b ∈G; ∀ a,b ∈G and a+b give the same result ev\\n\\n\\n\\n12 min read\\n\\n\\n\\n\\nTypes of Sets in Set Theory\\nIn mathematics, a Set is a fundamental concept representing a collection of well-defined objects or elements. Sets are typically denoted by capital letters, and the individual elements within a set are listed in curly braces, separated by commas. For example, A={1,2,3,4,5} represents a set A with elements 1, 2, 3, 4, and 5. The order of elements wi\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\n\\nMathematics | Graph Theory Basics - Set 2\\nGraph theory is a basic branch of discrete mathematics that mainly focuses on the relationship between objects. These objects are called vertices and these vertices are joined by edges. Graphs are common in computer science, network analysis, and many other everyday uses because they provide a good representation of connection, relationship, and pr\\n\\n\\n\\n10 min read\\n\\n\\n\\n\\nGroup in Maths: Group Theory\\nGroup theory is one of the most important branches of abstract algebra which is concerned with the concept of the group. A group consists of a set equipped with a binary operation that satisfies four key properties: specifically, it includes property of closure, associativity, the existence of an identity element, and the existence of inverse eleme\\n\\n\\n\\n13 min read\\n\\n\\n\\n\\nMatching (Graph Theory)\\nMatching (Graph Theory): In graph theory, matching is a fundamental concept used to describe a set of edges without common vertices. Matchings are used in various applications such as network design, job assignments, and scheduling. Understanding matchings is essential for solving problems involving optimal pairings and resource allocation. Table o\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nGATE CS\\n\\n\\nTheory of Computation\\n \\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Please Login to comment...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n76k+ interested Geeks \\n\\n\\n\\nCore Computer Science Subject for Interview Preparation \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n28k+ interested Geeks \\n\\n\\n\\nGATE Computer Science & Information Technology - 2025 \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n12k+ interested Geeks \\n\\n\\n\\nCBSE Class 12 Computer Science \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nExplore More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Corporate & Communications Address:- A-143, 9th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305) | Registered Address:- K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalCareersIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramExploreJob-A-Thon Hiring ChallengeHack-A-ThonGfG Weekly ContestOffline Classes (Delhi/NCR)DSA in JAVA/C++Master System DesignMaster CPGeeksforGeeks VideosGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonData Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython Programming ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceAccountancyBusiness StudiesEconomicsManagementHR ManagementFinanceIncome TaxDatabasesSQLMYSQLPostgreSQLPL/SQLMongoDBPreparation CornerCompany-Wise Recruitment ProcessResume TemplatesAptitude PreparationPuzzlesCompany-Wise PreparationCompaniesCollegesCompetitive ExamsJEE AdvancedUGC NETUPSCSSC CGLSBI POSBI ClerkIBPS POIBPS ClerkMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsRecent ArticlesFree Online ToolsTyping TestImage EditorCode FormattersCode ConvertersCurrency ConverterRandom Number GeneratorRandom Password GeneratorWrite & EarnWrite an ArticleImprove an ArticlePick Topics to WriteShare your ExperiencesInternships \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\\n\\n        acknowledge that you have read and understood our\\n\\n        Cookie Policy &\\n\\n        Privacy Policy\\n\\n\\n\\n        Got It !\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\nThis improvement is locked by another user right now. You can suggest the changes for now and it will be under \\'My Suggestions\\' Tab on Write.\\nYou will be notified via email once the article is available for improvement.\\n\\n                        Thank you for your valuable feedback!\\n\\n                    \\n\\nSuggest changes\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\n\\nmin 4 words, max CharLimit:2000\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n                        Can\\'t choose a topic to write? click here for suggested topics\\n                    \\n\\n\\n\\n                       Write and publish your own Article\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTheory Of Computation Notes PDF, Syllabus ✅ [2021] B Tech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHomeBest Courses\\n\\nGoogle Professional Certificates\\nHuman Resource\\n\\nHuman Resource Human Resource Management Human Resource Planning Organizational Culture Organization Development Organizational Behavior\\nLearning DealsAll Blog PostManagement\\n\\nBusiness Statistics Lean Six Sigma Management Operation Management Research Methodology Operations Research Procurement Management Production Management Supply Chain Strategic Management\\nMarketing\\n\\nEconomics Brand Management Business Business Communication Business Law Entrepreneurship Consumer Behaviour Marketing Essentials Marketing Management Sales Management Shark Tank India\\nBusiness Tech\\n\\nProject Management Business Analytics Management Information System Enterprise Resource Planning Technologies Cloud Computing\\nAbout\\n\\nAbout Us Cookie Policy DMCA Policy Disclaimer Contact Us\\nToggle website search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Category\\nClose\\n\\n\\n\\n\\n\\n\\nHome\\nBest Courses\\n\\nGoogle Professional Certificates\\n\\n\\nHuman Resource\\n\\nHuman Resource\\nHuman Resource Management\\nHuman Resource Planning\\nOrganizational Culture\\nOrganization Development\\nOrganizational Behavior\\n\\n\\nLearning Deals\\nAll Blog Post\\nManagement\\n\\nBusiness Statistics\\nLean Six Sigma\\nManagement\\nOperation Management\\nResearch Methodology\\nOperations Research\\nProcurement Management\\nProduction Management\\nSupply Chain\\nStrategic Management\\n\\n\\nMarketing\\n\\nEconomics\\nBrand Management\\nBusiness\\nBusiness Communication\\nBusiness Law\\nEntrepreneurship\\nConsumer Behaviour\\nMarketing Essentials\\nMarketing Management\\nSales Management\\nShark Tank India\\n\\n\\nBusiness Tech\\n\\nProject Management\\nBusiness Analytics\\nManagement Information System\\nEnterprise Resource Planning\\nTechnologies\\nCloud Computing\\n\\n\\nAbout\\n\\nAbout Us\\nCookie Policy\\nDMCA Policy\\nDisclaimer\\nContact Us\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\nHome>B Tech Study Material>Theory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\nPost last modified:30 March 2021\\nReading time:27 mins read\\nPost category:B Tech Study Material\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload Theory of Computation Notes PDF, syllabus for B Tech, BCA, MCA 2021. We provide a complete theory of computation pdf. Theory of Computation lecture notes includes a theory of computation notes, theory of computation book, theory of computation courses, theory of computation syllabus, theory of computation question paper, MCQ, case study, theory of computation interview questions and available in theory of computation pdf form.\\nTheory of Computation Notes\\nTheory of Computation subject is included in B Tech CSE, BCA, MCA, M Tech. So, students can able to download theory of computation notes pdf.\\n\\nTable of Content1 Theory of Computation Syllabus2 Theory of Computation PDF3 Theory of Computation Notes3.1 What is Theory of Computation?3.2 Theory of Computation Handwritten Notes4 Theory of Computation Interview Questions5 Theory of Computation Question Paper6 Theory of Computation Book\\n\\nTheory of Computation Notes can be downloaded in theory of computation pdf from the below article\\n\\nTheory of Computation Syllabus\\nA detailed theory of computation syllabus as prescribed by various Universities and colleges in India are as under. You can download the syllabus in the theory of computation pdf form.\\nUnit I\\n\\n\\n\\n\\n\\n\\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural Representations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, Proof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets Strings, Languages, Applications of Automata Theory. \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, Extending the Transition Function to Strings, The Language of a DFA \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The Languages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite Automata With Epsilon-Transitions: Uses of Î-Transitions, The Formal Notation for an Î-NFA, Epsilon-Closures, Extended Transitions and Languages for Î-NFA’s, Eliminating Î- Transitions.\\n Unit II\\nRegular Expressions and Languages: Regular Expressions: The Operators of regular Expressions, Building Regular Expressions, Precedence of Regular-Expression Operators, Precedence of Regular-Expression Operators Finite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions by Eliminating States, Converting Regular Expressions to Automata. \\nAlgebraic Laws for Regular Expressions: Properties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of the Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular Languages, Equivalence and Minimization of Automata, \\nContext-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations Using a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, Parse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and Parse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive Inferences, Applications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: Ambiguous Grammars, Removing Ambiguity.\\n\\n\\n\\n\\n\\nUnit III \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical Notation for PDA’s, Instantaneous Descriptions of a PDA, \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack to Final State, From Final State to Empty Stack Equivalence of PDA’s and CFG’s: From Grammars to Pushdown Automata, From PDA’s to Grammars \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and Deterministic PDA’s, DPDA’s and Context-Free Languages, DPDA’s and Ambiguous Grammars \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The Pumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, Decision Properties of CFL’s\\nUnit IV\\nIntroduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for Turing Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, Turing Machines and Halting Programming Techniques for Turing Machines, Extensions to the Basic Turing Machine, Restricted Turing Machines, Turing Machines and Computers\\nUNIT V\\nRecursive And Recursively Enumerable Languages: Properties of recursive and recursively enumerable languages, Universal Turing machine, The Halting problem, Undecidable problems about TMs. Context-sensitive language and linear bounded automata (LBA), Chomsky hierarchy, Decidability, Post’s correspondence problem (PCP), undecidability of PCP.\\n\\nTheory of Computation PDF\\n\\n\\n\\n\\n\\n Theory of Computation Notes PDF(How to download) Theory of Computation Notes Download  Theory of Computation Book Download Theory of Computation Syllabus Download  Theory of Computation Question Paper Download  Theory of Computation Interview Questions Download \\n\\nTheory of Computation Notes\\nWhat is Theory of Computation?\\n\\n\\n\\n\\n\\n Download PDF\\n\\n\\n\\n\\n\\n\\nTheory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory.\\nTheory of Computation Handwritten Notes\\n\\n\\n\\n Download PDF\\n\\nTheory of Computation Interview Questions\\nSome of the theory of computation interview questions are mentioned below. You can download the QnA in theory of computation pdf form.\\nWhat is TOC?What is Automata Theory in TOC?What is Regular Language in TOC?What is Grammer and Language in TOC?What is Null String in TOC?What is Grammer and Language in TOC?What is Regular Expression in TOC?What is Linear Bound Automata in TOC?What is Context-Free Language(CFL) in TOC?What is Recursive Language in TOC?What is the use of Lexical Analysis in TOC?What is Chomsky Classification of Languages in TOC?Define Kleene Star Closure in TOC?What is the Productions in TOC? Explain Production Rules.\\n\\nTheory of Computation Question Paper\\nIf you have already studied the theory of computation notes, now it’s time to move ahead and go through previous year theory of computation question paper. \\n Download PDF Fill Before Download\\nIt will help you to understand question paper pattern and type of theory of computation questions and answers asked in B Tech, BCA, MCA, M Tech theory of computation exam. You can download the syllabus in theory of computation pdf form.\\n\\nTheory of Computation Book\\nBelow is the list of theory of computation book recommended by the top university in India.\\nIntroduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, R.Motwani & J.D.Ullman (3rd Edition) – Pearson EducationTheory of Computer Science (Automata Language & Computations), by K.L.Mishra & N. Chandrashekhar, PHI\\n\\nDownload B Tech (CS) Study Material\\n\\n\\n\\nComputer Networks Notes ✅ [2020] PDF – Download \\n\\nComputer Networks Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Networks Notes) \\n\\n\\n\\nComputer Graphics Notes ✅ [2020] PDF – Download \\n\\nComputer Graphics Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Graphics Notes)\\n\\n\\n\\nOperating System Notes ✅ [2020] PDF – Download  \\n\\nOperating System Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operating System Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download\\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Compiler Design Notes)\\n\\n\\n\\nData Structures Notes ✅ [2020] PDF – Download \\n\\nData Structures Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Structures Notes)\\n\\n\\n\\nDigital Image Processing Notes ✅ [2020] PDF – Download \\n\\nDigital Image Processing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Digital Image Processing Notes) \\n\\n\\n\\nTheory of Computation Notes ✅ [2020] PDF – Download \\n\\nTheory of Computation Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Theory of Computation Notes) \\n\\n\\n\\nComputer Organization and Architecture Notes ✅ [2020] PDF – Download \\n\\nComputer Organization and Architecture Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Computer Organization and Architecture Notes) \\n\\n\\n\\nCloud Computing Notes ✅ [2020] PDF – Download \\n\\nCloud Computing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Cloud Computing Notes) \\n\\n\\n\\nData Communication and Networking Notes ✅ [2020] PDF – Download \\n\\nData Communication and Networking Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Communication and Networking Notes) \\n\\n\\n\\n Software Engineering Notes ✅ [2020] PDF – Download \\n\\nSoftware Engineering Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Software Engineering Notes) \\n\\n\\n\\nWeb Technologies Notes ✅ [2020] PDF – Download \\n\\nWeb Technologies Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Web Technologies Notes) \\n\\n\\n\\nMicroprocessor and Microcontrollers Notes ✅ [2020] PDF – Download \\n\\nMicroprocessor and Microcontrollers Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Microprocessor and Microcontrollers Notes) \\n\\n\\n\\nDesign and Analysis of Algorithm Notes ✅ [2020] PDF – Download \\n\\nDesign and Analysis of Algorithm Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Design and Analysis of Algorithm Notes) \\n\\n\\n\\nOperation Research Notes ✅ [2020] PDF – Download \\n\\nOperation Research Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operation Research Notes) \\n\\n\\n\\nDatabase Management Systems Notes ✅ [2020] PDF – Download \\n\\nDatabase Management Systems Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Database Management Systems Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download \\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Compiler Design Notes)\\n\\n\\n\\n\\n\\nIn the above article, a student can download theory of computation notes for B Tech, BCA, MCA, M Tech. Theory of Computation lecture notes and study material includes theory of computation notes, theory of computation books, theory of computation syllabus, theory of computation question paper, theory of computation case study, theory of computation interview questions, theory of computation courses in theory of computation pdf form.\\n\\nGo On, Share & Help your Friend\\n Did we miss something in B.Tech Computer Science Notes or You want something More? Come on! Tell us what you think about our post on Theory of Computation Notes | PDF, Syllabus, Book | B Tech 2020 in the comments section and Share this post with your friends.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead more articles\\n Previous PostWeb Technologies Notes | PDF, Syllabus, Book | B Tech 2021 Next PostDigital Image Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n\\nTags: B Tech, B Tech CS Books, B Tech CS Syllabus, B Tech CSE Books, B Tech CSE Notes, B Tech CSE Syllabus, B Tech Notes, B Tech Study Material, Computer Science Notes PDF, CS Notes, CSE Notes, Theory of Computation Book, Theory of Computation Course, Theory of Computation Interview Questions, Theory of Computation Notes, Theory of Computation PDF, Theory of Computation PPT, Theory of Computation Question Paper, Theory of Computation Syllabus, Theory of Computation Tutorial\\n\\n\\nYou Might Also Like\\n\\n\\n\\n\\nDigital Signal Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n \\n\\n\\nOperation Research Notes | PDF, Syllabus | MBA, B Tech 2024\\n\\n18 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDatabase Management Systems Notes | PDF | B Tech 2021\\n\\n22 March 2020\\n\\n\\n\\nAdvanced Java Programming Notes | PDF | B Tech (2024)\\n\\n23 November 2020\\n\\n\\n\\n\\n \\n\\n\\nComputer Networks Notes | PDF, Syllabus, Books | B Tech (2024)\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nSoftware Engineering Notes | PDF, Syllabus | B Tech 2021\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDesign and Analysis of Algorithm Notes PDF | B Tech (2024)\\n\\n20 March 2020\\n\\n\\n\\nDigital Communication Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nAnalog Communication PDF | Notes, Syllabus B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nBusiness Intelligence Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nHow to Download Notes on Geektonight\\n\\n27 June 2020\\n\\n\\n\\nWireless Networks Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n\\nLeave a Reply Cancel replyYou must be logged in to post a comment. \\n\\n\\n\\n\\n\\n\\nAll CategoryAll Category\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies\\nTreasury Management in Banking\\nTwitter Certification\\nUncategorized\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nWorld\\'s Best Online Courses at One Place \\n\\n\\n\\nWe’ve spent the time in finding, so you can spend your time in learning \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDigital Marketing \\n\\n\\n\\nGoogle Ads CourseFacebook Ads CourseSEO CourseInstagram Marketing CourseSEM CourseSocial Media CourseEmail Marketing CoursePinterest CourseChatbot CourseBlogging CourseContent Marketing CourseWooCommerce CourseClickbank Affiliate Marketing CourseAffiliate Marketing CourseAmazon Affiliate Marketing CourseShopify, eCommerce & Dropshipping CourseExcel Data Analysis CourseWordPress CourseGoogle Tag Manager CourseGoogle Analytics CourseDigital Marketing CourseYoutube Marketing CourseBing Ads CourseSocial Media Analytics Course \\n\\n\\n\\n\\n\\n\\n\\nBusiness \\n\\n\\n\\nProduct Strategy CourseSales CourseBrand Strategy CourseBusiness Law CourseStrategic Management CourseMarketing Analytics CourseBusiness Strategy CourseMarketing Management CourseHuman Resource CourseProduct Management CourseProduct Marketing CourseB2B Marketing CourseGrowth Hacking CoursePeople HR Analytics CourseEntrepreneurship CourseBusiness Statistics CourseProject Management CourseNegotiation CourseTime Management CourseLeadership CourseCareer Development CourseStress Management CourseAnxiety Management CourseDesign Thinking CourseEmotional Intelligence CourseTeam Building CourseBusiness Analytics CourseDigital Transformation Course \\n\\n\\n\\n\\n\\n\\n\\nPersonal Growth \\n\\n\\n\\nEnglish Grammar CourseVocabulary CourseSoft Skills CoursePublic Speaking CoursePhotography CourseBody Language CourseCommunication Skills CourseInterview Preparation CourseProductivity CourseMindfulness CourseMemory CourseSelf DisciplineSpeed ReadingAcademic WritingCopywriting CourseScientific Writing CourseNovel Writing CourseAcademic Writing CourseTravel Writer CourseCreative Writing CourseInterior Design CourseGraphic Design CourseDrawing CourseDigital Art CourseUI UX Designer Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFinance \\n\\n\\n\\nMutual Fund CourseFinancial Analysis CoursePersonal Finance CourseCost Accounting CourseAudit CourseFintech CourseValue Investing CourseTrading CourseFinancial Modeling CourseInvestment CourseProject Finance CourseStock Trading CourseFinancial & Capital Markets CourseAccounting CourseFinancial Engineering Course \\n\\n\\n\\n\\n\\n\\n\\nFinTech \\n\\n\\n\\nNFT CourseMongoDB CoursejQuery CourseBlockchain CourseCryptocurrency CourseSwift CourseAWS CourseRedux CourseGo CourseDeFi CourseSolidity CourseMetaverse CourseDjango CourseJIRA CourseConversion Rate Optimization (CRO) CourseAnalytics CourseCustomer Loyalty Course \\n\\n\\n\\n\\n\\n\\n\\nLanguage \\n\\n\\n\\nEnglish SpeakingKorean LanguageGerman LanguageSpanish LanguageFrench Language Italian Language Russian Language Japanese Language Arabic LanguageSwedish LanguageHindi LanguagePortuguese LanguageDutch LanguageLatin LanguageTurkish LanguageHungarian LanguageVietnameseAmerican AccentPronunciationSpelling Courses \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTech \\n\\n\\n\\nData Science CourseR Programming CourseBig Data CourseSQL CourseData Analytics CourseMachine Learning CoursePython CourseSQL Data Science CourseArtificial Intelligence CourseCloud Computing CourseData Warehouse CourseNLP Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment \\n\\n\\n\\nReact JS CoursesFront End Development CourseFull Stack Web Developer CourseC++ CourseData Engineering CourseHTML & CSS3 CourseMicrosoft SQL CourseMySQL CourseJava CourseJavaScript CourseTypeScript CourseBack End Development CourseDatabase CourseGraphQL Course \\n\\n\\n\\n\\n\\n\\n\\nExam Prep \\n\\n\\n\\nGRE PrepGMAT PrepMCAT PrepIELTS PrepDAT PrepPSAT PrepCFA PrepOAT PrepACT PrepLSAT PrepFRM PrepSSAT PrepCPA PrepTESOL PrepSAT PrepSSAT Prep \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython \\n\\n\\n\\nPython CourseDeep Learning Python CoursePython Data Science CoursePython for Marketing CoursePython for Finance CoursePython Pandas CoursePython Data Visualization CoursePython Machine Learning CoursePython Data Processing CoursePython Scripting CoursePython for Data Analysis CoursePython Data Structure CourseNLP Python CourseMatplotlib CourseData Cleaning CourseStatistical Modeling CourseKeras CoursePytorch CourseMachine Learning Finance Course \\n\\n\\n\\n\\n\\n\\n\\nTech \\n\\n\\n\\nSCADA CourseASP.net CourseScrum CourseSpring Boot and MVC CourseIT Support & Help Desk CourseRuby on Rails CourseKubernetes CourseDocker CourseNodeJs CourseAngular CoursePHP CourseAPI CourseAlteryx CoursePower BI CourseTableau CourseData Visualization CourseDAX CourseData Streaming CourseRegex CourseQlik Sense CoursePlotly Dash CourseData Modeling Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment \\n\\n\\n\\nAndroid CourseiOS Development CourseFlutter CourseKotlin CourseIonic CourseXamarin CourseVirtual Reality CourseMatlab CourseGit & GitHub CourseSelenium CourseShell Scripting CourseARKit CourseGame Design CourseUnity CourseUnreal Engine CourseGame Development CourseBlender CourseDreamweaver CourseVisual Studio CourseC# (C-Sharp) CourseBootstrap Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChild Care \\n\\n\\n\\nChild Nutrition CourseBaby Massage CourseChildcare & Early Education CourseBaby Sign Language CourseKids Art & Drawing CourseKids Coding CourseChild Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Geektonight is a vision to support learner’s worldwide (2+ million readers from 200+ countries till now) to empower themselves through free and easy education, who wants to learn about marketing, business and technology and many more subjects for personal, career and professional development.\\n\\n \\n\\nConnect With Us\\n\\nOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tab \\n\\n \\n\\nMoreAbout UsOpens in a new tabDisclaimerOpens in a new tabCookie PolicyOpens in a new tabPrivacy PolicyOpens in a new tabDMCA PolicyOpens in a new tab \\n\\nCategoriesCategories\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies\\nTreasury Management in Banking\\nTwitter Certification\\nUncategorized\\n\\n\\n \\n\\n\\n\\n\\n\\nCopyright 2023 Geektonight\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch this website\\n\\nType then hit enter to search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMBA NOTES \\nTHEORY OF COMPUTATION\\nBCS601T \\nTHEORY OF COMPUTATION \\nUnit – I \\n \\n(12 hours) \\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural\\nRepresentations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, \\nProof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets \\nStrings, Languages, Applications of Automata Theory. \\n \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of \\na Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, \\nExtending the Transition Function to Strings, The Language of a DFA \\n \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The \\nLanguages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite \\nAutomata With Epsilon-Transitions: Uses of \\uf0ce-Transitions, The Formal Notation for an \\n \\n\\uf0ce-NFA, Epsilon-Closures, Extended Transitions and Languages for \\uf0ce-NFA’s, Eliminating \\uf0ce-\\nTransitions. \\n \\nUnit – II \\n(12 hours) \\nRegular  Expressions\\nand  Languages:  Regular  Expressions:  The  Operators  of  regular\\nExpressions, Building \\nRegular  Expressions,  Precedence  of  Regular-Expression  Operators,\\n \\nPrecedence of Regular-Expression Operators \\n \\nFinite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting \\nDFA’s to Regular Expressions, Converting DFA’s to R egular Expressions by Eliminating States, \\nConverting Regular Expressions to Automata. \\n \\nAlgebraic Laws for Regular Expressions: \\n \\nProperties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of \\nthe Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular \\nLanguages, Equivalence and Minimization of Automata, \\n \\nUnit – III \\n(12 hours) \\nGrammar : Types of Grammar \\nContext-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations \\nUsing a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, \\n \\nParse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and \\nParse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive \\nInferences, \\n \\nApplications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: \\nAmbiguous Grammars, Removing Ambiguity From Grammars, Leftmost Derivations as a Way to \\nExpress Ambiguity, Inherent Anbiguity \\nwww.indiansbrain.com\\n \\nUnit – IV \\n \\n(12 hours) \\n \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical \\nNotation for PDA’s, Instantaneous Descriptions of a PDA, \\n \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack \\nto Final State, From Final State to Empty Stack \\n \\nEquivalence of PDA’s and CFG’s: From Grammars to Pu shdown Automata, From PDA’s to \\nGrammars \\n \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and \\nDeterministic PDA’s, DPDA’s and Context-Free La nguages, DPDA’s and Ambiguous Grammars \\n \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The \\nPumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, \\nDecision Properties of CFL’s \\n \\nUnit – V \\n \\n(12 hours) \\nIntroduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for \\nTuring Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, \\nTuring Machines and Halting \\n \\nProgramming Techniques for Turing Machines, Extensions to the Basic Turing Machine, \\nRestricted Turing Machines, Turing Machines and Computers, \\n \\nUndecidability: A Language That is Not Recursively Enumerable, Enumerating the Binary \\nStrings, Codes for Turing Machines, The Diagonalization Language \\n \\nAn Undecidable Problem That Is RE: Recursive Languages, Complements of Recursive and RE \\nlanguages, The Universal Languages, Undecidability of the Universal Language \\n \\nUndecidable Problems About Turing Machines: Reductions, Turing Machines That Accept the \\nEmpty Language. Post’s Correspondence Problem: Definition of Post’s Correspondence Problem, \\nThe “Modified” PCP, Other Undecidable Prob lems: Undecidability of Ambiguity for CFG’s \\n \\n \\n \\n \\nText Book: \\n \\n1. Introduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, \\nR.Motwani & J.D.Ullman (3rd Edition) – Pearson Education \\n \\nwww.indiansbrain.com\\n \\nUNIT - I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWhat is TOC? \\n \\nIn theoretical computer science, the theory of computation is the branch that deals with \\nwhether and how efficiently problems can be solved on a model of computation, using an \\nalgorithm. The field is divided into three major branches: automata theory, computability theory \\nand computational complexity theory. \\n \\nIn order to perform a rigorous study of computation, computer scientists work with a \\nmathematical abstraction of computers called a model of computation. There are several \\nmodels in use, but the most commonly examined is the Turing machine. \\n \\nAutomata theory \\n \\nIn theoretical computer science, automata theory is the study of abstract machines (or more \\nappropriately, abstract \\'mathematical\\' machines or systems) and the computational problems that \\ncan be solved using these machines. These abstract machines are called automata. \\n \\nThis automaton consists of \\n \\n\\uf095 states (represented in the figure by circles),\\uf020\\n\\uf095 and transitions (represented by arrows).\\uf020\\n \\nAs the automaton sees a symbol of input, it makes a transition (or jump) to another state, \\naccording to its transition function (which takes the current state and the recent symbol as \\nits inputs). \\nUses of Automata: compiler design and parsing. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIntroduction to formal proof: \\nBasic Symbols used : \\nU – Union \\n∩- Conjunction \\n \\nϵ - Empty String \\nΦ – NULL set \\n7- negation \\n \\n‘ – compliment \\n= > implies \\nwww.indiansbrain.com\\nAdditive inverse: a+(-a)=0 \\nMultiplicative inverse: a*1/a=1 \\nUniversal set U={1,2,3,4,5} \\nSubset A={1,3} \\nA’ ={2,4,5} \\nAbsorption law: AU(A ∩B) = A, A∩(AUB) = A \\n \\nDe Morgan’s Law: \\n \\n(AUB)’ =A’ ∩ B’ \\n(A∩B)’ = A’ U B’ \\nDouble compliment \\n(A’)’ =A \\n \\nA ∩ A’ = Φ \\n \\nLogic relations: \\na b = > 7a U b \\n7(a∩b)=7a U 7b \\n \\nRelations: \\n \\nLet a and b be two sets a relation R contains aXb. \\nRelations used in TOC: \\nReflexive: a = a \\nSymmetric: aRb = > bRa \\nTransition: aRb, bRc = > aRc \\n \\nIf a given relation is reflexive, symmentric and transitive then the relation is called equivalence \\nrelation. \\n \\nDeductive proof: Consists of sequence of statements whose truth lead us from some \\ninitial statement called the hypothesis or the give statement to a conclusion statement. \\n \\n \\n \\n \\nAdditional forms of proof: \\nProof of sets \\nProof by contradiction \\nProof by counter example \\n \\nDirect proof (AKA) Constructive proof: \\nIf p is true then q is true \\n \\nEg: if a and b are odd numbers then product is also an odd \\nnumber. Odd number can be represented as 2n+1 \\n \\na=2x+1, b=2y+1 \\nproduct of a X b = (2x+1) X (2y+1) \\n= 2(2xy+x+y)+1 = 2z+1 (odd number) \\nwww.indiansbrain.com\\n \\n \\n \\nProof by contrapositive: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof by Contradiction: \\n \\nH and not C implies falsehood. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBe regarded as an observation than a theorem. \\n \\n \\n \\n \\n \\n \\n \\n \\nFor any sets a,b,c if a∩b = Φ and c is a subset of b the prove that a∩c \\n=Φ Given : a∩b=Φ and c subset b \\n \\nAssume: a∩c  Φ \\n \\nThen \\n \\n \\n= > a∩b Φ = > a∩c=Φ(i.e., the assumption is wrong) \\nwww.indiansbrain.com\\nProof by mathematical Induction: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLanguages : \\n \\nThe languages we consider for our discussion is an abstraction of natural languages. That is, \\nour focus here is on formal languages that need precise and formal definitions. Programming \\nlanguages belong to this category. \\n \\nSymbols : \\n \\nSymbols are indivisible objects or entity that cannot be defined. That is, symbols are the atoms \\n \\nof the world of languages. A symbol is any single object such as begin, or do. \\n \\nAlphabets : \\n \\nAn alphabet is a finite, nonempty set of symbols. The alphabet of a language is \\nnormally denoted by \\n. When more than one alphabets are considered for discussion, \\nthen \\nsubscripts may be used (e.g.    \\netc) or sometimes other symbol like G may also be \\n \\nintroduced. \\n \\n \\n \\n \\n \\n \\n \\n \\nExample : \\n \\nStrings or Words over Alphabet : \\n \\nA string or word over an alphabet   \\nis a finite sequence of concatenated symbols of     \\n. \\n , a, 0, 1, #, \\nwww.indiansbrain.com\\nExample : 0110, 11, 001 are three strings over the binary alphabet { 0, 1 } . \\n \\naab, abcb, b, cc are four strings over the alphabet { a, b, c }. \\n \\nIt is not the case that a string over some alphabet should contain all the symbols from the alpha-\\nbet. For example, the string cc over the alphabet { a, b, c } does not contain the symbols a and b. \\nHence, it is true that a string over an alphabet is also a string over any superset of that alphabet. \\n \\nLength of a string : \\nThe number of symbols in a string w is called its length, denoted by |w|. \\n \\nExample : | 011 | = 4, |11| = 2, | b | = 1 \\n \\nConvention : We will use small case letters towards the beginning of the English alphabet \\nto denote symbols of an alphabet and small case letters towards the end to \\n \\ndenote strings over an alphabet. That is, \\n (symbols) and \\n \\nare strings. \\n \\nSome String Operations : \\nLet \\nand \\nbe two strings. The concatenation of x and y \\n \\ndenoted by xy, is the string   \\n. That is, the concatenation of x and y \\n \\ndenoted by xy is the string that has a copy of x followed by a copy of y without any intervening \\nspace between them. \\n \\nExample : Consider the string 011 over the binary alphabet. All the prefixes, suffixes and \\nsubstrings of this string are listed below. \\n \\nPrefixes:  , 0, 01, 011. \\n \\nSuffixes:  , 1, 11, 011. \\n \\nSubstrings:  , 0, 1, 01, 11, 011. \\n \\nNote that x is a prefix (suffix or substring) to x, for any string x and is a prefix (suffix or \\nsubstring) to any string. \\n \\nA string x is a proper prefix (suffix) of string y if x is a prefix (suffix) of y and x 蝤 y. \\n \\nIn the above example, all prefixes except 011 are proper prefixes. \\n \\nPowers of Strings : For any string x and integer \\n, we use \\nto denote the string formed \\nby sequentially concatenating n copies of x. We can also give an inductive \\ndefinition of \\nas follows: \\n= e, if n \\n= 0 ; otherwise \\n \\nwww.indiansbrain.com\\nExample : If x = 011, then  \\n= 011011011,  \\n= 011 and \\n \\nPowers of Alphabets : \\n \\nWe write \\n(for some integer k) to denote the set of strings of length k with symbols \\n \\nfrom \\n. In other words, \\n \\n= { w | w is a string over  \\n and | w | = k}. Hence, for any alphabet,   \\ndenotes the set \\n \\nof all strings of length zero. That is,   \\n= { e }. For the binary alphabet { 0, 1 } we have \\n \\nthe following. \\n \\n \\n \\n \\n \\n \\n \\n \\nThe set of all strings over an alphabet \\n is denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nThe set  \\n contains all the strings that can be generated by iteratively concatenating sym- \\nbols from      \\nany number of times. \\n \\nExample : If \\n= { a, b }, then \\n= {  , a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, …}. \\n \\nPlease note that if \\n, then \\n that is \\n. It may look odd that one can proceed \\nfrom the empty set to a non-empty set by iterated concatenation. But there is a reason for this \\nand we accept this convention \\n \\nThe set of all nonempty strings over an alphabet     \\nis denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nNote that  \\nis infinite. It contains no infinite strings but strings of arbitrary lengths. \\n \\nReversal : \\nFor any string  \\nthe reversal of the string is \\n. \\n \\nAn inductive definition of reversal can be given as follows: \\nwww.indiansbrain.com\\nLanguages : \\nA language over an alphabet is a set of strings over \\nthat alphabet. Therefore, a \\n \\nlanguage L is any subset of \\n. That is, any \\nis a language. \\nExample : \\n \\n \\n1. F is the empty language.  \\n \\n2.\\nis a language for any \\n. \\n \\n3. {e} is a language for any \\n. Note that, \\n. Because the language F does not \\n \\ncontain any string but {e} contains one string of length zero. \\n4. The set of all strings over { 0, 1 } containing equal number of 0\\'s and 1\\'s. \\n \\n5. The set of all strings over {a, b, c} that starts with a. \\n \\nConvention : Capital letters A, B, C, L, etc. with or without subscripts are normally used \\nto denote languages. \\n \\nSet operations on languages : Since languages are set of strings we can apply set operations to \\nlanguages. Here are some simple examples (though there is nothing new in it). \\n \\n \\nUnion : A string \\n \\n \\n \\n \\n \\n \\niff \\nor \\n \\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 0, 11, 01, 011, 111 } \\nIntersection  : \\nA   string, \\nxϵ  L1 \\n∩ L2 \\niff    x   ϵ  L1    and   x   ϵ  L2    .\\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 01 } \\n \\nComplement :  Usually, \\nis the universe that a complement is taken with respect to. \\n \\nThus for a language L, the complement is L(bar) = { \\n| \\n}. \\n \\nExample : Let L = { x | |x| is even }. Then its complement is the language { \\n| |x| is \\n \\nodd }. \\n \\nSimilarly we can define other usual set operations on languages like relative \\ncom-plement, symmetric difference, etc. \\n \\nReversal of a language : \\nThe reversal of a language L, denoted as \\n, is defined as:  \\n. \\n \\nExample : \\n \\n1.  Let L = { 0, 11, 01, 011 }. Then \\n= { 0, 11, 10, 110 }.\\nwww.indiansbrain.com\\n2.  Let L = { \\n| n is an integer }. Then  \\n=  { \\n| n is an integer }. \\n \\nLanguage concatenation : The concatenation of languages       \\nand \\nis defined as \\n \\n= { xy | \\nand \\n}. \\n \\nExample : { a, ab }{ b, ba } = { ab, aba, abb, abba }. \\n \\nNote that , \\n1. \\n  in general. \\n2. \\n \\n \\n3. \\n \\n \\nIterated concatenation of languages : Since we can concatenate two languages, we also repeat this to \\nconcatenate any number of languages. Or we can concatenate a language with itself any \\n \\nnumber of times. The operation L with itself n times. This is \\ndefined formally as follows: \\n \\n \\n \\n \\n \\n \\nExample :  Let L = { a, ab }. Then according to the definition, \\nwe have \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand so on. \\n \\n \\n \\nKleene\\'s Star operation : The Kleene star operation on a language L, denoted as is defined as \\nfollows : \\n \\n= ( Union n in N ) \\n \\n= \\n \\n \\n= { x | x is the concatenation of zero or more strings from L } \\n \\ndenotes the concatenation of \\nwww.indiansbrain.com\\nThus \\nis the set of all strings derivable by any number of concatenations of strings in \\nL. It is also useful to define \\n \\n=, i.e., all strings derivable by one or more concatenations of strings in L. That is \\n \\n= (Union n in N and n >0) \\n \\n= \\n \\nExample :  Let L = { a, ab }. Then we have, \\n \\n= \\n \\n= {e} \\n{a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\n= \\n \\n= {a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\nNote : \\nis in  \\n, for every language L, including . \\n \\nThe previously introduced definition of   \\nis an instance of Kleene star. \\n \\n \\n \\n \\n \\n(Generates) \\n(Recognizes) \\nGrammar \\nLanguage \\n  Automata \\n \\nAutomata: A algorithm or program that automatically recognizes if a particular string belongs to \\nthe language or not, by checking the grammar of the string. \\n \\nAn automata is an abstract computing device (or machine). There are different varities of such \\nabstract machines (also called models of computation) which can be defined mathematically. \\n \\nEvery Automaton fulfills the three basic requirements. \\n \\n• \\nEvery automaton consists of some essential features as in real computers. It has a mech-\\nanism for reading input. The input is assumed to be a sequence of symbols over a given \\nalphabet and is placed on an input tape(or written on an input file). The simpler automata \\ncan only read the input one symbol at a time from left to right but not change. Powerful \\nversions can both read (from left to right or right to left) and change the input. \\nwww.indiansbrain.com\\n\\uf095 The automaton can produce output of some form. If the output in response to an input \\nstring is binary (say, accept or reject), then it is called an accepter. If it produces an out-\\nput sequence in response to an input sequence, then it is called a transducer(or \\nautomaton with output).\\uf020\\n\\uf020\\n• \\nThe automaton may have a temporary storage, consisting of an unlimited number of \\ncells, each capable of holding a symbol from an alphabet ( whcih may be different from \\nthe input alphabet). The automaton can both read and change the contents of the storage \\ncells in the temporary storage. The accusing capability of this storage varies depending \\non the type of the storage. \\n \\n• \\nThe most important feature of the automaton is its control unit, which can be in any \\none of a finite number of interval states at any point. It can change state in some de-\\nfined manner determined by a transition function. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1: The figure above shows a diagrammatic representation of a generic \\nautoma-tion. \\n \\nOperation of the automation is defined as follows. \\n \\nAt any point of time the automaton is in some integral state and is reading a particular symbol \\nfrom the input tape by using the mechanism for reading input. In the next time step the automa-\\nton then moves to some other integral (or remain in the same state) as defined by the transition \\nfunction. The transition function is based on the current state, input symbol read, and the content \\nof the temporary storage. At the same time the content of the storage may be changed and the \\ninput read may be modifed. The automation may also produce some output during this transition. \\nThe internal state, input and the content of storage at any point defines the configuration of the \\nautomaton at that point. The transition from one configuration to the next ( as defined by the \\ntransition function) is called a move. Finite state machine or Finite Automation is the simplest \\ntype of abstract machine we consider. Any system that is at any point of time in one of a finite \\nnumber of interval state and moves among these states in a defined manner in response to some \\ninput, can be modeled by a finite automaton. It doesnot have any temporary storage and hence a \\nrestricted model of computation. \\nwww.indiansbrain.com\\nFinite Automata \\n \\nAutomata (singular : automation) are a particularly simple, but useful, model of compu-\\ntation. They were initially proposed as a simple model for the behavior of neurons. \\n \\nStates, Transitions and Finite-State Transition System : \\n \\n \\nLet us first give some intuitive idea about a state of a system and state transitions before \\ndescribing finite automata. \\n \\nInformally, a state of a system is an instantaneous description of that system which gives all \\nrelevant information necessary to determine how the system can evolve from that point on. \\n \\nTransitions are changes of states that can occur spontaneously or in response to inputs to the \\nstates. Though transitions usually take time, we assume that state transitions are instantaneous \\n(which is an abstraction). \\n \\nSome examples of state transition systems are: digital systems, vending machines, etc. A system \\n \\ncontaining only a finite number of states and transitions among them is \\ncalled a finite-state transition system. \\n \\nFinite-state transition systems can be modeled abstractly by a mathematical model called \\nfinite automation \\n \\nDeterministic Finite (-state) Automata \\n \\nInformally, a DFA (Deterministic Finite State Automaton) is a simple machine that reads an in-\\nput string -- one symbol at a time -- and then, after the input has been completely read, decides \\nwhether to accept or reject the input. As the symbols are read from the tape, the automaton can \\nchange its state, to reflect how it reacts to what it has seen so far. A machine for which a deter-\\nministic code can be formulated, and if there is only one unique way to formulate the code, then \\nthe machine is called deterministic finite automata. \\n \\nThus, a DFA conceptually consists of 3 parts: \\n \\n \\n \\n \\n1. A tape to hold the input string. The tape is divided into a finite number of cells. Each \\ncell holds a symbol from \\n. \\n2. A tape head for reading symbols from the tape \\n3. A control , which itself consists of 3 things: \\n \\no \\nfinite number of states that the machine is allowed to be in (zero or more states \\nare designated as accept or final states), \\n \\no a current state, initially set to a start state, \\nwww.indiansbrain.com\\no a state transition function for changing the current state. \\n \\nAn automaton processes a string on the tape by repeating the following actions until the \\ntape head has traversed the entire string: \\n \\n1. The tape head reads the current tape cell and sends the symbol s found there to \\nthe control. Then the tape head moves to the next cell. \\n \\n2. he control takes s and the current state and consults the state transition function to \\nget the next state, which becomes the new current state. \\n \\nOnce the entire string has been processed, the state in which the automation enters is examined. \\n \\nIf it is an accept state , the input string is accepted ; otherwise, the string is rejected . Summariz- \\n \\ning all the above we can formulate the following formal definition: \\n \\n \\nDeterministic Finite State Automaton : A Deterministic Finite State Automaton (DFA) is \\n \\na 5-tuple : \\n \\n\\uf095 Q is a finite set of states.\\uf020\\n• \\nis a finite set of input symbols or alphabet \\n \\n\\uf095 \\nis the “next state” transition function (which is total ). Intuitively, \\nis\\n a \\nfunction that tells which state to move to in response to an input, i.e., if M is in \\n \\nstate q and sees input a, it moves to state      \\n. \\n \\n\\uf095 \\nis the start state.\\uf020\\n• \\nis the set of accept or final states. \\n \\nAcceptance of Strings : \\n \\nA DFA accepts a string   \\nif there is a sequence of states     \\nin Q \\n \\nsuch that \\n \\n1.  \\nis the start state. \\n2.  \\nfor all \\n. \\n \\n3. \\n \\nLanguage Accepted or Recognized by a DFA : \\n \\nThe language accepted or recognized by a DFA M is the set of all strings accepted by M , and \\n \\nis denoted by \\ni.e. \\nThe  notion \\nof \\nacceptance can also be made more precise by extending the transition function \\n.\\n \\nExtended transition function : \\nwww.indiansbrain.com\\nExtend \\n(which is function on symbols) to a function on strings, i.e. . \\n \\n \\n \\nThat is, \\n is the state the automation reaches when it starts from the state q and finish \\nprocessing the string w. Formally, we can give an inductive definition as follows: \\n \\nThe language of the DFA M is the set of strings that can take the start state to one of \\nthe accepting states i.e. \\n \\n \\nL(M) = { \\n| M accepts w } \\n \\n= {\\n| \\n} \\n \\n \\nExample 1 : \\n \\n \\n \\n \\n \\n \\n \\n \\nis the start state \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is a formal description of a DFA. But it is hard to comprehend. For ex. The language of the \\nDFA is any string over { 0, 1} having at least one 1 \\n \\nWe can describe the same DFA by transition table or state transition diagram as follow-\\ning: \\n \\n \\n \\n \\nTransition Table : \\n \\n0    1 \\nwww.indiansbrain.com\\n \\n \\nIt is easy to comprehend the transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExplanation :  We cannot \\nreach find state \\nw/0 or in the i/p string. There can be any no. \\nof 0\\'s at the beginning. \\n( The self-loop at \\non label 0 indicates it ). Similarly there \\ncan be any no. of 0\\'s & 1\\'s in any order at the end of the string. \\n \\nTransition table : \\n \\nIt is basically a tabular representation of the transition function that takes two arguments (a \\nstate and a symbol) and returns a value (the “next state”). \\n \\n• \\nRows correspond to states, \\n• \\nColumns correspond to input symbols, \\n• \\nEntries correspond to next states \\n• \\nThe start state is marked with an arrow \\n• \\nThe accept states are marked with a star (*). \\n \\n \\n \\n0    1 \\n \\n \\n \\n(State) Transition diagram : \\n \\nA state transition diagram or simply a transition diagram is a directed graph which can \\nbe constructed as follows: \\n \\n1.  For each state in Q there is a node. \\n2. There is a directed edge from node q to node p labeled a iff \\n . (If there are \\nseveral input symbols that cause a transition, the edge is labeled by the list of these \\nsymbols.) \\n3. There is an arrow with no source into the start state. \\n4. Accepting states are indicated by double circle. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n5. \\n \\n6. Here is an informal description how a DFA operates. An input to a DFA can be any \\n \\nstring. \\nPut a pointer to the start state q. Read the input string w from left \\n \\nto right, one symbol at a time, moving the pointer according to the transition  \\n \\nfunction, \\n.  If the next symbol of w is a and the pointer is on state p, move the \\n \\npointer to \\n. When the end of the input string w is encountered, the pointer is on \\n \\nsome state, r. The string is said to be accepted by the DFA if \\nand \\n \\nrejected if \\n. Note that there is no formal mechanism for moving the pointer. \\n7. A language \\nis said to be regular if L = L(M) for some DFA M. \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying \\ncertain recursive rules as given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) is a RE \\n \\nii) is a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\n \\nIf \\n \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\nwww.indiansbrain.com\\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive \\nREs) by a finite no of applications of the recursive step (given in 2). \\n \\nExample : Let \\n = { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this \\nexpression by applying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n4 \\n(1+  ) \\nRule 2(iv) & Step 3 \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated \\nwith every RE). We will see later that REs are used to attribute regular languages. \\n \\n \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We \\ncan define the language L(r) associated with (or described by) a REs as follows. \\n \\n1. is the RE describing the empty language i.e. L( ) = . \\n \\n2. is a RE describing the language {\\n} i.e. L( ) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\nwww.indiansbrain.com\\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,. } {0} \\n{1} \\n \\n= {  , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nConsider the RE ab + c. The language described by the RE can be thought of either \\nL(a)L(b+c) or L (ab) L(c) as provided by the rules (of languages described by REs) \\ngiven already. But these two represents two different languages lending to \\nambiguity. To remove this ambiguity we can either \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. \\nLike other algebras mod in mathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) \\noperator. \\n \\nii) It is also important to note that concatenation & union (+) operators are \\nassociative and union operation is commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language \\nL(ab) L(c) i.e. it should be grouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For \\nexample, the language represented by the RE a(b+c) is L(a)L(b+c). \\nwww.indiansbrain.com\\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language \\nL(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all \\nstrings over {0,1} which are either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an \\neven number of 0\\'s followed by an odd number of 1\\'s i.e. \\n \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE \\nrr, \\ndenotes \\nr, and so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of \\nconsecutive 1\\'s} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and \\nwhat goes before is completely arbitrary. Considering these observations we can write \\nthe REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE \\n(0+1)*11(0+1)*+(0+1)*00(0+1)* represents the set of string over {0,1} that contains \\nthe substring 11 or 00. \\n \\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the \\nset of strings over {0,1} that contains exactly two 1\\'s. The presence of two 1\\'s in the \\nRE and any no of 0\\'s before, between and after the 1\\'s ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1\\'s. \\n \\nSolution : There must be at least two 1\\'s in the RE somewhere and what comes before, \\nbetween, and after is completely arbitrary. Hence we can write the RE as \\n(0+1)*1(0+1)*1(0+1)* . But following two REs also represent the same language, each \\nensuring presence of least two 1\\'s somewhere in the string \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\nwww.indiansbrain.com\\nL(r) = {\\n has no pair of consecutive 1\\'s} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We \\nobserver that, whenever a 1 occurs, it must be immediately followed by a 0. This \\nsubstring may be preceded & followed by any no of 0\\'s. So the final RE must be a \\nrepetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the RE is \\n(0*100*)*. But in this case the strings ending in 1 or consisting of all 0\\'s are not \\naccounted for. Taking these observations into consideration, the final RE is r = \\n(0*100*)(1+ )+0*(1+\\n). \\n \\nAlternative Solution : \\n \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as \\nr = (0+10)*(1+ ).This is a shorter expression but represents the same language. \\n \\nRegular Expression and Regular Language : \\n \\nEquivalence(of REs) with FA : \\n \\nRecall that, language that is accepted by some FAs are known as Regular language. \\nThe two concepts : REs and Regular language are essentially same i.e. (for) every \\nregular language can be developed by (there is) a RE, and for every RE there is a \\nRegular Langauge. This fact is rather suprising, because RE approach to describing \\nlanguage is fundamentally differnet from the FA approach. But REs and FA are \\nequivalent in their descriptive power. We can put this fact in the focus of the following \\nTheorem. \\n \\nTheorem : A language is regular iff some RE describes it. \\n \\nThis Theorem has two directions, and are stated & proved below as a separate lemma \\n \\n \\nRE to FA : \\n \\nREs denote regular languages : \\n \\nLemma : If L(r) is a language described by the RE r, then it is regular i.e. there is a FA \\nsuch that L(M)\\nL(r). \\n \\nProof : To prove the lemma, we apply structured index on the expression r. First, we \\n \\nshow how to construct FA for the basis elements: \\n, and for any \\n. Then we show \\nhow to combine these Finite Automata into Complex Automata that accept the Union, \\nConcatenation, Kleen Closure of the languages accepted by the original smaller \\nautomata. \\nwww.indiansbrain.com\\nUse of NFAs is helpful in the case i.e. we construct NFAs for every REs which are \\nrepresented by transition diagram only. \\n \\nBasis : \\n \\n\\uf095 Case (i) : \\n. Then \\n. Then \\nand the following NFA N \\nrecognizes L(r). Formally \\nwhere Q = {q} \\nand \\n.\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf095 Case (ii) : \\n. \\n, and the following NFA N accepts L(r). Formally\\uf020\\nwhere \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince the start state is also the accept step, and there is no any transition defined, it \\nwill accept the only string \\nand nothing else. \\n \\n\\uf095 Case (iii) : r = a for some \\n. Then L(r) = {a}, and the following NFA \\nN accepts L(r).\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFormally, \\nwhere \\nfor \\nor \\n \\n \\n \\n \\n \\nInduction : \\nwww.indiansbrain.com\\nAssume that the start of the theorem is true for REs \\nand \\n. Hence we can assume \\nthat we have automata \\nand \\nthat accepts languages denoted by REs \\nand \\n, \\n \\nrespectively i.e. \\nand \\n. The FAs are represented \\nschematically as shown below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEach has an initial state and a final state. There are four cases to consider. \\n \\n\\uf095 Case (i) : Consider the RE \\ndenoting the language \\n. We \\nconstruct FA \\n, from \\nand \\nto accept the language denoted by RE \\nas \\nfollows :\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new (initial) start state \\nand give \\n- transition to the initial state of \\n and \\n.This is the initial state of \\n. \\n \\n\\uf095 Create a final state \\nand give \\n-transition from the two final state of \\nand \\n. \\nis the only final state of \\nand final state of \\nand \\nwill be \\nordinary states in \\n.\\uf020\\n\\uf095 All the state of \\nand \\nare also state of \\n.\\uf020\\nwww.indiansbrain.com\\n\\uf095 All the moves of \\nand \\nare also moves of \\n. [ Formal Construction] \\n \\n \\nIt is easy to prove that \\n \\nProof: To show that \\nwe must show that \\n \\n= \\n \\n \\n= \\nby following transition of \\n \\n \\nStarts at initial state \\nand enters the start state of either \\nor \\nfollwoing the \\ntransition i.e. without consuming any input. WLOG, assume that, it enters the start state \\nof \\n. From this point onward it has to follow only the transition of \\nto enter the final \\n \\nstate of \\n, because this is the only way to enter the final state of M by following the e-\\ntransition.(Which is the last transition & no input is taken at hte transition). Hence the \\n \\nwhole input w is considered while traversing from the start state of \\nto the final \\nstate of \\n. Therefore \\nmust accept \\n. \\n \\nSay, \\nor \\n. \\n \\n \\nWLOG, say \\n \\nTherefore when \\nprocess the string w , it starts at the initial state and enters the final \\nstate when w consumed totally, by following its transition. Then \\nalso accepts w, by \\nstarting at state \\nand taking \\n-transition enters the start state of \\n-follows the moves \\n \\nof \\nto enter the final state of \\nconsuming input w thus takes -transition to \\n. Hence proved \\n \\n\\uf095 Case(ii) : Consider the RE \\ndenoting the language \\n. We construct \\nFA \\nfrom \\n& \\nto accept \\nas follows :\\uf020\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new start state \\nand a new final state \\n \\n1. Add \\n- transition from \\no \\nto the start state of \\n \\n \\no \\nto \\n \\n \\no final state of \\nto the start state of \\n \\n \\n2. All the states of \\nare also the states of \\n. \\nhas 2 more states than that of \\nnamely \\nand \\n. \\n3. All the moves of \\nare also included in \\n. \\n \\nBy the transition of type (b), \\ncan accept . \\nBy the transition of type (a), \\ncan enters the initial state of \\nw/o any input and then \\nfollow all kinds moves of \\nto enter the final state of \\nand then following \\n-transition \\ncan enter \\n. Hence if any \\nis accepted by \\nthen w is also accepted by \\n. By the \\ntransition of type (b), strings accepted by \\ncan be repeated by any no of times & thus \\naccepted by \\n. Hence \\naccepts \\nand any string accepted by \\nrepeated (i.e. \\n \\nconcatenated) any no of times. Hence \\n \\nCase(iv) : Let \\n=(\\n). Then the FA \\nis also the FA for (\\n), since the use of \\nparentheses does not change the language denoted by the expression \\n \\nNon-Deterministic Finite Automata \\nNondeterminism is an important abstraction in computer science. Importance of \\nnondeterminism is found in the design of algorithms. For examples, there are many \\nproblems with efficient nondeterministic solutions but no known efficient deterministic \\nsolutions. ( Travelling salesman, Hamiltonean cycle, clique, etc). Behaviour of a process \\nis in a distributed system is also a good example of nondeterministic situation. Because \\nwww.indiansbrain.com\\nthe behaviour of a process might depend on some messages from other processes \\nthat might arrive at arbitrary times with arbitrary contents. \\nIt is easy to construct and comprehend an NFA than DFA for a given regular \\nlanguage. The concept of NFA can also be used in proving many theorems and \\nresults. Hence, it plays an important role in this subject. \\nIn the context of FA nondeterminism can be incorporated naturally. That is, an NFA is \\ndefined in the same way as the DFA but with the following two exceptions: \\n\\uf095 multiple next state.\\uf020\\n\\uf020\\n\\uf095 \\n- transitions.\\uf020\\n \\nMultiple Next State : \\n \\n\\uf095 In contrast to a DFA, the next state is not necessarily uniquely determined by the \\ncurrent state and input symbol in case of an NFA. (Recall that, in a DFA there is \\nexactly one start state and exactly one transition out of every state for each \\nsymbol in \\n).\\uf020\\n\\uf095 \\nThis means that - in a state q and with input symbol a - there could be one, more \\nthan one or zero next state to go, i.e. the value of \\nis a subset of Q. Thus\\uf020\\n \\n= \\nwhich means that any one of \\ncould be the next \\nstate. \\n \\n\\uf095 The zero next state case is a special one giving \\n=\\n, which means that \\nthere is no next state on input symbol when the automata is in state q. In such \\na case, we may think that the automata \"hangs\" and the input will be rejected.\\uf020\\n \\n- transitions : \\n \\nIn an -transition, the tape head doesn\\'t do anything- it doesnot read and it doesnot move. \\nHowever, the state of the automata can be changed - that is can go to zero, one \\n \\nor more states. This is written formally as \\nimplying that the next \\nstate could by any one of \\nw/o consuming the next input symbol. \\n \\n \\n \\n \\nAcceptance : \\n \\nInformally, an NFA is said to accept its input \\nif it is possible to start in some start state \\nand process \\n, moving according to the transition rules and making choices along the way \\nwhenever the next state is not uniquely defined, such that when \\nis completely processed \\n(i.e. end of \\nis reached), the automata is in an accept state. There may be several \\npossible paths through the automation in response to an input \\nsince the start state is not \\ndetermined and there are choices along the way because of multiple next states. Some of \\nthese paths may lead to accpet states while others may not. The \\nwww.indiansbrain.com\\nautomation is said to accept \\nif at least one computation path on input \\nstarting from at \\nleast one start state leads to an accept state- otherwise, the automation rejects input \\n. \\nAlternatively, we can say that, \\nis accepted iff there exists a path with label \\nfrom some \\nstart state to some accept state. Since there is no mechanism for determining which state \\nto start in or which of the possible next moves to take (including the \\n- transitions) in \\nresponse to an input symbol we can think that the automation is having some \"guessing\" \\npower to chose the correct one in case the input is accepted \\n \\nExample 1 : Consider the language L = {\\n {0, 1}* | The 3rd symbol from the right \\nis 1}. The following four-state automation accepts L. \\n \\nThe m/c is not deterministic since there are two transitions from state \\non input 1 \\nand no transition (zero transition) from \\non both 0 & 1. \\n \\nFor any string \\nwhose 3rd symbol from the right is a 1, there exists a sequence of legal \\ntransitions leading from the start state q, to the accept state \\n. But for any string \\nwhere 3rd symbol from the right is 0, there is no possible sequence of legal \\n \\ntranisitons leading from \\nand \\n. Hence m/c accepts L. How does it accept any string \\nL? \\n \\nFormal definition of NFA : \\n \\nFormally, an NFA is a quituple \\nwhere Q, \\n, \\n, and F bear \\nthe same meaning as for a DFA, but \\n, the transition function is redefined as follows: \\n \\n \\n \\n \\nwhere P(Q) is the power set of Q i.e. \\n. \\n \\nThe Langauge of an NFA : \\n \\nFrom the discussion of the acceptance by an NFA, we can give the formal definition of a \\nlanguage accepted by an NFA as follows : \\n \\nIf \\nis an NFA, then the langauge accepted by N is writtten as L(N) \\nis given by \\n. \\n \\nThat is, L(N) is the set of all strings w in \\nsuch that \\ncontains at least \\none accepting state. \\nwww.indiansbrain.com\\nRemoving ϵ-transition: \\n \\n- transitions do not increase the power of an NFA . That is, any \\n- NFA ( NFA with \\ntransition), we can always construct an equivalent NFA without \\n-transitions. The \\n \\nequivalent NFA must keep track where the \\nNFA goes at every step during \\ncomputation. This can be done by adding extra transitions for removal of every \\n- \\ntransitions from the - NFA as follows. \\n \\nIf we removed the \\n- transition \\nfrom the - NFA , then we need to moves \\n \\nfrom state p to all the state on input symbol \\nwhich are reachable from state q \\n(in the - NFA ) on same input symbol q. This will allow the modified NFA to move \\nfrom state p to all states on some input symbols which were possible in case of \\n-NFA \\non the same input symbol. This process is stated formally in the following theories. \\n \\nTheorem if L is accepted by an - NFA N , then there is some \\nequivalent \\nwithout transitions accepting the same language L \\nProof: \\n \\nLet \\nbe the given \\nwith \\n \\n \\nWe construct \\n \\nWhere, \\nfor all \\nand \\nand \\n \\n \\n \\n \\n \\nOther elements of N\\' and N \\n \\nWe can show that \\ni.e. N\\' and N are equivalent. \\n \\nWe need to prove that \\n \\n \\n i.e. \\n \\n \\n \\n \\n \\nWe will show something more, that is, \\nwww.indiansbrain.com\\nWe will show something more, that is, \\n \\n \\nBasis : \\n, then \\n \\n \\nBut \\nby definition of \\n. \\n \\nInduction hypothesis Let the statement hold for all \\nwith \\n. \\n \\n \\nBy definition of extension of \\n \\n \\nBy inductions hypothesis. \\n \\nAssuming that \\n \\n \\n \\n \\nBy definition of \\n \\n \\nSince \\n \\n \\n \\nTo complete the proof we consider the case \\n \\nWhen \\ni.e. \\nthen \\nwww.indiansbrain.com\\n \\nand by the construction of \\nwherever \\nconstrains a state in F. \\n \\nIf \\n(and thus \\nis not in F ), then \\nwith \\nleads to an accepting state in N\\' iff it \\nlead to an accepting state in N ( by the construction of N\\' and N ). \\n \\nAlso, if (\\n , thus w is accepted by N\\' iff w is accepted by N (iff \\n) \\n \\nIf \\n(and, thus in M we load \\nin F ), thus is accepted by both N\\' and N . \\n \\nLet \\n. If w cannot lead to \\nin N , then \\n. (Since can add transitions to get an accept \\nstate). So there is no harm in making \\nan accept state in N\\'. \\n \\nEx: Consider the following NFA with \\n- transition. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition Diagram \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition diagram for \\n\\' for the equivalent NFA without - moves \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n          1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince \\nthe start state q0 must be final state in the equivalent NFA . \\n \\nSince \\nand \\nand \\nwe add moves \\nand \\nin the equivalent NFA . Other moves are also constructed accordingly. \\n \\n-closures: \\n \\nThe concept used in the above construction can be made more formal by defining the \\n-closure for a state (or a set of states). The idea of \\n-closure is that, when moving \\n \\nfrom a state p to a state q (or from a set of states Si to a set of states Sj ) an input \\n, we need to take account of all \\n-moves that could be made after the transition. \\nFormally, for a given state q, \\n \\n \\n-closures: \\n \\nSimilarly, for a given set \\n \\n \\n-closures: \\n \\n \\n \\nSo, in the construction of equivalent NFA N\\' without -transition from any NFA with \\n \\nmoves. the first rule can now be written as \\nwww.indiansbrain.com\\nEquivalence of NFA and DFA \\n \\nIt is worth noting that a DFA is a special type of NFA and hence the class of languages \\naccepted by DFA s is a subset of the class of languages accepted by NFA s. \\nSurprisingly, these two classes are in fact equal. NFA s appeared to have more power \\nthan DFA s because of generality enjoyed in terms of \\n-transition and multiple next \\nstates. But they are no more powerful than DFA s in terms of the languages they \\naccept. \\n \\nConverting DFA to NFA \\n \\n \\n \\n \\nTheorem: Every DFA has as equivalent NFA \\n \\nProof: A DFA is just a special type of an NFA . In a DFA , the transition functions is \\ndefined from \\nwhereas in case of an NFA it is defined from \\nand \\nbe a DFA . We construct an equivalent NFA \\nas follows. \\n \\n \\n \\n \\ni. e \\n \\nIf \\nand \\n \\nAll other elements of N are as in D. \\n \\nIf \\nthen there is a sequence of states \\nsuch that \\n \\n \\n \\nThen it is clear from the above construction of N that there is a sequence of states (in N) \\nsuch that \\nand \\nand hence \\n \\n \\nSimilarly we can show the converse. \\n \\nHence , \\n \\n \\nGiven any NFA we need to construct as equivalent DFA i.e. the DFA need to simulate \\nthe behaviour of the NFA . For this, the DFA have to keep track of all the states where \\nthe NFA could be in at every step during processing a given input string. \\nwww.indiansbrain.com\\nThere are \\npossible subsets of states for any NFA with n states. Every subset \\ncorresponds to one of the possibilities that the equivalent DFA must keep track of. Thus, \\n \\nthe equivalent DFA will have \\nstates. \\n \\nThe formal constructions of an equivalent DFA for any NFA is given below. We \\nfirst consider an NFA without \\ntransitions and then we incorporate the affects of \\ntransitions later. \\n \\nFormal construction of an equivalent DFA for a given NFA without transitions. \\n \\nGiven an \\nwithout - moves, we construct an equivalent DFA \\n \\n \\nas follows \\n \\ni.e. \\n \\n \\n \\n \\n \\n(i.e. every subset of Q which as an element in F is considered as a final stat\\nin DFA D ) \\n \\n \\n \\n \\nfor all \\nand \\n \\n \\nwhere \\n \\n \\nThat is, \\n \\n \\nTo show that this construction works we need to show that L(D)=L(N) i.e. \\n \\n \\n \\n \\n \\n \\nOr,\\n \\n \\nWe will prove the following which is a stranger statement thus required. \\nwww.indiansbrain.com\\n \\n \\nProof : We will show by inductions on \\n \\n \\nBasis If \\n=0, then w =  \\n \\nSo, \\nby definition. \\n \\nInductions hypothesis : Assume inductively that the statement holds \\nof \\nlength less than or equal to n. \\n \\nInductive step \\n \\nLet \\n, then \\nwith \\n \\n \\nNow, \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNow, given any NFA with -transition, we can first construct an equivalent NFA without \\n-transition and then use the above construction process to construct an equivalent \\nDFA , thus, proving the equivalence of NFA s and DFA s.. \\n \\nIt is also possible to construct an equivalent DFA directly from any given NFA with \\n- transition by integrating the concept of \\n-closure in the above construction. \\n \\nRecall that, for any \\n \\n \\n- closure : \\nwww.indiansbrain.com\\nIn the equivalent DFA , at every step, we need to modify the transition functions \\nto \\nkeep track of all the states where the NFA can go on \\n-transitions. This is done by \\nreplacing \\nby \\n-closure \\n, i.e. we now compute \\nat every step as \\nfollows: \\n \\n \\n \\nBesides this the initial state of the DFA D has to be modified to keep track of all the \\nstates that can be reached from the initial state of NFA on zero or more -transitions. \\nThis can be done by changing the initial state \\nto -closure (\\n ) . \\n \\nIt is clear that, at every step in the processing of an input string by the DFA D , it enters \\na state that corresponds to the subset of states that the NFA N could be in at that \\nparticular point. This has been proved in the constructions of an equivalent NFA for any \\n-NFA \\nIf the number of states in the NFA is n , then there are \\nstates in the DFA . That is, \\neach state in the DFA is a subset of state of the NFA . \\n \\nBut, it is important to note that most of these \\nstates are inaccessible from the start state \\nand hence can be removed from the DFA without changing the accepted language. Thus, \\nin fact, the number of states in the equivalent DFA would be much less \\n \\nthan \\n. \\nExample : Consider the NFA given below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n{\\n} \\n \\n \\n \\n \\nSince there are 3 states in the NFA \\nwww.indiansbrain.com\\nThere will be \\nstates (representing all possible subset of states) in the \\nequivalent DFA . The transition table of the DFA constructed by using the subset \\nconstructions process is produced here. \\n \\n0 \\n \\n1 The start state of the DFA is   - closures \\n \\n \\n \\n \\n The final states are all those subsets that contains \\n(since \\nin the NFA). \\n \\n{   } \\nLet us compute one entry, \\n \\n \\n \\n \\n \\n \\n \\n \\n Similarly, all other transitions can be computed \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCorresponding Transition fig. for DFA.Note that states \\n \\nare not accessible and hence can be removed. \\nThis gives us the following simplified DFA with only 3 states. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is interesting to note that we can avoid encountering all those inaccessible \\nor unnecessary states in the equivalent DFA by performing the following two \\nsteps inductively. \\n \\n1. If \\nis the start state of the NFA, then make \\n- closure ( \\n) the start state of the \\nequivalent DFA . This is definitely the only accessible state. \\n \\n2. If we have already computed a set \\nof states which are accessible. Then \\n. \\ncompute \\nbecause these set of states will also be accessible. \\n \\nFollowing these steps in the above example, we get the transition table given below \\nwww.indiansbrain.com\\nUNIT-II \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying certain recursive rules \\nas given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) \\nis a RE \\n \\nii) \\nis a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\nIf \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\n \\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive REs) by a finite no of \\napplications of the recursive step (given in 2). \\n \\nExample : Let \\n= { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this expression by \\napplying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n \\n \\nwww.indiansbrain.com\\n4 \\n(1+   ) \\nRule 2(iv) & Step 3 \\n \\n \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated with every RE). \\nWe will see later that REs are used to attribute regular languages. \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We can define the \\nlanguage L(r) associated with (or described by) a REs as follows. \\n \\n1. \\nis the RE describing the empty language i.e. L(\\n) = \\n. \\n \\n2. \\nis a RE describing the language {\\n} i.e. L(\\n) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\n \\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,........} {0} \\n{1} \\n \\n= {\\n , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nwww.indiansbrain.com\\nConsider the RE ab + c. The language described by the RE can be thought of either L(a)L(b+c) or \\n \\nL(ab)\\nL(c) as provided by the rules (of languages described by REs) given already. But these two \\nrepresents two different languages lending to ambiguity. To remove this ambiguity we can either \\n \\n \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. Like other algebras mod in \\nmathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) operator. \\n \\nii) It is also important to note that concatenation & union (+) operators are associative and union operation \\nis commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language L(ab) \\nL(c) i.e. it should be \\ngrouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For example, the \\nlanguage represented by the RE a(b+c) is L(a)L(b+c). \\n \\n \\n \\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language L(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all strings over {0,1} which \\nare either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an even number of 0\\'s \\n \\nfollowed by an odd number of 1\\'s i.e. \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE rr, \\ndenotes \\nr, \\nand so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of consecutive 1\\'s} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and what goes before is \\ncompletely arbitrary. Considering these observations we can write the REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE (0+1)*11(0+1)*+(0+1)*00(0+1)* \\nrepresents the set of string over {0,1} that contains the substring 11 or 00. \\nwww.indiansbrain.com\\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the set of strings over {0,1} \\nthat contains exactly two 1\\'s. The presence of two 1\\'s in the RE and any no of 0\\'s before, between and after the \\n1\\'s ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1\\'s. \\n \\nSolution : There must be at least two 1\\'s in the RE somewhere and what comes before, between, and after is \\ncompletely arbitrary. Hence we can write the RE as (0+1)*1(0+1)*1(0+1)*. But following two REs also \\nrepresent the same language, each ensuring presence of least two 1\\'s somewhere in the string \\n \\n \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\n \\nL(r) = {\\n has no pair of consecutive 1\\'s} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We observer that, whenever \\na 1 occurs, it must be immediately followed by a 0. This substring may be preceded & followed by any no of \\n0\\'s. So the final RE must be a repetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the \\nRE is (0*100*)*. But in this case the strings ending in 1 or consisting of all 0\\'s are not accounted for. Taking \\nthese observations into consideration, the final RE is  r = (0*100*)(1+ \\n)+0*(1+\\n). \\n \\nAlternative Solution : \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as r = (0+10)*(1+\\n).This \\nis a shorter expression but represents the same language. \\n \\nRegular Expression: \\n \\nFA to regular expressions: \\n \\nFA to RE (REs for Regular Languages) : \\n \\nLemma : If a language is regular, then there is a RE to describe it. i.e. if L = L(M) for some DFA M, then there is a \\nRE r such that L = L(r). \\n \\nProof : We need to construct a RE r such that \\n. Since M is a DFA, it has a finite no \\nof states. Let the set of states of M is Q = {1, 2, 3,..., n} for some integer n. [ Note : if the n states of M were \\ndenoted by some other symbols, we can always rename those to indicate as 1, 2, 3,..., n ]. The required RE is \\nconstructed inductively. \\n \\nNotations : \\nis a RE denoting the language which is the set of all strings w such that w is the label of a \\npath from state i to state j \\nin M, and that path has no intermediate state whose number is \\ngreater then k. ( i & j (begining and end pts) are not considered to be \"intermediate\" so i and /or j can be \\nwww.indiansbrain.com\\ngreater than k ) \\n \\nWe now construct \\ninductively, for all i, j \\nQ starting at k = 0 and finally reaching k = n. \\n \\nBasis : k = 0, \\ni.e. the paths must not have any intermediate state ( since all states are numbered 1 or \\nabove). There are only two possible paths meeting the above condition : \\n \\n1. A direct transition from state i to state j. \\no \\n= a if then is a transition from state i to state j on symbol the single symbol a. \\n \\no \\n= \\nif there are multiple transitions from state i to state j on symbols \\n \\n. \\no \\n= f if there is no transition at all from state i to state j. \\n \\n2. All paths consisting of only one node i.e. when i = j. This gives the path of length 0 (i.e. the RE \\ndenoting the string \\n) and all self loops. By simply adding Î to various cases above we get \\nthe corresponding REs i.e. \\no \\n= \\n+ a if there is a self loop on symbol a in state i . \\n \\no \\n= \\n+ \\nif there are self loops in state i as multiple symbols \\n \\n. \\n \\no \\n= \\nif there is no self loop on state i. \\n \\nInduction : \\n \\nAssume that there exists a path from state i to state j such that there is no intermediate state whose number is \\n \\ngreater than k. The corresponding Re for the label of the path is \\n. There are only two possible cases : \\n \\n1. The path dose not go through the state k at all i.e. number of all the intermediate states are less \\nthan k. So, the label of the path from state i to state j is tha language described by the RE \\n. \\n \\n2. The path goes through the state k at least once. The path may go from i to j and k may appear more \\nthan once. We can break the into pieces as shown in the figure 7. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7 \\n \\n1. The first part from the state i to the state k which is the first recurence. In this path, all \\nintermediate states are less than k and it starts at iand ends at k. So the RE \\ndenotes the \\nlanguage of the label of path. \\n \\n2. The last part from the last occurence of the state k in the path to state j. In this path also, no \\nintermediate state is numbered greater than k. Hence the RE \\ndenoting the language of the \\nlabel of the path. \\n \\n3. In the middle, for the first occurence of k to the last occurence of k , represents a loop which may be \\ntaken zero times, once or any no of times. And all states between two consecutive k\\'s are \\nnumbered less than k. \\n \\nHence the label of the path of the part is denoted by the RE \\n.The label of the path from state i to state \\nj is the concatenation of these 3 parts which is \\n \\n \\n \\nSince either case 1 or case 2 may happen the labels of all paths from state i to j is denoted by the following RE \\n \\n \\n \\n \\nWe can construct \\nfor all i, j \\n{1,2,..., n} in increasing order of k starting with the basis k = 0 upto k = n since \\ndepends only on expressions with a small superscript (and hence will be available). WLOG, assume \\n \\nthat state 1 is the start state and \\nare the m final states where ji \\n{1, 2, ... , n }, \\nand \\n \\n. According to the convention used, the language of the automatacan be denoted by the RE \\nwww.indiansbrain.com\\n \\n \\nSince \\nis the set of all strings that starts at start state 1 and finishes at final state \\nfollowing the transition of \\nthe FA with any value of the intermediate state (1, 2, ... , n) and hence accepted by the automata. \\n \\nRegular Grammar: \\n \\nA grammar \\nis right-linear if each production has one of the following three forms: \\n \\n\\uf095 \\nA\\ncB ,\\uf020\\n\\uf020\\n\\uf095 \\nA\\nc,\\uf020\\n\\uf095 \\nA\\n\\uf020\\n \\nWhere A, B \\n( with A = B allowed) and \\n. A grammar G is left-linear if each production has once of \\nthe following three forms. \\n \\nA\\nBc , A\\nc, A\\n \\n \\nA right or left-linear grammar is called a regular grammar. \\n \\nRegular grammar and Finite Automata are equivalent as stated in the following theorem. \\n \\nTheorem : A language L is regular iff it has a regular grammar. We use the following two lemmas to prove the \\nabove theorem. \\n \\nLemma 1 : If L is a regular language, then L is generated by some right-linear grammar. \\n \\nProof : Let \\nbe a DFA that accepts L. \\n \\nLet \\nand \\n. \\n \\nWe construct the right-linear grammar \\nby letting \\n \\nN = Q , \\nand \\n \\n[ Note: If \\n, then \\n] \\n \\nLet \\n. For M to accept w, there must be a sequence of states \\nsuch that \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\nBy construction, the grammar G will have one production for each of the above transitions. Therefore, we have \\nthe corresponding derivation. \\n \\n \\n \\n \\nHence w \\nL(g). \\n \\nConversely, if \\n, then the derivation of w in G must have the form as given above. \\nBut, then the construction of G from M implies that \\n \\n, where \\n, completing the proof. \\n \\nLemma 2 : Let \\nbe a right-linear grammar. Then L(G) is a regular \\nlanguage. Proof: To prove it, we construct a FA M from G to accept the same language. \\n \\nis constructed as follows: \\n \\n( \\nis a special sumbol not in N ) \\n \\n, \\n \\nFor any \\nand \\nand \\nis defined as \\n \\n \\nif \\n \\nand \\n, if \\n. \\nWe now show that this construction works. \\n \\nLet \\n. Then there is a derivation of w in G of the form \\nwww.indiansbrain.com\\n \\n \\nBy contradiction of M, there must be a sequence of transitions \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimplying that \\ni.e. w is accepted by M. \\n \\nConversely, if \\nis accepted by M, then because \\nis the only accepting state of M, the transitions \\ncausing w to be accepted by M will be of the form given above. These transitions corresponds to a \\n \\nderivationof w in the grammar G. Hence \\n, completing the proof of the lemma. \\n \\nGiven any left-linear grammar G with production of the form \\n, we can construct from it a \\nright-linear grammar \\nby replacing every production of G of the form \\nwith \\n \\n \\nIt is easy to prove that \\n. Since \\nis right-linear, \\nis regular. But then so are \\ni.e. \\nbecause regular languages are closed under reversal. \\n \\nPutting the two lemmas and the discussions in the above paragraph together we get the proof of the theorem- \\n \\nA language L is regular iff it has a regular grammar \\n \\nExample : Consider the grammar \\n \\n \\n \\nIt is easy to see that G generates the language denoted by the regular expression \\n(01)*0. The construction of lemma 2 for this grammar produces the follwoing FA. \\n \\nThis FA accepts exactly (01)*1. \\n \\nDecisions Algorithms for CFL \\n \\nIn this section, we examine some questions about CFLs we can answer. A CFL may be represented using a \\nCFG or PDA. But an algorithm that uses one representation can be made to work for the others, since we can \\nconstruct one from the other. \\nwww.indiansbrain.com\\nTesting Emptiness : \\n \\nTheorem : There are algorithms to test emptiness of a CFL. \\n \\nProof : Given any CFL L, there is a CFG G to generate it. We can determine, using the construction described \\n \\nin the context of elimination of useless symbols, whether the start symbol is useless. If so, then \\n; \\notherwise not. \\n \\nTesting Membership : \\n \\nGiven a CFL L and a string x, the membership, problem is to determine whether \\n? \\n \\nGiven a PDA P for L, simulating the PDA on input string x doesnot quite work, because the PDA can grow \\nits stack indefinitely on \\ninput, and the process may never terminate, even if the PDA is deterministic. \\n \\nSo, we assume that a CFG \\nis given such that L = L(G). \\n \\nLet us first present a simple but inefficient algorithm. \\n \\nConvert G to \\nin CNF generating \\n. If the input string \\n, then we need to \\n \\ndetermine whether \\nand it can easily be done using the technique given in the context of elimination of \\n \\n-production. If , \\nthen \\niff \\n. Consider a derivation under a grammar in CNF. At \\nevery step, a production in CNF in used, and hence it adds exactly one terminal symbol to the sentential form. \\n \\nHence, if the length of the input string x is n, then it takes exactly n steps to derive x ( provided x is in \\n). \\n \\nLet the maximum number of productions for any nonterminal in \\nis K. So at every step in derivation, there are \\natmost k choices. We may try out all these choices, systematically., to derive the string x in \\n. Since \\n \\nthere are atmost \\ni.e. \\nchoices. This algorithms is of exponential time complexity. We now present an \\nefficient (polynomial time) membership algorithm. \\n \\nPumping Lemma: \\n \\nLimitations of Finite Automata and Non regular Languages : \\n \\nThe class of languages recognized by FA s is strictly the regular set. There are certain languages which are \\nnon regular i.e. cannot be recognized by any FA \\n \\nConsider the language \\n \\n \\nIn order to accept is language, we find that, an automaton seems to need to remember when passing the \\ncenter point between a\\'s and b\\'s how many a\\'s it has seen so far. Because it would have to compare that \\nwith the number of b\\'s to either accept (when the two numbers are same) or reject (when they are not same) \\nthe input string. \\nwww.indiansbrain.com\\nBut the number of a\\'s is not limited and may be much larger than the number of states since the string may \\nbe arbitrarily long. So, the amount of information the automaton need to remember is unbounded. \\n \\nA finite automaton cannot remember this with only finite memory (i.e. finite number of states). The fact that \\nFA s have finite memory imposes some limitations on the structure of the languages recognized. Inductively, we \\ncan say that a language is regular only if in processing any string in this language, the information that has to \\nbe remembered at any point is strictly limited. The argument given above to show that \\nis non regular is \\ninformal. We now present a formal method for showing that certain languages such as \\nare non regular \\n \\nProperties of CFL’s \\n \\nClosure properties of CFL: \\n \\nWe consider some important closure properties of CFLs. \\n \\nTheorem : If \\nand \\nare CFLs then so is \\n \\n \\nProof : Let \\nand \\nbe CFGs generating. Without loss of generality, we \\ncan assume that \\n. Let \\nis a nonterminal not in \\nor \\n. We construct the grammar \\n \\nfrom \\nand \\n, where \\n \\n, \\n \\n \\n \\n \\n \\n \\n \\n \\nWe now show that \\n \\n \\nThus proving the theorem. \\n \\nLet \\n. Then \\n. All productions applied in their derivation are also in \\n. Hence \\ni.e. \\n \\n \\n \\nSimilarly, if \\n, then \\n \\n \\nThus \\n. \\nwww.indiansbrain.com\\nConversely, let \\n. Then \\nand the first step in this derivation must be either \\nor \\n. Considering the former case, we have \\n \\n \\nSince \\nand \\nare disjoint, the derivation \\nmust use the productions of \\nonly ( which are also in \\n \\n) Since \\nis the start symbol of \\n. Hence, \\ngiving \\n. \\n \\nUsing similar reasoning, in the latter case, we get \\n. Thus \\n. \\n \\nSo, \\n, as claimed \\n \\n \\nTheorem : If \\nand \\nare CFLs, then so is \\n. \\n \\nProof : Let \\nand \\nbe the CFGs generating \\nand \\nrespectively. \\nAgain, we assume that \\nand \\nare disjoint, and \\nis a nonterminal not in \\nor \\n. we construct the CFG \\nfrom \\nand \\n, where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe claim that \\n \\n \\n \\nTo prove it, we first assume that \\nand \\n. Then \\nand \\n. We can derive the string xy \\nin \\nas shown below. \\n \\n \\n \\n \\n \\nsince \\nand \\n. Hence \\n. \\nwww.indiansbrain.com\\nFor the converse, let \\n. Then the derivation of w in \\nwill be of the form \\n \\ni.e. the first step in the derivation must see the rule \\n. Again, since \\nand \\nare \\ndisjoint and \\nand \\n, some string x will be generated from \\nusing productions in \\n( which \\nare also in \\n) and such that \\n. \\n \\nThus \\n \\nHence \\nand \\n. \\n \\nThis means that w can be divided into two parts x, y such that \\nand \\n. Thus \\n.This \\n \\ncompletes the proof \\nTheorem : If L is a CFL, then so is \\n. \\nProof : Let \\nbe the CFG generating L. Let us construct the CFG \\n \\n \\nwhere \\n. \\n \\nWe now prove that \\n, which prove the theorem. \\n \\ncan generate \\nin one step by using the production \\nsince \\n, \\n Let \\nfor any n >1 we can \\nwrite \\nwhere \\nfor \\n \\n \\n \\n \\nusing following steps. \\n \\n \\n \\n \\n \\nFirst (n-1)-steps uses the production S\\nSS producing the sentential form of n numbers of S \\'s. The \\nnonterminal S in the i-th position then generates \\nusing production in P ( which are also in \\n) \\n \\nIt is also easy to see that G can generate the empty string, any string in L and any string \\nfor n >1 \\nand none other. \\n \\nHence \\n \\n \\nTheorem : CFLs are not closed under intersection \\n \\nProof : We prove it by giving a counter example. Consider the language \\n.The following \\nCFG generates L1 and hence a CFL \\n \\ncan generate any string in L. \\n. w can be generated by \\n \\nfrom G \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\nThe nonterminal X generates strings of the form \\nand C generates strings of the form \\n, \\n. These are the only types of strings generated by X and C. Hence, S generates \\n. \\n \\nUsing similar reasoning, it can be shown that the following grammar \\nand hence it is \\nalso a CFL. \\n \\n \\n \\n \\n \\n \\n \\nBut, \\nand is already shown to be not context-free. \\n \\nHence proof. \\n \\nTheorem : A CFL\\'s are not closed under complementations \\n \\nProof : Assume, for contradiction, that CFL\\'s are closed under complementation. SInce, CFL\\'s are also closed \\nunder union, the language \\n, where \\nand \\nare CFL\\'s must be CFL. But by DeMorgan\\'s law \\n \\n \\n \\n \\nThis contradicts the already proved fact that CFL\\'s are not closed under intersection. \\nBut it can be shown that the CFL\\'s are closed under intersection with a regular set. \\n \\nTheorem : If L is a CFL and R is a regular language, then \\nis a CFL. \\n \\nProof : Let \\nbe a PDA for L and let \\nbe a DFA for \\nR. We construct a PDA M from P and D as follows \\n \\n \\nwhere \\nis defined as \\n \\ncontains \\niff \\nwww.indiansbrain.com\\nand \\ncontains \\n \\n \\nThe idea is that M simulates the moves of P and D parallely on input w, and accepts w iff both P and \\nD accepts. That means, we want to show that \\n \\n \\n \\n \\nWe apply induction on n, the number of moves, to show that \\n \\niff \\n \\nand \\n \\nBasic Case is n=0. Hence \\n, \\nand \\n. For this case it is trivially true \\n \\nInductive hypothesis : Assume that the statement is true for n -1. \\n \\nInductive Step : Let w = xa and \\n \\n \\n \\nLet \\n \\nBy inductive hypothesis, \\nand \\n \\n \\nFrom the definition of \\nand considering the n-th move of the PDA M above, we have \\n \\nand \\n \\n \\nHence \\nand \\n \\n \\nIf \\nand \\n, then \\nand we got that if M accepts w, then both P and D accepts it. \\n \\nWe can show that converse, in a similar way. Hence \\nis a CFL ( since it is accepted by a PDA M ) \\nThis property is useful in showing that certain languages are not context-free. \\n \\nExample : Consider the language \\n \\n \\nIntersecting L with the regular set \\n, we get \\nwww.indiansbrain.com\\n \\n \\n \\nWhich is already known to be not context-free. Hence L is not context-free \\nTheorem : CFL\\'s are closed under reversal. That is if L is a CFL, then so is \\n \\n \\nProof : Let the CFG \\ngenerates L. We construct a CFG \\nwhere \\n \\n. We now show that \\n, thus proving the theorem. \\nWe need to prove that \\niff \\n. \\n \\nThe proof is by induction on n, the number of steps taken by the derivation. We assume, for simplicity (and \\nof course without loss of generality), that G and hence \\nare in CNF. \\n \\nThe basis is n=1 in which case it is trivial. Because \\nmust be either \\nor BC with \\n. \\n \\nHence \\niff \\n \\n \\nAssume that it is true for (n-1)-steps. Let \\n. Then the first step must apply a rule of the \\nform \\nand it gives \\n \\nwhere \\nand \\n \\n \\nBy constructing of G\\', \\n \\nHence \\n \\n \\nThe converse case is exactly similar \\nSubstitution : \\n \\n, let \\nbe a language (over any alphabet). This defines a function S, called substitution, on \\nwhich is \\n \\ndenoted as \\n- for all \\n \\n \\nThis definition of substitution can be extended further to apply strings and langauge as well. \\nIf \\n, where \\n, is a string in \\n, then \\n \\n. \\nSimilarly, for any language L, \\n \\nThe following theorem shows that CFLs are closed under substitution. \\n \\nThereom : Let \\nis a CFL, and s is a substitution on \\nsuch that \\nis a CFL for all \\n, \\nthus s(L) is a CFL \\n \\nProof : Let L = L(G) for a CFG \\nand for every \\n, \\nfor some \\n. Without loss of generality, assume that the sets of nonterminals N and \\n\\'s \\nare disjoint. \\nwww.indiansbrain.com\\nNow, we construct a grammar \\n, generating s(L), from G and \\n\\'s as follows : \\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095 \\nconsists of\\uf020\\n\\uf020\\n1. \\nand \\n \\n2. The production of P but with each terminal a in the right hand side of a production replaced \\nby \\neverywhere. \\nWe now want to prove that this construction works i.e. \\niff \\n. \\n \\nIf Part : Let \\nthen according to the definition there is some string \\nand \\nfor \\nsuch that \\n \\nWe will show that \\n. \\n \\nFrom the construction of \\n, we find that, there is a derivation \\ncorresponding to the string \\n \\n(since \\ncontains all productions of G but every ai replaced with \\nin the RHS of any \\nproduction). \\n \\nEvery \\nis the start symbol of \\nand all productions of \\nare also included in \\n. \\nHence \\n \\n \\n \\n \\n \\n \\n \\n \\nTherefore, \\n \\n \\n(Only-if Part) Let \\n. Then there must be a derivative as follows : \\n \\n(using the production of G include in \\nas modified by (step 2) of the construction of \\n.) \\n \\nEach \\n(\\n) can only generate a string \\n, since each \\n\\'s and N are disjoin. \\nTherefore, we get \\n \\n \\nsince \\n \\nwww.indiansbrain.com\\nsince \\n \\n \\n \\n \\n \\nThe string \\nis formed by substituting strings \\nfor each \\nand hence \\n. \\n \\nTheorem : CFL\\'s are closed under homomorphism \\n \\nProof : Let \\nbe a CFL, and h is a homomorphism on \\ni.e \\nfor some alphabets \\n. consider the \\nfollowing substitution S:Replace each symbol \\nby the language consisting of the only string h(a), i.e. \\n \\nfor all \\n. Then, it is clear that, h(L) = s(L). Hence, CFL\\'s being closed under \\nsubstitution must also be closed under homomorphism. \\nwww.indiansbrain.com\\n \\n \\nUNIT- III \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nGrammar \\n \\nA grammar is a mechanism used for describing languages. This is one of the most simple but yet powerful \\nmechanism. There are other notions to do the same, of course. \\n \\nIn everyday language, like English, we have a set of symbols (alphabet), a set of words constructed from \\nthese symbols, and a set of rules using which we can group the words to construct meaningful sentences. The \\ngrammar for English tells us what are the words in it and the rules to construct sentences. It also tells us \\nwhether a particular sentence is well-formed (as per the grammar) or not. But even if one follows the rules of \\nthe english grammar it may lead to some sentences which are not meaningful at all, because of impreciseness \\nand ambiguities involved in the language. In english grammar we use many other higher level constructs like \\nnoun-phrase, verb-phrase, article, noun, predicate, verb etc. A typical rule can be defined as \\n \\n< sentence >\\n< noun-phrase > < predicate > \\n \\nmeaning that \"a sentence can be constructed using a \\'noun-phrase\\' followed by a predicate\". \\n \\nSome more rules are as follows: \\n \\n< noun-phrase >\\n< article >< noun > \\n \\n< predicate > \\n< verb > \\n \\nwith similar kind of interpretation given above. \\n \\nIf we take {a, an, the} to be <article>; cow, bird, boy, Ram, pen to be examples of <noun>; and eats, runs, \\nswims, walks, are associated with <verb>, then we can construct the sentence- a cow runs, the boy eats, an \\npen walks- using the above rules. Even though all sentences are well-formed, the last one is not meaningful. \\nWe observe that we start with the higher level construct <sentence> and then reduce it to <noun-phrase>, \\n<article>, <noun>, <verb> successively, eventually leading to a group of words associated with these \\nconstructs. \\n \\nThese concepts are generalized in formal language leading to formal grammars. The word \\'formal\\' here refers \\nto the fact that the specified rules for the language are explicitly stated in terms of what strings or symbols can \\noccur. There can be no ambiguity in it. \\n \\nFormal definitions of a Grammar \\nwww.indiansbrain.com\\nA grammar G is defined as a quadruple. \\n \\n \\n \\n \\nN is a non-empty finite set of non-terminals or variables, \\n \\nis a non-empty finite set of terminal symbols such that \\n \\n \\n, is a special non-terminal (or variable) called the start symbol, and \\nis a \\nfinite set of production rules. \\n \\nThe binary relation defined by the set of production rules is denoted by \\n, i.e. \\niff \\n. \\n \\nIn other words, P is a finite set of production rules of the form \\n, where \\nand \\n \\n \\n \\nProduction rules: \\n \\nThe production rules specify how the grammar transforms one string to another. Given a string \\n, we say that \\nthe production rule \\nis applicable to this string, since it is possible to use the rule \\nto rewrite the \\n(in \\n) to \\nobtaining a new string \\n. We say that \\nderives \\nand is denoted as \\n \\n \\n \\n \\nSuccessive strings are dervied by applying the productions rules of the grammar in any arbitrary order. \\nA particular rule can be used if it is applicable, and it can be applied as many times as described. \\n \\nWe write \\nif the string \\ncan be derived from the string \\nin zero or more steps; \\nif \\ncan be \\nderived from \\nin one or more steps. \\n \\nBy applying the production rules in arbitrary order, any given grammar can generate many strings of terminal \\nsymbols starting with the special start symbol, S, of the grammar. The set of all such terminal strings is \\ncalled the language generated (or defined) by the grammar. \\n \\nFormaly, for a given grammar \\nthe language generated by G is \\n \\n \\n \\n \\n \\n \\nThat is \\niff \\n. \\nwww.indiansbrain.com\\nIf \\n, we must have for some \\n, \\n, denoted as a \\nderivation sequence of w, The strings \\n \\nare denoted as sentential forms of the \\nderivation.  \\n \\n \\nExample : Consider the grammar \\n \\n, where N = {S},={a, b} and P is the set of the following \\nproduction rules \\n \\n \\n \\n \\n{ S \\nab, SaSb} \\n \\nSome terminal strings generated by this grammar together with their derivation is given below. \\n \\nS \\nab \\n \\nS \\naSb\\naabb \\n \\nS \\naSb\\naaSbb\\naaabbb \\n \\nIt is easy to prove that the language generated by this grammar is \\n \\n \\n \\n \\nBy using the first production, it generates the string ab ( for i =1 ). \\n \\nTo generate any other string, it needs to start with the production S\\naSb and then the non-terminal S in the RHS can be \\nreplaced either by ab (in which we get the string aabb) or the same production S\\naSb can be used one or more \\ntimes. Every time it adds an \\'a\\' to the left and a \\'b\\' to the right of S, thus giving the sentential \\n \\nform \\n. When the non-terminal is replaced by ab (which is then only possibility for generating \\na terminal string) we get a terminal string of the form \\n. \\n \\nThere is no general rule for finding a grammar for a given language. For many languages we can devise \\ngrammars and there are many languages for which we cannot find any grammar. \\n \\nExample: Find a grammar for the language \\n. \\n \\nIt is possible to find a grammar for L by modifying the previous grammar since we need to generate an extra b \\nat the end of the string \\n. We can do this by adding a production S\\nBb where the non-terminal B \\ngenerates \\nas given in the previous example. \\n \\nUsing the above concept we devise the follwoing grammar for L. \\n \\n \\nwhere, N = { S, B }, P = { S\\nBb, B\\nab, B\\naBb } \\n \\nParse Trees: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\nConstruction of a Parse tree: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nYield of a Parse tree: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAmbiguity in languages and grammars: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\nUNIT-IV \\n \\n \\n \\n \\n \\n \\nPush down automata: \\n \\nRegular language can be charaterized as the language accepted by finite automata. Similarly, we can \\ncharacterize the context-free language as the langauge accepted by a class of machines called \\n\"Pushdown Automata\" (PDA). A pushdown automation is an extension of the NFA. \\n \\nIt is observed that FA have limited capability. (in the sense that the class of languages accepted or characterized by \\nthem is small). This is due to the \"finite memory\" (number of states) and \"no external memory\" involved with them. A \\nPDA is simply an NFA augmented with an \"external stack memory\". The addition of a stack provides the PDA with a \\nlast-in, first-out memory management cpapability. This \"Stack\" or \"pushdown store\" can be used to record a \\npotentially unbounded information. It is due to this memory management capability with the help of the stack that a \\nPDA can overcome the memory limitations that prevents a FA to \\n \\naccept many interesting languages like \\n. Although, a PDA can store an unbounded amount of \\ninformation on the stack, its access to the information on the stack is limited. It can push an element onto the \\ntop of the stack and pop off an element from the top of the stack. To read down into the stack the top elements \\nmust be popped off and are lost. Due to this limited access to the information on the stack, a PDA still has \\nsome limitations and cannot accept some other interesting languages. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAs shown in figure, a PDA has three components: an input tape with read only head, a finite control and \\na pushdown store. \\n \\nThe input head is read-only and may only move from left to right, one symbol (or cell) at a time. In each step, the \\nPDA pops the top symbol off the stack; based on this symbol, the input symbol it is currently reading, and \\nwww.indiansbrain.com\\nits present state, it can push a sequence of symbols onto the stack, move its read-only head one cell \\n(or symbol) to the right, and enter a new state, as defined by the transition rules of the PDA. \\n \\nPDA are nondeterministic, by default. That is, \\n- transitions are also allowed in which the PDA can pop and \\npush, and change state without reading the next input symbol or moving its read-only head. Besides this, \\nthere may be multiple options for possible next moves. \\n \\nFormal Definitions : Formally, a PDA M is a 7-tuple M =\\n \\nwhere, \\n \\n\\uf095 \\nis a finite set of states,\\uf020\\n\\uf095 \\nis a finite set of input symbols (input alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a finite set of stack symbols (stack alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a transition function from \\nto subset of \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the start state\\uf020\\n\\uf095 \\n, is the initial stack symbol, and\\uf020\\n\\uf020\\n\\uf095 \\n, is the final or accept states.\\uf020\\n \\nExplanation of the transition function, \\n: \\n \\nIf, for any \\n, \\n. This means intitutively that whenever \\nthe PDA is in state q reading input symbol a and z on top of the stack, it can nondeterministically for any i, \\n \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack (where \\n) (The usual convention is that if \\n, \\nthen \\nwill be at the top and \\nat the bottom.)\\uf020\\n\\uf095 \\nmove read head right one cell past the current symbol a.\\uf020\\n \\nIf a = \\n, then \\nmeans intitutively that whenver the PDA is in state \\nq with z on the top of the stack regardless of the current input symbol, it can nondeterministically for any \\n \\ni, \\n, \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack, and\\uf020\\n\\uf095 \\nleave its read-only head where it is.\\uf020\\nwww.indiansbrain.com\\nState transition diagram : A PDA can also be depicted by a state transition diagram. The labels on the arcs \\nindicate both the input and the stack operation. The transition \\n \\nfor \\nand \\nis depicted by \\n \\n \\n \\n \\n \\n \\n \\nFinal states are indicated by double circles and the start state is indicated by an arrow to it from nowhere. \\n \\n \\nConfiguration or Instantaneous Description (ID) : \\n \\nA configuration or an instantaneous description (ID) of PDA at any moment during its computation is an \\nelement of \\ndescribing the current state, the portion of the input remaining to be read (i.e. \\nunder and to the right of the read head), and the current stack contents. Only these three elements \\ncan affect the computation from that point on and, hence, are parts of the ID. \\n \\nThe start or inital configuartion (or ID) on input \\nis \\n. That is, the PDA always starts in its \\nstart state, \\nwith its read head pointing to the leftmost input symbol and the stack containing only \\nthe start/initial stack symbol, \\n. \\n \\nThe \"next move relation\" one figure describes how the PDA can move from one configuration to \\nanother in one step. \\n \\nFormally, \\n \\n \\n \\n \\niff \\n\\'a\\' may be \\nor an input symbol. \\n \\nLet I, J, K be IDs of a PDA. We define we write I\\nK, if ID I can become K after exactly i moves. The \\nrelations \\nand \\ndefine as follows \\n \\nI \\nK \\n \\nI \\nJ if \\nsuch that I \\nK and K\\n J \\n \\nI \\nJ if \\nsuch that I \\nJ. \\nwww.indiansbrain.com\\nThat is, \\nis the reflexive, transitive closure of \\n. We say that I \\nJ if the ID J follows from the ID I in \\nzero or more moves. \\n \\n( Note : subscript M can be dropped when the particular PDA M is understood. ) \\n \\nLanguage accepted by a PDA M \\n \\nThere are two alternative definiton of acceptance as given below. \\n \\n1. Acceptance by final state : \\n \\nConsider the PDA \\n. Informally, the PDA M is said to accept its input \\nby \\nfinal state if it enters any final state in zero or more moves after reading its entire input, starting in the start \\nconfiguration on input \\n. \\n \\nFormally, we define L(M), the language accepted by final state to be \\n \\n{ \\n| \\nfor some \\nand \\n} \\n \\n \\n \\n \\n2. Acceptance by empty stack (or Null stack) : The PDA M accepts its input \\nby empty stack if starting in the \\n \\nstart configuration on input \\n, it ever empties the stack w/o pushing anything back on after reading the \\nentire input. Formally, we define N(M), the language accepted by empty stack, to be \\n \\n{ \\n| \\nfor some \\n} \\n \\nNote that the set of final states, F is irrelevant in this case and we usually let the F to be the empty set i.e. F = \\nQ . \\n \\nExample 1 : Here is a PDA that accepts the language \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n, and \\nconsists of the following transitions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe PDA can also be described by the adjacent transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever the PDA M sees an input a in the start state \\nwith the start symbol z on the top of the stack \\nit pushes a onto the stack and changes state to \\n. (to remember that it has seen the first \\'a\\'). On state \\nif it \\nsees anymore a, it simply pushes it onto the stack. Note that when M is on state \\n, the symbol on the \\n \\ntop of the stack can only be a. On state \\nif it sees the first b with a on the top of the stack, then it needs to \\nstart comparison of numbers of a\\'s and b\\'s, since all the a\\'s at the begining of the input have already been \\npushed onto the stack. It start this process by popping off the a from the top of the stack and enters in state q3 \\n \\n(to remember that the comparison process has begun). On state \\n, it expects only b\\'s in the input (if it sees \\nany more a in the input thus the input will not be in the proper form of anbn). Hence there is no more on input a \\nwhen it is in state \\n. On state \\nit pops off an a from the top of the stack for every b in the input. When it sees \\nthe last b on state q3 (i.e. when the input is exaushted), then the last a from the stack will be popped off and the \\nstart symbol z is exposed. This is the only possible case when the input (i.e. on \\n-input ) the PDA M \\n \\nwill move to state \\nwhich is an accept state. \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\nLet the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) \\n \\n( using transition 2 ) \\n \\n( using transition 3 ) \\nwww.indiansbrain.com\\n( using transition 4 ), \\n( using transition 5 ) , \\nis final state. Hence , accept. So \\nthe string aabb is rightly accepted by M \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nExample 2 : We give an example of a PDA M that accepts the set of balanced strings of parentheses [] by \\nempty stack. \\nThe PDA M is given below. \\n \\n \\nwhere \\nis defined as \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever it sees a [, it will push the ] onto the stack. (first two transitions), and whenever it sees a ] \\nand the top of the stack symbol is [, it will pop the symbol [ off the stack. (The third transition). The fourth \\ntransition is used when the input is exhausted in order to pop z off the stack ( to empty the stack) and accept. \\nNote that there is only one state and no final state. The following is a sequence of configurations leading to the \\nacceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn\\'t matter which one we use, since \\nwww.indiansbrain.com\\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M\\' from M and the proof of equivalence of M & M\\' \\nare given below. \\n \\nThere are two cases to be considered. \\n \\nCASE I : PDA M accepts by final state, Let \\nLet qf be a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and M\\' are equivalent i.e. \\n \\nL(M) = L(\\n) \\n \\nLet \\nL(M) . Then \\nfor some \\nand \\n \\n \\nThen \\n \\n \\nThus \\naccepts \\n \\n \\nConversely, let \\naccepts \\ni.e. \\nL(\\n), then \\nfor \\ninherits all other moves except the last one from M. Hence \\nfor some \\n \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE II : PDA M accepts by empty stack. \\n \\nWe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand X\\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\n \\nand \\nwww.indiansbrain.com\\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M\\'s stack. From this point onward \\nwill simulate every move of M \\nsince all the transitions of M are also in \\n \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point, \\nwill enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. We will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. Then \\n \\nfor some \\n. But then \\n \\n \\n( by transition rule 1) \\n \\n( Since \\nincludes all the moves of M ) \\n \\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n. Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some \\n \\n \\nEvery move in the sequence, \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n \\n \\nEquivalence of PDA’s and CFG’s: \\nWe will now show that pushdown automata and context-free grammars are equivalent in expressive power, \\nthat is, the language accepted by PDAs are exactly the context-free languages. To show this, we have to \\nprove each of the following: \\n \\ni) \\nGiven any arbitrary CFG G there exists some PDA M that accepts exactly the same \\nlanguage generated by G. \\n \\nii) \\nGiven any arbitrary PDA M there exists a CFG G that generates exactly the same \\nlanguage accpeted by M. \\n \\n(i) CFA to PDA \\n \\nWe will first prove that the first part i.e. we want to show to convert a given CFG to an equivalent PDA. \\nwww.indiansbrain.com\\nLet the given CFG is \\n. Without loss of generality we can assume that G is in \\nGreibach Normal Form i.e. all productions of G are of the form . \\n \\n where \\nand \\n. \\n \\nFrom the given CFG G we now construct an equivalent PDA M that accepts by empty stack. Note that there \\nis only one state in M. Let \\n \\n, where \\n \\n\\uf095 \\nq is the only state\\uf020\\n\\uf020\\n\\uf095 \\nis the input alphabet,\\uf020\\n\\uf095 \\nN is the stack alphabet ,\\uf020\\n\\uf020\\n\\uf095 \\nq is the start state.\\uf020\\n\\uf095 \\nS is the start/initial stack symbol, and \\n, the transition relation is defined as follows\\uf020\\n \\nFor each production \\n, \\n. We now want to show \\nthat M and G are equivalent i.e. L(G)=N(M). i.e. for any \\n. \\niff \\n. \\n \\nIf \\n, then by definition of L(G), there must be a leftmost derivation starting with S and deriving w. \\n \\ni.e. \\n \\n \\nAgain if \\n, then one sysmbol. Therefore we need to show that for any \\n. \\n \\niff \\n. \\n \\nBut we will prove a more general result as given in the following lemma. Replacing A by S (the start \\nsymbol) and \\nby \\ngives the required proof. \\n \\nLemma For any \\n, \\nand \\n, \\nvia a leftmost derivative iff \\n. \\n \\nProof : The proof is by induction on n. \\n \\nBasis : n = 0 \\nwww.indiansbrain.com\\n \\niff \\ni.e. \\nand \\n \\n \\n \\niff \\n \\niff \\n \\n \\nInduction Step : \\n \\nFirst, assume that \\nvia a leftmost derivation. Let the last production applied in their derivation is \\nfor some \\nand \\n. \\n \\nThen, for some \\n, \\n \\n \\n \\n \\n \\n \\nwhere \\nand \\n \\n \\nNow by the indirection hypothesis, we get, \\n \\n.............................................................................(1) \\nAgain by the construction of M, we get \\n \\n \\nso, from (1), we get \\n \\n \\n \\n \\nsince \\nand \\n, we get \\n \\n \\nThat is, if \\n, then \\n. Conversely, assume that \\nand let \\nwww.indiansbrain.com\\nbe the transition used in the last move. Then for some \\n, \\nand \\n \\n \\n \\nwhere \\nand \\n. \\n \\nNow, by the induction hypothesis, we get \\n \\nvia a leftmost derivation. \\n \\nAgain, by the construction of M, \\nmust be a production of G. [ Since \\n]. Applying the production to the sentential form \\nwe get \\n \\n \\n \\n \\n \\ni.e. \\n \\n \\nvia a leftmost derivation. \\n \\nHence the proof. \\n \\nExample : Consider the CFG G in GNF \\n \\nS\\naAB \\n \\nA\\na / aA \\nB\\na / bB \\n \\nThe one state PDA M equivalent to G is shown below. For convenience, a production of G and \\nthe corresponding transition in M are marked by the same encircled number. \\n \\n(1) S\\naAB \\n \\n(2) A \\na \\n \\n(3) A\\naA \\n \\n(4) B \\na \\n \\n(5) B \\nbB \\n \\n \\n. We have used the same construction discussed earlier \\n \\nSome Useful Explanations : \\nConsider the moves of M on input aaaba leading to acceptance of the string. \\n \\nSteps \\nwww.indiansbrain.com\\n \\n1. (q, aaaba, s) \\n( q, aaba, AB ) \\n2. \\n( q, aba, AB ) \\n3. \\n( q, ba, B ) \\n4. \\n( q, a, B ) \\n5. \\n( q,   ,   )    Accept by empty stack. \\n \\nNote : encircled numbers here shows the transitions rule applied at every step. \\n \\nNow consider the derivation of the same string under grammar G. Once again, the production used at \\nevery step is shown with encircled number. \\n \\nS \\naAB \\naaAB \\naaaB \\naaabB \\naaaba \\n \\nSteps \\n1 \\n2 \\n3 \\n4 \\n5\\n \\nObservations: \\n\\uf095 \\nThere is an one-to-one correspondence of the sequence of moves of the PDA M and the derivation\\uf020\\n\\uf020\\nsequence under the CFG G for the same input string in the sense that - number of steps in both \\nthe cases are same and transition rule corresponding to the same production is used at every step \\n(as shown by encircled number). \\n\\uf020\\n\\uf095 \\nconsidering the moves of the PDA and derivation under G together, it is also observed that at \\nevery step the input read so far and the stack content together is exactly identical to the \\ncorresponding sentential form i.e.\\uf020\\n\\uf020\\n<what is Read><stack> = <sentential form> \\n \\nSay, at step 2, Read so far = \\na stack = AB \\nSentential form = aAB From this property we claim that \\niff \\n. If the claim is \\n \\ntrue, then apply with \\nand we get \\niff \\n or \\niff \\n( \\nby definition ) \\n \\nThus N(M) = L(G) as desired. Note that we have already proved a more general version of the \\nclaim PDA and CFG: \\nWe now want to show that for every PDA M that accpets by empty stack, there is a CFG G such that L(G) = \\nN(M) \\n \\nwe first see whether the \"reverse of the construction\" that was used in part (i) can be used here to construct \\nan equivalent CFG from any PDA M. \\n \\nIt can be show that this reverse construction works only for single state PDAs. \\nwww.indiansbrain.com\\n\\uf095 \\nThat is, for every one-state PDA M there is CFG G such that L(G) = N(M). For every move of the \\nPDA M \\nwe introduce a production \\nin the \\ngrammar \\nwhere N = T and \\n.\\uf020\\n \\nwe can now apply the proof in part (i) in the reverse direction to show that L(G) = N(M). \\n \\nBut the reverse construction does not work for PDAs with more than one state. For example, consider the PDA \\nM produced here to accept the langauge \\n \\n \\n \\n \\n \\nNow let us construct CFG \\nusing the \"reverse\" construction. \\n \\n( Note \\n). \\n \\nTransitions in M \\nCorresponding Production in G \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe can drive strings like aabaa which is in the language. \\n \\n \\n \\n \\nBut under this grammar we can also derive some strings which are not in the language. e.g \\n \\n \\n \\n \\nand \\n. But \\n \\n \\nTherefore, to complete the proof of part (ii) we need to prove the following claim also. \\n \\nClaim: For every PDA M there is some one-state PDA \\nsuch that \\n. \\n \\nIt is quite possible to prove the above claim. But here we will adopt a different approach. We start with \\nany arbitrary PDA M that accepts by empty stack and directly construct an equivalent CFG G. \\nwww.indiansbrain.com\\nPDA to CFG \\n \\nWe want to construct a CFG G to simulate any arbitrary PDA M with one or more states. Without loss \\nof generality we can assume that the PDA M accepts by empty stack. \\n \\nThe idea is to use nonterminal of the form <PAq> whenever PDA M in state P with A on top of the stack goes \\n \\nto state \\n. That is, for example, for a given transition of the PDA corresponding production in the grammar as \\nshown below, \\nAnd, we would like to show, in general, that \\niff the PDA M, when started from state P with A on \\n \\nthe top of the stack will finish processing \\n, arrive at state q and remove A from the stack. \\n \\nwe are now ready to give the construction of an equivalent CFG G from a given PDA M. we need to introduce \\ntwo kinds of producitons in the grammar as given below. The reason for introduction of the first kind of \\nproduction will be justified at a later point. Introduction of the second type of production has been justified in \\nthe above discussion. \\n \\nLet \\nbe a PDA. We construct from M a equivalent CFG \\n \\n \\nWhere \\n \\n\\uf095 \\nN is the set of nonterminals of the form <PAq> for \\nand \\nand P contains the follwoing\\uf020\\n \\ntwo kind of production \\n \\n1.  \\n \\n \\n2. If \\n, then for every choice of the sequence \\n,\\n \\n, \\n. \\n \\n \\nInclude the follwoing production \\n \\n \\n \\n \\nIf n = 0, then the production is \\n.For the whole exercise to be meaningful we want \\nmeans there is a sequence of transitions ( for PDA M ), starting in state q, ending in \\n, \\n \\nduring which the PDA M consumes the input string \\nand removes A from the stack (and, of course, all \\nother symbols pushed onto stack in A\\'s place, and so on.) \\n \\nThat is we want to claim that \\n \\niff \\n \\n \\nIf this claim is true, then let \\nto get \\niff \\nfor some \\n \\n. But for all \\nwe have \\nas production in G. Therefore, \\nwww.indiansbrain.com\\niff \\ni.e. \\niff PDA M accepts w by empty stack or L(G) = N(M) \\n \\nNow, to show that the above construction of CFG G from any PDA M works, we need to prove the \\nproposed claim. \\n \\nNote: At this point, the justification for introduction of the first type of production (of the form \\n) in \\nthe CFG G, is quite clear. This helps use deriving a string from the start symbol of the grammar. \\n \\nProof : Of the claim \\niff \\nfor some \\n, \\nand \\n \\n \\nThe proof is by induction on the number of steps in a derivation of G (which of course is equal to the number \\nof moves taken by M). Let the number of steps taken is n. \\n \\nThe proof consists of two parts: \\' if \\' part and \\' only if \\' part. First, consider the \\' if \\' part \\n \\nIf \\nthen \\n. \\n \\nBasis is n =1 \\n \\nThen \\n. In this case, it is clear that \\n. Hence, by construction \\nis a production of G. \\n \\nThen \\n \\nInductive Hypothesis : \\n \\n \\n \\n \\n \\nInductive Step : \\n \\n \\nFor n >1, let w = ax for some \\nand \\nconsider the first move of the PDA M which uses \\nthe general transition \\n= \\n \\n. Now M must remove \\nfrom stack \\nwhile consuming x in the remaining n-1 moves. \\n \\nLet \\n, where \\nis the prefix of x that M has consumed when \\nfirst appears at top \\nof the stack. Then there must exist a sequence of states in M (as per construction) \\n(with \\n \\n), such that \\nwww.indiansbrain.com\\n \\n [ This step implies \\n] \\n [ This step implies \\n] \\n \\n... \\n \\n \\n=\\n \\n \\n[ Note: Each step takes less than or equal to n -1 moves because the total number of moves required \\nassumed to be n-1.] \\n \\nThat is, in general \\n \\n \\n, \\n. \\n \\nSo, applying inductive hypothesis we get \\n \\n, \\n. But corresponding to the original move \\n \\nin M we have added the following production in G. \\n \\nWe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) , \\n( using transition 2 ) \\n \\n( using transition 3 ), \\n( using transition 4 ) \\n \\n( using transition 5 ) , \\nis final state. Hence, accept. \\n \\nSo the string aabb is rightly accepted by M. \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\nwww.indiansbrain.com\\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nThe following is a sequence of configurations leading to the acceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn\\'t matter which one we use, since \\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\n \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M\\' from M and the proof of equivalence of M & M\\' \\nare given below \\n \\nThere are two cases to be considered. \\n \\nCASE 1 : PDA M accepts by final state, Let \\n. Let \\nbe a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and \\nare equivalent i.e. \\n \\n \\n. \\n \\nLet \\n. Then \\nfor some \\nand \\n \\n \\nThen \\n. \\n \\nThus \\naccepts \\n. \\nwww.indiansbrain.com\\nConversely, let \\naccepts \\ni.e. \\n, then \\nfor some \\n. \\ninherits all other moves except the last one from M. Hence \\nfor some \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE 2 : PDA M accepts by empty stack. \\n \\nwe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand \\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\nand \\n \\n \\n \\n \\n \\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M\\'s stack. From this point onward M\\' will simulate every move of M \\n \\nsince all the transitions of M are also in \\n. \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point\\n, will enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. we will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. \\n \\nThen \\n \\nfor some \\n. But then, \\n \\n \\n( by transition rule 1 ) \\n \\n( since \\ninclude all the moves of M ) \\nwww.indiansbrain.com\\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n.Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some Q . \\n \\nEvery move in the sequence \\n \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n. \\n \\nDeterministic PDA: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRegular Languages and DPDA’s The DPDA’s accepts a class of languages that is in between the regular \\nlanguages and CFL’s. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDeterministic Pushdown Automata (DPDA) and Deterministic Context-free Languages (DCFLs) \\n \\nPushdown automata that we have already defined and discussed are nondeterministic by default, that is , there may be two or \\nmore moves involving the same combinations of state, input symbol, and top of the stock, and again, for some state and \\ntop of the stock the machine may either read and input symbol or make an \\n- transition (without consuming any input). \\n \\nIn deterministic PDA , there is never a choice of move in any situation. This is handled by preventing the above mentioned two \\ncases as described in the definition below. \\n \\nDefnition : Let \\nbe a PDA . Then M is deterministic if and only if both the following conditions are \\nsatisfied. \\n \\n1. \\nhas at most one element for any \\nand \\n(this condition prevents multiple choice f \\nany combination of \\n) \\n2. \\nIf \\nand \\nfor every \\n \\n \\n(This condition prevents the possibility of a choice between a move with or without an input symbol). \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEmpty Production Removal \\n \\nThe productions of context-free grammars can be coerced into a variety of forms without \\naffecting the expressive power of the grammars. If the empty string does not belong to a language, \\nthen there is a way to eliminate the productions of the form A→ λ from the grammar. \\n \\nIf the empty string belongs to a language, then we can eliminate λ from all productions \\n \\nsave for the single production S → λ. In this case we can also eliminate any occurrences of S \\nfrom the right-hand side of productions. \\n \\nProcedure to find CFG with out empty Productions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUnit production removal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLeft Recursion Removal \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNORMAL FORMS \\n \\nTwo kinds of normal forms viz., Chomsky Normal Form and Greibach Normal Form (GNF) \\nare considered here. \\n \\nChomsky Normal Form (CNF) \\n \\nAny context-free language L without any λ-production is generated by a grammar is \\nwhich productions are of the form A → BC or A→ a, where A, B ∈VN , and a ∈ V Τ. \\n \\nProcedure to find Equivalent Grammar in CNF \\n \\n(i) Eliminate the unit productions, and λ-productions if any, \\n \\n(ii) Eliminate the terminals on the right hand side of length two or more. \\n \\n(iii) Restrict the number of variables on the right hand side of productions to two. \\nProof: \\n \\nFor Step (i): Apply the following theorem: “Every context free language can be generated by \\na grammar with no useless symbols and no unit productions”. \\n \\nAt the end of this step the RHS of any production has a single terminal or two or more symbols. \\n \\nLet us assume the equivalent resulting grammar as G = (VN ,VT ,P ,S ). \\nFor Step (ii): Consider any production of the form \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExample \\n \\nObtain a grammar in Chomsky Normal Form (CNF) equivalent to the grammar G \\nwith productions P given \\n \\n \\n \\n \\n \\n \\nSolution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\nPumping Lemma for CFG \\nA “Pumping Lemma” is a theorem used to show that, if certain strings belong to a \\n \\nlanguage, then certain other strings must also belong to the language. Let us discuss a Pumping \\nLemma for CFL. We will show that , if L is a context-free language, then strings of L that are at \\nleast ‘m’ symbols long can be “pumped” to produce additional strings in L. The value of ‘m’ \\ndepends on the particular language. Let L be an infinite context-free language. Then there is some \\npositive integer ‘m’ such that, if S is a string of L of Length at least ‘m’, then \\n \\n(i) S = uvwxy (for some u, v, w, x, y) \\n \\n(ii) | vwx| \\uf064 m \\n(iii) | vx| \\uf0651 \\n(iv) uv iwx i y∈L. \\n \\nfor all non-negative values of i. \\nIt should be understood that \\n \\n(i) If S is sufficiently long string, then there are two substrings, v and x, somewhere in \\nS. There is stuff (u) before v, stuff (w) between v and x, and stuff (y), after x. \\n \\n(ii) The stuff between v and x won’t be too long, because | vwx | can’t be larger than m. \\n(iii) Substrings v and x won’t both be empty, though either one could be. \\n \\n(iv) If we duplicate substring v, some number (i) of times, and duplicate x the same \\nnumber of times, the resultant string will also be in L. \\n \\nDefinitions \\nA variable is useful if it occurs in the derivation of some string. This requires that \\n \\n(a) the variable occurs in some sentential form (you can get to the variable if you start from S), and \\n \\n(b) a string of terminals can be derived from the sentential form (the variable is not a “dead end”). \\nA variable is “recursive” if it can generate a string containing itself. For example, variable A is \\nrecursive if \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof of Pumping Lemma \\n \\n(a) Suppose we have a CFL given by L. Then there is some context-free Grammar G that \\ngenerates L. Suppose \\n(i) L is infinite, hence there is no proper upper bound on the length of strings belonging to L. \\n \\n(ii) L does not contain l. \\n(iii) G has no productions or l-productions. \\nwww.indiansbrain.com\\nThere are only a finite number of variables in a grammar and the productions for each \\n \\nvariable have finite lengths. The only way that a grammar can generate arbitrarily long strings is if \\none or more variables is both useful and recursive. Suppose no variable is recursive. Since the start \\nsymbol is non recursive, it must be defined only in terms of terminals and other variables. Then \\nsince those variables are non recursive, they have to be defined in terms of terminals and still other \\nvariables and so on. \\n \\nAfter a while we run out of “other variables” while the generated string is still finite. Therefore \\nthere is an upper bond on the length of the string which can be generated from the start \\nsymbol. This contradicts our statement that the language is finite. \\nHence, our assumption that no variable is recursive must be incorrect. \\n \\n(b) Let us consider a string X belonging to L. If X is sufficiently long, then the derivation of X \\nmust have involved recursive use of some variable A. Since A was used in the derivation, the \\nderivation should have started as \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUsage of Pumping Lemma \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHence our original assumption, that L is context free should be false. Hence the language L is not \\ncon text-free. \\n \\nExample \\n \\nCheck whether the language given by L \\uf03d {a mbmcn : m \\uf064 n \\uf064 2m} is a CFL or not. \\nSolution \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nClosure properties of CFL – Substitution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nApplications of substitution theorem \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReversal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInverse Homomorphism: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUNIT-V \\nTuring machine: \\n \\nInformal Definition: \\n \\nWe consider here a basic model of TM which is deterministic and have one-tape. There are many variations, \\nall are equally powerfull. \\n \\nThe basic model of TM has a finite set of states, a semi-infinite tape that has a leftmost cell but is infinite to \\nthe right and a tape head that can move left and right over the tape, reading and writing symbols. \\n \\nFor any input w with |w|=n, initially it is written on the n leftmost (continguous) tape cells. The infinitely many \\ncells to the right of the input all contain a blank symbol, B whcih is a special tape symbol that is not an input \\nsymbol. The machine starts in its start state with its head scanning the leftmost symbol of the input w. De-\\npending upon the symbol scanned by the tape head and the current state the machine makes a move which \\nconsists of the following: \\n \\n\\uf095 \\nwrites a new symbol on that tape cell,  \\uf095\\uf020\\n\\uf020\\nmoves its head one cell either to the left or to the right and \\n\\uf095 \\n(possibly) enters a new state.\\uf020\\n \\nThe action it takes in each step is determined by a transition functions. The machine continues computing (i.e. \\nmaking moves) until \\n \\n\\uf095 \\nit decides to \"accept\" its input by entering a special state called accept or final state or\\uf020\\n\\uf095 \\nhalts without accepting i.e. rejecting the input when there is no move defined.\\uf020\\n \\nOn some inputs the TM many keep on computing forever without ever accepting or rejecting the input, in \\nwhich case it is said to \"loop\" on that input \\n \\nFormal Definition : \\n \\nFormally, a deterministic turing machine (DTM) is a 7-tuple \\n, where \\n \\n\\uf095 \\nQ is a finite nonempty set of states.\\uf020\\n\\uf095 \\nis a finite non-empty set of tape symbols, callled the tape alphabet of M.\\uf020\\n\\uf020\\n\\uf095 \\nis a finite non-empty set of input symbols, called the input alphabet of M.\\uf020\\n\\uf095 \\nis the transition function of M,\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nis the initial or start state.\\uf020\\n\\uf095 \\nis the blank symbol\\uf020\\n\\uf020\\n\\uf095 \\nis the set of final state.\\uf020\\n \\nSo, given the current state and tape symbol being read, the transition function describes the next state, symbol \\nto be written on the tape, and the direction in which to move the tape head ( L and R denote left and right, \\nrespectively ). \\n \\nTransition function :\\n \\n \\n\\uf095 \\nThe heart of the TM is the transition function, \\nbecause it tells us how the machine gets one step \\nto the next.\\uf020\\n\\uf020\\n\\uf095 \\nwhen the machine is in a certain state q\\nQ and the head is currently scanning the tape symbol \\n, and if \\n, then the machine\\uf020\\n \\n1. replaces the symbol X by Y on the tape \\n \\n2. goes to state p, and \\n3. the tape head moves one cell ( i.e. one tape symbol ) to the left ( or right ) if D is L ( or R ). \\n \\nThe ID (instantaneous description) of a TM capture what is going out at any moment i.e. it contains all the \\ninformation to exactly capture the \"current state of the computations\". \\n \\nIt contains the following: \\n \\n\\uf095 \\nThe current state, q\\uf020\\n\\uf095 \\nThe position of the tape head,\\uf020\\n\\uf020\\n\\uf095 \\nThe constants of the tape up to the rightmost nonblank symbol or the symbol to the left of the head, \\nwhichever is rightmost.\\uf020\\n \\nNote that, although there is no limit on how far right the head may move and write nonblank symbols on the \\ntape, at any finite \\n \\ntime, the TM has visited only a finite prefix of the infinite tape. \\n \\nAn ID (or configuration) of a TM M is denoted by \\nwhere \\nand \\n \\n\\uf095 \\nis the tape contents to the left of the head\\uf020\\n\\uf095 \\nq is the current state.\\uf020\\n\\uf020\\n\\uf095 \\nis the tape contents at or to the right of the tape head\\uf020\\n \\nThat is, the tape head is currently scanning the leftmost tape symbol of \\n. ( Note that if \\n, then the \\ntape head is scanning a blank symbol) \\n \\nIf \\nis the start state and w is the input to a TM M then the starting or initial configuration of M is onviously \\ndenoted by \\n \\nwww.indiansbrain.com\\nMoves of Turing Machines \\n \\nTo indicate one move we use the symbol \\n. Similarly, zero, one, or more moves will be represented by \\n. A \\nmove of a TM \\n \\nM is defined as follows. \\n \\nLet \\nbe an ID of M where \\n, \\nand \\n. \\n \\nLet there exists a transition \\nof M. \\n \\nThen we write \\nmeaning that ID \\nyields \\n \\n \\n\\uf095 \\nAlternatively \\n, \\nif \\nis \\na \\ntransition \\nof \\nM, \\nthen \\nwe \\nwrite \\nwhich means that the ID \\nyields \\n\\uf020\\n\\uf020\\n\\uf095 \\nIn other words, when two IDs are related by the relation \\n, we say that the first one yields the \\nsecond ( or the second is the result of the first) by one move.\\uf020\\n\\uf095 \\nIf IDj results from IDi by zero, one or more (finite) moves then we write \\n( If the TM M is understand, \\nthen the subscript M can be dropped from \\nor \\n)\\uf020\\n \\nSpecial Boundary Cases \\n \\n\\uf095 \\nLet \\nbe an ID and \\nbe an transition of M. Then \\n. That is, the head is not \\nallowed to fall off the left end of the tape.\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure (Note that \\nis equivalent to \\n)\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n \\nThe language accepted by a TM \\n, denoted as L(M) is \\n \\nL(M) = { w | \\nand figure for some p\\nF and \\n} \\n \\nIn other words the TM M accepts a string \\nthat cause M to enter a final or accepting state when started \\nin its initial ID (i.e. \\n). That is a TM M accepts the string \\nif a sequence of IDs, \\n \\nexists such that \\n \\n\\uf095 \\nis the initial or starting ID of M\\uf020\\n\\uf095 \\n; \\n\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nThe representation of IDk contains an accepting state.\\uf020\\n \\nThe set of strings that M accepts is the language of M, denoted L(M), as defined \\nabove More about configuration and acceptance \\n \\n\\uf095 \\nAn ID \\nof M is called an accepting (or final) ID if \\n\\uf020\\n\\uf020\\n\\uf095 \\nAn ID \\nis called a blocking (or halting) ID if \\nis undefined i.e. the TM has no move at this \\npoint.\\uf020\\n\\uf095 \\nis called reactable from \\nif \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the initial (or starting) ID if \\nis the input to the TM and \\nis the initial (or start) state \\nof M.\\uf020\\n \\nOn any input string \\n \\n \\neither \\n \\n\\uf095 \\nM halts on w if there exists a blocking (configuration) ID, \\nsuch that \\n\\uf020\\n \\nThere are two cases to be considered \\n \\n\\uf095 \\nM accepts w if I is an accepting ID. The set of all \\naccepted by M is denoted as L(M) as\\uf020\\nalready defined \\n \\n\\uf095 \\nM rejects w if \\nis a blocking configuration. Denote by reject (M), the set of all \\nrejected by M.\\uf020\\n \\nor \\n \\n\\uf095 \\nM loops on w if it does not halt on w.\\uf020\\n \\nLet loop(M) be the set of all \\non which M loops for. \\n \\nIt is quite clear that \\n \\n \\n \\n \\nThat is, we assume that a TM M halts \\n \\n\\uf095 \\nWhen it enters an accepting \\nor\\uf020\\n\\uf095 \\nWhen it enters a blocking \\ni.e. when there is no next move.\\uf020\\n \\nHowever, on some input string, , \\n, it is possible that the TM M loops for ever i.e. it never halts \\nwww.indiansbrain.com\\n \\nThe Halting Problem \\n \\nThe input to a Turing machine is a string. Turing machines themselves can be written as strings. \\nSince these strings can be used as input to other Turing machines. A “Universal Turing \\nmachine” is one whose input consists of a description M of some arbitrary Turing machine, and \\nsome input w to which machine M is to be applied, we write this combined input as M + w. \\nThis produces the same output that would be produced by M. This is written as \\n \\nUniversal Turing Machine (M + w) = M (w). \\n \\nAs a Turing machine can be represented as a string, it is fully possible to supply a Turing \\n \\nmachine as input to itself, for example M (M). This is not even a particularly bizarre thing to do for \\nexample, suppose you have written a C pretty printer in C, then used the Pretty printer on itself. \\nAnother common usage is Bootstrapping—where some convenient languages used to write a \\nminimal compiler for some new language L, then used this minimal compiler for L to write a new, \\nimproved compiler for language L. Each time a new feature is added to language L, you can \\nrecompile and use this new feature in the next version of the compiler. Turing machines sometimes \\nhalt, and sometimes they enter an infinite loop. \\n \\nA Turing machine might halt for one input string, but go into an infinite loop when given \\nsome other string. The halting problem asks: “It is possible to tell, in general, whether a given \\n \\nmachine will halt for some given input?” If it is possible, then there is an effective procedure to look \\nat a Turing machine and its input and determine whether the machine will halt with that input. If \\nthere is an effective procedure, then we can build a Turing machine to implement it. Suppose we \\nhave a Turing machine “WillHalt” which, given an input string M + w, will halt and accept the string \\nif Turing machine M halts on input w and will halt and reject the string if Turing machine M does \\nnot halt on input w. When viewed as a Boolean function, “WillHalt (M, w)” halts and returns \\n“TRUE” in the first case, and (halts and) returns “FALSE” in the second. \\n \\nTheorem \\nTuring Machine “WillHalt (M, w)” does not exist. \\n \\nProof: This theorem is proved by contradiction. Suppose we could build a machine “WillHalt”. \\nThen we can certainly build a second machine, “LoopIfHalts”, that will go into an infinite loop \\nif and only if “WillHalt” accepts its input: \\n Function LoopIfHalts (M, \\nw): if WillHalt (M, w) then \\nwhile true do { } \\nelse \\nreturn false; \\n \\nWe will also define a machine “LoopIfHaltOnItSelf” that, for any given input M, representing a \\nTuring machine, will determine what will happen if M is applied to itself, and loops if M will halt \\nin this case. \\n Function LoopIfHaltsOnItself (M): \\nreturn LoopIfHalts (M, M): \\n \\nFinally, we ask what happens if we try: \\nFunc tion Impos sible: \\nreturn LoopIfHaltsOnItself (LoopIfHaltsOnItself): \\n \\nThis machine, when applied to itself, goes into an infinite loop if and only if it halts \\nwhen applied to itself. This is impossible. Hence the theorem is proved. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nImplications of Halting Problem \\n \\nProgramming \\nThe Theorem of “Halting Problem” does not say that we can never determine whether or not \\n \\na given program halts on a given input. Most of the times, for practical reasons, we could \\neliminate infinite loops from programs. Sometimes a “meta-program” is used to check another \\nprogram for potential infinite loops, and get this meta-program to work most of the time. \\n \\nThe theorem says that we cannot ever write such a meta-program and have it work all of the \\ntime. This result is also used to demonstrate that certain other programs are also impossible. \\nThe basic outline is as follows: \\n \\n(i) If we could solve a problem X, we could solve the Halting problem \\n(ii) We cannot solve the Halting Problem \\n(iii) Therefore, we cannot solve problem X \\n \\n \\n \\n \\n \\n \\nA Turing machine can be \"programmed,\" in much the same manner as a computer is \\n \\nprogrammed. When one specifies the function which we usually call δ for a Tm, he is really \\nwriting a program for the Tm. \\n \\n1. Storage in finite Control \\n \\nThe finite control can be used to hold a finite amount of information. To do so, the state is written \\nas a pair of elements, one exercising control and the other storing a symbol. It should be \\n \\nemphasized that this arrangement is for conceptual purposes only. No modification in the definition \\nof the Turing machine has been made. \\n \\nExample \\nConsider the Turing machine \\nSolution \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2. Multiple Tracks \\n \\nWe can imagine that the tape of the Turing machine is divided into k tracks, for any finite k. This \\narrangement is shown in Fig., with k = 3. What is actually done is that the symbols on the tape \\nare considered as k-tuples. One component for each track. \\n \\nExample \\nThe tape in Fig. can be imagined to be that of a Turing machine which takes a binary input \\n \\ngreater than 2, written on the first track, and determines if it is a prime. The input is surrounded by \\n¢ and $ on the first track. \\n \\nThus, the allowable input symbols are [¢, B, B], [0, B, B ], [1, B, B ], and [$, B, B]. These \\n \\nsymbols can be identified with ¢, 0, 1, and $, respectively, when viewed as input symbols. The blank \\nwww.indiansbrain.com\\nsymbol can be represented by [B, B, B ] \\n \\nTo test if its input is a prime, the Tm first writes the number two in binary on the second track \\n \\nand copies the first track onto the third track. Then, the second track is subtracted, as many times \\nas possible, from the third track, effectively dividing the third track by the second and leaving the \\nremainder. If the remainder is zero, the number on the first track is not a prime. If the remainder is \\nnonzero, increase the number on the second track by one. \\n \\nIf now the second track equals the first, the number on the first track is a prime, because it cannot \\nbe divided by any number between one and itself. If the second is less than the first, the whole \\noperation is repeated for the new number on the second track. In Fig., the Tm is testing to determine \\nif 47 is a prime. The Tm is dividing by 5; already 5 has been subtracted twice, so 37 appears on the \\nthird track. \\n \\n3. Subroutines \\nwww.indiansbrain.com\\n \\nUNDECIDABILITY \\n \\n \\nDesign a Turing machine to add two given integers. \\n \\nSolution: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSome unsolvable Problems are as follows: \\n(i) Does a given Turing machine M halts on all input? \\n(ii) Does Turing machine M halt for any input? \\n(iii) Is the language L(M) finite? \\n \\n(iv) Does L(M) contain a string of length k, for some given k? \\n \\n(v) Do two Turing machines M1 and M2 accept the same language? \\n \\nIt is very obvious that if there is no algorithm that decides, for an arbitrary given Turing machine \\nM and input string w, whether or not M accepts w. These problems for which no algorithms exist \\nare called “UNDECIDABLE” or “UNSOLVABLE”. \\n \\nCode for Turing Machine: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\nDiagonalization language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis table represents language acceptable by Turing machine \\nwww.indiansbrain.com\\nProof that Ld is not recursively enumerable: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRecursive Languages: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUniversal \\n \\nLanguage: \\nwww.indiansbrain.com\\n \\nUndecidability of Universal Language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProblem -Reduction : \\nIf P1 reduced to P2, \\n \\nThen P2 is at least as hard as P1. \\nTheorem: If P1 reduces to P2 then, \\n\\uf095 If P1 is undecidable the so is P2.\\uf020\\n\\uf095 If P1 is Non-RE then so is P2.\\uf020\\nwww.indiansbrain.com\\nPost\\'s Correspondence Problem (PCP) \\n \\nA post correspondence system consists of a finite set of ordered pairs \\nwhere \\nfor some alphabet \\n. \\n \\nAny sequence of numbers \\n \\n \\nis called a solution to a Post Correspondence System. \\n \\nThe Post\\'s Correspondence Problem is the problem of determining whether \\na Post Correspondence system has a solutions. \\n \\nExample 1 : Consider the post correspondence system \\n \\n The list 1,2,1,3 is a solution to it. \\n \\nBecause \\n \\n \\n \\n \\n \\n \\n \\ni \\n             xi                                 yi \\n \\n1 \\n \\n2 \\n \\n3 \\n \\n \\n(A post correspondence system is also denoted as an instance of the \\nPCP) Example 2 : The following PCP instance has no solution \\ni \\n          xi                          yi \\n \\n1 \\n \\n2 \\n \\nThis can be proved as follows. \\ncannot be chosen at the start, since than the LHS and RHS would \\ndiffer in the first symbol ( \\nin LHS and \\nin RHS). So, we must start with \\n. The next pair must be \\nso that the 3 rd symbol in the RHS becomes identical to that of the LHS, which is a . After this step, \\nLHS and RHS are not matching. If \\nis selected next, then would be mismatched in the 7 th symbol \\nwww.indiansbrain.com\\n( \\nin LHS and \\nin RHS). If \\nis selected, instead, there will not be any choice to match the both side \\nin the next step. \\n \\nExample3 : The list 1,3,2,3 is a solution to the following PCP instance. \\n \\n \\ni \\n   \\nx\\ni \\n   \\ny\\ni \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n1 \\n \\n1 \\n \\n101 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n2 \\n \\n10 \\n \\n00 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n3 \\n011 \\n11 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\nThe following properties can easily be proved. \\n \\nProposition The Post Correspondence System \\n \\n has solutions if and only if \\n \\n \\n \\n \\n \\n \\nCorollary : PCP over one-letter alphabet is decidable. \\n \\nProposition Any PCP instance over an alphabet \\nwith \\nis equivalent to a PCP instance over \\nan alphabet \\nwith \\n \\n \\nProof : Let \\n \\n \\nConsider \\nWe can now encode every \\nas \\nany PCP instance over \\nwill now \\nhave only two symbols, 0 and 1 and, hence, is equivalent to a PCP instance over \\n \\n \\nTheorem : PCP is undecidable. That is, there is no algorithm that determines whether an arbitrary Post \\nCorrespondence System has a solution. \\n \\nProof: The halting problem of turning machine can be reduced to PCP to show the undecidability of PCP. Since \\nhalting problem of TM is undecidable (already proved), This reduction shows that PCP is also undecidable. \\nThe proof is little bit lengthy and left as an exercise. \\n \\nSome undecidable problem in context-free languages \\n \\nWe can use the undecidability of PCP to show that many problem concerning the context-free languages \\nare undecidable. To prove this we reduce the PCP to each of these problem. The following discussion \\nmakes it clear how PCP can be used to serve this purpose. \\nwww.indiansbrain.com\\nLet \\nbe a Post Correspondence System over the alphabet \\n. We \\nconstruct two CFG\\'s Gx and Gy from the ordered pairs x,y respectively as follows. \\n \\n and \\n \\n where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\n \\nit is clear that the grammar \\ngenerates the strings that can appear in the LHS of a sequence while solving the \\nPCP followed by a sequence of numbers. The sequence of number at the end records the sequence of \\n \\nstrings from the PCP instance (in reverse order) that generates the string. Similarly, \\ngenerates the \\nstrings that can be obtained from the RHS of a sequence and the corresponding sequence of numbers (in \\nreverse order). \\n \\nNow, if the Post Correspondence System has a solution, then there must be a sequence \\n \\n \\n \\n \\n \\n \\n \\n \\nAccording to the construction of \\nand \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn this case \\nwww.indiansbrain.com\\n \\n \\nHence , \\nand \\nimplying \\n \\n \\n \\n \\n \\nConversely, let \\n \\n \\nHence, w must be in the form w1w2 where \\nand w2 in a sequence \\n(since, only that kind \\nof strings can be generated by each of \\nand \\n). \\n \\nNow, the string \\nis a solution to the Post Correspondence System. \\n \\nIt is interesting to note that we have here reduced PCP to the language of pairs of CFG,s whose intersection is \\nnonempty. The following result is a direct conclusion of the above. \\n \\nTheorem : Given any two CFG\\'s G1 and G2 the question \"Is \\n\" is undecidable. \\n \\nProof: Assume for contradiction that there exists an algorithm A to decide this question. This would imply \\nthat PCP is decidable as shown below. \\n \\nFor any Post Correspondence System, P construct grammars \\nand \\nby using the constructions \\nelaborated already. We can now use the algorithm A to decide whether and \\nThus, PCP is decidable, a contradiction. So, such an algorithm does not exist. \\n \\nIf \\nand \\nare CFG\\'s constructed from any arbitrary Post Correspondence System, than it is not difficult to \\n \\nshow that \\nand \\nare also context-free, even though the class of context-free languages are \\nnot closed under complementation. \\n \\nand their complements can be used in various ways to show that many other questions \\nrelated to CFL\\'s are undecidable. We prove here some of those. \\n \\nTheorem : Foe any two arbitrary CFG\\'s \\nthe following questions are undecidable \\n \\ni. \\nIs \\n \\n \\nii. \\nIs \\n \\nwww.indiansbrain.com\\niii. Is \\n \\nProof : \\ni. \\nIf \\nthen, \\n \\n \\nHence, it suffice to show that the question “Is \\n\" is undecidable. \\n \\nSince, \\nand \\nare CFl\\'s and CFL\\'s are closed under union, \\nis also context-\\nfree. By DeMorgan\\'s theorem, \\n \\n \\nIf there is an algorithm to decide whether \\nwe can use it to decide whether \\nor not. But this problem has already been proved to be undecidable. \\n \\n \\nHence there is no such algorithm to decide or not. \\nii. \\n \\nLet P be any arbitrary Post correspondence system and \\nand \\nare CFg\\'s constructed from the pairs of \\nstrings. \\n \\nmust be a CFL and let G1generates L1. That is, \\n \\n \\n \\n \\n \\nby De Morgan\\'s theorem, as shown already, any string, \\nrepresents a solution to the \\nPCP. Hence, \\ncontains all but those strings representing the solution to the PCP. \\n \\nLet \\nfor same CFG G2. \\n \\nIt is now obvious that \\nif and only if the PCP has no solutions, which is already proved to be \\nundecidable. Hence, the question “Is \\n?\" is undecidable. \\n \\niii. \\nwww.indiansbrain.com\\nLet \\nbe a CFG generating the language \\nand G2 be a CFG generating \\nwhere \\nand \\nare CFG.s constructed from same arbitrary instance of PCP. \\n \\niff \\n \\n \\ni.e. iff the PCP instance has no solutions as discussed in part (ii). \\n \\nHence the proof. \\n \\nTheorem : It is undecidable whether an arbitrary CFG is ambiguous. \\n \\nProof : Consider an arbitrary instance of PCP and construct the CFG\\'s \\nand \\nfrom the ordered pairs \\nof strings. \\n \\nWe construct a new grammar G from \\nand \\nas follows. \\n \\n where \\n \\n \\n \\n \\n \\nis same as that of \\nand \\n. \\n \\n \\n \\n \\n \\nThis constructions gives a reduction of PCP to the -------- of whether a CFG is ambiguous, thus leading to \\nthe undecidability of the given problem. That is, we will now show that the PCP has a solution if and only if G \\nis ambiguous. (where G is constructed from an arbitrary instance of PCP). \\n \\nOnly if Assume that \\nis a solution sequence to this instance of PCP. \\n \\nConsider the following two derivation in \\n. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBut , \\n \\n \\n \\n \\nis a solution to the PCP. Hence the same string of terminals \\nhas two derivations. Both these \\nderivations are, clearly, leftmost. Hence G is ambiguous. \\n \\nIf It is important to note that any string of terminals cannot have more than one derivation in \\nand \\n \\nBecause, every terminal string which are derivable under these grammars ends with a sequence of integers \\nThis sequence uniquely determines which productions must be used at every step of the derivation. \\n \\nHence, if a terminal string, \\n, has two leftmost derivations, then one of them must begin with \\nthe step. \\n \\nthen continues with derivations under \\n \\n \\nIn both derivations the resulting string must end with a sequence \\nfor same \\nThe reverse \\nof this sequence must be a solution to the PCP, because the string that precede in one case is \\n \\nand \\nin the other case. Since the string derived in both cases are identical, the \\n \\nsequence \\n \\nmust be a solution to the PCP. \\n \\nHence the proof \\nwww.indiansbrain.com\\nClass p-problem solvable in polynomial time: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNon deterministic polynomial time: \\n \\nA nondeterministic TM that never makes more than p(n) moves in any sequence of choices for \\nsome polynomial p is said to be non polynomial time NTM. \\n\\uf095 NP is the set of languags that are accepted by polynomial time NTM’s\\uf020\\n\\uf020\\n\\uf095 Many problems are in NP but appear not to be in p.\\uf020\\n\\uf095 One of the great mathematical questions of our age: is there anything in NP that is not in p?\\uf020\\nNP-complete problems: \\n\\uf020\\nIf We cannot resolve the “p=np question, we can at least demonstrate that certain problems in NP \\nare the hardest , in the sense that if any one of them were in P , then P=NP. \\n\\uf020\\n\\uf095 These are called NP-complete.\\uf020\\n\\uf020\\n\\uf095 Intellectual leverage: Each NP-complete problem’s apparent difficulty reinforces the belief \\nthat they are all hard.\\uf020\\n \\nMethods for proving NP-Complete problems: \\n \\n\\uf095 Polynomial time reduction (PTR): Take time that is some polynomial in the input size to \\nconvert instances of one problem to instances of another.\\uf020\\n\\uf020\\n\\uf095 If P1 PTR to P2 and P2 is in P1 the so is P1.\\uf020\\n\\uf095 Start by showing every problem in NP has a PTR to Satisfiability of Boolean formula.\\uf020\\n\\uf020\\n\\uf095 Then, more problems can be proven NP complete by showing that SAT PTRs to them \\ndirectly or indirectly.\\uf020\\nwww.indiansbrain.com\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'combined_theory_of_computation_content.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'combined_theory_of_computation_content.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automata Tutorial | Theory of Computation - JavatpointTutorials×PythonPython Django Numpy Pandas Tkinter Pytorch Flask OpenCVAI, ML and Data ScienceArtificial Intelligence Machine Learning Data Science Deep Learning TensorFlow Artificial Neural Network Matplotlib Python ScipyJavaJava Servlet JSP Spring Boot Spring Framework Hibernate JavaFX Java Web ServicesB.Tech and MCADBMS Data Structures Operating System Computer Network DAA Computer Organization Software Engineering Data MiningWeb TechnologyHTML CSS JavaScript Jquery Angular-8 React JS React Native Node JSSoftware TestingSoftware Testing Selenium JIRA JMeter Postman TestNG SoapUI CucumberInterview×Technical InterviewC C++ Php Java Python JavaScript TypeScriptJava InterviewJDBC Servlet Maven Jenkins Spring Spring Boot JDB Hibernate JSFWeb InterviewHTML CSS JavaScript Jquery Angular Node-JS AJAXDatabase InterviewDBMS SQL PL/SQL Oracle MySQL MongoDB Redis MariaDBCompany InterviewsIBM Adobe Microsoft Amazon TCS HCL Wipro DXC Accenture Capgemini Space X Ericsson Infosy IGate EXL IndiaMART SapientCompilerPythonJavaPhpCC++RHtmlJavascriptTypescriptSwiftHome Python Java JavaScriptHTML SQL PHP C# C++ DS Aptitude Reasoning Selenium DBMS C Andriod Interview QAutomata TutorialAutomata TutorialTheory of AutomataFinite AutomataTransition DiagramTransition TableDFAExamples of DFANFAExamples of NFAEliminating ε TransitionsConversion from NFA to DFAConversion from NFA with ε to DFAMinimization of DFARegular ExpressionRegular ExpressionExamples of Regular ExpressionConversion of RE to FAArden\\'s TheoremMoore MachineMealy MachineConversion from Mealy machine to Moore machineConversion from Moore machine to Mealy machineCFGContext-free GrammarDerivationDerivation TreeAmbiguity in GrammarUnambiguous GrammarSimplification of CFGChomsky\\'s Normal Form (CNF)Greibach Normal Form (GNF)PDAPushdown AutomataPDA AcceptanceNon-deterministic Pushdown AutomataCFG to PDA ConversionTuring MachineApplication of Different Automata | Theory of ComputationIntroduction to Computational Complexity TheoryAutomata and Game TheoryRecursive Descent Parsernext →Automata TutorialTheory of automata is a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. An automaton with a finite number of states is called a Finite automaton.In this tutorial, we are going to learn how to construct deterministic finite automata, non-deterministic finite automata, Regular expression, context-free grammar, context-free language, Push down automata, Turning machines, etc.PrerequisiteBefore learning Automata, you should have a basic understanding of string, language, alphabets, symbols.AudienceOur Automata Tutorial is designed to help beginners and professionals.ProblemsWe assure that you will not find any problem in this Automata Tutorial. But if there is any mistake, please post the problem in contact form.Next TopicTheory of Automatanext →Latest CoursesWe provides tutorials and interview questions of all technology like java tutorial, android, java frameworksContact info G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India[email\\xa0protected].Follow usLatest PostPRIVACY POLICYTutorialsJava Data Structures C Programming C++ Tutorial C# Tutorial PHP Tutorial HTML Tutorial JavaScript Tutorial jQuery Tutorial Spring TutorialInterview QuestionsTcs Intuit Wipro Adobe Infosys Amazon Accenture Cognizant Capgemini MicrosoftOnline CompilerC R C++ Php Java Html Swift Python JavaScript TypeScript© Copyright 2024 Javatpoint. All Rights Reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction of Theory of Computation - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA to DevelopmentNewly Launched!Android with KotlinGenerative AI & ChatGPTMaster Django FrameworkBecome AWS CertifiedFor Working ProfessionalsInterview 101: DSA & System DesignData Science Training ProgramJAVA Backend Development (Live)DevOps Engineering (LIVE)Software Testing & Automation (Live)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation CourseData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive Programming (Live)Full Stack Development with React & Node JS (Live)GATE Exam CoursesGATE CS & IT (Self-Paced)GATE DS & AI (Self-Paced)All CoursesTutorialsData Structures & AlgorithmsDSA for BeginnersData StructuresArraysMatrixStringsLinked ListStackQueueTreeGeneric TreeBinary TreeBinary Search TreeAVL TreeB TreeB+ TreeRed Black TreeTree Data Structure TutorialHeapHashingGraphSet Data StructureMap Data StructureAdvanced Data StructureData Structures TutorialAlgorithmsAnalysis of AlgorithmsSearching AlgorithmsLinear SearchBinary SearchSearching Algorithms TutorialSorting AlgorithmsSelection SortBubble SortInsertion SortMerge SortQuick SortHeap SortCounting SortRadix SortBucket SortSorting Algorithms TutorialGreedy AlgorithmsDynamic ProgrammingGraph AlgorithmsPattern SearchingRecursionBacktrackingDivide and ConquerMathematical AlgorithmsGeometric AlgorithmsBitwise AlgorithmsRandomized AlgorithmsBranch and BoundAlgorithms TutorialComplete DSA TutorialCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE SheetNetflix SDE SheetGoogle SDE SheetWipro Coding SheetInfosys Coding SheetTCS Coding SheetCognizant Coding SheetHCL Coding SheetDSA Cheat SheetsDSA Sheet for BeginnersSDE SheetsFAANG Coding SheetLove Babbaar SheetMass Recruiter SheetProduct-Based Coding SheetCompany-Wise Preparation SheetTop 100 DSA Interview Questions Topic-wise100 Days of CodePythonPython TutorialPython ExercisesPython List ExercisePython String ExercisePython Tuple ExercisePython Dictionary ExercisePython Set ExercisePython Excercises Topic wisePython QuizPython ProgramsAdvanced Python TutorialPython API TutorialPython Database TutorialPython JSONPython Cheat SheetPython ProjectsPython Interview QuestionsML & Data ScienceMachine LearningMachine Learning TutorialMaths for MLML Projects100 Days of Machine LearningData Science TutorialData Science PackagesPandas TutorialNumPy TutorialData VisualizationData Visualization with PythonData Visualization with RTableauPower BIData AnalysisData Analysis with PythonData Analysis with R100 Days of Data AnalyticsDeep LearningNLP TutorialOpenCV TutorialInterview QuestionsMachine Learning Interview QuestionsDeep Learning Interview QuestionsR Interview QuestionsSystem DesignSystem Design TutorialSoftware Design PatternsSystem Design RoadmapTop 10 System Design Interview QuestionsInterview CornerCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsPuzzles for InterviewsLanguagesCC++JavaPythonR TutorialC#SQLScalaPerlGo LanguageWeb DevelopmentHTMLHTML TutorialFree HTML CourseHTML Cheat SheetCSSCSS TutorialFree CSS CourseCSS Cheat SheetJavaScriptJavaScript TutorialJavaScript QuestionsJavaScript Cheat SheetDSA using JavaScriptFree JavaScript CourseJavaScript A to Z Complete GuideTypeScriptReactJSReactJS TutorialFree ReactJS CourseReactJS Cheat SheetNextJSNode.jsPHPAngularJSjQueryWeb Development Using PythonDjangoFlaskSeleniumPostmanGithubWeb Design100 Days of Web DevelopmentCS SubjectsOperating SystemDBMSComputer NetworksEngineering MathematicsComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOps And LinuxDevOps TutorialGITAWSKubernetesDockerMicrosoft Azure TutorialGoogle Cloud PlatformDevOps RoadmapDevOps Interview QuestionsLinuxLinux TutorialLinux Commands A-ZLinux Commands CheatsheetFile Permissions in LinuxLinux System AdministrationLinux Shell ScriptingLinux NetworkingLinux Interview QuestionsSchool LearningClass 8 Study MaterialClass 9 Study MaterialClass 10 Study MaterialClass 11Study MaterialClass 12 Study MaterialEnglish GrammarGfG SchoolCommerceGATEGATE Computer Science NotesLast Minute NotesGATE CS Solved PapersGATE CS Original Papers and Official KeysGATE CS 2025 SyllabusGATE DA 2025 SyllabusOther CS ExamsISROUGC NETGeeksforGeeks VideosJobsGet Hired: Apply for JobsCorporate Hiring SolutionsFiltered JobsJobs for FreshersJobs for ExperiencedAll JobsPracticePractice Coding ProblemsAll DSA ProblemsProblem of the DayCompany Wise Coding PracticeAmazonMicrosoftFlipkartExplore AllGfG SDE SheetPractice Problems Difficulty WiseBasicEasyMediumHardLanguage Wise Coding PracticeCPPJavaPythonCurated DSA ListsBeginner\\'s DSA SheetLove Babbar SheetTop 50 Array ProblemsTop 50 String ProblemsTop 50 DP ProblemsTop 50 Graph ProblemsTop 50 Tree ProblemsContestsJob-A-Thon Hiring ChallengeGfG Weekly [Rated Contest]All Contests and Events\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\n\\n                                        You\\'re all caught up!!\\n\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAptitudeEngineering MathematicsDiscrete MathematicsOperating SystemDBMSComputer NetworksDigital Logic and DesignC ProgrammingData StructuresAlgorithmsTheory of ComputationCompiler DesignComputer Org and Architecture \\n\\n\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGo PremiumShare Your ExperiencesAutomata TutorialAutomata _ IntroductionIntroduction of Theory of ComputationChomsky Hierarchy in Theory of ComputationApplications of various AutomataRegular Expression and Finite AutomataIntroduction of Finite AutomataArden\\'s Theorem in Theory of ComputationArden\\'s Theorem and Challenging Applications | Set 2L-graphs and what they represent in TOCHypothesis (language regularity) and algorithm (L-graph to NFA) in TOCRegular Expressions, Regular Grammar and Regular LanguagesHow to identify if a language is regular or notDesigning Finite Automata from Regular Expression (Set 1)Star Height of Regular Expression and Regular LanguageGenerating regular expression from Finite AutomataDesigning Deterministic Finite Automata (Set 1)Designing Deterministic Finite Automata (Set 2)DFA for Strings not ending with \"THE\"DFA of a string with at least two 0’s and at least two 1’sDFA for accepting the language L = {  anbm | n+m=even }DFA machines accepting odd number of 0’s or/and even number of 1’sDFA of a string in which 2nd symbol from RHS is \\'a\\'Union process in DFAConcatenation process in DFADFA in LEX code which accepts even number of zeros and even number of onesConversion from NFA to DFAMinimization of DFAReversing Deterministic Finite AutomataComplementation process in DFAKleene\\'s Theorem in TOC | Part-1Mealy\\xa0and\\xa0Moore\\xa0Machines in TOCDifference Between Mealy Machine and Moore MachineCFGRelationship between grammar and language in Theory of ComputationSimplifying Context Free GrammarsClosure Properties of Context Free LanguagesUnion and Intersection of Regular languages with CFLConverting Context Free Grammar to Chomsky Normal FormConverting Context Free Grammar to Greibach Normal FormPumping Lemma in Theory of ComputationCheck if the language is Context Free or NotAmbiguity in Context free Grammar and Context free LanguagesOperator grammar and precedence parser in TOCContext-sensitive Grammar (CSG) and Language (CSL)PDA (Pushdown Automata)Introduction of Pushdown AutomataPushdown Automata Acceptance by Final StateConstruct Pushdown Automata for given languagesConstruct Pushdown Automata for all length palindromeDetailed Study of PushDown AutomataNPDA for accepting the language  L = {an bm cn | m,n>=1}NPDA for accepting the language L = {an bn cm | m,n>=1}NPDA for accepting the language  L = {an bn | n>=1}NPDA for accepting the language  L = {am b(2m) | m>=1}NPDA for accepting the language  L = {am bn cp dq | m+n=p+q ; m,n,p,q>=1}Construct Pushdown automata for L = {0n1m2m3n | m,n ≥ 0}Construct Pushdown automata for L = {0n1m2(n+m) | m,n ≥ 0}NPDA for accepting the language L = {ambnc(m+n) | m,n ≥ 1}NPDA for accepting the language L = {amb(m+n)cn | m,n ≥ 1}NPDA for accepting the language L = {a2mb3m | m ≥ 1}NPDA for accepting the language L = {amb(2m+1) | m ≥ 1}NPDA for accepting the language L = {aibjckdl | i==k or j==l,i>=1,j>=1}Construct Pushdown automata for L = {a(2*m)c(4*n)dnbm | m,n ≥ 0}NPDA for L =  {0i1j2k | i==j or j==k ; i , j , k >= 1}NPDA for accepting the language L = {anb(2n) | n>=1} U {anbn | n>=1}NPDA for the language L ={w∈ {a,b}*| w contains equal no. of a\\'s and b\\'s}Turing MachineRecursive and Recursive Enumerable Languages in TOCTuring Machine in TOCTuring Machine for additionTuring machine for subtraction | Set 1Turing machine for multiplicationTuring machine for copying dataConstruct a Turing Machine for language L = {0n1n2n | n≥1}Construct a Turing Machine for language L = {wwr | w ∈ {0, 1}}Construct a Turing Machine for language L = {ww | w ∈ {0,1}}Construct Turing machine for L = {an bm a(n+m) | n,m≥1}Construct a Turing machine for L = {aibjck | i*j = k; i, j, k ≥ 1}Turing machine for 1\\'s and 2’s complementRecursive and Recursive Enumerable Languages in TOCTuring Machine for subtraction | Set 2Halting Problem in Theory of ComputationTuring Machine as ComparatorDecidabilityDecidable and Undecidable Problems in Theory of ComputationUndecidability and Reducibility in TOCComputable and non-computable problems in TOCTOC Interview preparationLast Minute Notes - Theory of ComputationTOC  Quiz and PYQ\\'s in TOCTheory of Computation - GATE CSE Previous Year QuestionsRegular languages and finite automataContext free languages and Push-down automataRecursively enumerable sets and Turing machinesUndecidabilityDSA to Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction of Theory of Computation\\n\\n\\n\\nLast Updated : \\n27 Sep, 2024\\n\\n\\n\\n\\n\\n\\nSummarize\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\nFollow\\n\\n\\n\\n\\n\\nAutomata theory (also referred to as the Theory Of Computation) is a branch of Computer Science and Mathematics that studies how machines compute functions and solve problems. This field is mainly focused on mathematical structures called automata and is crucial for the purpose of studying processes occurring in discrete systems.\\nWhat is Automata Theory?\\nIn automata theory, scientists and engineers can predict the behavior of computing systems thereby improving problem-solving approaches. Originally developed to describe and explain the dynamics of systems, automata theory is the theoretical base of the formal languages theory, grammar, and computational complexity.\\nBasic Terminologies of Theory of Computation\\nNow, let’s understand the basic terminologies, which are important and frequently used in the Theory of Computation.\\xa0\\nSymbol\\nA symbol (often also called a character) is the smallest building block, which can be any alphabet, letter, or picture.\\xa0\\n\\nAlphabets (Σ)\\nAlphabets are a set of symbols, which are always finite.\\xa0\\n\\nString\\xa0\\nA string is a finite sequence of symbols from some alphabet. A string is generally denoted as w and the length of a string is denoted as |w|.\\xa0\\nEmpty string is the string with zero occurrence of symbols, represented as ε.\\nNumber of Strings (of length 2) that can be generated over the alphabet {a, b}:                     -   -                     a   a                     a   b                     b   a                     b   bLength of String |w| = 2Number of Strings = 4Conclusion:For alphabet {a, b} with length n, number of strings can be generated = 2n.\\n__mask-blockquote__index=1__\\n\\nThe Theory of Computation explores automata, languages, and complexity. If you want to dive deeper into this subject for GATE, the GATE CS Self-Paced Course covers it extensively.\\n\\nClosure Representation in TOC\\nL+: It is a Positive Closure that represents a set of all strings except Null or ε-strings.\\nL*: It is “Kleene Closure“, that represents the occurrence of certain alphabets for given language alphabets from zero to the infinite number of times. In which ε-string is also included.\\nFrom the above two statements, it can be concluded that:\\nL* = εL+\\nExample:(a) Regular expression for language accepting all combination of g\\'s over Σ={g}:                                         R = g*                               R={ε,g,gg,ggg,gggg,ggggg,...}(b) Regular Expression for language accepting all combination of g\\'s over Σ={g} : R = g+                               R={g,gg,ggg,gggg,ggggg,gggggg,...}\\nNote: Σ* is a set of all possible strings(often power set(need not be unique here or we can say multiset) of string) So this implies that language is a subset of Σ*.This is also called a “Kleene Star”.\\nKleene Star is also called a “Kleene Operator” or “Kleene Closure”. Engineers and IT professionals make use of Kleene Star to achieve all set of strings which is to be included from a given set of characters or symbols. It is one kind of Unary operator. In Kleene Star methodology all individual elements of a given string must be present but additional elements or combinations of these alphabets can be included to any extent.\\nExample:Input String: \"GFG\".Σ* = { ε,\"GFG\",\"GGFG\",\"GGFG\",\"GFGGGGGGGG\",\"GGGGGGGGFFFFFFFFFGGGGGGGG\",...}  (Kleene Star is an infinite set but if we provide any grammar rules then it can work as a finite set.Please note that we can include ε string also in given Kleene star representation.)\\nLanguage\\nA language is a set of strings, chosen from some Σ* or we can say- ‘A language is a subset of Σ* ‘. A language that can be formed over ‘ Σ ‘ can be Finite or Infinite.\\nExample of Finite Language:           L1 = { set of string of 2 }         L1 = { xy, yx, xx, yy }Example of Infinite Language:         L1 = { set of all strings starts with \\'b\\' }         L1 = { babb, baa, ba, bbb, baab, ....... }\\nConclusion\\nIt is an important branch of computation that is concerned with formal languages, and automata theory in particular. It provides a basis for other courses such as Turing machines and computational complexity that are very important in computer science.\\nIntroduction of Theory of Computation – FAQs\\nWhat is the relevance of the automata theory in computer science?\\n\\nAutomata theory is used in modeling computational problems hence enhancing the understanding and design of systems such as compilers, interpreters among others.\\n\\nwhat is the purpose of using Kleene Star in the study of formal languages?\\n\\nThe Kleene Star extends symbols from a given alphabet where one is able to create infinite strings from it or even the null string.\\n\\nIs it possible to implement automata theory into real life?\\n\\nOf course, automata theory has found its use in certain areas like compiler design, artificial intelligence, network security and natural language processing.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nabhishek1 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Article\\n\\n\\n\\nAutomata Tutorial\\n\\n\\n\\n\\nNext Article\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\n\\n\\n\\n\\n\\n\\nRead More\\n\\n\\n\\nSimilar Reads\\n\\n\\n\\nIntroduction to Computation Complex Theory\\nBroad Overview : Complexity theory, in a nutshell, a complexity word is a quite fancy word, literally, it sounds complex, but it is not an intimidating topic. What it really means is analyzing the program or we can say analyzing the efficiency of the program, figuring out whether the program is correct, figuring out whether one program is better th\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nIntroduction To Grammar in Theory of Computation\\nPrerequisite - Theory of ComputationGrammar :It is a finite set of formal rules for generating syntactically correct sentences or meaningful correct sentences.Constitute Of Grammar :Grammar is basically composed of two basic elements - Terminal Symbols - Terminal symbols are those which are the components of the sentences generated using a grammar\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nTheory of Computation - GATE CSE Previous Year Questions\\nSolving GATE Previous Year\\'s Questions (PYQs) not only clears the concepts but also helps to gain flexibility, speed, accuracy, and understanding of the level of questions generally asked in the GATE exam, and that eventually helps you to gain good marks in the examination. Previous Year Questions help a candidate practice and revise for GATE, whic\\n\\n\\n\\n5 min read\\n\\n\\n\\n\\n\\nRelationship between grammar and language in Theory of Computation\\nA grammar is a set of production rules which are used to generate strings of a language. In this article, we have discussed how to find the language generated by a grammar and vice versa as well. Language generated by a grammar - Given a grammar G, its corresponding language L(G) represents the set of all strings generated from G. Consider the foll\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nTheory of Computation | Regular languages and finite automata | Question 2\\nWhat is the complement of the language accepted by the NFA shown below? (A) A (B) B (C) C (D) D Answer: (B) Explanation: Quiz of this QuestionPlease comment below if you find anything wrong in the above post\\n\\n\\n\\n1 min read\\n\\n\\n\\n\\nArden\\'s Theorem in Theory of Computation\\nArden\\'s theorem state that: \"If P and Q are two regular expressions over \"∑\", and if P does not contain \"∈\" , then the following equation in R given by R = Q + RP has a unique solution i.e., R = QP*.\" That means, whenever we get any equation in the form of R = Q + RP, then we can directly replace it with R = QP*. So, here we will first prove that R\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nDecidability Table in Theory of Computation\\nPrerequisite - Undecidability, Decidable and undecidable problems Identifying languages (or problems*) as decidable, undecidable or partially decidable is a very common question in GATE. With correct knowledge and ample experience, this question becomes very easy to solve. A language is undecidable if it is not decidable. An undecidable language ma\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\nAccording to Chomsky hierarchy, grammar is divided into 4 types as follows: Type 0 is known as unrestricted grammar.Type 1 is known as context-sensitive grammar.Type 2 is known as a context-free grammar.Type 3 Regular Grammar.Type 0: Unrestricted Grammar: Type-0 grammars include all formal grammar. Type 0 grammar languages are recognized by turing\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nPumping Lemma in Theory of Computation\\nThere are two Pumping Lemmas, which are defined for 1. Regular Languages, and 2. Context - Free Languages Pumping Lemma for Regular Languages For any regular language L, there exists an integer n, such that for all x ? L with |x| ? n, there exists u, v, w ? ?*, such that x = uvw, and (1) |uv| ? n (2) |v| ? 1 (3) for all i ? 0: uviw ? L In simple te\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nDecidable and Undecidable Problems in Theory of Computation\\nIn the Theory of Computation, problems can be classified into decidable and undecidable categories based on whether they can be solved using an algorithm. A decidable problem is one for which a solution can be found in a finite amount of time, meaning there exists an algorithm that can always provide a correct answer. While an undecidable problem i\\n\\n\\n\\n6 min read\\n\\n\\n\\n\\nHalting Problem in Theory of Computation\\nTo understand better the halting problem, we must know Decidability , Undecidability and Turing machine , decision problems and also a theory named as Computability theory and Computational complexity theory. Some important terms: Computability theory - The branch of theory of computation that studies which problems are computationally solvable usi\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nAutomata Theory | Set 2\\nFollowing questions have been asked in GATE CS 2012 exam. 1) What is the complement of the language accepted by the NFA shown below? Assume ∑ = {a} and ε is the empty string (A) Φ (B) ε (C) a (D) {a, ε} Answer (B) The given alphabet ∑ contains only one symbol {a} and the given NFA accepts all strings with any number of occurrences of \\'a\\'. In other\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 3\\nFollowing questions have been asked in GATE CS 2011 exam. 1) The lexical analysis for a modern language such as Java needs the power of which one of the following machine models in a necessary and sufficient sense? (A) Finite state automata (B) Deterministic pushdown automata (C) Non-deterministic pushdown automata (D) Turing machine Answer (A) Lex\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nAutomata Theory | Set 4\\nFollowing questions have been asked in GATE CS 2011 exam. 1) Let P be a regular language and Q be context-free language such that Q ⊆ P. (For example, let P be the language represented by the regular expression p*q* and Q be {pnqn|n ∈ N}). Then which of the following is ALWAYS regular? (A) P ∩ Q (B) P - Q (C) ∑* - P (D) ∑* - Q\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\nAutomata Theory | Set 5\\nFollowing questions have been asked in GATE CS 2009 exam. 1) S --> aSa| bSb| a| b ;The language generated by the above grammar over the alphabet {a,b} is the set of (A) All palindromes. (B) All odd length palindromes. (C) Strings that begin and end with the same symbol (D) All even length palindromes. Answer (B) The strings accepted by language are\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 7\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider L= {(TM) | TM is the Turing machine that halts on all input and L(TM)= L\\' for some undecidable language L\\'}. Here, (TM) is the encoding of a Turing machine as a string over alphabet {0, 1} then L is: (A) decidable and recursively enumerable (B) decidable and recursive (C) decid\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 8\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Which one of the following language is Regular? (A) {wxwR | w,x ∈ (a+b)+} (B) {wxwR | w ∈ (a+b)*, x ∈ {a,b}} (C) {wwRx | w,x ∈ (a+b)+} (D) {wwR | w ∈ (a+b)*} Explanation: (A) It is correct, since this language can form regular expression which is {{ a(a + b)+a } + {b(a + b)+b}}, i.e., s\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n\\nAutomata Theory | Set 9\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider the following two statements with respect to Countability: Statement-1: If X union of \\'Y\\' is uncountable, then both set \\'X\\' and set \\'Y\\' must be uncountable. Statement-2: The Cartesian product of two countable sets \\'X\\' and \\'Y\\' is countable. Which of the following option is true\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nAutomata Theory | Set 10\\nThese questions for practice purpose of GATE CS Exam. Ques-1: Consider the following statements: X: For any language either a language L or its complement L\\' must be finite.Y: DFA for language which contains epsilon must have initial state as final state.Z: Non-deterministic finite automata is more powerful than deterministic finite automata. Which\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nRegular Graph in Graph Theory\\nPrerequisite: Graph Theory Basics – Set 1, Set 2 Regular Graph: A graph is called regular graph if degree of each vertex is equal. A graph is called K regular if degree of each vertex in the graph is K. Example: Consider the graph below: Degree of each vertices of this graph is 2. So, the graph is 2 Regular. Similarly, below graphs are 3 Regular an\\n\\n\\n\\n2 min read\\n\\n\\n\\n\\n5 Color Theorem in Graph Theory\\nThe graph is a data structure that is used extensively in real-life. Planar Graph: If a graph can be drawn on the plane without crossing, it is said to be planar. Coloring of a simple graph is the assignment of color to each vertex of the graph so that no two adjacent vertices are assigned the same color. Bi-Partite Graphs: A bipartite graph, also\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nMathematics | Graph Theory Basics - Set 1\\nA graph is a data structure that is defined by two components : A node or a vertex.An edge E or ordered pair is a connection between two nodes u,v that is identified by unique pair(u,v). The pair (u,v) is ordered because (u,v) is not same as (v,u) in case of directed graph.The edge may have a weight or is set to one in case of unweighted graph.Cons\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nMathematics | Graph theory practice questions\\nProblem 1 - There are 25 telephones in Geeksland. Is it possible to connect them with wires so that each telephone is connected with exactly 7 others. Solution - Let us suppose that such an arrangement is possible. This can be viewed as a graph in which telephones are represented using vertices and wires using the edges. Now we have 25 vertices in\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\nSet Theory Operations in Relational Algebra\\nRelational Algebra in DBMS These Set Theory operations are the standard mathematical operations on set. These operations are Binary operations that are, operated on 2 relations unlike PROJECT, SELECT and RENAME operations. These operations are used to merge 2 sets in various ways. The set operation is mainly categorized into the following: Union op\\n\\n\\n\\n3 min read\\n\\n\\n\\n\\nApplications of Group Theory\\nGroup theory is the branch of mathematics that includes the study of elements in a group. Group is the fundamental concept of algebraic structure like other algebraic structures like rings and fields. Group: A non-empty set G with * as operation, (G, *) is called a group if it follows the closure, associativity, identity, and inverse properties. Pr\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\nQuotient Group in Group Theory\\nWe can say that \"o\" is the binary operation on set G if: G is a non-empty set & G * G = { (a,b): a, b∈ G } and o: G * G --> G. Here, aob denotes the image of ordered pair (a,b) under the function/operation o.Example - \"+\" is called a binary operation on G (any non-empty set ) if & only if: a+b ∈G; ∀ a,b ∈G and a+b give the same result ev\\n\\n\\n\\n12 min read\\n\\n\\n\\n\\nTypes of Sets in Set Theory\\nIn mathematics, a Set is a fundamental concept representing a collection of well-defined objects or elements. Sets are typically denoted by capital letters, and the individual elements within a set are listed in curly braces, separated by commas. For example, A={1,2,3,4,5} represents a set A with elements 1, 2, 3, 4, and 5. The order of elements wi\\n\\n\\n\\n7 min read\\n\\n\\n\\n\\n\\nMathematics | Graph Theory Basics - Set 2\\nGraph theory is a basic branch of discrete mathematics that mainly focuses on the relationship between objects. These objects are called vertices and these vertices are joined by edges. Graphs are common in computer science, network analysis, and many other everyday uses because they provide a good representation of connection, relationship, and pr\\n\\n\\n\\n10 min read\\n\\n\\n\\n\\nGroup in Maths: Group Theory\\nGroup theory is one of the most important branches of abstract algebra which is concerned with the concept of the group. A group consists of a set equipped with a binary operation that satisfies four key properties: specifically, it includes property of closure, associativity, the existence of an identity element, and the existence of inverse eleme\\n\\n\\n\\n13 min read\\n\\n\\n\\n\\nMatching (Graph Theory)\\nMatching (Graph Theory): In graph theory, matching is a fundamental concept used to describe a set of edges without common vertices. Matchings are used in various applications such as network design, job assignments, and scheduling. Understanding matchings is essential for solving problems involving optimal pairings and resource allocation. Table o\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nGATE CS\\n\\n\\nTheory of Computation\\n \\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Please Login to comment...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n76k+ interested Geeks \\n\\n\\n\\nCore Computer Science Subject for Interview Preparation \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n28k+ interested Geeks \\n\\n\\n\\nGATE Computer Science & Information Technology - 2025 \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n12k+ interested Geeks \\n\\n\\n\\nCBSE Class 12 Computer Science \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nExplore More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                      Corporate & Communications Address:- A-143, 9th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305) | Registered Address:- K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalCareersIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramExploreJob-A-Thon Hiring ChallengeHack-A-ThonGfG Weekly ContestOffline Classes (Delhi/NCR)DSA in JAVA/C++Master System DesignMaster CPGeeksforGeeks VideosGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonData Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython Programming ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceAccountancyBusiness StudiesEconomicsManagementHR ManagementFinanceIncome TaxDatabasesSQLMYSQLPostgreSQLPL/SQLMongoDBPreparation CornerCompany-Wise Recruitment ProcessResume TemplatesAptitude PreparationPuzzlesCompany-Wise PreparationCompaniesCollegesCompetitive ExamsJEE AdvancedUGC NETUPSCSSC CGLSBI POSBI ClerkIBPS POIBPS ClerkMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsRecent ArticlesFree Online ToolsTyping TestImage EditorCode FormattersCode ConvertersCurrency ConverterRandom Number GeneratorRandom Password GeneratorWrite & EarnWrite an ArticleImprove an ArticlePick Topics to WriteShare your ExperiencesInternships \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\\n\\n        acknowledge that you have read and understood our\\n\\n        Cookie Policy &\\n\\n        Privacy Policy\\n\\n\\n\\n        Got It !\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\nThis improvement is locked by another user right now. You can suggest the changes for now and it will be under \\'My Suggestions\\' Tab on Write.\\nYou will be notified via email once the article is available for improvement.\\n\\n                        Thank you for your valuable feedback!\\n\\n                    \\n\\nSuggest changes\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\n\\nmin 4 words, max CharLimit:2000\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n                        Can\\'t choose a topic to write? click here for suggested topics\\n                    \\n\\n\\n\\n                       Write and publish your own Article\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTheory Of Computation Notes PDF, Syllabus ✅ [2021] B Tech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHomeBest Courses\\n\\nGoogle Professional Certificates\\nHuman Resource\\n\\nHuman Resource Human Resource Management Human Resource Planning Organizational Culture Organization Development Organizational Behavior\\nLearning DealsAll Blog PostManagement\\n\\nBusiness Statistics Lean Six Sigma Management Operation Management Research Methodology Operations Research Procurement Management Production Management Supply Chain Strategic Management\\nMarketing\\n\\nEconomics Brand Management Business Business Communication Business Law Entrepreneurship Consumer Behaviour Marketing Essentials Marketing Management Sales Management Shark Tank India\\nBusiness Tech\\n\\nProject Management Business Analytics Management Information System Enterprise Resource Planning Technologies Cloud Computing\\nAbout\\n\\nAbout Us Cookie Policy DMCA Policy Disclaimer Contact Us\\nToggle website search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Category\\nClose\\n\\n\\n\\n\\n\\n\\nHome\\nBest Courses\\n\\nGoogle Professional Certificates\\n\\n\\nHuman Resource\\n\\nHuman Resource\\nHuman Resource Management\\nHuman Resource Planning\\nOrganizational Culture\\nOrganization Development\\nOrganizational Behavior\\n\\n\\nLearning Deals\\nAll Blog Post\\nManagement\\n\\nBusiness Statistics\\nLean Six Sigma\\nManagement\\nOperation Management\\nResearch Methodology\\nOperations Research\\nProcurement Management\\nProduction Management\\nSupply Chain\\nStrategic Management\\n\\n\\nMarketing\\n\\nEconomics\\nBrand Management\\nBusiness\\nBusiness Communication\\nBusiness Law\\nEntrepreneurship\\nConsumer Behaviour\\nMarketing Essentials\\nMarketing Management\\nSales Management\\nShark Tank India\\n\\n\\nBusiness Tech\\n\\nProject Management\\nBusiness Analytics\\nManagement Information System\\nEnterprise Resource Planning\\nTechnologies\\nCloud Computing\\n\\n\\nAbout\\n\\nAbout Us\\nCookie Policy\\nDMCA Policy\\nDisclaimer\\nContact Us\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\nHome>B Tech Study Material>Theory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\nPost last modified:30 March 2021\\nReading time:27 mins read\\nPost category:B Tech Study Material\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload Theory of Computation Notes PDF, syllabus for B Tech, BCA, MCA 2021. We provide a complete theory of computation pdf. Theory of Computation lecture notes includes a theory of computation notes, theory of computation book, theory of computation courses, theory of computation syllabus, theory of computation question paper, MCQ, case study, theory of computation interview questions and available in theory of computation pdf form.\\nTheory of Computation Notes\\nTheory of Computation subject is included in B Tech CSE, BCA, MCA, M Tech. So, students can able to download theory of computation notes pdf.\\n\\nTable of Content1 Theory of Computation Syllabus2 Theory of Computation PDF3 Theory of Computation Notes3.1 What is Theory of Computation?3.2 Theory of Computation Handwritten Notes4 Theory of Computation Interview Questions5 Theory of Computation Question Paper6 Theory of Computation Book\\n\\nTheory of Computation Notes can be downloaded in theory of computation pdf from the below article\\n\\nTheory of Computation Syllabus\\nA detailed theory of computation syllabus as prescribed by various Universities and colleges in India are as under. You can download the syllabus in the theory of computation pdf form.\\nUnit I\\n\\n\\n\\n\\n\\n\\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural Representations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, Proof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets Strings, Languages, Applications of Automata Theory. \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, Extending the Transition Function to Strings, The Language of a DFA \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The Languages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite Automata With Epsilon-Transitions: Uses of Î-Transitions, The Formal Notation for an Î-NFA, Epsilon-Closures, Extended Transitions and Languages for Î-NFA’s, Eliminating Î- Transitions.\\n Unit II\\nRegular Expressions and Languages: Regular Expressions: The Operators of regular Expressions, Building Regular Expressions, Precedence of Regular-Expression Operators, Precedence of Regular-Expression Operators Finite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions by Eliminating States, Converting Regular Expressions to Automata. \\nAlgebraic Laws for Regular Expressions: Properties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of the Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular Languages, Equivalence and Minimization of Automata, \\nContext-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations Using a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, Parse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and Parse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive Inferences, Applications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: Ambiguous Grammars, Removing Ambiguity.\\n\\n\\n\\n\\n\\nUnit III \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical Notation for PDA’s, Instantaneous Descriptions of a PDA, \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack to Final State, From Final State to Empty Stack Equivalence of PDA’s and CFG’s: From Grammars to Pushdown Automata, From PDA’s to Grammars \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and Deterministic PDA’s, DPDA’s and Context-Free Languages, DPDA’s and Ambiguous Grammars \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The Pumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, Decision Properties of CFL’s\\nUnit IV\\nIntroduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for Turing Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, Turing Machines and Halting Programming Techniques for Turing Machines, Extensions to the Basic Turing Machine, Restricted Turing Machines, Turing Machines and Computers\\nUNIT V\\nRecursive And Recursively Enumerable Languages: Properties of recursive and recursively enumerable languages, Universal Turing machine, The Halting problem, Undecidable problems about TMs. Context-sensitive language and linear bounded automata (LBA), Chomsky hierarchy, Decidability, Post’s correspondence problem (PCP), undecidability of PCP.\\n\\nTheory of Computation PDF\\n\\n\\n\\n\\n\\n Theory of Computation Notes PDF(How to download) Theory of Computation Notes Download  Theory of Computation Book Download Theory of Computation Syllabus Download  Theory of Computation Question Paper Download  Theory of Computation Interview Questions Download \\n\\nTheory of Computation Notes\\nWhat is Theory of Computation?\\n\\n\\n\\n\\n\\n Download PDF\\n\\n\\n\\n\\n\\n\\nTheory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory.\\nTheory of Computation Handwritten Notes\\n\\n\\n\\n Download PDF\\n\\nTheory of Computation Interview Questions\\nSome of the theory of computation interview questions are mentioned below. You can download the QnA in theory of computation pdf form.\\nWhat is TOC?What is Automata Theory in TOC?What is Regular Language in TOC?What is Grammer and Language in TOC?What is Null String in TOC?What is Grammer and Language in TOC?What is Regular Expression in TOC?What is Linear Bound Automata in TOC?What is Context-Free Language(CFL) in TOC?What is Recursive Language in TOC?What is the use of Lexical Analysis in TOC?What is Chomsky Classification of Languages in TOC?Define Kleene Star Closure in TOC?What is the Productions in TOC? Explain Production Rules.\\n\\nTheory of Computation Question Paper\\nIf you have already studied the theory of computation notes, now it’s time to move ahead and go through previous year theory of computation question paper. \\n Download PDF Fill Before Download\\nIt will help you to understand question paper pattern and type of theory of computation questions and answers asked in B Tech, BCA, MCA, M Tech theory of computation exam. You can download the syllabus in theory of computation pdf form.\\n\\nTheory of Computation Book\\nBelow is the list of theory of computation book recommended by the top university in India.\\nIntroduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, R.Motwani & J.D.Ullman (3rd Edition) – Pearson EducationTheory of Computer Science (Automata Language & Computations), by K.L.Mishra & N. Chandrashekhar, PHI\\n\\nDownload B Tech (CS) Study Material\\n\\n\\n\\nComputer Networks Notes ✅ [2020] PDF – Download \\n\\nComputer Networks Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Networks Notes) \\n\\n\\n\\nComputer Graphics Notes ✅ [2020] PDF – Download \\n\\nComputer Graphics Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Graphics Notes)\\n\\n\\n\\nOperating System Notes ✅ [2020] PDF – Download  \\n\\nOperating System Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operating System Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download\\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Compiler Design Notes)\\n\\n\\n\\nData Structures Notes ✅ [2020] PDF – Download \\n\\nData Structures Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Structures Notes)\\n\\n\\n\\nDigital Image Processing Notes ✅ [2020] PDF – Download \\n\\nDigital Image Processing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Digital Image Processing Notes) \\n\\n\\n\\nTheory of Computation Notes ✅ [2020] PDF – Download \\n\\nTheory of Computation Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Theory of Computation Notes) \\n\\n\\n\\nComputer Organization and Architecture Notes ✅ [2020] PDF – Download \\n\\nComputer Organization and Architecture Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Computer Organization and Architecture Notes) \\n\\n\\n\\nCloud Computing Notes ✅ [2020] PDF – Download \\n\\nCloud Computing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Cloud Computing Notes) \\n\\n\\n\\nData Communication and Networking Notes ✅ [2020] PDF – Download \\n\\nData Communication and Networking Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Communication and Networking Notes) \\n\\n\\n\\n Software Engineering Notes ✅ [2020] PDF – Download \\n\\nSoftware Engineering Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Software Engineering Notes) \\n\\n\\n\\nWeb Technologies Notes ✅ [2020] PDF – Download \\n\\nWeb Technologies Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Web Technologies Notes) \\n\\n\\n\\nMicroprocessor and Microcontrollers Notes ✅ [2020] PDF – Download \\n\\nMicroprocessor and Microcontrollers Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Microprocessor and Microcontrollers Notes) \\n\\n\\n\\nDesign and Analysis of Algorithm Notes ✅ [2020] PDF – Download \\n\\nDesign and Analysis of Algorithm Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Design and Analysis of Algorithm Notes) \\n\\n\\n\\nOperation Research Notes ✅ [2020] PDF – Download \\n\\nOperation Research Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operation Research Notes) \\n\\n\\n\\nDatabase Management Systems Notes ✅ [2020] PDF – Download \\n\\nDatabase Management Systems Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Database Management Systems Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download \\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Compiler Design Notes)\\n\\n\\n\\n\\n\\nIn the above article, a student can download theory of computation notes for B Tech, BCA, MCA, M Tech. Theory of Computation lecture notes and study material includes theory of computation notes, theory of computation books, theory of computation syllabus, theory of computation question paper, theory of computation case study, theory of computation interview questions, theory of computation courses in theory of computation pdf form.\\n\\nGo On, Share & Help your Friend\\n Did we miss something in B.Tech Computer Science Notes or You want something More? Come on! Tell us what you think about our post on Theory of Computation Notes | PDF, Syllabus, Book | B Tech 2020 in the comments section and Share this post with your friends.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead more articles\\n Previous PostWeb Technologies Notes | PDF, Syllabus, Book | B Tech 2021 Next PostDigital Image Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n\\nTags: B Tech, B Tech CS Books, B Tech CS Syllabus, B Tech CSE Books, B Tech CSE Notes, B Tech CSE Syllabus, B Tech Notes, B Tech Study Material, Computer Science Notes PDF, CS Notes, CSE Notes, Theory of Computation Book, Theory of Computation Course, Theory of Computation Interview Questions, Theory of Computation Notes, Theory of Computation PDF, Theory of Computation PPT, Theory of Computation Question Paper, Theory of Computation Syllabus, Theory of Computation Tutorial\\n\\n\\nYou Might Also Like\\n\\n\\n\\n\\nDigital Signal Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n \\n\\n\\nOperation Research Notes | PDF, Syllabus | MBA, B Tech 2024\\n\\n18 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDatabase Management Systems Notes | PDF | B Tech 2021\\n\\n22 March 2020\\n\\n\\n\\nAdvanced Java Programming Notes | PDF | B Tech (2024)\\n\\n23 November 2020\\n\\n\\n\\n\\n \\n\\n\\nComputer Networks Notes | PDF, Syllabus, Books | B Tech (2024)\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nSoftware Engineering Notes | PDF, Syllabus | B Tech 2021\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDesign and Analysis of Algorithm Notes PDF | B Tech (2024)\\n\\n20 March 2020\\n\\n\\n\\nDigital Communication Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nAnalog Communication PDF | Notes, Syllabus B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nBusiness Intelligence Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nHow to Download Notes on Geektonight\\n\\n27 June 2020\\n\\n\\n\\nWireless Networks Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n\\nLeave a Reply Cancel replyYou must be logged in to post a comment. \\n\\n\\n\\n\\n\\n\\nAll CategoryAll Category\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies\\nTreasury Management in Banking\\nTwitter Certification\\nUncategorized\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nWorld\\'s Best Online Courses at One Place \\n\\n\\n\\nWe’ve spent the time in finding, so you can spend your time in learning \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDigital Marketing \\n\\n\\n\\nGoogle Ads CourseFacebook Ads CourseSEO CourseInstagram Marketing CourseSEM CourseSocial Media CourseEmail Marketing CoursePinterest CourseChatbot CourseBlogging CourseContent Marketing CourseWooCommerce CourseClickbank Affiliate Marketing CourseAffiliate Marketing CourseAmazon Affiliate Marketing CourseShopify, eCommerce & Dropshipping CourseExcel Data Analysis CourseWordPress CourseGoogle Tag Manager CourseGoogle Analytics CourseDigital Marketing CourseYoutube Marketing CourseBing Ads CourseSocial Media Analytics Course \\n\\n\\n\\n\\n\\n\\n\\nBusiness \\n\\n\\n\\nProduct Strategy CourseSales CourseBrand Strategy CourseBusiness Law CourseStrategic Management CourseMarketing Analytics CourseBusiness Strategy CourseMarketing Management CourseHuman Resource CourseProduct Management CourseProduct Marketing CourseB2B Marketing CourseGrowth Hacking CoursePeople HR Analytics CourseEntrepreneurship CourseBusiness Statistics CourseProject Management CourseNegotiation CourseTime Management CourseLeadership CourseCareer Development CourseStress Management CourseAnxiety Management CourseDesign Thinking CourseEmotional Intelligence CourseTeam Building CourseBusiness Analytics CourseDigital Transformation Course \\n\\n\\n\\n\\n\\n\\n\\nPersonal Growth \\n\\n\\n\\nEnglish Grammar CourseVocabulary CourseSoft Skills CoursePublic Speaking CoursePhotography CourseBody Language CourseCommunication Skills CourseInterview Preparation CourseProductivity CourseMindfulness CourseMemory CourseSelf DisciplineSpeed ReadingAcademic WritingCopywriting CourseScientific Writing CourseNovel Writing CourseAcademic Writing CourseTravel Writer CourseCreative Writing CourseInterior Design CourseGraphic Design CourseDrawing CourseDigital Art CourseUI UX Designer Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFinance \\n\\n\\n\\nMutual Fund CourseFinancial Analysis CoursePersonal Finance CourseCost Accounting CourseAudit CourseFintech CourseValue Investing CourseTrading CourseFinancial Modeling CourseInvestment CourseProject Finance CourseStock Trading CourseFinancial & Capital Markets CourseAccounting CourseFinancial Engineering Course \\n\\n\\n\\n\\n\\n\\n\\nFinTech \\n\\n\\n\\nNFT CourseMongoDB CoursejQuery CourseBlockchain CourseCryptocurrency CourseSwift CourseAWS CourseRedux CourseGo CourseDeFi CourseSolidity CourseMetaverse CourseDjango CourseJIRA CourseConversion Rate Optimization (CRO) CourseAnalytics CourseCustomer Loyalty Course \\n\\n\\n\\n\\n\\n\\n\\nLanguage \\n\\n\\n\\nEnglish SpeakingKorean LanguageGerman LanguageSpanish LanguageFrench Language Italian Language Russian Language Japanese Language Arabic LanguageSwedish LanguageHindi LanguagePortuguese LanguageDutch LanguageLatin LanguageTurkish LanguageHungarian LanguageVietnameseAmerican AccentPronunciationSpelling Courses \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTech \\n\\n\\n\\nData Science CourseR Programming CourseBig Data CourseSQL CourseData Analytics CourseMachine Learning CoursePython CourseSQL Data Science CourseArtificial Intelligence CourseCloud Computing CourseData Warehouse CourseNLP Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment \\n\\n\\n\\nReact JS CoursesFront End Development CourseFull Stack Web Developer CourseC++ CourseData Engineering CourseHTML & CSS3 CourseMicrosoft SQL CourseMySQL CourseJava CourseJavaScript CourseTypeScript CourseBack End Development CourseDatabase CourseGraphQL Course \\n\\n\\n\\n\\n\\n\\n\\nExam Prep \\n\\n\\n\\nGRE PrepGMAT PrepMCAT PrepIELTS PrepDAT PrepPSAT PrepCFA PrepOAT PrepACT PrepLSAT PrepFRM PrepSSAT PrepCPA PrepTESOL PrepSAT PrepSSAT Prep \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython \\n\\n\\n\\nPython CourseDeep Learning Python CoursePython Data Science CoursePython for Marketing CoursePython for Finance CoursePython Pandas CoursePython Data Visualization CoursePython Machine Learning CoursePython Data Processing CoursePython Scripting CoursePython for Data Analysis CoursePython Data Structure CourseNLP Python CourseMatplotlib CourseData Cleaning CourseStatistical Modeling CourseKeras CoursePytorch CourseMachine Learning Finance Course \\n\\n\\n\\n\\n\\n\\n\\nTech \\n\\n\\n\\nSCADA CourseASP.net CourseScrum CourseSpring Boot and MVC CourseIT Support & Help Desk CourseRuby on Rails CourseKubernetes CourseDocker CourseNodeJs CourseAngular CoursePHP CourseAPI CourseAlteryx CoursePower BI CourseTableau CourseData Visualization CourseDAX CourseData Streaming CourseRegex CourseQlik Sense CoursePlotly Dash CourseData Modeling Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment \\n\\n\\n\\nAndroid CourseiOS Development CourseFlutter CourseKotlin CourseIonic CourseXamarin CourseVirtual Reality CourseMatlab CourseGit & GitHub CourseSelenium CourseShell Scripting CourseARKit CourseGame Design CourseUnity CourseUnreal Engine CourseGame Development CourseBlender CourseDreamweaver CourseVisual Studio CourseC# (C-Sharp) CourseBootstrap Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChild Care \\n\\n\\n\\nChild Nutrition CourseBaby Massage CourseChildcare & Early Education CourseBaby Sign Language CourseKids Art & Drawing CourseKids Coding CourseChild Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Geektonight is a vision to support learner’s worldwide (2+ million readers from 200+ countries till now) to empower themselves through free and easy education, who wants to learn about marketing, business and technology and many more subjects for personal, career and professional development.\\n\\n \\n\\nConnect With Us\\n\\nOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tab \\n\\n \\n\\nMoreAbout UsOpens in a new tabDisclaimerOpens in a new tabCookie PolicyOpens in a new tabPrivacy PolicyOpens in a new tabDMCA PolicyOpens in a new tab \\n\\nCategoriesCategories\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies\\nTreasury Management in Banking\\nTwitter Certification\\nUncategorized\\n\\n\\n \\n\\n\\n\\n\\n\\nCopyright 2023 Geektonight\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch this website\\n\\nType then hit enter to search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMBA NOTES \\nTHEORY OF COMPUTATION\\nBCS601T \\nTHEORY OF COMPUTATION \\nUnit – I \\n \\n(12 hours) \\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural\\nRepresentations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, \\nProof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets \\nStrings, Languages, Applications of Automata Theory. \\n \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of \\na Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, \\nExtending the Transition Function to Strings, The Language of a DFA \\n \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The \\nLanguages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite \\nAutomata With Epsilon-Transitions: Uses of \\uf0ce-Transitions, The Formal Notation for an \\n \\n\\uf0ce-NFA, Epsilon-Closures, Extended Transitions and Languages for \\uf0ce-NFA’s, Eliminating \\uf0ce-\\nTransitions. \\n \\nUnit – II \\n(12 hours) \\nRegular  Expressions\\nand  Languages:  Regular  Expressions:  The  Operators  of  regular\\nExpressions, Building \\nRegular  Expressions,  Precedence  of  Regular-Expression  Operators,\\n \\nPrecedence of Regular-Expression Operators \\n \\nFinite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting \\nDFA’s to Regular Expressions, Converting DFA’s to R egular Expressions by Eliminating States, \\nConverting Regular Expressions to Automata. \\n \\nAlgebraic Laws for Regular Expressions: \\n \\nProperties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of \\nthe Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular \\nLanguages, Equivalence and Minimization of Automata, \\n \\nUnit – III \\n(12 hours) \\nGrammar : Types of Grammar \\nContext-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations \\nUsing a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, \\n \\nParse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and \\nParse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive \\nInferences, \\n \\nApplications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: \\nAmbiguous Grammars, Removing Ambiguity From Grammars, Leftmost Derivations as a Way to \\nExpress Ambiguity, Inherent Anbiguity \\nwww.indiansbrain.com\\n \\nUnit – IV \\n \\n(12 hours) \\n \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical \\nNotation for PDA’s, Instantaneous Descriptions of a PDA, \\n \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack \\nto Final State, From Final State to Empty Stack \\n \\nEquivalence of PDA’s and CFG’s: From Grammars to Pu shdown Automata, From PDA’s to \\nGrammars \\n \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and \\nDeterministic PDA’s, DPDA’s and Context-Free La nguages, DPDA’s and Ambiguous Grammars \\n \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The \\nPumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, \\nDecision Properties of CFL’s \\n \\nUnit – V \\n \\n(12 hours) \\nIntroduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for \\nTuring Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, \\nTuring Machines and Halting \\n \\nProgramming Techniques for Turing Machines, Extensions to the Basic Turing Machine, \\nRestricted Turing Machines, Turing Machines and Computers, \\n \\nUndecidability: A Language That is Not Recursively Enumerable, Enumerating the Binary \\nStrings, Codes for Turing Machines, The Diagonalization Language \\n \\nAn Undecidable Problem That Is RE: Recursive Languages, Complements of Recursive and RE \\nlanguages, The Universal Languages, Undecidability of the Universal Language \\n \\nUndecidable Problems About Turing Machines: Reductions, Turing Machines That Accept the \\nEmpty Language. Post’s Correspondence Problem: Definition of Post’s Correspondence Problem, \\nThe “Modified” PCP, Other Undecidable Prob lems: Undecidability of Ambiguity for CFG’s \\n \\n \\n \\n \\nText Book: \\n \\n1. Introduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, \\nR.Motwani & J.D.Ullman (3rd Edition) – Pearson Education \\n \\nwww.indiansbrain.com\\n \\nUNIT - I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWhat is TOC? \\n \\nIn theoretical computer science, the theory of computation is the branch that deals with \\nwhether and how efficiently problems can be solved on a model of computation, using an \\nalgorithm. The field is divided into three major branches: automata theory, computability theory \\nand computational complexity theory. \\n \\nIn order to perform a rigorous study of computation, computer scientists work with a \\nmathematical abstraction of computers called a model of computation. There are several \\nmodels in use, but the most commonly examined is the Turing machine. \\n \\nAutomata theory \\n \\nIn theoretical computer science, automata theory is the study of abstract machines (or more \\nappropriately, abstract \\'mathematical\\' machines or systems) and the computational problems that \\ncan be solved using these machines. These abstract machines are called automata. \\n \\nThis automaton consists of \\n \\n\\uf095 states (represented in the figure by circles),\\uf020\\n\\uf095 and transitions (represented by arrows).\\uf020\\n \\nAs the automaton sees a symbol of input, it makes a transition (or jump) to another state, \\naccording to its transition function (which takes the current state and the recent symbol as \\nits inputs). \\nUses of Automata: compiler design and parsing. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIntroduction to formal proof: \\nBasic Symbols used : \\nU – Union \\n∩- Conjunction \\n \\nϵ - Empty String \\nΦ – NULL set \\n7- negation \\n \\n‘ – compliment \\n= > implies \\nwww.indiansbrain.com\\nAdditive inverse: a+(-a)=0 \\nMultiplicative inverse: a*1/a=1 \\nUniversal set U={1,2,3,4,5} \\nSubset A={1,3} \\nA’ ={2,4,5} \\nAbsorption law: AU(A ∩B) = A, A∩(AUB) = A \\n \\nDe Morgan’s Law: \\n \\n(AUB)’ =A’ ∩ B’ \\n(A∩B)’ = A’ U B’ \\nDouble compliment \\n(A’)’ =A \\n \\nA ∩ A’ = Φ \\n \\nLogic relations: \\na b = > 7a U b \\n7(a∩b)=7a U 7b \\n \\nRelations: \\n \\nLet a and b be two sets a relation R contains aXb. \\nRelations used in TOC: \\nReflexive: a = a \\nSymmetric: aRb = > bRa \\nTransition: aRb, bRc = > aRc \\n \\nIf a given relation is reflexive, symmentric and transitive then the relation is called equivalence \\nrelation. \\n \\nDeductive proof: Consists of sequence of statements whose truth lead us from some \\ninitial statement called the hypothesis or the give statement to a conclusion statement. \\n \\n \\n \\n \\nAdditional forms of proof: \\nProof of sets \\nProof by contradiction \\nProof by counter example \\n \\nDirect proof (AKA) Constructive proof: \\nIf p is true then q is true \\n \\nEg: if a and b are odd numbers then product is also an odd \\nnumber. Odd number can be represented as 2n+1 \\n \\na=2x+1, b=2y+1 \\nproduct of a X b = (2x+1) X (2y+1) \\n= 2(2xy+x+y)+1 = 2z+1 (odd number) \\nwww.indiansbrain.com\\n \\n \\n \\nProof by contrapositive: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof by Contradiction: \\n \\nH and not C implies falsehood. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBe regarded as an observation than a theorem. \\n \\n \\n \\n \\n \\n \\n \\n \\nFor any sets a,b,c if a∩b = Φ and c is a subset of b the prove that a∩c \\n=Φ Given : a∩b=Φ and c subset b \\n \\nAssume: a∩c  Φ \\n \\nThen \\n \\n \\n= > a∩b Φ = > a∩c=Φ(i.e., the assumption is wrong) \\nwww.indiansbrain.com\\nProof by mathematical Induction: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLanguages : \\n \\nThe languages we consider for our discussion is an abstraction of natural languages. That is, \\nour focus here is on formal languages that need precise and formal definitions. Programming \\nlanguages belong to this category. \\n \\nSymbols : \\n \\nSymbols are indivisible objects or entity that cannot be defined. That is, symbols are the atoms \\n \\nof the world of languages. A symbol is any single object such as begin, or do. \\n \\nAlphabets : \\n \\nAn alphabet is a finite, nonempty set of symbols. The alphabet of a language is \\nnormally denoted by \\n. When more than one alphabets are considered for discussion, \\nthen \\nsubscripts may be used (e.g.    \\netc) or sometimes other symbol like G may also be \\n \\nintroduced. \\n \\n \\n \\n \\n \\n \\n \\n \\nExample : \\n \\nStrings or Words over Alphabet : \\n \\nA string or word over an alphabet   \\nis a finite sequence of concatenated symbols of     \\n. \\n , a, 0, 1, #, \\nwww.indiansbrain.com\\nExample : 0110, 11, 001 are three strings over the binary alphabet { 0, 1 } . \\n \\naab, abcb, b, cc are four strings over the alphabet { a, b, c }. \\n \\nIt is not the case that a string over some alphabet should contain all the symbols from the alpha-\\nbet. For example, the string cc over the alphabet { a, b, c } does not contain the symbols a and b. \\nHence, it is true that a string over an alphabet is also a string over any superset of that alphabet. \\n \\nLength of a string : \\nThe number of symbols in a string w is called its length, denoted by |w|. \\n \\nExample : | 011 | = 4, |11| = 2, | b | = 1 \\n \\nConvention : We will use small case letters towards the beginning of the English alphabet \\nto denote symbols of an alphabet and small case letters towards the end to \\n \\ndenote strings over an alphabet. That is, \\n (symbols) and \\n \\nare strings. \\n \\nSome String Operations : \\nLet \\nand \\nbe two strings. The concatenation of x and y \\n \\ndenoted by xy, is the string   \\n. That is, the concatenation of x and y \\n \\ndenoted by xy is the string that has a copy of x followed by a copy of y without any intervening \\nspace between them. \\n \\nExample : Consider the string 011 over the binary alphabet. All the prefixes, suffixes and \\nsubstrings of this string are listed below. \\n \\nPrefixes:  , 0, 01, 011. \\n \\nSuffixes:  , 1, 11, 011. \\n \\nSubstrings:  , 0, 1, 01, 11, 011. \\n \\nNote that x is a prefix (suffix or substring) to x, for any string x and is a prefix (suffix or \\nsubstring) to any string. \\n \\nA string x is a proper prefix (suffix) of string y if x is a prefix (suffix) of y and x 蝤 y. \\n \\nIn the above example, all prefixes except 011 are proper prefixes. \\n \\nPowers of Strings : For any string x and integer \\n, we use \\nto denote the string formed \\nby sequentially concatenating n copies of x. We can also give an inductive \\ndefinition of \\nas follows: \\n= e, if n \\n= 0 ; otherwise \\n \\nwww.indiansbrain.com\\nExample : If x = 011, then  \\n= 011011011,  \\n= 011 and \\n \\nPowers of Alphabets : \\n \\nWe write \\n(for some integer k) to denote the set of strings of length k with symbols \\n \\nfrom \\n. In other words, \\n \\n= { w | w is a string over  \\n and | w | = k}. Hence, for any alphabet,   \\ndenotes the set \\n \\nof all strings of length zero. That is,   \\n= { e }. For the binary alphabet { 0, 1 } we have \\n \\nthe following. \\n \\n \\n \\n \\n \\n \\n \\n \\nThe set of all strings over an alphabet \\n is denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nThe set  \\n contains all the strings that can be generated by iteratively concatenating sym- \\nbols from      \\nany number of times. \\n \\nExample : If \\n= { a, b }, then \\n= {  , a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, …}. \\n \\nPlease note that if \\n, then \\n that is \\n. It may look odd that one can proceed \\nfrom the empty set to a non-empty set by iterated concatenation. But there is a reason for this \\nand we accept this convention \\n \\nThe set of all nonempty strings over an alphabet     \\nis denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nNote that  \\nis infinite. It contains no infinite strings but strings of arbitrary lengths. \\n \\nReversal : \\nFor any string  \\nthe reversal of the string is \\n. \\n \\nAn inductive definition of reversal can be given as follows: \\nwww.indiansbrain.com\\nLanguages : \\nA language over an alphabet is a set of strings over \\nthat alphabet. Therefore, a \\n \\nlanguage L is any subset of \\n. That is, any \\nis a language. \\nExample : \\n \\n \\n1. F is the empty language.  \\n \\n2.\\nis a language for any \\n. \\n \\n3. {e} is a language for any \\n. Note that, \\n. Because the language F does not \\n \\ncontain any string but {e} contains one string of length zero. \\n4. The set of all strings over { 0, 1 } containing equal number of 0\\'s and 1\\'s. \\n \\n5. The set of all strings over {a, b, c} that starts with a. \\n \\nConvention : Capital letters A, B, C, L, etc. with or without subscripts are normally used \\nto denote languages. \\n \\nSet operations on languages : Since languages are set of strings we can apply set operations to \\nlanguages. Here are some simple examples (though there is nothing new in it). \\n \\n \\nUnion : A string \\n \\n \\n \\n \\n \\n \\niff \\nor \\n \\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 0, 11, 01, 011, 111 } \\nIntersection  : \\nA   string, \\nxϵ  L1 \\n∩ L2 \\niff    x   ϵ  L1    and   x   ϵ  L2    .\\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 01 } \\n \\nComplement :  Usually, \\nis the universe that a complement is taken with respect to. \\n \\nThus for a language L, the complement is L(bar) = { \\n| \\n}. \\n \\nExample : Let L = { x | |x| is even }. Then its complement is the language { \\n| |x| is \\n \\nodd }. \\n \\nSimilarly we can define other usual set operations on languages like relative \\ncom-plement, symmetric difference, etc. \\n \\nReversal of a language : \\nThe reversal of a language L, denoted as \\n, is defined as:  \\n. \\n \\nExample : \\n \\n1.  Let L = { 0, 11, 01, 011 }. Then \\n= { 0, 11, 10, 110 }.\\nwww.indiansbrain.com\\n2.  Let L = { \\n| n is an integer }. Then  \\n=  { \\n| n is an integer }. \\n \\nLanguage concatenation : The concatenation of languages       \\nand \\nis defined as \\n \\n= { xy | \\nand \\n}. \\n \\nExample : { a, ab }{ b, ba } = { ab, aba, abb, abba }. \\n \\nNote that , \\n1. \\n  in general. \\n2. \\n \\n \\n3. \\n \\n \\nIterated concatenation of languages : Since we can concatenate two languages, we also repeat this to \\nconcatenate any number of languages. Or we can concatenate a language with itself any \\n \\nnumber of times. The operation L with itself n times. This is \\ndefined formally as follows: \\n \\n \\n \\n \\n \\n \\nExample :  Let L = { a, ab }. Then according to the definition, \\nwe have \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand so on. \\n \\n \\n \\nKleene\\'s Star operation : The Kleene star operation on a language L, denoted as is defined as \\nfollows : \\n \\n= ( Union n in N ) \\n \\n= \\n \\n \\n= { x | x is the concatenation of zero or more strings from L } \\n \\ndenotes the concatenation of \\nwww.indiansbrain.com\\nThus \\nis the set of all strings derivable by any number of concatenations of strings in \\nL. It is also useful to define \\n \\n=, i.e., all strings derivable by one or more concatenations of strings in L. That is \\n \\n= (Union n in N and n >0) \\n \\n= \\n \\nExample :  Let L = { a, ab }. Then we have, \\n \\n= \\n \\n= {e} \\n{a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\n= \\n \\n= {a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\nNote : \\nis in  \\n, for every language L, including . \\n \\nThe previously introduced definition of   \\nis an instance of Kleene star. \\n \\n \\n \\n \\n \\n(Generates) \\n(Recognizes) \\nGrammar \\nLanguage \\n  Automata \\n \\nAutomata: A algorithm or program that automatically recognizes if a particular string belongs to \\nthe language or not, by checking the grammar of the string. \\n \\nAn automata is an abstract computing device (or machine). There are different varities of such \\nabstract machines (also called models of computation) which can be defined mathematically. \\n \\nEvery Automaton fulfills the three basic requirements. \\n \\n• \\nEvery automaton consists of some essential features as in real computers. It has a mech-\\nanism for reading input. The input is assumed to be a sequence of symbols over a given \\nalphabet and is placed on an input tape(or written on an input file). The simpler automata \\ncan only read the input one symbol at a time from left to right but not change. Powerful \\nversions can both read (from left to right or right to left) and change the input. \\nwww.indiansbrain.com\\n\\uf095 The automaton can produce output of some form. If the output in response to an input \\nstring is binary (say, accept or reject), then it is called an accepter. If it produces an out-\\nput sequence in response to an input sequence, then it is called a transducer(or \\nautomaton with output).\\uf020\\n\\uf020\\n• \\nThe automaton may have a temporary storage, consisting of an unlimited number of \\ncells, each capable of holding a symbol from an alphabet ( whcih may be different from \\nthe input alphabet). The automaton can both read and change the contents of the storage \\ncells in the temporary storage. The accusing capability of this storage varies depending \\non the type of the storage. \\n \\n• \\nThe most important feature of the automaton is its control unit, which can be in any \\none of a finite number of interval states at any point. It can change state in some de-\\nfined manner determined by a transition function. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1: The figure above shows a diagrammatic representation of a generic \\nautoma-tion. \\n \\nOperation of the automation is defined as follows. \\n \\nAt any point of time the automaton is in some integral state and is reading a particular symbol \\nfrom the input tape by using the mechanism for reading input. In the next time step the automa-\\nton then moves to some other integral (or remain in the same state) as defined by the transition \\nfunction. The transition function is based on the current state, input symbol read, and the content \\nof the temporary storage. At the same time the content of the storage may be changed and the \\ninput read may be modifed. The automation may also produce some output during this transition. \\nThe internal state, input and the content of storage at any point defines the configuration of the \\nautomaton at that point. The transition from one configuration to the next ( as defined by the \\ntransition function) is called a move. Finite state machine or Finite Automation is the simplest \\ntype of abstract machine we consider. Any system that is at any point of time in one of a finite \\nnumber of interval state and moves among these states in a defined manner in response to some \\ninput, can be modeled by a finite automaton. It doesnot have any temporary storage and hence a \\nrestricted model of computation. \\nwww.indiansbrain.com\\nFinite Automata \\n \\nAutomata (singular : automation) are a particularly simple, but useful, model of compu-\\ntation. They were initially proposed as a simple model for the behavior of neurons. \\n \\nStates, Transitions and Finite-State Transition System : \\n \\n \\nLet us first give some intuitive idea about a state of a system and state transitions before \\ndescribing finite automata. \\n \\nInformally, a state of a system is an instantaneous description of that system which gives all \\nrelevant information necessary to determine how the system can evolve from that point on. \\n \\nTransitions are changes of states that can occur spontaneously or in response to inputs to the \\nstates. Though transitions usually take time, we assume that state transitions are instantaneous \\n(which is an abstraction). \\n \\nSome examples of state transition systems are: digital systems, vending machines, etc. A system \\n \\ncontaining only a finite number of states and transitions among them is \\ncalled a finite-state transition system. \\n \\nFinite-state transition systems can be modeled abstractly by a mathematical model called \\nfinite automation \\n \\nDeterministic Finite (-state) Automata \\n \\nInformally, a DFA (Deterministic Finite State Automaton) is a simple machine that reads an in-\\nput string -- one symbol at a time -- and then, after the input has been completely read, decides \\nwhether to accept or reject the input. As the symbols are read from the tape, the automaton can \\nchange its state, to reflect how it reacts to what it has seen so far. A machine for which a deter-\\nministic code can be formulated, and if there is only one unique way to formulate the code, then \\nthe machine is called deterministic finite automata. \\n \\nThus, a DFA conceptually consists of 3 parts: \\n \\n \\n \\n \\n1. A tape to hold the input string. The tape is divided into a finite number of cells. Each \\ncell holds a symbol from \\n. \\n2. A tape head for reading symbols from the tape \\n3. A control , which itself consists of 3 things: \\n \\no \\nfinite number of states that the machine is allowed to be in (zero or more states \\nare designated as accept or final states), \\n \\no a current state, initially set to a start state, \\nwww.indiansbrain.com\\no a state transition function for changing the current state. \\n \\nAn automaton processes a string on the tape by repeating the following actions until the \\ntape head has traversed the entire string: \\n \\n1. The tape head reads the current tape cell and sends the symbol s found there to \\nthe control. Then the tape head moves to the next cell. \\n \\n2. he control takes s and the current state and consults the state transition function to \\nget the next state, which becomes the new current state. \\n \\nOnce the entire string has been processed, the state in which the automation enters is examined. \\n \\nIf it is an accept state , the input string is accepted ; otherwise, the string is rejected . Summariz- \\n \\ning all the above we can formulate the following formal definition: \\n \\n \\nDeterministic Finite State Automaton : A Deterministic Finite State Automaton (DFA) is \\n \\na 5-tuple : \\n \\n\\uf095 Q is a finite set of states.\\uf020\\n• \\nis a finite set of input symbols or alphabet \\n \\n\\uf095 \\nis the “next state” transition function (which is total ). Intuitively, \\nis\\n a \\nfunction that tells which state to move to in response to an input, i.e., if M is in \\n \\nstate q and sees input a, it moves to state      \\n. \\n \\n\\uf095 \\nis the start state.\\uf020\\n• \\nis the set of accept or final states. \\n \\nAcceptance of Strings : \\n \\nA DFA accepts a string   \\nif there is a sequence of states     \\nin Q \\n \\nsuch that \\n \\n1.  \\nis the start state. \\n2.  \\nfor all \\n. \\n \\n3. \\n \\nLanguage Accepted or Recognized by a DFA : \\n \\nThe language accepted or recognized by a DFA M is the set of all strings accepted by M , and \\n \\nis denoted by \\ni.e. \\nThe  notion \\nof \\nacceptance can also be made more precise by extending the transition function \\n.\\n \\nExtended transition function : \\nwww.indiansbrain.com\\nExtend \\n(which is function on symbols) to a function on strings, i.e. . \\n \\n \\n \\nThat is, \\n is the state the automation reaches when it starts from the state q and finish \\nprocessing the string w. Formally, we can give an inductive definition as follows: \\n \\nThe language of the DFA M is the set of strings that can take the start state to one of \\nthe accepting states i.e. \\n \\n \\nL(M) = { \\n| M accepts w } \\n \\n= {\\n| \\n} \\n \\n \\nExample 1 : \\n \\n \\n \\n \\n \\n \\n \\n \\nis the start state \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is a formal description of a DFA. But it is hard to comprehend. For ex. The language of the \\nDFA is any string over { 0, 1} having at least one 1 \\n \\nWe can describe the same DFA by transition table or state transition diagram as follow-\\ning: \\n \\n \\n \\n \\nTransition Table : \\n \\n0    1 \\nwww.indiansbrain.com\\n \\n \\nIt is easy to comprehend the transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExplanation :  We cannot \\nreach find state \\nw/0 or in the i/p string. There can be any no. \\nof 0\\'s at the beginning. \\n( The self-loop at \\non label 0 indicates it ). Similarly there \\ncan be any no. of 0\\'s & 1\\'s in any order at the end of the string. \\n \\nTransition table : \\n \\nIt is basically a tabular representation of the transition function that takes two arguments (a \\nstate and a symbol) and returns a value (the “next state”). \\n \\n• \\nRows correspond to states, \\n• \\nColumns correspond to input symbols, \\n• \\nEntries correspond to next states \\n• \\nThe start state is marked with an arrow \\n• \\nThe accept states are marked with a star (*). \\n \\n \\n \\n0    1 \\n \\n \\n \\n(State) Transition diagram : \\n \\nA state transition diagram or simply a transition diagram is a directed graph which can \\nbe constructed as follows: \\n \\n1.  For each state in Q there is a node. \\n2. There is a directed edge from node q to node p labeled a iff \\n . (If there are \\nseveral input symbols that cause a transition, the edge is labeled by the list of these \\nsymbols.) \\n3. There is an arrow with no source into the start state. \\n4. Accepting states are indicated by double circle. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n5. \\n \\n6. Here is an informal description how a DFA operates. An input to a DFA can be any \\n \\nstring. \\nPut a pointer to the start state q. Read the input string w from left \\n \\nto right, one symbol at a time, moving the pointer according to the transition  \\n \\nfunction, \\n.  If the next symbol of w is a and the pointer is on state p, move the \\n \\npointer to \\n. When the end of the input string w is encountered, the pointer is on \\n \\nsome state, r. The string is said to be accepted by the DFA if \\nand \\n \\nrejected if \\n. Note that there is no formal mechanism for moving the pointer. \\n7. A language \\nis said to be regular if L = L(M) for some DFA M. \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying \\ncertain recursive rules as given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) is a RE \\n \\nii) is a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\n \\nIf \\n \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\nwww.indiansbrain.com\\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive \\nREs) by a finite no of applications of the recursive step (given in 2). \\n \\nExample : Let \\n = { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this \\nexpression by applying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n4 \\n(1+  ) \\nRule 2(iv) & Step 3 \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated \\nwith every RE). We will see later that REs are used to attribute regular languages. \\n \\n \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We \\ncan define the language L(r) associated with (or described by) a REs as follows. \\n \\n1. is the RE describing the empty language i.e. L( ) = . \\n \\n2. is a RE describing the language {\\n} i.e. L( ) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\nwww.indiansbrain.com\\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,. } {0} \\n{1} \\n \\n= {  , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nConsider the RE ab + c. The language described by the RE can be thought of either \\nL(a)L(b+c) or L (ab) L(c) as provided by the rules (of languages described by REs) \\ngiven already. But these two represents two different languages lending to \\nambiguity. To remove this ambiguity we can either \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. \\nLike other algebras mod in mathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) \\noperator. \\n \\nii) It is also important to note that concatenation & union (+) operators are \\nassociative and union operation is commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language \\nL(ab) L(c) i.e. it should be grouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For \\nexample, the language represented by the RE a(b+c) is L(a)L(b+c). \\nwww.indiansbrain.com\\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language \\nL(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all \\nstrings over {0,1} which are either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an \\neven number of 0\\'s followed by an odd number of 1\\'s i.e. \\n \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE \\nrr, \\ndenotes \\nr, and so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of \\nconsecutive 1\\'s} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and \\nwhat goes before is completely arbitrary. Considering these observations we can write \\nthe REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE \\n(0+1)*11(0+1)*+(0+1)*00(0+1)* represents the set of string over {0,1} that contains \\nthe substring 11 or 00. \\n \\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the \\nset of strings over {0,1} that contains exactly two 1\\'s. The presence of two 1\\'s in the \\nRE and any no of 0\\'s before, between and after the 1\\'s ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1\\'s. \\n \\nSolution : There must be at least two 1\\'s in the RE somewhere and what comes before, \\nbetween, and after is completely arbitrary. Hence we can write the RE as \\n(0+1)*1(0+1)*1(0+1)* . But following two REs also represent the same language, each \\nensuring presence of least two 1\\'s somewhere in the string \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\nwww.indiansbrain.com\\nL(r) = {\\n has no pair of consecutive 1\\'s} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We \\nobserver that, whenever a 1 occurs, it must be immediately followed by a 0. This \\nsubstring may be preceded & followed by any no of 0\\'s. So the final RE must be a \\nrepetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the RE is \\n(0*100*)*. But in this case the strings ending in 1 or consisting of all 0\\'s are not \\naccounted for. Taking these observations into consideration, the final RE is r = \\n(0*100*)(1+ )+0*(1+\\n). \\n \\nAlternative Solution : \\n \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as \\nr = (0+10)*(1+ ).This is a shorter expression but represents the same language. \\n \\nRegular Expression and Regular Language : \\n \\nEquivalence(of REs) with FA : \\n \\nRecall that, language that is accepted by some FAs are known as Regular language. \\nThe two concepts : REs and Regular language are essentially same i.e. (for) every \\nregular language can be developed by (there is) a RE, and for every RE there is a \\nRegular Langauge. This fact is rather suprising, because RE approach to describing \\nlanguage is fundamentally differnet from the FA approach. But REs and FA are \\nequivalent in their descriptive power. We can put this fact in the focus of the following \\nTheorem. \\n \\nTheorem : A language is regular iff some RE describes it. \\n \\nThis Theorem has two directions, and are stated & proved below as a separate lemma \\n \\n \\nRE to FA : \\n \\nREs denote regular languages : \\n \\nLemma : If L(r) is a language described by the RE r, then it is regular i.e. there is a FA \\nsuch that L(M)\\nL(r). \\n \\nProof : To prove the lemma, we apply structured index on the expression r. First, we \\n \\nshow how to construct FA for the basis elements: \\n, and for any \\n. Then we show \\nhow to combine these Finite Automata into Complex Automata that accept the Union, \\nConcatenation, Kleen Closure of the languages accepted by the original smaller \\nautomata. \\nwww.indiansbrain.com\\nUse of NFAs is helpful in the case i.e. we construct NFAs for every REs which are \\nrepresented by transition diagram only. \\n \\nBasis : \\n \\n\\uf095 Case (i) : \\n. Then \\n. Then \\nand the following NFA N \\nrecognizes L(r). Formally \\nwhere Q = {q} \\nand \\n.\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf095 Case (ii) : \\n. \\n, and the following NFA N accepts L(r). Formally\\uf020\\nwhere \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince the start state is also the accept step, and there is no any transition defined, it \\nwill accept the only string \\nand nothing else. \\n \\n\\uf095 Case (iii) : r = a for some \\n. Then L(r) = {a}, and the following NFA \\nN accepts L(r).\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFormally, \\nwhere \\nfor \\nor \\n \\n \\n \\n \\n \\nInduction : \\nwww.indiansbrain.com\\nAssume that the start of the theorem is true for REs \\nand \\n. Hence we can assume \\nthat we have automata \\nand \\nthat accepts languages denoted by REs \\nand \\n, \\n \\nrespectively i.e. \\nand \\n. The FAs are represented \\nschematically as shown below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEach has an initial state and a final state. There are four cases to consider. \\n \\n\\uf095 Case (i) : Consider the RE \\ndenoting the language \\n. We \\nconstruct FA \\n, from \\nand \\nto accept the language denoted by RE \\nas \\nfollows :\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new (initial) start state \\nand give \\n- transition to the initial state of \\n and \\n.This is the initial state of \\n. \\n \\n\\uf095 Create a final state \\nand give \\n-transition from the two final state of \\nand \\n. \\nis the only final state of \\nand final state of \\nand \\nwill be \\nordinary states in \\n.\\uf020\\n\\uf095 All the state of \\nand \\nare also state of \\n.\\uf020\\nwww.indiansbrain.com\\n\\uf095 All the moves of \\nand \\nare also moves of \\n. [ Formal Construction] \\n \\n \\nIt is easy to prove that \\n \\nProof: To show that \\nwe must show that \\n \\n= \\n \\n \\n= \\nby following transition of \\n \\n \\nStarts at initial state \\nand enters the start state of either \\nor \\nfollwoing the \\ntransition i.e. without consuming any input. WLOG, assume that, it enters the start state \\nof \\n. From this point onward it has to follow only the transition of \\nto enter the final \\n \\nstate of \\n, because this is the only way to enter the final state of M by following the e-\\ntransition.(Which is the last transition & no input is taken at hte transition). Hence the \\n \\nwhole input w is considered while traversing from the start state of \\nto the final \\nstate of \\n. Therefore \\nmust accept \\n. \\n \\nSay, \\nor \\n. \\n \\n \\nWLOG, say \\n \\nTherefore when \\nprocess the string w , it starts at the initial state and enters the final \\nstate when w consumed totally, by following its transition. Then \\nalso accepts w, by \\nstarting at state \\nand taking \\n-transition enters the start state of \\n-follows the moves \\n \\nof \\nto enter the final state of \\nconsuming input w thus takes -transition to \\n. Hence proved \\n \\n\\uf095 Case(ii) : Consider the RE \\ndenoting the language \\n. We construct \\nFA \\nfrom \\n& \\nto accept \\nas follows :\\uf020\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new start state \\nand a new final state \\n \\n1. Add \\n- transition from \\no \\nto the start state of \\n \\n \\no \\nto \\n \\n \\no final state of \\nto the start state of \\n \\n \\n2. All the states of \\nare also the states of \\n. \\nhas 2 more states than that of \\nnamely \\nand \\n. \\n3. All the moves of \\nare also included in \\n. \\n \\nBy the transition of type (b), \\ncan accept . \\nBy the transition of type (a), \\ncan enters the initial state of \\nw/o any input and then \\nfollow all kinds moves of \\nto enter the final state of \\nand then following \\n-transition \\ncan enter \\n. Hence if any \\nis accepted by \\nthen w is also accepted by \\n. By the \\ntransition of type (b), strings accepted by \\ncan be repeated by any no of times & thus \\naccepted by \\n. Hence \\naccepts \\nand any string accepted by \\nrepeated (i.e. \\n \\nconcatenated) any no of times. Hence \\n \\nCase(iv) : Let \\n=(\\n). Then the FA \\nis also the FA for (\\n), since the use of \\nparentheses does not change the language denoted by the expression \\n \\nNon-Deterministic Finite Automata \\nNondeterminism is an important abstraction in computer science. Importance of \\nnondeterminism is found in the design of algorithms. For examples, there are many \\nproblems with efficient nondeterministic solutions but no known efficient deterministic \\nsolutions. ( Travelling salesman, Hamiltonean cycle, clique, etc). Behaviour of a process \\nis in a distributed system is also a good example of nondeterministic situation. Because \\nwww.indiansbrain.com\\nthe behaviour of a process might depend on some messages from other processes \\nthat might arrive at arbitrary times with arbitrary contents. \\nIt is easy to construct and comprehend an NFA than DFA for a given regular \\nlanguage. The concept of NFA can also be used in proving many theorems and \\nresults. Hence, it plays an important role in this subject. \\nIn the context of FA nondeterminism can be incorporated naturally. That is, an NFA is \\ndefined in the same way as the DFA but with the following two exceptions: \\n\\uf095 multiple next state.\\uf020\\n\\uf020\\n\\uf095 \\n- transitions.\\uf020\\n \\nMultiple Next State : \\n \\n\\uf095 In contrast to a DFA, the next state is not necessarily uniquely determined by the \\ncurrent state and input symbol in case of an NFA. (Recall that, in a DFA there is \\nexactly one start state and exactly one transition out of every state for each \\nsymbol in \\n).\\uf020\\n\\uf095 \\nThis means that - in a state q and with input symbol a - there could be one, more \\nthan one or zero next state to go, i.e. the value of \\nis a subset of Q. Thus\\uf020\\n \\n= \\nwhich means that any one of \\ncould be the next \\nstate. \\n \\n\\uf095 The zero next state case is a special one giving \\n=\\n, which means that \\nthere is no next state on input symbol when the automata is in state q. In such \\na case, we may think that the automata \"hangs\" and the input will be rejected.\\uf020\\n \\n- transitions : \\n \\nIn an -transition, the tape head doesn\\'t do anything- it doesnot read and it doesnot move. \\nHowever, the state of the automata can be changed - that is can go to zero, one \\n \\nor more states. This is written formally as \\nimplying that the next \\nstate could by any one of \\nw/o consuming the next input symbol. \\n \\n \\n \\n \\nAcceptance : \\n \\nInformally, an NFA is said to accept its input \\nif it is possible to start in some start state \\nand process \\n, moving according to the transition rules and making choices along the way \\nwhenever the next state is not uniquely defined, such that when \\nis completely processed \\n(i.e. end of \\nis reached), the automata is in an accept state. There may be several \\npossible paths through the automation in response to an input \\nsince the start state is not \\ndetermined and there are choices along the way because of multiple next states. Some of \\nthese paths may lead to accpet states while others may not. The \\nwww.indiansbrain.com\\nautomation is said to accept \\nif at least one computation path on input \\nstarting from at \\nleast one start state leads to an accept state- otherwise, the automation rejects input \\n. \\nAlternatively, we can say that, \\nis accepted iff there exists a path with label \\nfrom some \\nstart state to some accept state. Since there is no mechanism for determining which state \\nto start in or which of the possible next moves to take (including the \\n- transitions) in \\nresponse to an input symbol we can think that the automation is having some \"guessing\" \\npower to chose the correct one in case the input is accepted \\n \\nExample 1 : Consider the language L = {\\n {0, 1}* | The 3rd symbol from the right \\nis 1}. The following four-state automation accepts L. \\n \\nThe m/c is not deterministic since there are two transitions from state \\non input 1 \\nand no transition (zero transition) from \\non both 0 & 1. \\n \\nFor any string \\nwhose 3rd symbol from the right is a 1, there exists a sequence of legal \\ntransitions leading from the start state q, to the accept state \\n. But for any string \\nwhere 3rd symbol from the right is 0, there is no possible sequence of legal \\n \\ntranisitons leading from \\nand \\n. Hence m/c accepts L. How does it accept any string \\nL? \\n \\nFormal definition of NFA : \\n \\nFormally, an NFA is a quituple \\nwhere Q, \\n, \\n, and F bear \\nthe same meaning as for a DFA, but \\n, the transition function is redefined as follows: \\n \\n \\n \\n \\nwhere P(Q) is the power set of Q i.e. \\n. \\n \\nThe Langauge of an NFA : \\n \\nFrom the discussion of the acceptance by an NFA, we can give the formal definition of a \\nlanguage accepted by an NFA as follows : \\n \\nIf \\nis an NFA, then the langauge accepted by N is writtten as L(N) \\nis given by \\n. \\n \\nThat is, L(N) is the set of all strings w in \\nsuch that \\ncontains at least \\none accepting state. \\nwww.indiansbrain.com\\nRemoving ϵ-transition: \\n \\n- transitions do not increase the power of an NFA . That is, any \\n- NFA ( NFA with \\ntransition), we can always construct an equivalent NFA without \\n-transitions. The \\n \\nequivalent NFA must keep track where the \\nNFA goes at every step during \\ncomputation. This can be done by adding extra transitions for removal of every \\n- \\ntransitions from the - NFA as follows. \\n \\nIf we removed the \\n- transition \\nfrom the - NFA , then we need to moves \\n \\nfrom state p to all the state on input symbol \\nwhich are reachable from state q \\n(in the - NFA ) on same input symbol q. This will allow the modified NFA to move \\nfrom state p to all states on some input symbols which were possible in case of \\n-NFA \\non the same input symbol. This process is stated formally in the following theories. \\n \\nTheorem if L is accepted by an - NFA N , then there is some \\nequivalent \\nwithout transitions accepting the same language L \\nProof: \\n \\nLet \\nbe the given \\nwith \\n \\n \\nWe construct \\n \\nWhere, \\nfor all \\nand \\nand \\n \\n \\n \\n \\n \\nOther elements of N\\' and N \\n \\nWe can show that \\ni.e. N\\' and N are equivalent. \\n \\nWe need to prove that \\n \\n \\n i.e. \\n \\n \\n \\n \\n \\nWe will show something more, that is, \\nwww.indiansbrain.com\\nWe will show something more, that is, \\n \\n \\nBasis : \\n, then \\n \\n \\nBut \\nby definition of \\n. \\n \\nInduction hypothesis Let the statement hold for all \\nwith \\n. \\n \\n \\nBy definition of extension of \\n \\n \\nBy inductions hypothesis. \\n \\nAssuming that \\n \\n \\n \\n \\nBy definition of \\n \\n \\nSince \\n \\n \\n \\nTo complete the proof we consider the case \\n \\nWhen \\ni.e. \\nthen \\nwww.indiansbrain.com\\n \\nand by the construction of \\nwherever \\nconstrains a state in F. \\n \\nIf \\n(and thus \\nis not in F ), then \\nwith \\nleads to an accepting state in N\\' iff it \\nlead to an accepting state in N ( by the construction of N\\' and N ). \\n \\nAlso, if (\\n , thus w is accepted by N\\' iff w is accepted by N (iff \\n) \\n \\nIf \\n(and, thus in M we load \\nin F ), thus is accepted by both N\\' and N . \\n \\nLet \\n. If w cannot lead to \\nin N , then \\n. (Since can add transitions to get an accept \\nstate). So there is no harm in making \\nan accept state in N\\'. \\n \\nEx: Consider the following NFA with \\n- transition. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition Diagram \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition diagram for \\n\\' for the equivalent NFA without - moves \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n          1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince \\nthe start state q0 must be final state in the equivalent NFA . \\n \\nSince \\nand \\nand \\nwe add moves \\nand \\nin the equivalent NFA . Other moves are also constructed accordingly. \\n \\n-closures: \\n \\nThe concept used in the above construction can be made more formal by defining the \\n-closure for a state (or a set of states). The idea of \\n-closure is that, when moving \\n \\nfrom a state p to a state q (or from a set of states Si to a set of states Sj ) an input \\n, we need to take account of all \\n-moves that could be made after the transition. \\nFormally, for a given state q, \\n \\n \\n-closures: \\n \\nSimilarly, for a given set \\n \\n \\n-closures: \\n \\n \\n \\nSo, in the construction of equivalent NFA N\\' without -transition from any NFA with \\n \\nmoves. the first rule can now be written as \\nwww.indiansbrain.com\\nEquivalence of NFA and DFA \\n \\nIt is worth noting that a DFA is a special type of NFA and hence the class of languages \\naccepted by DFA s is a subset of the class of languages accepted by NFA s. \\nSurprisingly, these two classes are in fact equal. NFA s appeared to have more power \\nthan DFA s because of generality enjoyed in terms of \\n-transition and multiple next \\nstates. But they are no more powerful than DFA s in terms of the languages they \\naccept. \\n \\nConverting DFA to NFA \\n \\n \\n \\n \\nTheorem: Every DFA has as equivalent NFA \\n \\nProof: A DFA is just a special type of an NFA . In a DFA , the transition functions is \\ndefined from \\nwhereas in case of an NFA it is defined from \\nand \\nbe a DFA . We construct an equivalent NFA \\nas follows. \\n \\n \\n \\n \\ni. e \\n \\nIf \\nand \\n \\nAll other elements of N are as in D. \\n \\nIf \\nthen there is a sequence of states \\nsuch that \\n \\n \\n \\nThen it is clear from the above construction of N that there is a sequence of states (in N) \\nsuch that \\nand \\nand hence \\n \\n \\nSimilarly we can show the converse. \\n \\nHence , \\n \\n \\nGiven any NFA we need to construct as equivalent DFA i.e. the DFA need to simulate \\nthe behaviour of the NFA . For this, the DFA have to keep track of all the states where \\nthe NFA could be in at every step during processing a given input string. \\nwww.indiansbrain.com\\nThere are \\npossible subsets of states for any NFA with n states. Every subset \\ncorresponds to one of the possibilities that the equivalent DFA must keep track of. Thus, \\n \\nthe equivalent DFA will have \\nstates. \\n \\nThe formal constructions of an equivalent DFA for any NFA is given below. We \\nfirst consider an NFA without \\ntransitions and then we incorporate the affects of \\ntransitions later. \\n \\nFormal construction of an equivalent DFA for a given NFA without transitions. \\n \\nGiven an \\nwithout - moves, we construct an equivalent DFA \\n \\n \\nas follows \\n \\ni.e. \\n \\n \\n \\n \\n \\n(i.e. every subset of Q which as an element in F is considered as a final stat\\nin DFA D ) \\n \\n \\n \\n \\nfor all \\nand \\n \\n \\nwhere \\n \\n \\nThat is, \\n \\n \\nTo show that this construction works we need to show that L(D)=L(N) i.e. \\n \\n \\n \\n \\n \\n \\nOr,\\n \\n \\nWe will prove the following which is a stranger statement thus required. \\nwww.indiansbrain.com\\n \\n \\nProof : We will show by inductions on \\n \\n \\nBasis If \\n=0, then w =  \\n \\nSo, \\nby definition. \\n \\nInductions hypothesis : Assume inductively that the statement holds \\nof \\nlength less than or equal to n. \\n \\nInductive step \\n \\nLet \\n, then \\nwith \\n \\n \\nNow, \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNow, given any NFA with -transition, we can first construct an equivalent NFA without \\n-transition and then use the above construction process to construct an equivalent \\nDFA , thus, proving the equivalence of NFA s and DFA s.. \\n \\nIt is also possible to construct an equivalent DFA directly from any given NFA with \\n- transition by integrating the concept of \\n-closure in the above construction. \\n \\nRecall that, for any \\n \\n \\n- closure : \\nwww.indiansbrain.com\\nIn the equivalent DFA , at every step, we need to modify the transition functions \\nto \\nkeep track of all the states where the NFA can go on \\n-transitions. This is done by \\nreplacing \\nby \\n-closure \\n, i.e. we now compute \\nat every step as \\nfollows: \\n \\n \\n \\nBesides this the initial state of the DFA D has to be modified to keep track of all the \\nstates that can be reached from the initial state of NFA on zero or more -transitions. \\nThis can be done by changing the initial state \\nto -closure (\\n ) . \\n \\nIt is clear that, at every step in the processing of an input string by the DFA D , it enters \\na state that corresponds to the subset of states that the NFA N could be in at that \\nparticular point. This has been proved in the constructions of an equivalent NFA for any \\n-NFA \\nIf the number of states in the NFA is n , then there are \\nstates in the DFA . That is, \\neach state in the DFA is a subset of state of the NFA . \\n \\nBut, it is important to note that most of these \\nstates are inaccessible from the start state \\nand hence can be removed from the DFA without changing the accepted language. Thus, \\nin fact, the number of states in the equivalent DFA would be much less \\n \\nthan \\n. \\nExample : Consider the NFA given below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n{\\n} \\n \\n \\n \\n \\nSince there are 3 states in the NFA \\nwww.indiansbrain.com\\nThere will be \\nstates (representing all possible subset of states) in the \\nequivalent DFA . The transition table of the DFA constructed by using the subset \\nconstructions process is produced here. \\n \\n0 \\n \\n1 The start state of the DFA is   - closures \\n \\n \\n \\n \\n The final states are all those subsets that contains \\n(since \\nin the NFA). \\n \\n{   } \\nLet us compute one entry, \\n \\n \\n \\n \\n \\n \\n \\n \\n Similarly, all other transitions can be computed \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCorresponding Transition fig. for DFA.Note that states \\n \\nare not accessible and hence can be removed. \\nThis gives us the following simplified DFA with only 3 states. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is interesting to note that we can avoid encountering all those inaccessible \\nor unnecessary states in the equivalent DFA by performing the following two \\nsteps inductively. \\n \\n1. If \\nis the start state of the NFA, then make \\n- closure ( \\n) the start state of the \\nequivalent DFA . This is definitely the only accessible state. \\n \\n2. If we have already computed a set \\nof states which are accessible. Then \\n. \\ncompute \\nbecause these set of states will also be accessible. \\n \\nFollowing these steps in the above example, we get the transition table given below \\nwww.indiansbrain.com\\nUNIT-II \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying certain recursive rules \\nas given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) \\nis a RE \\n \\nii) \\nis a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\nIf \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\n \\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive REs) by a finite no of \\napplications of the recursive step (given in 2). \\n \\nExample : Let \\n= { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this expression by \\napplying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n \\n \\nwww.indiansbrain.com\\n4 \\n(1+   ) \\nRule 2(iv) & Step 3 \\n \\n \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated with every RE). \\nWe will see later that REs are used to attribute regular languages. \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We can define the \\nlanguage L(r) associated with (or described by) a REs as follows. \\n \\n1. \\nis the RE describing the empty language i.e. L(\\n) = \\n. \\n \\n2. \\nis a RE describing the language {\\n} i.e. L(\\n) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\n \\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,........} {0} \\n{1} \\n \\n= {\\n , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nwww.indiansbrain.com\\nConsider the RE ab + c. The language described by the RE can be thought of either L(a)L(b+c) or \\n \\nL(ab)\\nL(c) as provided by the rules (of languages described by REs) given already. But these two \\nrepresents two different languages lending to ambiguity. To remove this ambiguity we can either \\n \\n \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. Like other algebras mod in \\nmathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) operator. \\n \\nii) It is also important to note that concatenation & union (+) operators are associative and union operation \\nis commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language L(ab) \\nL(c) i.e. it should be \\ngrouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For example, the \\nlanguage represented by the RE a(b+c) is L(a)L(b+c). \\n \\n \\n \\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language L(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all strings over {0,1} which \\nare either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an even number of 0\\'s \\n \\nfollowed by an odd number of 1\\'s i.e. \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE rr, \\ndenotes \\nr, \\nand so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of consecutive 1\\'s} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and what goes before is \\ncompletely arbitrary. Considering these observations we can write the REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE (0+1)*11(0+1)*+(0+1)*00(0+1)* \\nrepresents the set of string over {0,1} that contains the substring 11 or 00. \\nwww.indiansbrain.com\\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the set of strings over {0,1} \\nthat contains exactly two 1\\'s. The presence of two 1\\'s in the RE and any no of 0\\'s before, between and after the \\n1\\'s ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1\\'s. \\n \\nSolution : There must be at least two 1\\'s in the RE somewhere and what comes before, between, and after is \\ncompletely arbitrary. Hence we can write the RE as (0+1)*1(0+1)*1(0+1)*. But following two REs also \\nrepresent the same language, each ensuring presence of least two 1\\'s somewhere in the string \\n \\n \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\n \\nL(r) = {\\n has no pair of consecutive 1\\'s} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We observer that, whenever \\na 1 occurs, it must be immediately followed by a 0. This substring may be preceded & followed by any no of \\n0\\'s. So the final RE must be a repetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the \\nRE is (0*100*)*. But in this case the strings ending in 1 or consisting of all 0\\'s are not accounted for. Taking \\nthese observations into consideration, the final RE is  r = (0*100*)(1+ \\n)+0*(1+\\n). \\n \\nAlternative Solution : \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as r = (0+10)*(1+\\n).This \\nis a shorter expression but represents the same language. \\n \\nRegular Expression: \\n \\nFA to regular expressions: \\n \\nFA to RE (REs for Regular Languages) : \\n \\nLemma : If a language is regular, then there is a RE to describe it. i.e. if L = L(M) for some DFA M, then there is a \\nRE r such that L = L(r). \\n \\nProof : We need to construct a RE r such that \\n. Since M is a DFA, it has a finite no \\nof states. Let the set of states of M is Q = {1, 2, 3,..., n} for some integer n. [ Note : if the n states of M were \\ndenoted by some other symbols, we can always rename those to indicate as 1, 2, 3,..., n ]. The required RE is \\nconstructed inductively. \\n \\nNotations : \\nis a RE denoting the language which is the set of all strings w such that w is the label of a \\npath from state i to state j \\nin M, and that path has no intermediate state whose number is \\ngreater then k. ( i & j (begining and end pts) are not considered to be \"intermediate\" so i and /or j can be \\nwww.indiansbrain.com\\ngreater than k ) \\n \\nWe now construct \\ninductively, for all i, j \\nQ starting at k = 0 and finally reaching k = n. \\n \\nBasis : k = 0, \\ni.e. the paths must not have any intermediate state ( since all states are numbered 1 or \\nabove). There are only two possible paths meeting the above condition : \\n \\n1. A direct transition from state i to state j. \\no \\n= a if then is a transition from state i to state j on symbol the single symbol a. \\n \\no \\n= \\nif there are multiple transitions from state i to state j on symbols \\n \\n. \\no \\n= f if there is no transition at all from state i to state j. \\n \\n2. All paths consisting of only one node i.e. when i = j. This gives the path of length 0 (i.e. the RE \\ndenoting the string \\n) and all self loops. By simply adding Î to various cases above we get \\nthe corresponding REs i.e. \\no \\n= \\n+ a if there is a self loop on symbol a in state i . \\n \\no \\n= \\n+ \\nif there are self loops in state i as multiple symbols \\n \\n. \\n \\no \\n= \\nif there is no self loop on state i. \\n \\nInduction : \\n \\nAssume that there exists a path from state i to state j such that there is no intermediate state whose number is \\n \\ngreater than k. The corresponding Re for the label of the path is \\n. There are only two possible cases : \\n \\n1. The path dose not go through the state k at all i.e. number of all the intermediate states are less \\nthan k. So, the label of the path from state i to state j is tha language described by the RE \\n. \\n \\n2. The path goes through the state k at least once. The path may go from i to j and k may appear more \\nthan once. We can break the into pieces as shown in the figure 7. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7 \\n \\n1. The first part from the state i to the state k which is the first recurence. In this path, all \\nintermediate states are less than k and it starts at iand ends at k. So the RE \\ndenotes the \\nlanguage of the label of path. \\n \\n2. The last part from the last occurence of the state k in the path to state j. In this path also, no \\nintermediate state is numbered greater than k. Hence the RE \\ndenoting the language of the \\nlabel of the path. \\n \\n3. In the middle, for the first occurence of k to the last occurence of k , represents a loop which may be \\ntaken zero times, once or any no of times. And all states between two consecutive k\\'s are \\nnumbered less than k. \\n \\nHence the label of the path of the part is denoted by the RE \\n.The label of the path from state i to state \\nj is the concatenation of these 3 parts which is \\n \\n \\n \\nSince either case 1 or case 2 may happen the labels of all paths from state i to j is denoted by the following RE \\n \\n \\n \\n \\nWe can construct \\nfor all i, j \\n{1,2,..., n} in increasing order of k starting with the basis k = 0 upto k = n since \\ndepends only on expressions with a small superscript (and hence will be available). WLOG, assume \\n \\nthat state 1 is the start state and \\nare the m final states where ji \\n{1, 2, ... , n }, \\nand \\n \\n. According to the convention used, the language of the automatacan be denoted by the RE \\nwww.indiansbrain.com\\n \\n \\nSince \\nis the set of all strings that starts at start state 1 and finishes at final state \\nfollowing the transition of \\nthe FA with any value of the intermediate state (1, 2, ... , n) and hence accepted by the automata. \\n \\nRegular Grammar: \\n \\nA grammar \\nis right-linear if each production has one of the following three forms: \\n \\n\\uf095 \\nA\\ncB ,\\uf020\\n\\uf020\\n\\uf095 \\nA\\nc,\\uf020\\n\\uf095 \\nA\\n\\uf020\\n \\nWhere A, B \\n( with A = B allowed) and \\n. A grammar G is left-linear if each production has once of \\nthe following three forms. \\n \\nA\\nBc , A\\nc, A\\n \\n \\nA right or left-linear grammar is called a regular grammar. \\n \\nRegular grammar and Finite Automata are equivalent as stated in the following theorem. \\n \\nTheorem : A language L is regular iff it has a regular grammar. We use the following two lemmas to prove the \\nabove theorem. \\n \\nLemma 1 : If L is a regular language, then L is generated by some right-linear grammar. \\n \\nProof : Let \\nbe a DFA that accepts L. \\n \\nLet \\nand \\n. \\n \\nWe construct the right-linear grammar \\nby letting \\n \\nN = Q , \\nand \\n \\n[ Note: If \\n, then \\n] \\n \\nLet \\n. For M to accept w, there must be a sequence of states \\nsuch that \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\nBy construction, the grammar G will have one production for each of the above transitions. Therefore, we have \\nthe corresponding derivation. \\n \\n \\n \\n \\nHence w \\nL(g). \\n \\nConversely, if \\n, then the derivation of w in G must have the form as given above. \\nBut, then the construction of G from M implies that \\n \\n, where \\n, completing the proof. \\n \\nLemma 2 : Let \\nbe a right-linear grammar. Then L(G) is a regular \\nlanguage. Proof: To prove it, we construct a FA M from G to accept the same language. \\n \\nis constructed as follows: \\n \\n( \\nis a special sumbol not in N ) \\n \\n, \\n \\nFor any \\nand \\nand \\nis defined as \\n \\n \\nif \\n \\nand \\n, if \\n. \\nWe now show that this construction works. \\n \\nLet \\n. Then there is a derivation of w in G of the form \\nwww.indiansbrain.com\\n \\n \\nBy contradiction of M, there must be a sequence of transitions \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimplying that \\ni.e. w is accepted by M. \\n \\nConversely, if \\nis accepted by M, then because \\nis the only accepting state of M, the transitions \\ncausing w to be accepted by M will be of the form given above. These transitions corresponds to a \\n \\nderivationof w in the grammar G. Hence \\n, completing the proof of the lemma. \\n \\nGiven any left-linear grammar G with production of the form \\n, we can construct from it a \\nright-linear grammar \\nby replacing every production of G of the form \\nwith \\n \\n \\nIt is easy to prove that \\n. Since \\nis right-linear, \\nis regular. But then so are \\ni.e. \\nbecause regular languages are closed under reversal. \\n \\nPutting the two lemmas and the discussions in the above paragraph together we get the proof of the theorem- \\n \\nA language L is regular iff it has a regular grammar \\n \\nExample : Consider the grammar \\n \\n \\n \\nIt is easy to see that G generates the language denoted by the regular expression \\n(01)*0. The construction of lemma 2 for this grammar produces the follwoing FA. \\n \\nThis FA accepts exactly (01)*1. \\n \\nDecisions Algorithms for CFL \\n \\nIn this section, we examine some questions about CFLs we can answer. A CFL may be represented using a \\nCFG or PDA. But an algorithm that uses one representation can be made to work for the others, since we can \\nconstruct one from the other. \\nwww.indiansbrain.com\\nTesting Emptiness : \\n \\nTheorem : There are algorithms to test emptiness of a CFL. \\n \\nProof : Given any CFL L, there is a CFG G to generate it. We can determine, using the construction described \\n \\nin the context of elimination of useless symbols, whether the start symbol is useless. If so, then \\n; \\notherwise not. \\n \\nTesting Membership : \\n \\nGiven a CFL L and a string x, the membership, problem is to determine whether \\n? \\n \\nGiven a PDA P for L, simulating the PDA on input string x doesnot quite work, because the PDA can grow \\nits stack indefinitely on \\ninput, and the process may never terminate, even if the PDA is deterministic. \\n \\nSo, we assume that a CFG \\nis given such that L = L(G). \\n \\nLet us first present a simple but inefficient algorithm. \\n \\nConvert G to \\nin CNF generating \\n. If the input string \\n, then we need to \\n \\ndetermine whether \\nand it can easily be done using the technique given in the context of elimination of \\n \\n-production. If , \\nthen \\niff \\n. Consider a derivation under a grammar in CNF. At \\nevery step, a production in CNF in used, and hence it adds exactly one terminal symbol to the sentential form. \\n \\nHence, if the length of the input string x is n, then it takes exactly n steps to derive x ( provided x is in \\n). \\n \\nLet the maximum number of productions for any nonterminal in \\nis K. So at every step in derivation, there are \\natmost k choices. We may try out all these choices, systematically., to derive the string x in \\n. Since \\n \\nthere are atmost \\ni.e. \\nchoices. This algorithms is of exponential time complexity. We now present an \\nefficient (polynomial time) membership algorithm. \\n \\nPumping Lemma: \\n \\nLimitations of Finite Automata and Non regular Languages : \\n \\nThe class of languages recognized by FA s is strictly the regular set. There are certain languages which are \\nnon regular i.e. cannot be recognized by any FA \\n \\nConsider the language \\n \\n \\nIn order to accept is language, we find that, an automaton seems to need to remember when passing the \\ncenter point between a\\'s and b\\'s how many a\\'s it has seen so far. Because it would have to compare that \\nwith the number of b\\'s to either accept (when the two numbers are same) or reject (when they are not same) \\nthe input string. \\nwww.indiansbrain.com\\nBut the number of a\\'s is not limited and may be much larger than the number of states since the string may \\nbe arbitrarily long. So, the amount of information the automaton need to remember is unbounded. \\n \\nA finite automaton cannot remember this with only finite memory (i.e. finite number of states). The fact that \\nFA s have finite memory imposes some limitations on the structure of the languages recognized. Inductively, we \\ncan say that a language is regular only if in processing any string in this language, the information that has to \\nbe remembered at any point is strictly limited. The argument given above to show that \\nis non regular is \\ninformal. We now present a formal method for showing that certain languages such as \\nare non regular \\n \\nProperties of CFL’s \\n \\nClosure properties of CFL: \\n \\nWe consider some important closure properties of CFLs. \\n \\nTheorem : If \\nand \\nare CFLs then so is \\n \\n \\nProof : Let \\nand \\nbe CFGs generating. Without loss of generality, we \\ncan assume that \\n. Let \\nis a nonterminal not in \\nor \\n. We construct the grammar \\n \\nfrom \\nand \\n, where \\n \\n, \\n \\n \\n \\n \\n \\n \\n \\n \\nWe now show that \\n \\n \\nThus proving the theorem. \\n \\nLet \\n. Then \\n. All productions applied in their derivation are also in \\n. Hence \\ni.e. \\n \\n \\n \\nSimilarly, if \\n, then \\n \\n \\nThus \\n. \\nwww.indiansbrain.com\\nConversely, let \\n. Then \\nand the first step in this derivation must be either \\nor \\n. Considering the former case, we have \\n \\n \\nSince \\nand \\nare disjoint, the derivation \\nmust use the productions of \\nonly ( which are also in \\n \\n) Since \\nis the start symbol of \\n. Hence, \\ngiving \\n. \\n \\nUsing similar reasoning, in the latter case, we get \\n. Thus \\n. \\n \\nSo, \\n, as claimed \\n \\n \\nTheorem : If \\nand \\nare CFLs, then so is \\n. \\n \\nProof : Let \\nand \\nbe the CFGs generating \\nand \\nrespectively. \\nAgain, we assume that \\nand \\nare disjoint, and \\nis a nonterminal not in \\nor \\n. we construct the CFG \\nfrom \\nand \\n, where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe claim that \\n \\n \\n \\nTo prove it, we first assume that \\nand \\n. Then \\nand \\n. We can derive the string xy \\nin \\nas shown below. \\n \\n \\n \\n \\n \\nsince \\nand \\n. Hence \\n. \\nwww.indiansbrain.com\\nFor the converse, let \\n. Then the derivation of w in \\nwill be of the form \\n \\ni.e. the first step in the derivation must see the rule \\n. Again, since \\nand \\nare \\ndisjoint and \\nand \\n, some string x will be generated from \\nusing productions in \\n( which \\nare also in \\n) and such that \\n. \\n \\nThus \\n \\nHence \\nand \\n. \\n \\nThis means that w can be divided into two parts x, y such that \\nand \\n. Thus \\n.This \\n \\ncompletes the proof \\nTheorem : If L is a CFL, then so is \\n. \\nProof : Let \\nbe the CFG generating L. Let us construct the CFG \\n \\n \\nwhere \\n. \\n \\nWe now prove that \\n, which prove the theorem. \\n \\ncan generate \\nin one step by using the production \\nsince \\n, \\n Let \\nfor any n >1 we can \\nwrite \\nwhere \\nfor \\n \\n \\n \\n \\nusing following steps. \\n \\n \\n \\n \\n \\nFirst (n-1)-steps uses the production S\\nSS producing the sentential form of n numbers of S \\'s. The \\nnonterminal S in the i-th position then generates \\nusing production in P ( which are also in \\n) \\n \\nIt is also easy to see that G can generate the empty string, any string in L and any string \\nfor n >1 \\nand none other. \\n \\nHence \\n \\n \\nTheorem : CFLs are not closed under intersection \\n \\nProof : We prove it by giving a counter example. Consider the language \\n.The following \\nCFG generates L1 and hence a CFL \\n \\ncan generate any string in L. \\n. w can be generated by \\n \\nfrom G \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\nThe nonterminal X generates strings of the form \\nand C generates strings of the form \\n, \\n. These are the only types of strings generated by X and C. Hence, S generates \\n. \\n \\nUsing similar reasoning, it can be shown that the following grammar \\nand hence it is \\nalso a CFL. \\n \\n \\n \\n \\n \\n \\n \\nBut, \\nand is already shown to be not context-free. \\n \\nHence proof. \\n \\nTheorem : A CFL\\'s are not closed under complementations \\n \\nProof : Assume, for contradiction, that CFL\\'s are closed under complementation. SInce, CFL\\'s are also closed \\nunder union, the language \\n, where \\nand \\nare CFL\\'s must be CFL. But by DeMorgan\\'s law \\n \\n \\n \\n \\nThis contradicts the already proved fact that CFL\\'s are not closed under intersection. \\nBut it can be shown that the CFL\\'s are closed under intersection with a regular set. \\n \\nTheorem : If L is a CFL and R is a regular language, then \\nis a CFL. \\n \\nProof : Let \\nbe a PDA for L and let \\nbe a DFA for \\nR. We construct a PDA M from P and D as follows \\n \\n \\nwhere \\nis defined as \\n \\ncontains \\niff \\nwww.indiansbrain.com\\nand \\ncontains \\n \\n \\nThe idea is that M simulates the moves of P and D parallely on input w, and accepts w iff both P and \\nD accepts. That means, we want to show that \\n \\n \\n \\n \\nWe apply induction on n, the number of moves, to show that \\n \\niff \\n \\nand \\n \\nBasic Case is n=0. Hence \\n, \\nand \\n. For this case it is trivially true \\n \\nInductive hypothesis : Assume that the statement is true for n -1. \\n \\nInductive Step : Let w = xa and \\n \\n \\n \\nLet \\n \\nBy inductive hypothesis, \\nand \\n \\n \\nFrom the definition of \\nand considering the n-th move of the PDA M above, we have \\n \\nand \\n \\n \\nHence \\nand \\n \\n \\nIf \\nand \\n, then \\nand we got that if M accepts w, then both P and D accepts it. \\n \\nWe can show that converse, in a similar way. Hence \\nis a CFL ( since it is accepted by a PDA M ) \\nThis property is useful in showing that certain languages are not context-free. \\n \\nExample : Consider the language \\n \\n \\nIntersecting L with the regular set \\n, we get \\nwww.indiansbrain.com\\n \\n \\n \\nWhich is already known to be not context-free. Hence L is not context-free \\nTheorem : CFL\\'s are closed under reversal. That is if L is a CFL, then so is \\n \\n \\nProof : Let the CFG \\ngenerates L. We construct a CFG \\nwhere \\n \\n. We now show that \\n, thus proving the theorem. \\nWe need to prove that \\niff \\n. \\n \\nThe proof is by induction on n, the number of steps taken by the derivation. We assume, for simplicity (and \\nof course without loss of generality), that G and hence \\nare in CNF. \\n \\nThe basis is n=1 in which case it is trivial. Because \\nmust be either \\nor BC with \\n. \\n \\nHence \\niff \\n \\n \\nAssume that it is true for (n-1)-steps. Let \\n. Then the first step must apply a rule of the \\nform \\nand it gives \\n \\nwhere \\nand \\n \\n \\nBy constructing of G\\', \\n \\nHence \\n \\n \\nThe converse case is exactly similar \\nSubstitution : \\n \\n, let \\nbe a language (over any alphabet). This defines a function S, called substitution, on \\nwhich is \\n \\ndenoted as \\n- for all \\n \\n \\nThis definition of substitution can be extended further to apply strings and langauge as well. \\nIf \\n, where \\n, is a string in \\n, then \\n \\n. \\nSimilarly, for any language L, \\n \\nThe following theorem shows that CFLs are closed under substitution. \\n \\nThereom : Let \\nis a CFL, and s is a substitution on \\nsuch that \\nis a CFL for all \\n, \\nthus s(L) is a CFL \\n \\nProof : Let L = L(G) for a CFG \\nand for every \\n, \\nfor some \\n. Without loss of generality, assume that the sets of nonterminals N and \\n\\'s \\nare disjoint. \\nwww.indiansbrain.com\\nNow, we construct a grammar \\n, generating s(L), from G and \\n\\'s as follows : \\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095 \\nconsists of\\uf020\\n\\uf020\\n1. \\nand \\n \\n2. The production of P but with each terminal a in the right hand side of a production replaced \\nby \\neverywhere. \\nWe now want to prove that this construction works i.e. \\niff \\n. \\n \\nIf Part : Let \\nthen according to the definition there is some string \\nand \\nfor \\nsuch that \\n \\nWe will show that \\n. \\n \\nFrom the construction of \\n, we find that, there is a derivation \\ncorresponding to the string \\n \\n(since \\ncontains all productions of G but every ai replaced with \\nin the RHS of any \\nproduction). \\n \\nEvery \\nis the start symbol of \\nand all productions of \\nare also included in \\n. \\nHence \\n \\n \\n \\n \\n \\n \\n \\n \\nTherefore, \\n \\n \\n(Only-if Part) Let \\n. Then there must be a derivative as follows : \\n \\n(using the production of G include in \\nas modified by (step 2) of the construction of \\n.) \\n \\nEach \\n(\\n) can only generate a string \\n, since each \\n\\'s and N are disjoin. \\nTherefore, we get \\n \\n \\nsince \\n \\nwww.indiansbrain.com\\nsince \\n \\n \\n \\n \\n \\nThe string \\nis formed by substituting strings \\nfor each \\nand hence \\n. \\n \\nTheorem : CFL\\'s are closed under homomorphism \\n \\nProof : Let \\nbe a CFL, and h is a homomorphism on \\ni.e \\nfor some alphabets \\n. consider the \\nfollowing substitution S:Replace each symbol \\nby the language consisting of the only string h(a), i.e. \\n \\nfor all \\n. Then, it is clear that, h(L) = s(L). Hence, CFL\\'s being closed under \\nsubstitution must also be closed under homomorphism. \\nwww.indiansbrain.com\\n \\n \\nUNIT- III \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nGrammar \\n \\nA grammar is a mechanism used for describing languages. This is one of the most simple but yet powerful \\nmechanism. There are other notions to do the same, of course. \\n \\nIn everyday language, like English, we have a set of symbols (alphabet), a set of words constructed from \\nthese symbols, and a set of rules using which we can group the words to construct meaningful sentences. The \\ngrammar for English tells us what are the words in it and the rules to construct sentences. It also tells us \\nwhether a particular sentence is well-formed (as per the grammar) or not. But even if one follows the rules of \\nthe english grammar it may lead to some sentences which are not meaningful at all, because of impreciseness \\nand ambiguities involved in the language. In english grammar we use many other higher level constructs like \\nnoun-phrase, verb-phrase, article, noun, predicate, verb etc. A typical rule can be defined as \\n \\n< sentence >\\n< noun-phrase > < predicate > \\n \\nmeaning that \"a sentence can be constructed using a \\'noun-phrase\\' followed by a predicate\". \\n \\nSome more rules are as follows: \\n \\n< noun-phrase >\\n< article >< noun > \\n \\n< predicate > \\n< verb > \\n \\nwith similar kind of interpretation given above. \\n \\nIf we take {a, an, the} to be <article>; cow, bird, boy, Ram, pen to be examples of <noun>; and eats, runs, \\nswims, walks, are associated with <verb>, then we can construct the sentence- a cow runs, the boy eats, an \\npen walks- using the above rules. Even though all sentences are well-formed, the last one is not meaningful. \\nWe observe that we start with the higher level construct <sentence> and then reduce it to <noun-phrase>, \\n<article>, <noun>, <verb> successively, eventually leading to a group of words associated with these \\nconstructs. \\n \\nThese concepts are generalized in formal language leading to formal grammars. The word \\'formal\\' here refers \\nto the fact that the specified rules for the language are explicitly stated in terms of what strings or symbols can \\noccur. There can be no ambiguity in it. \\n \\nFormal definitions of a Grammar \\nwww.indiansbrain.com\\nA grammar G is defined as a quadruple. \\n \\n \\n \\n \\nN is a non-empty finite set of non-terminals or variables, \\n \\nis a non-empty finite set of terminal symbols such that \\n \\n \\n, is a special non-terminal (or variable) called the start symbol, and \\nis a \\nfinite set of production rules. \\n \\nThe binary relation defined by the set of production rules is denoted by \\n, i.e. \\niff \\n. \\n \\nIn other words, P is a finite set of production rules of the form \\n, where \\nand \\n \\n \\n \\nProduction rules: \\n \\nThe production rules specify how the grammar transforms one string to another. Given a string \\n, we say that \\nthe production rule \\nis applicable to this string, since it is possible to use the rule \\nto rewrite the \\n(in \\n) to \\nobtaining a new string \\n. We say that \\nderives \\nand is denoted as \\n \\n \\n \\n \\nSuccessive strings are dervied by applying the productions rules of the grammar in any arbitrary order. \\nA particular rule can be used if it is applicable, and it can be applied as many times as described. \\n \\nWe write \\nif the string \\ncan be derived from the string \\nin zero or more steps; \\nif \\ncan be \\nderived from \\nin one or more steps. \\n \\nBy applying the production rules in arbitrary order, any given grammar can generate many strings of terminal \\nsymbols starting with the special start symbol, S, of the grammar. The set of all such terminal strings is \\ncalled the language generated (or defined) by the grammar. \\n \\nFormaly, for a given grammar \\nthe language generated by G is \\n \\n \\n \\n \\n \\n \\nThat is \\niff \\n. \\nwww.indiansbrain.com\\nIf \\n, we must have for some \\n, \\n, denoted as a \\nderivation sequence of w, The strings \\n \\nare denoted as sentential forms of the \\nderivation.  \\n \\n \\nExample : Consider the grammar \\n \\n, where N = {S},={a, b} and P is the set of the following \\nproduction rules \\n \\n \\n \\n \\n{ S \\nab, SaSb} \\n \\nSome terminal strings generated by this grammar together with their derivation is given below. \\n \\nS \\nab \\n \\nS \\naSb\\naabb \\n \\nS \\naSb\\naaSbb\\naaabbb \\n \\nIt is easy to prove that the language generated by this grammar is \\n \\n \\n \\n \\nBy using the first production, it generates the string ab ( for i =1 ). \\n \\nTo generate any other string, it needs to start with the production S\\naSb and then the non-terminal S in the RHS can be \\nreplaced either by ab (in which we get the string aabb) or the same production S\\naSb can be used one or more \\ntimes. Every time it adds an \\'a\\' to the left and a \\'b\\' to the right of S, thus giving the sentential \\n \\nform \\n. When the non-terminal is replaced by ab (which is then only possibility for generating \\na terminal string) we get a terminal string of the form \\n. \\n \\nThere is no general rule for finding a grammar for a given language. For many languages we can devise \\ngrammars and there are many languages for which we cannot find any grammar. \\n \\nExample: Find a grammar for the language \\n. \\n \\nIt is possible to find a grammar for L by modifying the previous grammar since we need to generate an extra b \\nat the end of the string \\n. We can do this by adding a production S\\nBb where the non-terminal B \\ngenerates \\nas given in the previous example. \\n \\nUsing the above concept we devise the follwoing grammar for L. \\n \\n \\nwhere, N = { S, B }, P = { S\\nBb, B\\nab, B\\naBb } \\n \\nParse Trees: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\nConstruction of a Parse tree: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nYield of a Parse tree: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAmbiguity in languages and grammars: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\nUNIT-IV \\n \\n \\n \\n \\n \\n \\nPush down automata: \\n \\nRegular language can be charaterized as the language accepted by finite automata. Similarly, we can \\ncharacterize the context-free language as the langauge accepted by a class of machines called \\n\"Pushdown Automata\" (PDA). A pushdown automation is an extension of the NFA. \\n \\nIt is observed that FA have limited capability. (in the sense that the class of languages accepted or characterized by \\nthem is small). This is due to the \"finite memory\" (number of states) and \"no external memory\" involved with them. A \\nPDA is simply an NFA augmented with an \"external stack memory\". The addition of a stack provides the PDA with a \\nlast-in, first-out memory management cpapability. This \"Stack\" or \"pushdown store\" can be used to record a \\npotentially unbounded information. It is due to this memory management capability with the help of the stack that a \\nPDA can overcome the memory limitations that prevents a FA to \\n \\naccept many interesting languages like \\n. Although, a PDA can store an unbounded amount of \\ninformation on the stack, its access to the information on the stack is limited. It can push an element onto the \\ntop of the stack and pop off an element from the top of the stack. To read down into the stack the top elements \\nmust be popped off and are lost. Due to this limited access to the information on the stack, a PDA still has \\nsome limitations and cannot accept some other interesting languages. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAs shown in figure, a PDA has three components: an input tape with read only head, a finite control and \\na pushdown store. \\n \\nThe input head is read-only and may only move from left to right, one symbol (or cell) at a time. In each step, the \\nPDA pops the top symbol off the stack; based on this symbol, the input symbol it is currently reading, and \\nwww.indiansbrain.com\\nits present state, it can push a sequence of symbols onto the stack, move its read-only head one cell \\n(or symbol) to the right, and enter a new state, as defined by the transition rules of the PDA. \\n \\nPDA are nondeterministic, by default. That is, \\n- transitions are also allowed in which the PDA can pop and \\npush, and change state without reading the next input symbol or moving its read-only head. Besides this, \\nthere may be multiple options for possible next moves. \\n \\nFormal Definitions : Formally, a PDA M is a 7-tuple M =\\n \\nwhere, \\n \\n\\uf095 \\nis a finite set of states,\\uf020\\n\\uf095 \\nis a finite set of input symbols (input alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a finite set of stack symbols (stack alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a transition function from \\nto subset of \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the start state\\uf020\\n\\uf095 \\n, is the initial stack symbol, and\\uf020\\n\\uf020\\n\\uf095 \\n, is the final or accept states.\\uf020\\n \\nExplanation of the transition function, \\n: \\n \\nIf, for any \\n, \\n. This means intitutively that whenever \\nthe PDA is in state q reading input symbol a and z on top of the stack, it can nondeterministically for any i, \\n \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack (where \\n) (The usual convention is that if \\n, \\nthen \\nwill be at the top and \\nat the bottom.)\\uf020\\n\\uf095 \\nmove read head right one cell past the current symbol a.\\uf020\\n \\nIf a = \\n, then \\nmeans intitutively that whenver the PDA is in state \\nq with z on the top of the stack regardless of the current input symbol, it can nondeterministically for any \\n \\ni, \\n, \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack, and\\uf020\\n\\uf095 \\nleave its read-only head where it is.\\uf020\\nwww.indiansbrain.com\\nState transition diagram : A PDA can also be depicted by a state transition diagram. The labels on the arcs \\nindicate both the input and the stack operation. The transition \\n \\nfor \\nand \\nis depicted by \\n \\n \\n \\n \\n \\n \\n \\nFinal states are indicated by double circles and the start state is indicated by an arrow to it from nowhere. \\n \\n \\nConfiguration or Instantaneous Description (ID) : \\n \\nA configuration or an instantaneous description (ID) of PDA at any moment during its computation is an \\nelement of \\ndescribing the current state, the portion of the input remaining to be read (i.e. \\nunder and to the right of the read head), and the current stack contents. Only these three elements \\ncan affect the computation from that point on and, hence, are parts of the ID. \\n \\nThe start or inital configuartion (or ID) on input \\nis \\n. That is, the PDA always starts in its \\nstart state, \\nwith its read head pointing to the leftmost input symbol and the stack containing only \\nthe start/initial stack symbol, \\n. \\n \\nThe \"next move relation\" one figure describes how the PDA can move from one configuration to \\nanother in one step. \\n \\nFormally, \\n \\n \\n \\n \\niff \\n\\'a\\' may be \\nor an input symbol. \\n \\nLet I, J, K be IDs of a PDA. We define we write I\\nK, if ID I can become K after exactly i moves. The \\nrelations \\nand \\ndefine as follows \\n \\nI \\nK \\n \\nI \\nJ if \\nsuch that I \\nK and K\\n J \\n \\nI \\nJ if \\nsuch that I \\nJ. \\nwww.indiansbrain.com\\nThat is, \\nis the reflexive, transitive closure of \\n. We say that I \\nJ if the ID J follows from the ID I in \\nzero or more moves. \\n \\n( Note : subscript M can be dropped when the particular PDA M is understood. ) \\n \\nLanguage accepted by a PDA M \\n \\nThere are two alternative definiton of acceptance as given below. \\n \\n1. Acceptance by final state : \\n \\nConsider the PDA \\n. Informally, the PDA M is said to accept its input \\nby \\nfinal state if it enters any final state in zero or more moves after reading its entire input, starting in the start \\nconfiguration on input \\n. \\n \\nFormally, we define L(M), the language accepted by final state to be \\n \\n{ \\n| \\nfor some \\nand \\n} \\n \\n \\n \\n \\n2. Acceptance by empty stack (or Null stack) : The PDA M accepts its input \\nby empty stack if starting in the \\n \\nstart configuration on input \\n, it ever empties the stack w/o pushing anything back on after reading the \\nentire input. Formally, we define N(M), the language accepted by empty stack, to be \\n \\n{ \\n| \\nfor some \\n} \\n \\nNote that the set of final states, F is irrelevant in this case and we usually let the F to be the empty set i.e. F = \\nQ . \\n \\nExample 1 : Here is a PDA that accepts the language \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n, and \\nconsists of the following transitions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe PDA can also be described by the adjacent transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever the PDA M sees an input a in the start state \\nwith the start symbol z on the top of the stack \\nit pushes a onto the stack and changes state to \\n. (to remember that it has seen the first \\'a\\'). On state \\nif it \\nsees anymore a, it simply pushes it onto the stack. Note that when M is on state \\n, the symbol on the \\n \\ntop of the stack can only be a. On state \\nif it sees the first b with a on the top of the stack, then it needs to \\nstart comparison of numbers of a\\'s and b\\'s, since all the a\\'s at the begining of the input have already been \\npushed onto the stack. It start this process by popping off the a from the top of the stack and enters in state q3 \\n \\n(to remember that the comparison process has begun). On state \\n, it expects only b\\'s in the input (if it sees \\nany more a in the input thus the input will not be in the proper form of anbn). Hence there is no more on input a \\nwhen it is in state \\n. On state \\nit pops off an a from the top of the stack for every b in the input. When it sees \\nthe last b on state q3 (i.e. when the input is exaushted), then the last a from the stack will be popped off and the \\nstart symbol z is exposed. This is the only possible case when the input (i.e. on \\n-input ) the PDA M \\n \\nwill move to state \\nwhich is an accept state. \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\nLet the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) \\n \\n( using transition 2 ) \\n \\n( using transition 3 ) \\nwww.indiansbrain.com\\n( using transition 4 ), \\n( using transition 5 ) , \\nis final state. Hence , accept. So \\nthe string aabb is rightly accepted by M \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nExample 2 : We give an example of a PDA M that accepts the set of balanced strings of parentheses [] by \\nempty stack. \\nThe PDA M is given below. \\n \\n \\nwhere \\nis defined as \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever it sees a [, it will push the ] onto the stack. (first two transitions), and whenever it sees a ] \\nand the top of the stack symbol is [, it will pop the symbol [ off the stack. (The third transition). The fourth \\ntransition is used when the input is exhausted in order to pop z off the stack ( to empty the stack) and accept. \\nNote that there is only one state and no final state. The following is a sequence of configurations leading to the \\nacceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn\\'t matter which one we use, since \\nwww.indiansbrain.com\\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M\\' from M and the proof of equivalence of M & M\\' \\nare given below. \\n \\nThere are two cases to be considered. \\n \\nCASE I : PDA M accepts by final state, Let \\nLet qf be a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and M\\' are equivalent i.e. \\n \\nL(M) = L(\\n) \\n \\nLet \\nL(M) . Then \\nfor some \\nand \\n \\n \\nThen \\n \\n \\nThus \\naccepts \\n \\n \\nConversely, let \\naccepts \\ni.e. \\nL(\\n), then \\nfor \\ninherits all other moves except the last one from M. Hence \\nfor some \\n \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE II : PDA M accepts by empty stack. \\n \\nWe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand X\\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\n \\nand \\nwww.indiansbrain.com\\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M\\'s stack. From this point onward \\nwill simulate every move of M \\nsince all the transitions of M are also in \\n \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point, \\nwill enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. We will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. Then \\n \\nfor some \\n. But then \\n \\n \\n( by transition rule 1) \\n \\n( Since \\nincludes all the moves of M ) \\n \\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n. Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some \\n \\n \\nEvery move in the sequence, \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n \\n \\nEquivalence of PDA’s and CFG’s: \\nWe will now show that pushdown automata and context-free grammars are equivalent in expressive power, \\nthat is, the language accepted by PDAs are exactly the context-free languages. To show this, we have to \\nprove each of the following: \\n \\ni) \\nGiven any arbitrary CFG G there exists some PDA M that accepts exactly the same \\nlanguage generated by G. \\n \\nii) \\nGiven any arbitrary PDA M there exists a CFG G that generates exactly the same \\nlanguage accpeted by M. \\n \\n(i) CFA to PDA \\n \\nWe will first prove that the first part i.e. we want to show to convert a given CFG to an equivalent PDA. \\nwww.indiansbrain.com\\nLet the given CFG is \\n. Without loss of generality we can assume that G is in \\nGreibach Normal Form i.e. all productions of G are of the form . \\n \\n where \\nand \\n. \\n \\nFrom the given CFG G we now construct an equivalent PDA M that accepts by empty stack. Note that there \\nis only one state in M. Let \\n \\n, where \\n \\n\\uf095 \\nq is the only state\\uf020\\n\\uf020\\n\\uf095 \\nis the input alphabet,\\uf020\\n\\uf095 \\nN is the stack alphabet ,\\uf020\\n\\uf020\\n\\uf095 \\nq is the start state.\\uf020\\n\\uf095 \\nS is the start/initial stack symbol, and \\n, the transition relation is defined as follows\\uf020\\n \\nFor each production \\n, \\n. We now want to show \\nthat M and G are equivalent i.e. L(G)=N(M). i.e. for any \\n. \\niff \\n. \\n \\nIf \\n, then by definition of L(G), there must be a leftmost derivation starting with S and deriving w. \\n \\ni.e. \\n \\n \\nAgain if \\n, then one sysmbol. Therefore we need to show that for any \\n. \\n \\niff \\n. \\n \\nBut we will prove a more general result as given in the following lemma. Replacing A by S (the start \\nsymbol) and \\nby \\ngives the required proof. \\n \\nLemma For any \\n, \\nand \\n, \\nvia a leftmost derivative iff \\n. \\n \\nProof : The proof is by induction on n. \\n \\nBasis : n = 0 \\nwww.indiansbrain.com\\n \\niff \\ni.e. \\nand \\n \\n \\n \\niff \\n \\niff \\n \\n \\nInduction Step : \\n \\nFirst, assume that \\nvia a leftmost derivation. Let the last production applied in their derivation is \\nfor some \\nand \\n. \\n \\nThen, for some \\n, \\n \\n \\n \\n \\n \\n \\nwhere \\nand \\n \\n \\nNow by the indirection hypothesis, we get, \\n \\n.............................................................................(1) \\nAgain by the construction of M, we get \\n \\n \\nso, from (1), we get \\n \\n \\n \\n \\nsince \\nand \\n, we get \\n \\n \\nThat is, if \\n, then \\n. Conversely, assume that \\nand let \\nwww.indiansbrain.com\\nbe the transition used in the last move. Then for some \\n, \\nand \\n \\n \\n \\nwhere \\nand \\n. \\n \\nNow, by the induction hypothesis, we get \\n \\nvia a leftmost derivation. \\n \\nAgain, by the construction of M, \\nmust be a production of G. [ Since \\n]. Applying the production to the sentential form \\nwe get \\n \\n \\n \\n \\n \\ni.e. \\n \\n \\nvia a leftmost derivation. \\n \\nHence the proof. \\n \\nExample : Consider the CFG G in GNF \\n \\nS\\naAB \\n \\nA\\na / aA \\nB\\na / bB \\n \\nThe one state PDA M equivalent to G is shown below. For convenience, a production of G and \\nthe corresponding transition in M are marked by the same encircled number. \\n \\n(1) S\\naAB \\n \\n(2) A \\na \\n \\n(3) A\\naA \\n \\n(4) B \\na \\n \\n(5) B \\nbB \\n \\n \\n. We have used the same construction discussed earlier \\n \\nSome Useful Explanations : \\nConsider the moves of M on input aaaba leading to acceptance of the string. \\n \\nSteps \\nwww.indiansbrain.com\\n \\n1. (q, aaaba, s) \\n( q, aaba, AB ) \\n2. \\n( q, aba, AB ) \\n3. \\n( q, ba, B ) \\n4. \\n( q, a, B ) \\n5. \\n( q,   ,   )    Accept by empty stack. \\n \\nNote : encircled numbers here shows the transitions rule applied at every step. \\n \\nNow consider the derivation of the same string under grammar G. Once again, the production used at \\nevery step is shown with encircled number. \\n \\nS \\naAB \\naaAB \\naaaB \\naaabB \\naaaba \\n \\nSteps \\n1 \\n2 \\n3 \\n4 \\n5\\n \\nObservations: \\n\\uf095 \\nThere is an one-to-one correspondence of the sequence of moves of the PDA M and the derivation\\uf020\\n\\uf020\\nsequence under the CFG G for the same input string in the sense that - number of steps in both \\nthe cases are same and transition rule corresponding to the same production is used at every step \\n(as shown by encircled number). \\n\\uf020\\n\\uf095 \\nconsidering the moves of the PDA and derivation under G together, it is also observed that at \\nevery step the input read so far and the stack content together is exactly identical to the \\ncorresponding sentential form i.e.\\uf020\\n\\uf020\\n<what is Read><stack> = <sentential form> \\n \\nSay, at step 2, Read so far = \\na stack = AB \\nSentential form = aAB From this property we claim that \\niff \\n. If the claim is \\n \\ntrue, then apply with \\nand we get \\niff \\n or \\niff \\n( \\nby definition ) \\n \\nThus N(M) = L(G) as desired. Note that we have already proved a more general version of the \\nclaim PDA and CFG: \\nWe now want to show that for every PDA M that accpets by empty stack, there is a CFG G such that L(G) = \\nN(M) \\n \\nwe first see whether the \"reverse of the construction\" that was used in part (i) can be used here to construct \\nan equivalent CFG from any PDA M. \\n \\nIt can be show that this reverse construction works only for single state PDAs. \\nwww.indiansbrain.com\\n\\uf095 \\nThat is, for every one-state PDA M there is CFG G such that L(G) = N(M). For every move of the \\nPDA M \\nwe introduce a production \\nin the \\ngrammar \\nwhere N = T and \\n.\\uf020\\n \\nwe can now apply the proof in part (i) in the reverse direction to show that L(G) = N(M). \\n \\nBut the reverse construction does not work for PDAs with more than one state. For example, consider the PDA \\nM produced here to accept the langauge \\n \\n \\n \\n \\n \\nNow let us construct CFG \\nusing the \"reverse\" construction. \\n \\n( Note \\n). \\n \\nTransitions in M \\nCorresponding Production in G \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe can drive strings like aabaa which is in the language. \\n \\n \\n \\n \\nBut under this grammar we can also derive some strings which are not in the language. e.g \\n \\n \\n \\n \\nand \\n. But \\n \\n \\nTherefore, to complete the proof of part (ii) we need to prove the following claim also. \\n \\nClaim: For every PDA M there is some one-state PDA \\nsuch that \\n. \\n \\nIt is quite possible to prove the above claim. But here we will adopt a different approach. We start with \\nany arbitrary PDA M that accepts by empty stack and directly construct an equivalent CFG G. \\nwww.indiansbrain.com\\nPDA to CFG \\n \\nWe want to construct a CFG G to simulate any arbitrary PDA M with one or more states. Without loss \\nof generality we can assume that the PDA M accepts by empty stack. \\n \\nThe idea is to use nonterminal of the form <PAq> whenever PDA M in state P with A on top of the stack goes \\n \\nto state \\n. That is, for example, for a given transition of the PDA corresponding production in the grammar as \\nshown below, \\nAnd, we would like to show, in general, that \\niff the PDA M, when started from state P with A on \\n \\nthe top of the stack will finish processing \\n, arrive at state q and remove A from the stack. \\n \\nwe are now ready to give the construction of an equivalent CFG G from a given PDA M. we need to introduce \\ntwo kinds of producitons in the grammar as given below. The reason for introduction of the first kind of \\nproduction will be justified at a later point. Introduction of the second type of production has been justified in \\nthe above discussion. \\n \\nLet \\nbe a PDA. We construct from M a equivalent CFG \\n \\n \\nWhere \\n \\n\\uf095 \\nN is the set of nonterminals of the form <PAq> for \\nand \\nand P contains the follwoing\\uf020\\n \\ntwo kind of production \\n \\n1.  \\n \\n \\n2. If \\n, then for every choice of the sequence \\n,\\n \\n, \\n. \\n \\n \\nInclude the follwoing production \\n \\n \\n \\n \\nIf n = 0, then the production is \\n.For the whole exercise to be meaningful we want \\nmeans there is a sequence of transitions ( for PDA M ), starting in state q, ending in \\n, \\n \\nduring which the PDA M consumes the input string \\nand removes A from the stack (and, of course, all \\nother symbols pushed onto stack in A\\'s place, and so on.) \\n \\nThat is we want to claim that \\n \\niff \\n \\n \\nIf this claim is true, then let \\nto get \\niff \\nfor some \\n \\n. But for all \\nwe have \\nas production in G. Therefore, \\nwww.indiansbrain.com\\niff \\ni.e. \\niff PDA M accepts w by empty stack or L(G) = N(M) \\n \\nNow, to show that the above construction of CFG G from any PDA M works, we need to prove the \\nproposed claim. \\n \\nNote: At this point, the justification for introduction of the first type of production (of the form \\n) in \\nthe CFG G, is quite clear. This helps use deriving a string from the start symbol of the grammar. \\n \\nProof : Of the claim \\niff \\nfor some \\n, \\nand \\n \\n \\nThe proof is by induction on the number of steps in a derivation of G (which of course is equal to the number \\nof moves taken by M). Let the number of steps taken is n. \\n \\nThe proof consists of two parts: \\' if \\' part and \\' only if \\' part. First, consider the \\' if \\' part \\n \\nIf \\nthen \\n. \\n \\nBasis is n =1 \\n \\nThen \\n. In this case, it is clear that \\n. Hence, by construction \\nis a production of G. \\n \\nThen \\n \\nInductive Hypothesis : \\n \\n \\n \\n \\n \\nInductive Step : \\n \\n \\nFor n >1, let w = ax for some \\nand \\nconsider the first move of the PDA M which uses \\nthe general transition \\n= \\n \\n. Now M must remove \\nfrom stack \\nwhile consuming x in the remaining n-1 moves. \\n \\nLet \\n, where \\nis the prefix of x that M has consumed when \\nfirst appears at top \\nof the stack. Then there must exist a sequence of states in M (as per construction) \\n(with \\n \\n), such that \\nwww.indiansbrain.com\\n \\n [ This step implies \\n] \\n [ This step implies \\n] \\n \\n... \\n \\n \\n=\\n \\n \\n[ Note: Each step takes less than or equal to n -1 moves because the total number of moves required \\nassumed to be n-1.] \\n \\nThat is, in general \\n \\n \\n, \\n. \\n \\nSo, applying inductive hypothesis we get \\n \\n, \\n. But corresponding to the original move \\n \\nin M we have added the following production in G. \\n \\nWe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) , \\n( using transition 2 ) \\n \\n( using transition 3 ), \\n( using transition 4 ) \\n \\n( using transition 5 ) , \\nis final state. Hence, accept. \\n \\nSo the string aabb is rightly accepted by M. \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\nwww.indiansbrain.com\\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nThe following is a sequence of configurations leading to the acceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn\\'t matter which one we use, since \\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\n \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M\\' from M and the proof of equivalence of M & M\\' \\nare given below \\n \\nThere are two cases to be considered. \\n \\nCASE 1 : PDA M accepts by final state, Let \\n. Let \\nbe a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and \\nare equivalent i.e. \\n \\n \\n. \\n \\nLet \\n. Then \\nfor some \\nand \\n \\n \\nThen \\n. \\n \\nThus \\naccepts \\n. \\nwww.indiansbrain.com\\nConversely, let \\naccepts \\ni.e. \\n, then \\nfor some \\n. \\ninherits all other moves except the last one from M. Hence \\nfor some \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE 2 : PDA M accepts by empty stack. \\n \\nwe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand \\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\nand \\n \\n \\n \\n \\n \\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M\\'s stack. From this point onward M\\' will simulate every move of M \\n \\nsince all the transitions of M are also in \\n. \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point\\n, will enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. we will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. \\n \\nThen \\n \\nfor some \\n. But then, \\n \\n \\n( by transition rule 1 ) \\n \\n( since \\ninclude all the moves of M ) \\nwww.indiansbrain.com\\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n.Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some Q . \\n \\nEvery move in the sequence \\n \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n. \\n \\nDeterministic PDA: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRegular Languages and DPDA’s The DPDA’s accepts a class of languages that is in between the regular \\nlanguages and CFL’s. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDeterministic Pushdown Automata (DPDA) and Deterministic Context-free Languages (DCFLs) \\n \\nPushdown automata that we have already defined and discussed are nondeterministic by default, that is , there may be two or \\nmore moves involving the same combinations of state, input symbol, and top of the stock, and again, for some state and \\ntop of the stock the machine may either read and input symbol or make an \\n- transition (without consuming any input). \\n \\nIn deterministic PDA , there is never a choice of move in any situation. This is handled by preventing the above mentioned two \\ncases as described in the definition below. \\n \\nDefnition : Let \\nbe a PDA . Then M is deterministic if and only if both the following conditions are \\nsatisfied. \\n \\n1. \\nhas at most one element for any \\nand \\n(this condition prevents multiple choice f \\nany combination of \\n) \\n2. \\nIf \\nand \\nfor every \\n \\n \\n(This condition prevents the possibility of a choice between a move with or without an input symbol). \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEmpty Production Removal \\n \\nThe productions of context-free grammars can be coerced into a variety of forms without \\naffecting the expressive power of the grammars. If the empty string does not belong to a language, \\nthen there is a way to eliminate the productions of the form A→ λ from the grammar. \\n \\nIf the empty string belongs to a language, then we can eliminate λ from all productions \\n \\nsave for the single production S → λ. In this case we can also eliminate any occurrences of S \\nfrom the right-hand side of productions. \\n \\nProcedure to find CFG with out empty Productions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUnit production removal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLeft Recursion Removal \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNORMAL FORMS \\n \\nTwo kinds of normal forms viz., Chomsky Normal Form and Greibach Normal Form (GNF) \\nare considered here. \\n \\nChomsky Normal Form (CNF) \\n \\nAny context-free language L without any λ-production is generated by a grammar is \\nwhich productions are of the form A → BC or A→ a, where A, B ∈VN , and a ∈ V Τ. \\n \\nProcedure to find Equivalent Grammar in CNF \\n \\n(i) Eliminate the unit productions, and λ-productions if any, \\n \\n(ii) Eliminate the terminals on the right hand side of length two or more. \\n \\n(iii) Restrict the number of variables on the right hand side of productions to two. \\nProof: \\n \\nFor Step (i): Apply the following theorem: “Every context free language can be generated by \\na grammar with no useless symbols and no unit productions”. \\n \\nAt the end of this step the RHS of any production has a single terminal or two or more symbols. \\n \\nLet us assume the equivalent resulting grammar as G = (VN ,VT ,P ,S ). \\nFor Step (ii): Consider any production of the form \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExample \\n \\nObtain a grammar in Chomsky Normal Form (CNF) equivalent to the grammar G \\nwith productions P given \\n \\n \\n \\n \\n \\n \\nSolution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\nPumping Lemma for CFG \\nA “Pumping Lemma” is a theorem used to show that, if certain strings belong to a \\n \\nlanguage, then certain other strings must also belong to the language. Let us discuss a Pumping \\nLemma for CFL. We will show that , if L is a context-free language, then strings of L that are at \\nleast ‘m’ symbols long can be “pumped” to produce additional strings in L. The value of ‘m’ \\ndepends on the particular language. Let L be an infinite context-free language. Then there is some \\npositive integer ‘m’ such that, if S is a string of L of Length at least ‘m’, then \\n \\n(i) S = uvwxy (for some u, v, w, x, y) \\n \\n(ii) | vwx| \\uf064 m \\n(iii) | vx| \\uf0651 \\n(iv) uv iwx i y∈L. \\n \\nfor all non-negative values of i. \\nIt should be understood that \\n \\n(i) If S is sufficiently long string, then there are two substrings, v and x, somewhere in \\nS. There is stuff (u) before v, stuff (w) between v and x, and stuff (y), after x. \\n \\n(ii) The stuff between v and x won’t be too long, because | vwx | can’t be larger than m. \\n(iii) Substrings v and x won’t both be empty, though either one could be. \\n \\n(iv) If we duplicate substring v, some number (i) of times, and duplicate x the same \\nnumber of times, the resultant string will also be in L. \\n \\nDefinitions \\nA variable is useful if it occurs in the derivation of some string. This requires that \\n \\n(a) the variable occurs in some sentential form (you can get to the variable if you start from S), and \\n \\n(b) a string of terminals can be derived from the sentential form (the variable is not a “dead end”). \\nA variable is “recursive” if it can generate a string containing itself. For example, variable A is \\nrecursive if \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof of Pumping Lemma \\n \\n(a) Suppose we have a CFL given by L. Then there is some context-free Grammar G that \\ngenerates L. Suppose \\n(i) L is infinite, hence there is no proper upper bound on the length of strings belonging to L. \\n \\n(ii) L does not contain l. \\n(iii) G has no productions or l-productions. \\nwww.indiansbrain.com\\nThere are only a finite number of variables in a grammar and the productions for each \\n \\nvariable have finite lengths. The only way that a grammar can generate arbitrarily long strings is if \\none or more variables is both useful and recursive. Suppose no variable is recursive. Since the start \\nsymbol is non recursive, it must be defined only in terms of terminals and other variables. Then \\nsince those variables are non recursive, they have to be defined in terms of terminals and still other \\nvariables and so on. \\n \\nAfter a while we run out of “other variables” while the generated string is still finite. Therefore \\nthere is an upper bond on the length of the string which can be generated from the start \\nsymbol. This contradicts our statement that the language is finite. \\nHence, our assumption that no variable is recursive must be incorrect. \\n \\n(b) Let us consider a string X belonging to L. If X is sufficiently long, then the derivation of X \\nmust have involved recursive use of some variable A. Since A was used in the derivation, the \\nderivation should have started as \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUsage of Pumping Lemma \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHence our original assumption, that L is context free should be false. Hence the language L is not \\ncon text-free. \\n \\nExample \\n \\nCheck whether the language given by L \\uf03d {a mbmcn : m \\uf064 n \\uf064 2m} is a CFL or not. \\nSolution \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nClosure properties of CFL – Substitution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nApplications of substitution theorem \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReversal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInverse Homomorphism: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUNIT-V \\nTuring machine: \\n \\nInformal Definition: \\n \\nWe consider here a basic model of TM which is deterministic and have one-tape. There are many variations, \\nall are equally powerfull. \\n \\nThe basic model of TM has a finite set of states, a semi-infinite tape that has a leftmost cell but is infinite to \\nthe right and a tape head that can move left and right over the tape, reading and writing symbols. \\n \\nFor any input w with |w|=n, initially it is written on the n leftmost (continguous) tape cells. The infinitely many \\ncells to the right of the input all contain a blank symbol, B whcih is a special tape symbol that is not an input \\nsymbol. The machine starts in its start state with its head scanning the leftmost symbol of the input w. De-\\npending upon the symbol scanned by the tape head and the current state the machine makes a move which \\nconsists of the following: \\n \\n\\uf095 \\nwrites a new symbol on that tape cell,  \\uf095\\uf020\\n\\uf020\\nmoves its head one cell either to the left or to the right and \\n\\uf095 \\n(possibly) enters a new state.\\uf020\\n \\nThe action it takes in each step is determined by a transition functions. The machine continues computing (i.e. \\nmaking moves) until \\n \\n\\uf095 \\nit decides to \"accept\" its input by entering a special state called accept or final state or\\uf020\\n\\uf095 \\nhalts without accepting i.e. rejecting the input when there is no move defined.\\uf020\\n \\nOn some inputs the TM many keep on computing forever without ever accepting or rejecting the input, in \\nwhich case it is said to \"loop\" on that input \\n \\nFormal Definition : \\n \\nFormally, a deterministic turing machine (DTM) is a 7-tuple \\n, where \\n \\n\\uf095 \\nQ is a finite nonempty set of states.\\uf020\\n\\uf095 \\nis a finite non-empty set of tape symbols, callled the tape alphabet of M.\\uf020\\n\\uf020\\n\\uf095 \\nis a finite non-empty set of input symbols, called the input alphabet of M.\\uf020\\n\\uf095 \\nis the transition function of M,\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nis the initial or start state.\\uf020\\n\\uf095 \\nis the blank symbol\\uf020\\n\\uf020\\n\\uf095 \\nis the set of final state.\\uf020\\n \\nSo, given the current state and tape symbol being read, the transition function describes the next state, symbol \\nto be written on the tape, and the direction in which to move the tape head ( L and R denote left and right, \\nrespectively ). \\n \\nTransition function :\\n \\n \\n\\uf095 \\nThe heart of the TM is the transition function, \\nbecause it tells us how the machine gets one step \\nto the next.\\uf020\\n\\uf020\\n\\uf095 \\nwhen the machine is in a certain state q\\nQ and the head is currently scanning the tape symbol \\n, and if \\n, then the machine\\uf020\\n \\n1. replaces the symbol X by Y on the tape \\n \\n2. goes to state p, and \\n3. the tape head moves one cell ( i.e. one tape symbol ) to the left ( or right ) if D is L ( or R ). \\n \\nThe ID (instantaneous description) of a TM capture what is going out at any moment i.e. it contains all the \\ninformation to exactly capture the \"current state of the computations\". \\n \\nIt contains the following: \\n \\n\\uf095 \\nThe current state, q\\uf020\\n\\uf095 \\nThe position of the tape head,\\uf020\\n\\uf020\\n\\uf095 \\nThe constants of the tape up to the rightmost nonblank symbol or the symbol to the left of the head, \\nwhichever is rightmost.\\uf020\\n \\nNote that, although there is no limit on how far right the head may move and write nonblank symbols on the \\ntape, at any finite \\n \\ntime, the TM has visited only a finite prefix of the infinite tape. \\n \\nAn ID (or configuration) of a TM M is denoted by \\nwhere \\nand \\n \\n\\uf095 \\nis the tape contents to the left of the head\\uf020\\n\\uf095 \\nq is the current state.\\uf020\\n\\uf020\\n\\uf095 \\nis the tape contents at or to the right of the tape head\\uf020\\n \\nThat is, the tape head is currently scanning the leftmost tape symbol of \\n. ( Note that if \\n, then the \\ntape head is scanning a blank symbol) \\n \\nIf \\nis the start state and w is the input to a TM M then the starting or initial configuration of M is onviously \\ndenoted by \\n \\nwww.indiansbrain.com\\nMoves of Turing Machines \\n \\nTo indicate one move we use the symbol \\n. Similarly, zero, one, or more moves will be represented by \\n. A \\nmove of a TM \\n \\nM is defined as follows. \\n \\nLet \\nbe an ID of M where \\n, \\nand \\n. \\n \\nLet there exists a transition \\nof M. \\n \\nThen we write \\nmeaning that ID \\nyields \\n \\n \\n\\uf095 \\nAlternatively \\n, \\nif \\nis \\na \\ntransition \\nof \\nM, \\nthen \\nwe \\nwrite \\nwhich means that the ID \\nyields \\n\\uf020\\n\\uf020\\n\\uf095 \\nIn other words, when two IDs are related by the relation \\n, we say that the first one yields the \\nsecond ( or the second is the result of the first) by one move.\\uf020\\n\\uf095 \\nIf IDj results from IDi by zero, one or more (finite) moves then we write \\n( If the TM M is understand, \\nthen the subscript M can be dropped from \\nor \\n)\\uf020\\n \\nSpecial Boundary Cases \\n \\n\\uf095 \\nLet \\nbe an ID and \\nbe an transition of M. Then \\n. That is, the head is not \\nallowed to fall off the left end of the tape.\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure (Note that \\nis equivalent to \\n)\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n \\nThe language accepted by a TM \\n, denoted as L(M) is \\n \\nL(M) = { w | \\nand figure for some p\\nF and \\n} \\n \\nIn other words the TM M accepts a string \\nthat cause M to enter a final or accepting state when started \\nin its initial ID (i.e. \\n). That is a TM M accepts the string \\nif a sequence of IDs, \\n \\nexists such that \\n \\n\\uf095 \\nis the initial or starting ID of M\\uf020\\n\\uf095 \\n; \\n\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nThe representation of IDk contains an accepting state.\\uf020\\n \\nThe set of strings that M accepts is the language of M, denoted L(M), as defined \\nabove More about configuration and acceptance \\n \\n\\uf095 \\nAn ID \\nof M is called an accepting (or final) ID if \\n\\uf020\\n\\uf020\\n\\uf095 \\nAn ID \\nis called a blocking (or halting) ID if \\nis undefined i.e. the TM has no move at this \\npoint.\\uf020\\n\\uf095 \\nis called reactable from \\nif \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the initial (or starting) ID if \\nis the input to the TM and \\nis the initial (or start) state \\nof M.\\uf020\\n \\nOn any input string \\n \\n \\neither \\n \\n\\uf095 \\nM halts on w if there exists a blocking (configuration) ID, \\nsuch that \\n\\uf020\\n \\nThere are two cases to be considered \\n \\n\\uf095 \\nM accepts w if I is an accepting ID. The set of all \\naccepted by M is denoted as L(M) as\\uf020\\nalready defined \\n \\n\\uf095 \\nM rejects w if \\nis a blocking configuration. Denote by reject (M), the set of all \\nrejected by M.\\uf020\\n \\nor \\n \\n\\uf095 \\nM loops on w if it does not halt on w.\\uf020\\n \\nLet loop(M) be the set of all \\non which M loops for. \\n \\nIt is quite clear that \\n \\n \\n \\n \\nThat is, we assume that a TM M halts \\n \\n\\uf095 \\nWhen it enters an accepting \\nor\\uf020\\n\\uf095 \\nWhen it enters a blocking \\ni.e. when there is no next move.\\uf020\\n \\nHowever, on some input string, , \\n, it is possible that the TM M loops for ever i.e. it never halts \\nwww.indiansbrain.com\\n \\nThe Halting Problem \\n \\nThe input to a Turing machine is a string. Turing machines themselves can be written as strings. \\nSince these strings can be used as input to other Turing machines. A “Universal Turing \\nmachine” is one whose input consists of a description M of some arbitrary Turing machine, and \\nsome input w to which machine M is to be applied, we write this combined input as M + w. \\nThis produces the same output that would be produced by M. This is written as \\n \\nUniversal Turing Machine (M + w) = M (w). \\n \\nAs a Turing machine can be represented as a string, it is fully possible to supply a Turing \\n \\nmachine as input to itself, for example M (M). This is not even a particularly bizarre thing to do for \\nexample, suppose you have written a C pretty printer in C, then used the Pretty printer on itself. \\nAnother common usage is Bootstrapping—where some convenient languages used to write a \\nminimal compiler for some new language L, then used this minimal compiler for L to write a new, \\nimproved compiler for language L. Each time a new feature is added to language L, you can \\nrecompile and use this new feature in the next version of the compiler. Turing machines sometimes \\nhalt, and sometimes they enter an infinite loop. \\n \\nA Turing machine might halt for one input string, but go into an infinite loop when given \\nsome other string. The halting problem asks: “It is possible to tell, in general, whether a given \\n \\nmachine will halt for some given input?” If it is possible, then there is an effective procedure to look \\nat a Turing machine and its input and determine whether the machine will halt with that input. If \\nthere is an effective procedure, then we can build a Turing machine to implement it. Suppose we \\nhave a Turing machine “WillHalt” which, given an input string M + w, will halt and accept the string \\nif Turing machine M halts on input w and will halt and reject the string if Turing machine M does \\nnot halt on input w. When viewed as a Boolean function, “WillHalt (M, w)” halts and returns \\n“TRUE” in the first case, and (halts and) returns “FALSE” in the second. \\n \\nTheorem \\nTuring Machine “WillHalt (M, w)” does not exist. \\n \\nProof: This theorem is proved by contradiction. Suppose we could build a machine “WillHalt”. \\nThen we can certainly build a second machine, “LoopIfHalts”, that will go into an infinite loop \\nif and only if “WillHalt” accepts its input: \\n Function LoopIfHalts (M, \\nw): if WillHalt (M, w) then \\nwhile true do { } \\nelse \\nreturn false; \\n \\nWe will also define a machine “LoopIfHaltOnItSelf” that, for any given input M, representing a \\nTuring machine, will determine what will happen if M is applied to itself, and loops if M will halt \\nin this case. \\n Function LoopIfHaltsOnItself (M): \\nreturn LoopIfHalts (M, M): \\n \\nFinally, we ask what happens if we try: \\nFunc tion Impos sible: \\nreturn LoopIfHaltsOnItself (LoopIfHaltsOnItself): \\n \\nThis machine, when applied to itself, goes into an infinite loop if and only if it halts \\nwhen applied to itself. This is impossible. Hence the theorem is proved. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nImplications of Halting Problem \\n \\nProgramming \\nThe Theorem of “Halting Problem” does not say that we can never determine whether or not \\n \\na given program halts on a given input. Most of the times, for practical reasons, we could \\neliminate infinite loops from programs. Sometimes a “meta-program” is used to check another \\nprogram for potential infinite loops, and get this meta-program to work most of the time. \\n \\nThe theorem says that we cannot ever write such a meta-program and have it work all of the \\ntime. This result is also used to demonstrate that certain other programs are also impossible. \\nThe basic outline is as follows: \\n \\n(i) If we could solve a problem X, we could solve the Halting problem \\n(ii) We cannot solve the Halting Problem \\n(iii) Therefore, we cannot solve problem X \\n \\n \\n \\n \\n \\n \\nA Turing machine can be \"programmed,\" in much the same manner as a computer is \\n \\nprogrammed. When one specifies the function which we usually call δ for a Tm, he is really \\nwriting a program for the Tm. \\n \\n1. Storage in finite Control \\n \\nThe finite control can be used to hold a finite amount of information. To do so, the state is written \\nas a pair of elements, one exercising control and the other storing a symbol. It should be \\n \\nemphasized that this arrangement is for conceptual purposes only. No modification in the definition \\nof the Turing machine has been made. \\n \\nExample \\nConsider the Turing machine \\nSolution \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2. Multiple Tracks \\n \\nWe can imagine that the tape of the Turing machine is divided into k tracks, for any finite k. This \\narrangement is shown in Fig., with k = 3. What is actually done is that the symbols on the tape \\nare considered as k-tuples. One component for each track. \\n \\nExample \\nThe tape in Fig. can be imagined to be that of a Turing machine which takes a binary input \\n \\ngreater than 2, written on the first track, and determines if it is a prime. The input is surrounded by \\n¢ and $ on the first track. \\n \\nThus, the allowable input symbols are [¢, B, B], [0, B, B ], [1, B, B ], and [$, B, B]. These \\n \\nsymbols can be identified with ¢, 0, 1, and $, respectively, when viewed as input symbols. The blank \\nwww.indiansbrain.com\\nsymbol can be represented by [B, B, B ] \\n \\nTo test if its input is a prime, the Tm first writes the number two in binary on the second track \\n \\nand copies the first track onto the third track. Then, the second track is subtracted, as many times \\nas possible, from the third track, effectively dividing the third track by the second and leaving the \\nremainder. If the remainder is zero, the number on the first track is not a prime. If the remainder is \\nnonzero, increase the number on the second track by one. \\n \\nIf now the second track equals the first, the number on the first track is a prime, because it cannot \\nbe divided by any number between one and itself. If the second is less than the first, the whole \\noperation is repeated for the new number on the second track. In Fig., the Tm is testing to determine \\nif 47 is a prime. The Tm is dividing by 5; already 5 has been subtracted twice, so 37 appears on the \\nthird track. \\n \\n3. Subroutines \\nwww.indiansbrain.com\\n \\nUNDECIDABILITY \\n \\n \\nDesign a Turing machine to add two given integers. \\n \\nSolution: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSome unsolvable Problems are as follows: \\n(i) Does a given Turing machine M halts on all input? \\n(ii) Does Turing machine M halt for any input? \\n(iii) Is the language L(M) finite? \\n \\n(iv) Does L(M) contain a string of length k, for some given k? \\n \\n(v) Do two Turing machines M1 and M2 accept the same language? \\n \\nIt is very obvious that if there is no algorithm that decides, for an arbitrary given Turing machine \\nM and input string w, whether or not M accepts w. These problems for which no algorithms exist \\nare called “UNDECIDABLE” or “UNSOLVABLE”. \\n \\nCode for Turing Machine: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\nDiagonalization language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis table represents language acceptable by Turing machine \\nwww.indiansbrain.com\\nProof that Ld is not recursively enumerable: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRecursive Languages: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUniversal \\n \\nLanguage: \\nwww.indiansbrain.com\\n \\nUndecidability of Universal Language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProblem -Reduction : \\nIf P1 reduced to P2, \\n \\nThen P2 is at least as hard as P1. \\nTheorem: If P1 reduces to P2 then, \\n\\uf095 If P1 is undecidable the so is P2.\\uf020\\n\\uf095 If P1 is Non-RE then so is P2.\\uf020\\nwww.indiansbrain.com\\nPost\\'s Correspondence Problem (PCP) \\n \\nA post correspondence system consists of a finite set of ordered pairs \\nwhere \\nfor some alphabet \\n. \\n \\nAny sequence of numbers \\n \\n \\nis called a solution to a Post Correspondence System. \\n \\nThe Post\\'s Correspondence Problem is the problem of determining whether \\na Post Correspondence system has a solutions. \\n \\nExample 1 : Consider the post correspondence system \\n \\n The list 1,2,1,3 is a solution to it. \\n \\nBecause \\n \\n \\n \\n \\n \\n \\n \\ni \\n             xi                                 yi \\n \\n1 \\n \\n2 \\n \\n3 \\n \\n \\n(A post correspondence system is also denoted as an instance of the \\nPCP) Example 2 : The following PCP instance has no solution \\ni \\n          xi                          yi \\n \\n1 \\n \\n2 \\n \\nThis can be proved as follows. \\ncannot be chosen at the start, since than the LHS and RHS would \\ndiffer in the first symbol ( \\nin LHS and \\nin RHS). So, we must start with \\n. The next pair must be \\nso that the 3 rd symbol in the RHS becomes identical to that of the LHS, which is a . After this step, \\nLHS and RHS are not matching. If \\nis selected next, then would be mismatched in the 7 th symbol \\nwww.indiansbrain.com\\n( \\nin LHS and \\nin RHS). If \\nis selected, instead, there will not be any choice to match the both side \\nin the next step. \\n \\nExample3 : The list 1,3,2,3 is a solution to the following PCP instance. \\n \\n \\ni \\n   \\nx\\ni \\n   \\ny\\ni \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n1 \\n \\n1 \\n \\n101 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n2 \\n \\n10 \\n \\n00 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n3 \\n011 \\n11 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\nThe following properties can easily be proved. \\n \\nProposition The Post Correspondence System \\n \\n has solutions if and only if \\n \\n \\n \\n \\n \\n \\nCorollary : PCP over one-letter alphabet is decidable. \\n \\nProposition Any PCP instance over an alphabet \\nwith \\nis equivalent to a PCP instance over \\nan alphabet \\nwith \\n \\n \\nProof : Let \\n \\n \\nConsider \\nWe can now encode every \\nas \\nany PCP instance over \\nwill now \\nhave only two symbols, 0 and 1 and, hence, is equivalent to a PCP instance over \\n \\n \\nTheorem : PCP is undecidable. That is, there is no algorithm that determines whether an arbitrary Post \\nCorrespondence System has a solution. \\n \\nProof: The halting problem of turning machine can be reduced to PCP to show the undecidability of PCP. Since \\nhalting problem of TM is undecidable (already proved), This reduction shows that PCP is also undecidable. \\nThe proof is little bit lengthy and left as an exercise. \\n \\nSome undecidable problem in context-free languages \\n \\nWe can use the undecidability of PCP to show that many problem concerning the context-free languages \\nare undecidable. To prove this we reduce the PCP to each of these problem. The following discussion \\nmakes it clear how PCP can be used to serve this purpose. \\nwww.indiansbrain.com\\nLet \\nbe a Post Correspondence System over the alphabet \\n. We \\nconstruct two CFG\\'s Gx and Gy from the ordered pairs x,y respectively as follows. \\n \\n and \\n \\n where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\n \\nit is clear that the grammar \\ngenerates the strings that can appear in the LHS of a sequence while solving the \\nPCP followed by a sequence of numbers. The sequence of number at the end records the sequence of \\n \\nstrings from the PCP instance (in reverse order) that generates the string. Similarly, \\ngenerates the \\nstrings that can be obtained from the RHS of a sequence and the corresponding sequence of numbers (in \\nreverse order). \\n \\nNow, if the Post Correspondence System has a solution, then there must be a sequence \\n \\n \\n \\n \\n \\n \\n \\n \\nAccording to the construction of \\nand \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn this case \\nwww.indiansbrain.com\\n \\n \\nHence , \\nand \\nimplying \\n \\n \\n \\n \\n \\nConversely, let \\n \\n \\nHence, w must be in the form w1w2 where \\nand w2 in a sequence \\n(since, only that kind \\nof strings can be generated by each of \\nand \\n). \\n \\nNow, the string \\nis a solution to the Post Correspondence System. \\n \\nIt is interesting to note that we have here reduced PCP to the language of pairs of CFG,s whose intersection is \\nnonempty. The following result is a direct conclusion of the above. \\n \\nTheorem : Given any two CFG\\'s G1 and G2 the question \"Is \\n\" is undecidable. \\n \\nProof: Assume for contradiction that there exists an algorithm A to decide this question. This would imply \\nthat PCP is decidable as shown below. \\n \\nFor any Post Correspondence System, P construct grammars \\nand \\nby using the constructions \\nelaborated already. We can now use the algorithm A to decide whether and \\nThus, PCP is decidable, a contradiction. So, such an algorithm does not exist. \\n \\nIf \\nand \\nare CFG\\'s constructed from any arbitrary Post Correspondence System, than it is not difficult to \\n \\nshow that \\nand \\nare also context-free, even though the class of context-free languages are \\nnot closed under complementation. \\n \\nand their complements can be used in various ways to show that many other questions \\nrelated to CFL\\'s are undecidable. We prove here some of those. \\n \\nTheorem : Foe any two arbitrary CFG\\'s \\nthe following questions are undecidable \\n \\ni. \\nIs \\n \\n \\nii. \\nIs \\n \\nwww.indiansbrain.com\\niii. Is \\n \\nProof : \\ni. \\nIf \\nthen, \\n \\n \\nHence, it suffice to show that the question “Is \\n\" is undecidable. \\n \\nSince, \\nand \\nare CFl\\'s and CFL\\'s are closed under union, \\nis also context-\\nfree. By DeMorgan\\'s theorem, \\n \\n \\nIf there is an algorithm to decide whether \\nwe can use it to decide whether \\nor not. But this problem has already been proved to be undecidable. \\n \\n \\nHence there is no such algorithm to decide or not. \\nii. \\n \\nLet P be any arbitrary Post correspondence system and \\nand \\nare CFg\\'s constructed from the pairs of \\nstrings. \\n \\nmust be a CFL and let G1generates L1. That is, \\n \\n \\n \\n \\n \\nby De Morgan\\'s theorem, as shown already, any string, \\nrepresents a solution to the \\nPCP. Hence, \\ncontains all but those strings representing the solution to the PCP. \\n \\nLet \\nfor same CFG G2. \\n \\nIt is now obvious that \\nif and only if the PCP has no solutions, which is already proved to be \\nundecidable. Hence, the question “Is \\n?\" is undecidable. \\n \\niii. \\nwww.indiansbrain.com\\nLet \\nbe a CFG generating the language \\nand G2 be a CFG generating \\nwhere \\nand \\nare CFG.s constructed from same arbitrary instance of PCP. \\n \\niff \\n \\n \\ni.e. iff the PCP instance has no solutions as discussed in part (ii). \\n \\nHence the proof. \\n \\nTheorem : It is undecidable whether an arbitrary CFG is ambiguous. \\n \\nProof : Consider an arbitrary instance of PCP and construct the CFG\\'s \\nand \\nfrom the ordered pairs \\nof strings. \\n \\nWe construct a new grammar G from \\nand \\nas follows. \\n \\n where \\n \\n \\n \\n \\n \\nis same as that of \\nand \\n. \\n \\n \\n \\n \\n \\nThis constructions gives a reduction of PCP to the -------- of whether a CFG is ambiguous, thus leading to \\nthe undecidability of the given problem. That is, we will now show that the PCP has a solution if and only if G \\nis ambiguous. (where G is constructed from an arbitrary instance of PCP). \\n \\nOnly if Assume that \\nis a solution sequence to this instance of PCP. \\n \\nConsider the following two derivation in \\n. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBut , \\n \\n \\n \\n \\nis a solution to the PCP. Hence the same string of terminals \\nhas two derivations. Both these \\nderivations are, clearly, leftmost. Hence G is ambiguous. \\n \\nIf It is important to note that any string of terminals cannot have more than one derivation in \\nand \\n \\nBecause, every terminal string which are derivable under these grammars ends with a sequence of integers \\nThis sequence uniquely determines which productions must be used at every step of the derivation. \\n \\nHence, if a terminal string, \\n, has two leftmost derivations, then one of them must begin with \\nthe step. \\n \\nthen continues with derivations under \\n \\n \\nIn both derivations the resulting string must end with a sequence \\nfor same \\nThe reverse \\nof this sequence must be a solution to the PCP, because the string that precede in one case is \\n \\nand \\nin the other case. Since the string derived in both cases are identical, the \\n \\nsequence \\n \\nmust be a solution to the PCP. \\n \\nHence the proof \\nwww.indiansbrain.com\\nClass p-problem solvable in polynomial time: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNon deterministic polynomial time: \\n \\nA nondeterministic TM that never makes more than p(n) moves in any sequence of choices for \\nsome polynomial p is said to be non polynomial time NTM. \\n\\uf095 NP is the set of languags that are accepted by polynomial time NTM’s\\uf020\\n\\uf020\\n\\uf095 Many problems are in NP but appear not to be in p.\\uf020\\n\\uf095 One of the great mathematical questions of our age: is there anything in NP that is not in p?\\uf020\\nNP-complete problems: \\n\\uf020\\nIf We cannot resolve the “p=np question, we can at least demonstrate that certain problems in NP \\nare the hardest , in the sense that if any one of them were in P , then P=NP. \\n\\uf020\\n\\uf095 These are called NP-complete.\\uf020\\n\\uf020\\n\\uf095 Intellectual leverage: Each NP-complete problem’s apparent difficulty reinforces the belief \\nthat they are all hard.\\uf020\\n \\nMethods for proving NP-Complete problems: \\n \\n\\uf095 Polynomial time reduction (PTR): Take time that is some polynomial in the input size to \\nconvert instances of one problem to instances of another.\\uf020\\n\\uf020\\n\\uf095 If P1 PTR to P2 and P2 is in P1 the so is P1.\\uf020\\n\\uf095 Start by showing every problem in NP has a PTR to Satisfiability of Boolean formula.\\uf020\\n\\uf020\\n\\uf095 Then, more problems can be proven NP complete by showing that SAT PTRs to them \\ndirectly or indirectly.\\uf020\\nwww.indiansbrain.com\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Loaded Data into Smaller Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the recursive text splitter with adjusted separators\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Load combined content from file\n",
    "with open(\"combined_theory_of_computation_content.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    combined_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Automata Tutorial | Theory of Computation - JavatpointTutorials×PythonPython Django Numpy Pandas Tkinter Pytorch Flask OpenCVAI, ML and Data ScienceArtificial Intelligence Machine Learning Data Science Deep Learning TensorFlow Artificial Neural Network Matplotlib Python ScipyJavaJava Servlet JSP Spring Boot Spring Framework Hibernate JavaFX Java Web ServicesB.Tech and MCADBMS Data Structures Operating System Computer Network DAA Computer Organization Software Engineering Data MiningWeb TechnologyHTML CSS'),\n",
       " Document(metadata={}, page_content='JavaScript Jquery Angular-8 React JS React Native Node JSSoftware TestingSoftware Testing Selenium JIRA JMeter Postman TestNG SoapUI CucumberInterview×Technical InterviewC C++ Php Java Python JavaScript TypeScriptJava InterviewJDBC Servlet Maven Jenkins Spring Spring Boot JDB Hibernate JSFWeb InterviewHTML CSS JavaScript Jquery Angular Node-JS AJAXDatabase InterviewDBMS SQL PL/SQL Oracle MySQL MongoDB Redis MariaDBCompany InterviewsIBM Adobe Microsoft Amazon TCS HCL Wipro DXC Accenture Capgemini Space X'),\n",
       " Document(metadata={}, page_content='Ericsson Infosy IGate EXL IndiaMART SapientCompilerPythonJavaPhpCC++RHtmlJavascriptTypescriptSwiftHome Python Java JavaScriptHTML SQL PHP C# C++ DS Aptitude Reasoning Selenium DBMS C Andriod Interview QAutomata TutorialAutomata TutorialTheory of AutomataFinite AutomataTransition DiagramTransition TableDFAExamples of DFANFAExamples of NFAEliminating ε TransitionsConversion from NFA to DFAConversion from NFA with ε to DFAMinimization of DFARegular ExpressionRegular ExpressionExamples of Regular'),\n",
       " Document(metadata={}, page_content=\"ExpressionConversion of RE to FAArden's TheoremMoore MachineMealy MachineConversion from Mealy machine to Moore machineConversion from Moore machine to Mealy machineCFGContext-free GrammarDerivationDerivation TreeAmbiguity in GrammarUnambiguous GrammarSimplification of CFGChomsky's Normal Form (CNF)Greibach Normal Form (GNF)PDAPushdown AutomataPDA AcceptanceNon-deterministic Pushdown AutomataCFG to PDA ConversionTuring MachineApplication of Different Automata | Theory of ComputationIntroduction to\"),\n",
       " Document(metadata={}, page_content='Computational Complexity TheoryAutomata and Game TheoryRecursive Descent Parsernext →Automata TutorialTheory of automata is a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. An automaton with a finite number of states is called a Finite automaton.In this tutorial, we are going to learn how to construct deterministic finite automata,'),\n",
       " Document(metadata={}, page_content='non-deterministic finite automata, Regular expression, context-free grammar, context-free language, Push down automata, Turning machines, etc.PrerequisiteBefore learning Automata, you should have a basic understanding of string, language, alphabets, symbols.AudienceOur Automata Tutorial is designed to help beginners and professionals.ProblemsWe assure that you will not find any problem in this Automata Tutorial. But if there is any mistake, please post the problem in contact form.Next TopicTheory of'),\n",
       " Document(metadata={}, page_content='Automatanext →Latest CoursesWe provides tutorials and interview questions of all technology like java tutorial, android, java frameworksContact info G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India[email\\xa0protected].Follow usLatest PostPRIVACY POLICYTutorialsJava Data Structures C Programming C++ Tutorial C# Tutorial PHP Tutorial HTML Tutorial JavaScript Tutorial jQuery Tutorial Spring TutorialInterview QuestionsTcs Intuit Wipro Adobe Infosys Amazon Accenture Cognizant Capgemini MicrosoftOnline CompilerC R'),\n",
       " Document(metadata={}, page_content='C++ Php Java Html Swift Python JavaScript TypeScript© Copyright 2024 Javatpoint. All Rights Reserved.'),\n",
       " Document(metadata={}, page_content='Introduction of Theory of Computation - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content'),\n",
       " Document(metadata={}, page_content='CoursesDSA to DevelopmentNewly Launched!Android with KotlinGenerative AI & ChatGPTMaster Django FrameworkBecome AWS CertifiedFor Working ProfessionalsInterview 101: DSA & System DesignData Science Training ProgramJAVA Backend Development (Live)DevOps Engineering (LIVE)Software Testing & Automation (Live)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation CourseData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive Programming (Live)Full Stack Development'),\n",
       " Document(metadata={}, page_content='with React & Node JS (Live)GATE Exam CoursesGATE CS & IT (Self-Paced)GATE DS & AI (Self-Paced)All CoursesTutorialsData Structures & AlgorithmsDSA for BeginnersData StructuresArraysMatrixStringsLinked ListStackQueueTreeGeneric TreeBinary TreeBinary Search TreeAVL TreeB TreeB+ TreeRed Black TreeTree Data Structure TutorialHeapHashingGraphSet Data StructureMap Data StructureAdvanced Data StructureData Structures TutorialAlgorithmsAnalysis of AlgorithmsSearching AlgorithmsLinear SearchBinary SearchSearching'),\n",
       " Document(metadata={}, page_content='Algorithms TutorialSorting AlgorithmsSelection SortBubble SortInsertion SortMerge SortQuick SortHeap SortCounting SortRadix SortBucket SortSorting Algorithms TutorialGreedy AlgorithmsDynamic ProgrammingGraph AlgorithmsPattern SearchingRecursionBacktrackingDivide and ConquerMathematical AlgorithmsGeometric AlgorithmsBitwise AlgorithmsRandomized AlgorithmsBranch and BoundAlgorithms TutorialComplete DSA TutorialCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE'),\n",
       " Document(metadata={}, page_content='SheetNetflix SDE SheetGoogle SDE SheetWipro Coding SheetInfosys Coding SheetTCS Coding SheetCognizant Coding SheetHCL Coding SheetDSA Cheat SheetsDSA Sheet for BeginnersSDE SheetsFAANG Coding SheetLove Babbaar SheetMass Recruiter SheetProduct-Based Coding SheetCompany-Wise Preparation SheetTop 100 DSA Interview Questions Topic-wise100 Days of CodePythonPython TutorialPython ExercisesPython List ExercisePython String ExercisePython Tuple ExercisePython Dictionary ExercisePython Set ExercisePython Excercises'),\n",
       " Document(metadata={}, page_content='Topic wisePython QuizPython ProgramsAdvanced Python TutorialPython API TutorialPython Database TutorialPython JSONPython Cheat SheetPython ProjectsPython Interview QuestionsML & Data ScienceMachine LearningMachine Learning TutorialMaths for MLML Projects100 Days of Machine LearningData Science TutorialData Science PackagesPandas TutorialNumPy TutorialData VisualizationData Visualization with PythonData Visualization with RTableauPower BIData AnalysisData Analysis with PythonData Analysis with R100 Days of'),\n",
       " Document(metadata={}, page_content='Data AnalyticsDeep LearningNLP TutorialOpenCV TutorialInterview QuestionsMachine Learning Interview QuestionsDeep Learning Interview QuestionsR Interview QuestionsSystem DesignSystem Design TutorialSoftware Design PatternsSystem Design RoadmapTop 10 System Design Interview QuestionsInterview CornerCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsPuzzles for'),\n",
       " Document(metadata={}, page_content='InterviewsLanguagesCC++JavaPythonR TutorialC#SQLScalaPerlGo LanguageWeb DevelopmentHTMLHTML TutorialFree HTML CourseHTML Cheat SheetCSSCSS TutorialFree CSS CourseCSS Cheat SheetJavaScriptJavaScript TutorialJavaScript QuestionsJavaScript Cheat SheetDSA using JavaScriptFree JavaScript CourseJavaScript A to Z Complete GuideTypeScriptReactJSReactJS TutorialFree ReactJS CourseReactJS Cheat SheetNextJSNode.jsPHPAngularJSjQueryWeb Development Using PythonDjangoFlaskSeleniumPostmanGithubWeb Design100 Days of Web'),\n",
       " Document(metadata={}, page_content='DevelopmentCS SubjectsOperating SystemDBMSComputer NetworksEngineering MathematicsComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOps And LinuxDevOps TutorialGITAWSKubernetesDockerMicrosoft Azure TutorialGoogle Cloud PlatformDevOps RoadmapDevOps Interview QuestionsLinuxLinux TutorialLinux Commands A-ZLinux Commands CheatsheetFile Permissions in LinuxLinux System AdministrationLinux Shell ScriptingLinux NetworkingLinux Interview QuestionsSchool'),\n",
       " Document(metadata={}, page_content='LearningClass 8 Study MaterialClass 9 Study MaterialClass 10 Study MaterialClass 11Study MaterialClass 12 Study MaterialEnglish GrammarGfG SchoolCommerceGATEGATE Computer Science NotesLast Minute NotesGATE CS Solved PapersGATE CS Original Papers and Official KeysGATE CS 2025 SyllabusGATE DA 2025 SyllabusOther CS ExamsISROUGC NETGeeksforGeeks VideosJobsGet Hired: Apply for JobsCorporate Hiring SolutionsFiltered JobsJobs for FreshersJobs for ExperiencedAll JobsPracticePractice Coding ProblemsAll DSA'),\n",
       " Document(metadata={}, page_content=\"ProblemsProblem of the DayCompany Wise Coding PracticeAmazonMicrosoftFlipkartExplore AllGfG SDE SheetPractice Problems Difficulty WiseBasicEasyMediumHardLanguage Wise Coding PracticeCPPJavaPythonCurated DSA ListsBeginner's DSA SheetLove Babbar SheetTop 50 Array ProblemsTop 50 String ProblemsTop 50 DP ProblemsTop 50 Graph ProblemsTop 50 Tree ProblemsContestsJob-A-Thon Hiring ChallengeGfG Weekly [Rated Contest]All Contests and Events\"),\n",
       " Document(metadata={}, page_content=\"Notifications\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\n\\n                                        You're all caught up!!\\n\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAptitudeEngineering MathematicsDiscrete MathematicsOperating SystemDBMSComputer NetworksDigital Logic and DesignC ProgrammingData StructuresAlgorithmsTheory of ComputationCompiler DesignComputer Org and Architecture \\n\\n\\n\\n\\n▲\"),\n",
       " Document(metadata={}, page_content='Open In App'),\n",
       " Document(metadata={}, page_content=\"Go PremiumShare Your ExperiencesAutomata TutorialAutomata _ IntroductionIntroduction of Theory of ComputationChomsky Hierarchy in Theory of ComputationApplications of various AutomataRegular Expression and Finite AutomataIntroduction of Finite AutomataArden's Theorem in Theory of ComputationArden's Theorem and Challenging Applications | Set 2L-graphs and what they represent in TOCHypothesis (language regularity) and algorithm (L-graph to NFA) in TOCRegular Expressions, Regular Grammar and Regular\"),\n",
       " Document(metadata={}, page_content='LanguagesHow to identify if a language is regular or notDesigning Finite Automata from Regular Expression (Set 1)Star Height of Regular Expression and Regular LanguageGenerating regular expression from Finite AutomataDesigning Deterministic Finite Automata (Set 1)Designing Deterministic Finite Automata (Set 2)DFA for Strings not ending with \"THE\"DFA of a string with at least two 0’s and at least two 1’sDFA for accepting the language L = {  anbm | n+m=even }DFA machines accepting odd number of 0’s or/and'),\n",
       " Document(metadata={}, page_content=\"even number of 1’sDFA of a string in which 2nd symbol from RHS is 'a'Union process in DFAConcatenation process in DFADFA in LEX code which accepts even number of zeros and even number of onesConversion from NFA to DFAMinimization of DFAReversing Deterministic Finite AutomataComplementation process in DFAKleene's Theorem in TOC | Part-1Mealy\\xa0and\\xa0Moore\\xa0Machines in TOCDifference Between Mealy Machine and Moore MachineCFGRelationship between grammar and language in Theory of ComputationSimplifying Context Free\"),\n",
       " Document(metadata={}, page_content='GrammarsClosure Properties of Context Free LanguagesUnion and Intersection of Regular languages with CFLConverting Context Free Grammar to Chomsky Normal FormConverting Context Free Grammar to Greibach Normal FormPumping Lemma in Theory of ComputationCheck if the language is Context Free or NotAmbiguity in Context free Grammar and Context free LanguagesOperator grammar and precedence parser in TOCContext-sensitive Grammar (CSG) and Language (CSL)PDA (Pushdown Automata)Introduction of Pushdown'),\n",
       " Document(metadata={}, page_content='AutomataPushdown Automata Acceptance by Final StateConstruct Pushdown Automata for given languagesConstruct Pushdown Automata for all length palindromeDetailed Study of PushDown AutomataNPDA for accepting the language  L = {an bm cn | m,n>=1}NPDA for accepting the language L = {an bn cm | m,n>=1}NPDA for accepting the language  L = {an bn | n>=1}NPDA for accepting the language  L = {am b(2m) | m>=1}NPDA for accepting the language  L = {am bn cp dq | m+n=p+q ; m,n,p,q>=1}Construct Pushdown automata for L ='),\n",
       " Document(metadata={}, page_content='{0n1m2m3n | m,n ≥ 0}Construct Pushdown automata for L = {0n1m2(n+m) | m,n ≥ 0}NPDA for accepting the language L = {ambnc(m+n) | m,n ≥ 1}NPDA for accepting the language L = {amb(m+n)cn | m,n ≥ 1}NPDA for accepting the language L = {a2mb3m | m ≥ 1}NPDA for accepting the language L = {amb(2m+1) | m ≥ 1}NPDA for accepting the language L = {aibjckdl | i==k or j==l,i>=1,j>=1}Construct Pushdown automata for L = {a(2*m)c(4*n)dnbm | m,n ≥ 0}NPDA for L =  {0i1j2k | i==j or j==k ; i , j , k >= 1}NPDA for accepting'),\n",
       " Document(metadata={}, page_content=\"the language L = {anb(2n) | n>=1} U {anbn | n>=1}NPDA for the language L ={w∈ {a,b}*| w contains equal no. of a's and b's}Turing MachineRecursive and Recursive Enumerable Languages in TOCTuring Machine in TOCTuring Machine for additionTuring machine for subtraction | Set 1Turing machine for multiplicationTuring machine for copying dataConstruct a Turing Machine for language L = {0n1n2n | n≥1}Construct a Turing Machine for language L = {wwr | w ∈ {0, 1}}Construct a Turing Machine for language L = {ww | w ∈\"),\n",
       " Document(metadata={}, page_content=\"{0,1}}Construct Turing machine for L = {an bm a(n+m) | n,m≥1}Construct a Turing machine for L = {aibjck | i*j = k; i, j, k ≥ 1}Turing machine for 1's and 2’s complementRecursive and Recursive Enumerable Languages in TOCTuring Machine for subtraction | Set 2Halting Problem in Theory of ComputationTuring Machine as ComparatorDecidabilityDecidable and Undecidable Problems in Theory of ComputationUndecidability and Reducibility in TOCComputable and non-computable problems in TOCTOC Interview preparationLast\"),\n",
       " Document(metadata={}, page_content=\"Minute Notes - Theory of ComputationTOC  Quiz and PYQ's in TOCTheory of Computation - GATE CSE Previous Year QuestionsRegular languages and finite automataContext free languages and Push-down automataRecursively enumerable sets and Turing machinesUndecidabilityDSA to Development Course\"),\n",
       " Document(metadata={}, page_content='Introduction of Theory of Computation\\n\\n\\n\\nLast Updated : \\n27 Sep, 2024\\n\\n\\n\\n\\n\\n\\nSummarize\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\nFollow'),\n",
       " Document(metadata={}, page_content='Automata theory (also referred to as the Theory Of Computation) is a branch of Computer Science and Mathematics that studies how machines compute functions and solve problems. This field is mainly focused on mathematical structures called automata and is crucial for the purpose of studying processes occurring in discrete systems.\\nWhat is Automata Theory?'),\n",
       " Document(metadata={}, page_content='In automata theory, scientists and engineers can predict the behavior of computing systems thereby improving problem-solving approaches. Originally developed to describe and explain the dynamics of systems, automata theory is the theoretical base of the formal languages theory, grammar, and computational complexity.\\nBasic Terminologies of Theory of Computation\\nNow, let’s understand the basic terminologies, which are important and frequently used in the Theory of Computation.\\xa0\\nSymbol'),\n",
       " Document(metadata={}, page_content='A symbol (often also called a character) is the smallest building block, which can be any alphabet, letter, or picture.'),\n",
       " Document(metadata={}, page_content='Alphabets (Σ)\\nAlphabets are a set of symbols, which are always finite.'),\n",
       " Document(metadata={}, page_content='String\\xa0\\nA string is a finite sequence of symbols from some alphabet. A string is generally denoted as w and the length of a string is denoted as |w|.\\xa0\\nEmpty string is the string with zero occurrence of symbols, represented as ε.'),\n",
       " Document(metadata={}, page_content='Number of Strings (of length 2) that can be generated over the alphabet {a, b}:                     -   -                     a   a                     a   b                     b   a                     b   bLength of String |w| = 2Number of Strings = 4Conclusion:For alphabet {a, b} with length n, number of strings can be generated = 2n.\\n__mask-blockquote__index=1__'),\n",
       " Document(metadata={}, page_content='The Theory of Computation explores automata, languages, and complexity. If you want to dive deeper into this subject for GATE, the GATE CS Self-Paced Course covers it extensively.'),\n",
       " Document(metadata={}, page_content='Closure Representation in TOC\\nL+: It is a Positive Closure that represents a set of all strings except Null or ε-strings.\\nL*: It is “Kleene Closure“, that represents the occurrence of certain alphabets for given language alphabets from zero to the infinite number of times. In which ε-string is also included.\\nFrom the above two statements, it can be concluded that:\\nL* = εL+'),\n",
       " Document(metadata={}, page_content=\"Example:(a) Regular expression for language accepting all combination of g's over Σ={g}:                                         R = g*                               R={ε,g,gg,ggg,gggg,ggggg,...}(b) Regular Expression for language accepting all combination of g's over Σ={g} : R = g+                               R={g,gg,ggg,gggg,ggggg,gggggg,...}\"),\n",
       " Document(metadata={}, page_content='Note: Σ* is a set of all possible strings(often power set(need not be unique here or we can say multiset) of string) So this implies that language is a subset of Σ*.This is also called a “Kleene Star”.'),\n",
       " Document(metadata={}, page_content='Kleene Star is also called a “Kleene Operator” or “Kleene Closure”. Engineers and IT professionals make use of Kleene Star to achieve all set of strings which is to be included from a given set of characters or symbols. It is one kind of Unary operator. In Kleene Star methodology all individual elements of a given string must be present but additional elements or combinations of these alphabets can be included to any extent.'),\n",
       " Document(metadata={}, page_content='Example:Input String: \"GFG\".Σ* = { ε,\"GFG\",\"GGFG\",\"GGFG\",\"GFGGGGGGGG\",\"GGGGGGGGFFFFFFFFFGGGGGGGG\",...}  (Kleene Star is an infinite set but if we provide any grammar rules then it can work as a finite set.Please note that we can include ε string also in given Kleene star representation.)\\nLanguage\\nA language is a set of strings, chosen from some Σ* or we can say- ‘A language is a subset of Σ* ‘. A language that can be formed over ‘ Σ ‘ can be Finite or Infinite.'),\n",
       " Document(metadata={}, page_content=\"Example of Finite Language:           L1 = { set of string of 2 }         L1 = { xy, yx, xx, yy }Example of Infinite Language:         L1 = { set of all strings starts with 'b' }         L1 = { babb, baa, ba, bbb, baab, ....... }\\nConclusion\\nIt is an important branch of computation that is concerned with formal languages, and automata theory in particular. It provides a basis for other courses such as Turing machines and computational complexity that are very important in computer science.\"),\n",
       " Document(metadata={}, page_content='Introduction of Theory of Computation – FAQs\\nWhat is the relevance of the automata theory in computer science?'),\n",
       " Document(metadata={}, page_content='Automata theory is used in modeling computational problems hence enhancing the understanding and design of systems such as compilers, interpreters among others.\\n\\nwhat is the purpose of using Kleene Star in the study of formal languages?\\n\\nThe Kleene Star extends symbols from a given alphabet where one is able to create infinite strings from it or even the null string.\\n\\nIs it possible to implement automata theory into real life?'),\n",
       " Document(metadata={}, page_content='Of course, automata theory has found its use in certain areas like compiler design, artificial intelligence, network security and natural language processing.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nabhishek1 \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious Article\\n\\n\\n\\nAutomata Tutorial\\n\\n\\n\\n\\nNext Article\\n\\n\\n\\n\\nChomsky Hierarchy in Theory of Computation\\n\\n\\n\\n\\n\\n\\nRead More\\n\\n\\n\\nSimilar Reads'),\n",
       " Document(metadata={}, page_content='Introduction to Computation Complex Theory\\nBroad Overview : Complexity theory, in a nutshell, a complexity word is a quite fancy word, literally, it sounds complex, but it is not an intimidating topic. What it really means is analyzing the program or we can say analyzing the efficiency of the program, figuring out whether the program is correct, figuring out whether one program is better th\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Introduction To Grammar in Theory of Computation\\nPrerequisite - Theory of ComputationGrammar :It is a finite set of formal rules for generating syntactically correct sentences or meaningful correct sentences.Constitute Of Grammar :Grammar is basically composed of two basic elements - Terminal Symbols - Terminal symbols are those which are the components of the sentences generated using a grammar\\n\\n\\n\\n3 min read'),\n",
       " Document(metadata={}, page_content=\"Theory of Computation - GATE CSE Previous Year Questions\\nSolving GATE Previous Year's Questions (PYQs) not only clears the concepts but also helps to gain flexibility, speed, accuracy, and understanding of the level of questions generally asked in the GATE exam, and that eventually helps you to gain good marks in the examination. Previous Year Questions help a candidate practice and revise for GATE, whic\\n\\n\\n\\n5 min read\"),\n",
       " Document(metadata={}, page_content='Relationship between grammar and language in Theory of Computation\\nA grammar is a set of production rules which are used to generate strings of a language. In this article, we have discussed how to find the language generated by a grammar and vice versa as well. Language generated by a grammar - Given a grammar G, its corresponding language L(G) represents the set of all strings generated from G. Consider the foll\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Theory of Computation | Regular languages and finite automata | Question 2\\nWhat is the complement of the language accepted by the NFA shown below? (A) A (B) B (C) C (D) D Answer: (B) Explanation: Quiz of this QuestionPlease comment below if you find anything wrong in the above post\\n\\n\\n\\n1 min read'),\n",
       " Document(metadata={}, page_content='Arden\\'s Theorem in Theory of Computation\\nArden\\'s theorem state that: \"If P and Q are two regular expressions over \"∑\", and if P does not contain \"∈\" , then the following equation in R given by R = Q + RP has a unique solution i.e., R = QP*.\" That means, whenever we get any equation in the form of R = Q + RP, then we can directly replace it with R = QP*. So, here we will first prove that R\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Decidability Table in Theory of Computation\\nPrerequisite - Undecidability, Decidable and undecidable problems Identifying languages (or problems*) as decidable, undecidable or partially decidable is a very common question in GATE. With correct knowledge and ample experience, this question becomes very easy to solve. A language is undecidable if it is not decidable. An undecidable language ma\\n\\n\\n\\n2 min read'),\n",
       " Document(metadata={}, page_content='Chomsky Hierarchy in Theory of Computation\\nAccording to Chomsky hierarchy, grammar is divided into 4 types as follows: Type 0 is known as unrestricted grammar.Type 1 is known as context-sensitive grammar.Type 2 is known as a context-free grammar.Type 3 Regular Grammar.Type 0: Unrestricted Grammar: Type-0 grammars include all formal grammar. Type 0 grammar languages are recognized by turing\\n\\n\\n\\n2 min read'),\n",
       " Document(metadata={}, page_content='Pumping Lemma in Theory of Computation\\nThere are two Pumping Lemmas, which are defined for 1. Regular Languages, and 2. Context - Free Languages Pumping Lemma for Regular Languages For any regular language L, there exists an integer n, such that for all x ? L with |x| ? n, there exists u, v, w ? ?*, such that x = uvw, and (1) |uv| ? n (2) |v| ? 1 (3) for all i ? 0: uviw ? L In simple te\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Decidable and Undecidable Problems in Theory of Computation\\nIn the Theory of Computation, problems can be classified into decidable and undecidable categories based on whether they can be solved using an algorithm. A decidable problem is one for which a solution can be found in a finite amount of time, meaning there exists an algorithm that can always provide a correct answer. While an undecidable problem i\\n\\n\\n\\n6 min read'),\n",
       " Document(metadata={}, page_content='Halting Problem in Theory of Computation\\nTo understand better the halting problem, we must know Decidability , Undecidability and Turing machine , decision problems and also a theory named as Computability theory and Computational complexity theory. Some important terms: Computability theory - The branch of theory of computation that studies which problems are computationally solvable usi\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content=\"Automata Theory | Set 2\\nFollowing questions have been asked in GATE CS 2012 exam. 1) What is the complement of the language accepted by the NFA shown below? Assume ∑ = {a} and ε is the empty string (A) Φ (B) ε (C) a (D) {a, ε} Answer (B) The given alphabet ∑ contains only one symbol {a} and the given NFA accepts all strings with any number of occurrences of 'a'. In other\\n\\n\\n\\n3 min read\"),\n",
       " Document(metadata={}, page_content='Automata Theory | Set 3\\nFollowing questions have been asked in GATE CS 2011 exam. 1) The lexical analysis for a modern language such as Java needs the power of which one of the following machine models in a necessary and sufficient sense? (A) Finite state automata (B) Deterministic pushdown automata (C) Non-deterministic pushdown automata (D) Turing machine Answer (A) Lex\\n\\n\\n\\n2 min read'),\n",
       " Document(metadata={}, page_content='Automata Theory | Set 4\\nFollowing questions have been asked in GATE CS 2011 exam. 1) Let P be a regular language and Q be context-free language such that Q ⊆ P. (For example, let P be the language represented by the regular expression p*q* and Q be {pnqn|n ∈ N}). Then which of the following is ALWAYS regular? (A) P ∩ Q (B) P - Q (C) ∑* - P (D) ∑* - Q\\n\\n\\n\\n2 min read'),\n",
       " Document(metadata={}, page_content='Automata Theory | Set 5\\nFollowing questions have been asked in GATE CS 2009 exam. 1) S --> aSa| bSb| a| b ;The language generated by the above grammar over the alphabet {a,b} is the set of (A) All palindromes. (B) All odd length palindromes. (C) Strings that begin and end with the same symbol (D) All even length palindromes. Answer (B) The strings accepted by language are\\n\\n\\n\\n3 min read'),\n",
       " Document(metadata={}, page_content=\"Automata Theory | Set 7\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider L= {(TM) | TM is the Turing machine that halts on all input and L(TM)= L' for some undecidable language L'}. Here, (TM) is the encoding of a Turing machine as a string over alphabet {0, 1} then L is: (A) decidable and recursively enumerable (B) decidable and recursive (C) decid\\n\\n\\n\\n3 min read\"),\n",
       " Document(metadata={}, page_content='Automata Theory | Set 8\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Which one of the following language is Regular? (A) {wxwR | w,x ∈ (a+b)+} (B) {wxwR | w ∈ (a+b)*, x ∈ {a,b}} (C) {wwRx | w,x ∈ (a+b)+} (D) {wwR | w ∈ (a+b)*} Explanation: (A) It is correct, since this language can form regular expression which is {{ a(a + b)+a } + {b(a + b)+b}}, i.e., s\\n\\n\\n\\n2 min read'),\n",
       " Document(metadata={}, page_content=\"Automata Theory | Set 9\\nThese questions for practice purpose for GATE CS Exam. Ques-1: Consider the following two statements with respect to Countability: Statement-1: If X union of 'Y' is uncountable, then both set 'X' and set 'Y' must be uncountable. Statement-2: The Cartesian product of two countable sets 'X' and 'Y' is countable. Which of the following option is true\\n\\n\\n\\n3 min read\"),\n",
       " Document(metadata={}, page_content=\"Automata Theory | Set 10\\nThese questions for practice purpose of GATE CS Exam. Ques-1: Consider the following statements: X: For any language either a language L or its complement L' must be finite.Y: DFA for language which contains epsilon must have initial state as final state.Z: Non-deterministic finite automata is more powerful than deterministic finite automata. Which\\n\\n\\n\\n3 min read\"),\n",
       " Document(metadata={}, page_content='Regular Graph in Graph Theory\\nPrerequisite: Graph Theory Basics – Set 1, Set 2 Regular Graph: A graph is called regular graph if degree of each vertex is equal. A graph is called K regular if degree of each vertex in the graph is K. Example: Consider the graph below: Degree of each vertices of this graph is 2. So, the graph is 2 Regular. Similarly, below graphs are 3 Regular an\\n\\n\\n\\n2 min read'),\n",
       " Document(metadata={}, page_content='5 Color Theorem in Graph Theory\\nThe graph is a data structure that is used extensively in real-life. Planar Graph: If a graph can be drawn on the plane without crossing, it is said to be planar. Coloring of a simple graph is the assignment of color to each vertex of the graph so that no two adjacent vertices are assigned the same color. Bi-Partite Graphs: A bipartite graph, also\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Mathematics | Graph Theory Basics - Set 1\\nA graph is a data structure that is defined by two components : A node or a vertex.An edge E or ordered pair is a connection between two nodes u,v that is identified by unique pair(u,v). The pair (u,v) is ordered because (u,v) is not same as (v,u) in case of directed graph.The edge may have a weight or is set to one in case of unweighted graph.Cons\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Mathematics | Graph theory practice questions\\nProblem 1 - There are 25 telephones in Geeksland. Is it possible to connect them with wires so that each telephone is connected with exactly 7 others. Solution - Let us suppose that such an arrangement is possible. This can be viewed as a graph in which telephones are represented using vertices and wires using the edges. Now we have 25 vertices in\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Set Theory Operations in Relational Algebra\\nRelational Algebra in DBMS These Set Theory operations are the standard mathematical operations on set. These operations are Binary operations that are, operated on 2 relations unlike PROJECT, SELECT and RENAME operations. These operations are used to merge 2 sets in various ways. The set operation is mainly categorized into the following: Union op\\n\\n\\n\\n3 min read'),\n",
       " Document(metadata={}, page_content='Applications of Group Theory\\nGroup theory is the branch of mathematics that includes the study of elements in a group. Group is the fundamental concept of algebraic structure like other algebraic structures like rings and fields. Group: A non-empty set G with * as operation, (G, *) is called a group if it follows the closure, associativity, identity, and inverse properties. Pr\\n\\n\\n\\n4 min read'),\n",
       " Document(metadata={}, page_content='Quotient Group in Group Theory\\nWe can say that \"o\" is the binary operation on set G if: G is a non-empty set & G * G = { (a,b): a, b∈ G } and o: G * G --> G. Here, aob denotes the image of ordered pair (a,b) under the function/operation o.Example - \"+\" is called a binary operation on G (any non-empty set ) if & only if: a+b ∈G; ∀ a,b ∈G and a+b give the same result ev\\n\\n\\n\\n12 min read'),\n",
       " Document(metadata={}, page_content='Types of Sets in Set Theory\\nIn mathematics, a Set is a fundamental concept representing a collection of well-defined objects or elements. Sets are typically denoted by capital letters, and the individual elements within a set are listed in curly braces, separated by commas. For example, A={1,2,3,4,5} represents a set A with elements 1, 2, 3, 4, and 5. The order of elements wi\\n\\n\\n\\n7 min read'),\n",
       " Document(metadata={}, page_content='Mathematics | Graph Theory Basics - Set 2\\nGraph theory is a basic branch of discrete mathematics that mainly focuses on the relationship between objects. These objects are called vertices and these vertices are joined by edges. Graphs are common in computer science, network analysis, and many other everyday uses because they provide a good representation of connection, relationship, and pr\\n\\n\\n\\n10 min read'),\n",
       " Document(metadata={}, page_content='Group in Maths: Group Theory\\nGroup theory is one of the most important branches of abstract algebra which is concerned with the concept of the group. A group consists of a set equipped with a binary operation that satisfies four key properties: specifically, it includes property of closure, associativity, the existence of an identity element, and the existence of inverse eleme\\n\\n\\n\\n13 min read'),\n",
       " Document(metadata={}, page_content='Matching (Graph Theory)\\nMatching (Graph Theory): In graph theory, matching is a fundamental concept used to describe a set of edges without common vertices. Matchings are used in various applications such as network design, job assignments, and scheduling. Understanding matchings is essential for solving problems involving optimal pairings and resource allocation. Table o\\n\\n\\n\\n4 min read\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nGATE CS\\n\\n\\nTheory of Computation\\n \\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Please Login to comment...'),\n",
       " Document(metadata={}, page_content='76k+ interested Geeks \\n\\n\\n\\nCore Computer Science Subject for Interview Preparation \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n28k+ interested Geeks \\n\\n\\n\\nGATE Computer Science & Information Technology - 2025 \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n12k+ interested Geeks \\n\\n\\n\\nCBSE Class 12 Computer Science \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nExplore More'),\n",
       " Document(metadata={}, page_content='Corporate & Communications Address:- A-143, 9th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305) | Registered Address:- K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305'),\n",
       " Document(metadata={}, page_content='CompanyAbout UsLegalCareersIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramExploreJob-A-Thon Hiring ChallengeHack-A-ThonGfG Weekly ContestOffline Classes (Delhi/NCR)DSA in JAVA/C++Master System DesignMaster CPGeeksforGeeks VideosGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonData'),\n",
       " Document(metadata={}, page_content='Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython Programming ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem'),\n",
       " Document(metadata={}, page_content='DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceAccountancyBusiness StudiesEconomicsManagementHR ManagementFinanceIncome TaxDatabasesSQLMYSQLPostgreSQLPL/SQLMongoDBPreparation CornerCompany-Wise Recruitment ProcessResume TemplatesAptitude PreparationPuzzlesCompany-Wise PreparationCompaniesCollegesCompetitive ExamsJEE AdvancedUGC NETUPSCSSC'),\n",
       " Document(metadata={}, page_content='CGLSBI POSBI ClerkIBPS POIBPS ClerkMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsRecent ArticlesFree Online ToolsTyping TestImage EditorCode FormattersCode ConvertersCurrency ConverterRandom Number GeneratorRandom Password GeneratorWrite & EarnWrite an ArticleImprove an ArticlePick Topics to WriteShare your ExperiencesInternships'),\n",
       " Document(metadata={}, page_content='@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\\n\\n        acknowledge that you have read and understood our\\n\\n        Cookie Policy &\\n\\n        Privacy Policy\\n\\n\\n\\n        Got It !\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement'),\n",
       " Document(metadata={}, page_content=\"Please go through our recently updated Improvement Guidelines before submitting any improvements.\\nThis improvement is locked by another user right now. You can suggest the changes for now and it will be under 'My Suggestions' Tab on Write.\\nYou will be notified via email once the article is available for improvement.\\n\\n                        Thank you for your valuable feedback!\\n\\n                    \\n\\nSuggest changes\"),\n",
       " Document(metadata={}, page_content='Please go through our recently updated Improvement Guidelines before submitting any improvements.\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\n\\nmin 4 words, max CharLimit:2000'),\n",
       " Document(metadata={}, page_content=\"Create Improvement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n                        Can't choose a topic to write? click here for suggested topics\\n                    \\n\\n\\n\\n                       Write and publish your own Article\"),\n",
       " Document(metadata={}, page_content='Theory Of Computation Notes PDF, Syllabus ✅ [2021] B Tech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHomeBest Courses\\n\\nGoogle Professional Certificates\\nHuman Resource\\n\\nHuman Resource Human Resource Management Human Resource Planning Organizational Culture Organization Development Organizational Behavior\\nLearning DealsAll Blog PostManagement'),\n",
       " Document(metadata={}, page_content='Business Statistics Lean Six Sigma Management Operation Management Research Methodology Operations Research Procurement Management Production Management Supply Chain Strategic Management\\nMarketing\\n\\nEconomics Brand Management Business Business Communication Business Law Entrepreneurship Consumer Behaviour Marketing Essentials Marketing Management Sales Management Shark Tank India\\nBusiness Tech'),\n",
       " Document(metadata={}, page_content='Project Management Business Analytics Management Information System Enterprise Resource Planning Technologies Cloud Computing\\nAbout\\n\\nAbout Us Cookie Policy DMCA Policy Disclaimer Contact Us\\nToggle website search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Category\\nClose\\n\\n\\n\\n\\n\\n\\nHome\\nBest Courses\\n\\nGoogle Professional Certificates\\n\\n\\nHuman Resource\\n\\nHuman Resource\\nHuman Resource Management\\nHuman Resource Planning\\nOrganizational Culture\\nOrganization Development\\nOrganizational Behavior\\n\\n\\nLearning Deals\\nAll Blog Post\\nManagement'),\n",
       " Document(metadata={}, page_content='Business Statistics\\nLean Six Sigma\\nManagement\\nOperation Management\\nResearch Methodology\\nOperations Research\\nProcurement Management\\nProduction Management\\nSupply Chain\\nStrategic Management\\n\\n\\nMarketing\\n\\nEconomics\\nBrand Management\\nBusiness\\nBusiness Communication\\nBusiness Law\\nEntrepreneurship\\nConsumer Behaviour\\nMarketing Essentials\\nMarketing Management\\nSales Management\\nShark Tank India\\n\\n\\nBusiness Tech'),\n",
       " Document(metadata={}, page_content='Project Management\\nBusiness Analytics\\nManagement Information System\\nEnterprise Resource Planning\\nTechnologies\\nCloud Computing\\n\\n\\nAbout\\n\\nAbout Us\\nCookie Policy\\nDMCA Policy\\nDisclaimer\\nContact Us\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021\\nHome>B Tech Study Material>Theory of Computation Notes | PDF, Syllabus | B Tech 2021\\n\\n\\n\\n\\n\\n\\n\\nTheory of Computation Notes | PDF, Syllabus | B Tech 2021'),\n",
       " Document(metadata={}, page_content='Post last modified:30 March 2021\\nReading time:27 mins read\\nPost category:B Tech Study Material'),\n",
       " Document(metadata={}, page_content='Download Theory of Computation Notes PDF, syllabus for B Tech, BCA, MCA 2021. We provide a complete theory of computation pdf. Theory of Computation lecture notes includes a theory of computation notes, theory of computation book, theory of computation courses, theory of computation syllabus, theory of computation question paper, MCQ, case study, theory of computation interview questions and available in theory of computation pdf form.\\nTheory of Computation Notes'),\n",
       " Document(metadata={}, page_content='Theory of Computation subject is included in B Tech CSE, BCA, MCA, M Tech. So, students can able to download theory of computation notes pdf.'),\n",
       " Document(metadata={}, page_content='Table of Content1 Theory of Computation Syllabus2 Theory of Computation PDF3 Theory of Computation Notes3.1 What is Theory of Computation?3.2 Theory of Computation Handwritten Notes4 Theory of Computation Interview Questions5 Theory of Computation Question Paper6 Theory of Computation Book\\n\\nTheory of Computation Notes can be downloaded in theory of computation pdf from the below article'),\n",
       " Document(metadata={}, page_content='Theory of Computation Syllabus\\nA detailed theory of computation syllabus as prescribed by various Universities and colleges in India are as under. You can download the syllabus in the theory of computation pdf form.\\nUnit I'),\n",
       " Document(metadata={}, page_content='Introduction to Automata: The Methods Introduction to Finite Automata, Structural Representations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, Proof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets Strings, Languages, Applications of Automata Theory.'),\n",
       " Document(metadata={}, page_content='Finite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, Extending the Transition Function to Strings, The Language of a DFA'),\n",
       " Document(metadata={}, page_content='Nondeterministic Finite Automata: An Informal View. The Extended Transition Function, The Languages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite Automata With Epsilon-Transitions: Uses of Î-Transitions, The Formal Notation for an Î-NFA, Epsilon-Closures, Extended Transitions and Languages for Î-NFA’s, Eliminating Î- Transitions.\\n Unit II'),\n",
       " Document(metadata={}, page_content='Regular Expressions and Languages: Regular Expressions: The Operators of regular Expressions, Building Regular Expressions, Precedence of Regular-Expression Operators, Precedence of Regular-Expression Operators Finite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions by Eliminating States, Converting Regular Expressions to Automata.'),\n",
       " Document(metadata={}, page_content='Algebraic Laws for Regular Expressions: Properties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of the Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular Languages, Equivalence and Minimization of Automata,'),\n",
       " Document(metadata={}, page_content='Context-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations Using a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, Parse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and Parse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive Inferences, Applications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: Ambiguous Grammars, Removing Ambiguity.'),\n",
       " Document(metadata={}, page_content='Unit III \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical Notation for PDA’s, Instantaneous Descriptions of a PDA, \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack to Final State, From Final State to Empty Stack Equivalence of PDA’s and CFG’s: From Grammars to Pushdown Automata, From PDA’s to Grammars'),\n",
       " Document(metadata={}, page_content='Deterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and Deterministic PDA’s, DPDA’s and Context-Free Languages, DPDA’s and Ambiguous Grammars \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The Pumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, Decision Properties of CFL’s\\nUnit IV'),\n",
       " Document(metadata={}, page_content='Introduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for Turing Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, Turing Machines and Halting Programming Techniques for Turing Machines, Extensions to the Basic Turing Machine, Restricted Turing Machines, Turing Machines and Computers\\nUNIT V'),\n",
       " Document(metadata={}, page_content='Recursive And Recursively Enumerable Languages: Properties of recursive and recursively enumerable languages, Universal Turing machine, The Halting problem, Undecidable problems about TMs. Context-sensitive language and linear bounded automata (LBA), Chomsky hierarchy, Decidability, Post’s correspondence problem (PCP), undecidability of PCP.'),\n",
       " Document(metadata={}, page_content='Theory of Computation PDF\\n\\n\\n\\n\\n\\n Theory of Computation Notes PDF(How to download) Theory of Computation Notes Download  Theory of Computation Book Download Theory of Computation Syllabus Download  Theory of Computation Question Paper Download  Theory of Computation Interview Questions Download \\n\\nTheory of Computation Notes\\nWhat is Theory of Computation?\\n\\n\\n\\n\\n\\n Download PDF'),\n",
       " Document(metadata={}, page_content='Theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory.\\nTheory of Computation Handwritten Notes\\n\\n\\n\\n Download PDF'),\n",
       " Document(metadata={}, page_content='Theory of Computation Interview Questions\\nSome of the theory of computation interview questions are mentioned below. You can download the QnA in theory of computation pdf form.'),\n",
       " Document(metadata={}, page_content='What is TOC?What is Automata Theory in TOC?What is Regular Language in TOC?What is Grammer and Language in TOC?What is Null String in TOC?What is Grammer and Language in TOC?What is Regular Expression in TOC?What is Linear Bound Automata in TOC?What is Context-Free Language(CFL) in TOC?What is Recursive Language in TOC?What is the use of Lexical Analysis in TOC?What is Chomsky Classification of Languages in TOC?Define Kleene Star Closure in TOC?What is the Productions in TOC? Explain Production Rules.'),\n",
       " Document(metadata={}, page_content='Theory of Computation Question Paper\\nIf you have already studied the theory of computation notes, now it’s time to move ahead and go through previous year theory of computation question paper. \\n Download PDF Fill Before Download\\nIt will help you to understand question paper pattern and type of theory of computation questions and answers asked in B Tech, BCA, MCA, M Tech theory of computation exam. You can download the syllabus in theory of computation pdf form.'),\n",
       " Document(metadata={}, page_content='Theory of Computation Book\\nBelow is the list of theory of computation book recommended by the top university in India.\\nIntroduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, R.Motwani & J.D.Ullman (3rd Edition) – Pearson EducationTheory of Computer Science (Automata Language & Computations), by K.L.Mishra & N. Chandrashekhar, PHI\\n\\nDownload B Tech (CS) Study Material\\n\\n\\n\\nComputer Networks Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Computer Networks Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Networks Notes) \\n\\n\\n\\nComputer Graphics Notes ✅ [2020] PDF – Download \\n\\nComputer Graphics Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Graphics Notes)\\n\\n\\n\\nOperating System Notes ✅ [2020] PDF – Download  \\n\\nOperating System Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operating System Notes)'),\n",
       " Document(metadata={}, page_content='Compiler Design Notes ✅ [2020] PDF – Download\\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Compiler Design Notes)\\n\\n\\n\\nData Structures Notes ✅ [2020] PDF – Download \\n\\nData Structures Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Structures Notes)\\n\\n\\n\\nDigital Image Processing Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Digital Image Processing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Digital Image Processing Notes) \\n\\n\\n\\nTheory of Computation Notes ✅ [2020] PDF – Download \\n\\nTheory of Computation Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Theory of Computation Notes) \\n\\n\\n\\nComputer Organization and Architecture Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Computer Organization and Architecture Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Computer Organization and Architecture Notes) \\n\\n\\n\\nCloud Computing Notes ✅ [2020] PDF – Download \\n\\nCloud Computing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Cloud Computing Notes) \\n\\n\\n\\nData Communication and Networking Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Data Communication and Networking Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Communication and Networking Notes) \\n\\n\\n\\n Software Engineering Notes ✅ [2020] PDF – Download \\n\\nSoftware Engineering Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Software Engineering Notes) \\n\\n\\n\\nWeb Technologies Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Web Technologies Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Web Technologies Notes) \\n\\n\\n\\nMicroprocessor and Microcontrollers Notes ✅ [2020] PDF – Download \\n\\nMicroprocessor and Microcontrollers Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Microprocessor and Microcontrollers Notes) \\n\\n\\n\\nDesign and Analysis of Algorithm Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Design and Analysis of Algorithm Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Design and Analysis of Algorithm Notes) \\n\\n\\n\\nOperation Research Notes ✅ [2020] PDF – Download \\n\\nOperation Research Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operation Research Notes) \\n\\n\\n\\nDatabase Management Systems Notes ✅ [2020] PDF – Download'),\n",
       " Document(metadata={}, page_content='Database Management Systems Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Database Management Systems Notes)\\n\\n\\n\\nCompiler Design Notes ✅ [2020] PDF – Download \\n\\nCompiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Compiler Design Notes)'),\n",
       " Document(metadata={}, page_content='In the above article, a student can download theory of computation notes for B Tech, BCA, MCA, M Tech. Theory of Computation lecture notes and study material includes theory of computation notes, theory of computation books, theory of computation syllabus, theory of computation question paper, theory of computation case study, theory of computation interview questions, theory of computation courses in theory of computation pdf form.'),\n",
       " Document(metadata={}, page_content='Go On, Share & Help your Friend\\n Did we miss something in B.Tech Computer Science Notes or You want something More? Come on! Tell us what you think about our post on Theory of Computation Notes | PDF, Syllabus, Book | B Tech 2020 in the comments section and Share this post with your friends.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead more articles\\n Previous PostWeb Technologies Notes | PDF, Syllabus, Book | B Tech 2021 Next PostDigital Image Processing Notes | PDF, Syllabus | B Tech 2021'),\n",
       " Document(metadata={}, page_content='Tags: B Tech, B Tech CS Books, B Tech CS Syllabus, B Tech CSE Books, B Tech CSE Notes, B Tech CSE Syllabus, B Tech Notes, B Tech Study Material, Computer Science Notes PDF, CS Notes, CSE Notes, Theory of Computation Book, Theory of Computation Course, Theory of Computation Interview Questions, Theory of Computation Notes, Theory of Computation PDF, Theory of Computation PPT, Theory of Computation Question Paper, Theory of Computation Syllabus, Theory of Computation Tutorial\\n\\n\\nYou Might Also Like'),\n",
       " Document(metadata={}, page_content='Digital Signal Processing Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n \\n\\n\\nOperation Research Notes | PDF, Syllabus | MBA, B Tech 2024\\n\\n18 March 2020\\n\\n\\n\\n\\n \\n\\n\\nDatabase Management Systems Notes | PDF | B Tech 2021\\n\\n22 March 2020\\n\\n\\n\\nAdvanced Java Programming Notes | PDF | B Tech (2024)\\n\\n23 November 2020\\n\\n\\n\\n\\n \\n\\n\\nComputer Networks Notes | PDF, Syllabus, Books | B Tech (2024)\\n\\n21 March 2020\\n\\n\\n\\n\\n \\n\\n\\nSoftware Engineering Notes | PDF, Syllabus | B Tech 2021\\n\\n21 March 2020'),\n",
       " Document(metadata={}, page_content='Design and Analysis of Algorithm Notes PDF | B Tech (2024)\\n\\n20 March 2020\\n\\n\\n\\nDigital Communication Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nAnalog Communication PDF | Notes, Syllabus B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nBusiness Intelligence Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\nHow to Download Notes on Geektonight\\n\\n27 June 2020\\n\\n\\n\\nWireless Networks Notes | PDF, Syllabus | B Tech 2021\\n\\n3 July 2020\\n\\n\\n\\n\\n\\nLeave a Reply Cancel replyYou must be logged in to post a comment.'),\n",
       " Document(metadata={}, page_content='All CategoryAll Category\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics'),\n",
       " Document(metadata={}, page_content='Enterprise Resource Planning\\nEntrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management'),\n",
       " Document(metadata={}, page_content='MBA Study Material\\nMCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies'),\n",
       " Document(metadata={}, page_content='Treasury Management in Banking\\nTwitter Certification\\nUncategorized'),\n",
       " Document(metadata={}, page_content=\"World's Best Online Courses at One Place \\n\\n\\n\\nWe’ve spent the time in finding, so you can spend your time in learning \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDigital Marketing\"),\n",
       " Document(metadata={}, page_content='Google Ads CourseFacebook Ads CourseSEO CourseInstagram Marketing CourseSEM CourseSocial Media CourseEmail Marketing CoursePinterest CourseChatbot CourseBlogging CourseContent Marketing CourseWooCommerce CourseClickbank Affiliate Marketing CourseAffiliate Marketing CourseAmazon Affiliate Marketing CourseShopify, eCommerce & Dropshipping CourseExcel Data Analysis CourseWordPress CourseGoogle Tag Manager CourseGoogle Analytics CourseDigital Marketing CourseYoutube Marketing CourseBing Ads CourseSocial Media'),\n",
       " Document(metadata={}, page_content='Analytics Course'),\n",
       " Document(metadata={}, page_content='Business'),\n",
       " Document(metadata={}, page_content='Product Strategy CourseSales CourseBrand Strategy CourseBusiness Law CourseStrategic Management CourseMarketing Analytics CourseBusiness Strategy CourseMarketing Management CourseHuman Resource CourseProduct Management CourseProduct Marketing CourseB2B Marketing CourseGrowth Hacking CoursePeople HR Analytics CourseEntrepreneurship CourseBusiness Statistics CourseProject Management CourseNegotiation CourseTime Management CourseLeadership CourseCareer Development CourseStress Management CourseAnxiety'),\n",
       " Document(metadata={}, page_content='Management CourseDesign Thinking CourseEmotional Intelligence CourseTeam Building CourseBusiness Analytics CourseDigital Transformation Course'),\n",
       " Document(metadata={}, page_content='Personal Growth'),\n",
       " Document(metadata={}, page_content='English Grammar CourseVocabulary CourseSoft Skills CoursePublic Speaking CoursePhotography CourseBody Language CourseCommunication Skills CourseInterview Preparation CourseProductivity CourseMindfulness CourseMemory CourseSelf DisciplineSpeed ReadingAcademic WritingCopywriting CourseScientific Writing CourseNovel Writing CourseAcademic Writing CourseTravel Writer CourseCreative Writing CourseInterior Design CourseGraphic Design CourseDrawing CourseDigital Art CourseUI UX Designer Course'),\n",
       " Document(metadata={}, page_content='Finance \\n\\n\\n\\nMutual Fund CourseFinancial Analysis CoursePersonal Finance CourseCost Accounting CourseAudit CourseFintech CourseValue Investing CourseTrading CourseFinancial Modeling CourseInvestment CourseProject Finance CourseStock Trading CourseFinancial & Capital Markets CourseAccounting CourseFinancial Engineering Course \\n\\n\\n\\n\\n\\n\\n\\nFinTech'),\n",
       " Document(metadata={}, page_content='NFT CourseMongoDB CoursejQuery CourseBlockchain CourseCryptocurrency CourseSwift CourseAWS CourseRedux CourseGo CourseDeFi CourseSolidity CourseMetaverse CourseDjango CourseJIRA CourseConversion Rate Optimization (CRO) CourseAnalytics CourseCustomer Loyalty Course \\n\\n\\n\\n\\n\\n\\n\\nLanguage'),\n",
       " Document(metadata={}, page_content='English SpeakingKorean LanguageGerman LanguageSpanish LanguageFrench Language Italian Language Russian Language Japanese Language Arabic LanguageSwedish LanguageHindi LanguagePortuguese LanguageDutch LanguageLatin LanguageTurkish LanguageHungarian LanguageVietnameseAmerican AccentPronunciationSpelling Courses \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTech'),\n",
       " Document(metadata={}, page_content='Data Science CourseR Programming CourseBig Data CourseSQL CourseData Analytics CourseMachine Learning CoursePython CourseSQL Data Science CourseArtificial Intelligence CourseCloud Computing CourseData Warehouse CourseNLP Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment'),\n",
       " Document(metadata={}, page_content='React JS CoursesFront End Development CourseFull Stack Web Developer CourseC++ CourseData Engineering CourseHTML & CSS3 CourseMicrosoft SQL CourseMySQL CourseJava CourseJavaScript CourseTypeScript CourseBack End Development CourseDatabase CourseGraphQL Course \\n\\n\\n\\n\\n\\n\\n\\nExam Prep \\n\\n\\n\\nGRE PrepGMAT PrepMCAT PrepIELTS PrepDAT PrepPSAT PrepCFA PrepOAT PrepACT PrepLSAT PrepFRM PrepSSAT PrepCPA PrepTESOL PrepSAT PrepSSAT Prep \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython'),\n",
       " Document(metadata={}, page_content='Python CourseDeep Learning Python CoursePython Data Science CoursePython for Marketing CoursePython for Finance CoursePython Pandas CoursePython Data Visualization CoursePython Machine Learning CoursePython Data Processing CoursePython Scripting CoursePython for Data Analysis CoursePython Data Structure CourseNLP Python CourseMatplotlib CourseData Cleaning CourseStatistical Modeling CourseKeras CoursePytorch CourseMachine Learning Finance Course \\n\\n\\n\\n\\n\\n\\n\\nTech'),\n",
       " Document(metadata={}, page_content='SCADA CourseASP.net CourseScrum CourseSpring Boot and MVC CourseIT Support & Help Desk CourseRuby on Rails CourseKubernetes CourseDocker CourseNodeJs CourseAngular CoursePHP CourseAPI CourseAlteryx CoursePower BI CourseTableau CourseData Visualization CourseDAX CourseData Streaming CourseRegex CourseQlik Sense CoursePlotly Dash CourseData Modeling Course \\n\\n\\n\\n\\n\\n\\n\\nDevelopment'),\n",
       " Document(metadata={}, page_content='Android CourseiOS Development CourseFlutter CourseKotlin CourseIonic CourseXamarin CourseVirtual Reality CourseMatlab CourseGit & GitHub CourseSelenium CourseShell Scripting CourseARKit CourseGame Design CourseUnity CourseUnreal Engine CourseGame Development CourseBlender CourseDreamweaver CourseVisual Studio CourseC# (C-Sharp) CourseBootstrap Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChild Care'),\n",
       " Document(metadata={}, page_content='Child Nutrition CourseBaby Massage CourseChildcare & Early Education CourseBaby Sign Language CourseKids Art & Drawing CourseKids Coding CourseChild Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Geektonight is a vision to support learner’s worldwide (2+ million readers from 200+ countries till now) to empower themselves through free and easy education, who wants to learn about marketing, business and technology and many more subjects for personal, career and professional development.'),\n",
       " Document(metadata={}, page_content='Connect With Us\\n\\nOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tab \\n\\n \\n\\nMoreAbout UsOpens in a new tabDisclaimerOpens in a new tabCookie PolicyOpens in a new tabPrivacy PolicyOpens in a new tabDMCA PolicyOpens in a new tab'),\n",
       " Document(metadata={}, page_content='CategoriesCategories\\nSelect Category\\nAccounting\\nB Tech Study Material\\nBBA Study Material\\nBCOM Study Material\\nBest Online Course\\nBest Online Deal\\nBest Software Review\\nBrand Management\\nBusiness\\nBusiness Analytics\\nBusiness Communication\\nBusiness Ethics\\nBusiness Law\\nBusiness Statistics\\nCertification Answers\\nCloud Computing\\nComputer Network\\nConsumer Behaviour\\nCorporate Finance\\nCorporate Social Responsibility\\nCustomer Relationship Management\\nDigital Marketing\\nE-Business\\nEconomics\\nEnterprise Resource Planning'),\n",
       " Document(metadata={}, page_content='Entrepreneurship\\nFinance\\nFinancial Accounting\\nFinancial Institutions & Markets\\nFinancial Management\\nGoogle Career Certificates\\nGoogle Certification\\nHotel Management\\nHubSpot Certification\\nHuman Resource\\nHuman Resource Development\\nHuman Resource Management\\nHuman Resource Planning\\nInsurance and Risk Management\\nInternational Banking\\nLean Six Sigma\\nLinkedIn Certification\\nManagement\\nManagement Accounting\\nManagement Information System\\nMarketing Essentials\\nMarketing Management\\nMBA Study Material'),\n",
       " Document(metadata={}, page_content='MCOM Study Material\\nMicrosoft Certification\\nOperation Management\\nOperations Research\\nOrganization Development\\nOrganizational Behavior\\nOrganizational Culture\\nPerformance Management\\nPortfolio Management\\nProcurement Management\\nProduction Management\\nProject Management\\nPuzzle\\nResearch Methodology\\nSales Management\\nSEMrush Certification\\nService Operations Management\\nShark Tank India\\nShipping and Insurance\\nSoftware Engineering\\nStrategic Management\\nStrategy Tools\\nSupply Chain\\nTechnologies'),\n",
       " Document(metadata={}, page_content='Treasury Management in Banking\\nTwitter Certification\\nUncategorized'),\n",
       " Document(metadata={}, page_content='Copyright 2023 Geektonight\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch this website\\n\\nType then hit enter to search'),\n",
       " Document(metadata={}, page_content='MBA NOTES \\nTHEORY OF COMPUTATION\\nBCS601T \\nTHEORY OF COMPUTATION \\nUnit – I \\n \\n(12 hours) \\nIntroduction to Automata: The Methods Introduction to Finite Automata, Structural\\nRepresentations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, \\nProof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets \\nStrings, Languages, Applications of Automata Theory. \\n \\nFinite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of'),\n",
       " Document(metadata={}, page_content='a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, \\nExtending the Transition Function to Strings, The Language of a DFA \\n \\nNondeterministic Finite Automata: An Informal View. The Extended Transition Function, The \\nLanguages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite \\nAutomata With Epsilon-Transitions: Uses of \\uf0ce-Transitions, The Formal Notation for an'),\n",
       " Document(metadata={}, page_content='\\uf0ce-NFA, Epsilon-Closures, Extended Transitions and Languages for \\uf0ce-NFA’s, Eliminating \\uf0ce-\\nTransitions. \\n \\nUnit – II \\n(12 hours) \\nRegular  Expressions\\nand  Languages:  Regular  Expressions:  The  Operators  of  regular\\nExpressions, Building \\nRegular  Expressions,  Precedence  of  Regular-Expression  Operators,\\n \\nPrecedence of Regular-Expression Operators \\n \\nFinite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting'),\n",
       " Document(metadata={}, page_content='DFA’s to Regular Expressions, Converting DFA’s to R egular Expressions by Eliminating States, \\nConverting Regular Expressions to Automata. \\n \\nAlgebraic Laws for Regular Expressions: \\n \\nProperties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of \\nthe Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular \\nLanguages, Equivalence and Minimization of Automata, \\n \\nUnit – III \\n(12 hours) \\nGrammar : Types of Grammar'),\n",
       " Document(metadata={}, page_content='Context-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations \\nUsing a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, \\n \\nParse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and \\nParse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive \\nInferences, \\n \\nApplications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages:'),\n",
       " Document(metadata={}, page_content='Ambiguous Grammars, Removing Ambiguity From Grammars, Leftmost Derivations as a Way to \\nExpress Ambiguity, Inherent Anbiguity \\nwww.indiansbrain.com\\n \\nUnit – IV \\n \\n(12 hours) \\n \\nPushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical \\nNotation for PDA’s, Instantaneous Descriptions of a PDA, \\n \\nLanguages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack \\nto Final State, From Final State to Empty Stack'),\n",
       " Document(metadata={}, page_content='Equivalence of PDA’s and CFG’s: From Grammars to Pu shdown Automata, From PDA’s to \\nGrammars \\n \\nDeterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and \\nDeterministic PDA’s, DPDA’s and Context-Free La nguages, DPDA’s and Ambiguous Grammars \\n \\nProperties of Context-Free Languages: Normal Forms for Context-Free Grammars, The \\nPumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, \\nDecision Properties of CFL’s \\n \\nUnit – V \\n \\n(12 hours)'),\n",
       " Document(metadata={}, page_content='Introduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for \\nTuring Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, \\nTuring Machines and Halting \\n \\nProgramming Techniques for Turing Machines, Extensions to the Basic Turing Machine, \\nRestricted Turing Machines, Turing Machines and Computers, \\n \\nUndecidability: A Language That is Not Recursively Enumerable, Enumerating the Binary'),\n",
       " Document(metadata={}, page_content='Strings, Codes for Turing Machines, The Diagonalization Language \\n \\nAn Undecidable Problem That Is RE: Recursive Languages, Complements of Recursive and RE \\nlanguages, The Universal Languages, Undecidability of the Universal Language \\n \\nUndecidable Problems About Turing Machines: Reductions, Turing Machines That Accept the \\nEmpty Language. Post’s Correspondence Problem: Definition of Post’s Correspondence Problem, \\nThe “Modified” PCP, Other Undecidable Prob lems: Undecidability of Ambiguity for CFG’s'),\n",
       " Document(metadata={}, page_content='Text Book: \\n \\n1. Introduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, \\nR.Motwani & J.D.Ullman (3rd Edition) – Pearson Education \\n \\nwww.indiansbrain.com\\n \\nUNIT - I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWhat is TOC? \\n \\nIn theoretical computer science, the theory of computation is the branch that deals with \\nwhether and how efficiently problems can be solved on a model of computation, using an \\nalgorithm. The field is divided into three major branches: automata theory, computability theory'),\n",
       " Document(metadata={}, page_content=\"and computational complexity theory. \\n \\nIn order to perform a rigorous study of computation, computer scientists work with a \\nmathematical abstraction of computers called a model of computation. There are several \\nmodels in use, but the most commonly examined is the Turing machine. \\n \\nAutomata theory \\n \\nIn theoretical computer science, automata theory is the study of abstract machines (or more \\nappropriately, abstract 'mathematical' machines or systems) and the computational problems that\"),\n",
       " Document(metadata={}, page_content='can be solved using these machines. These abstract machines are called automata. \\n \\nThis automaton consists of \\n \\n\\uf095 states (represented in the figure by circles),\\uf020\\n\\uf095 and transitions (represented by arrows).\\uf020\\n \\nAs the automaton sees a symbol of input, it makes a transition (or jump) to another state, \\naccording to its transition function (which takes the current state and the recent symbol as \\nits inputs). \\nUses of Automata: compiler design and parsing. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIntroduction to formal proof:'),\n",
       " Document(metadata={}, page_content='Basic Symbols used : \\nU – Union \\n∩- Conjunction \\n \\nϵ - Empty String \\nΦ – NULL set \\n7- negation \\n \\n‘ – compliment \\n= > implies \\nwww.indiansbrain.com\\nAdditive inverse: a+(-a)=0 \\nMultiplicative inverse: a*1/a=1 \\nUniversal set U={1,2,3,4,5} \\nSubset A={1,3} \\nA’ ={2,4,5} \\nAbsorption law: AU(A ∩B) = A, A∩(AUB) = A \\n \\nDe Morgan’s Law: \\n \\n(AUB)’ =A’ ∩ B’ \\n(A∩B)’ = A’ U B’ \\nDouble compliment \\n(A’)’ =A \\n \\nA ∩ A’ = Φ \\n \\nLogic relations: \\na b = > 7a U b \\n7(a∩b)=7a U 7b \\n \\nRelations:'),\n",
       " Document(metadata={}, page_content='Let a and b be two sets a relation R contains aXb. \\nRelations used in TOC: \\nReflexive: a = a \\nSymmetric: aRb = > bRa \\nTransition: aRb, bRc = > aRc \\n \\nIf a given relation is reflexive, symmentric and transitive then the relation is called equivalence \\nrelation. \\n \\nDeductive proof: Consists of sequence of statements whose truth lead us from some \\ninitial statement called the hypothesis or the give statement to a conclusion statement. \\n \\n \\n \\n \\nAdditional forms of proof: \\nProof of sets \\nProof by contradiction'),\n",
       " Document(metadata={}, page_content='Proof by counter example \\n \\nDirect proof (AKA) Constructive proof: \\nIf p is true then q is true \\n \\nEg: if a and b are odd numbers then product is also an odd \\nnumber. Odd number can be represented as 2n+1 \\n \\na=2x+1, b=2y+1 \\nproduct of a X b = (2x+1) X (2y+1) \\n= 2(2xy+x+y)+1 = 2z+1 (odd number) \\nwww.indiansbrain.com\\n \\n \\n \\nProof by contrapositive: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProof by Contradiction: \\n \\nH and not C implies falsehood.'),\n",
       " Document(metadata={}, page_content='Be regarded as an observation than a theorem. \\n \\n \\n \\n \\n \\n \\n \\n \\nFor any sets a,b,c if a∩b = Φ and c is a subset of b the prove that a∩c \\n=Φ Given : a∩b=Φ and c subset b \\n \\nAssume: a∩c  Φ \\n \\nThen \\n \\n \\n= > a∩b Φ = > a∩c=Φ(i.e., the assumption is wrong) \\nwww.indiansbrain.com\\nProof by mathematical Induction: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLanguages : \\n \\nThe languages we consider for our discussion is an abstraction of natural languages. That is,'),\n",
       " Document(metadata={}, page_content='our focus here is on formal languages that need precise and formal definitions. Programming \\nlanguages belong to this category. \\n \\nSymbols : \\n \\nSymbols are indivisible objects or entity that cannot be defined. That is, symbols are the atoms \\n \\nof the world of languages. A symbol is any single object such as begin, or do. \\n \\nAlphabets : \\n \\nAn alphabet is a finite, nonempty set of symbols. The alphabet of a language is \\nnormally denoted by \\n. When more than one alphabets are considered for discussion, \\nthen'),\n",
       " Document(metadata={}, page_content='subscripts may be used (e.g.    \\netc) or sometimes other symbol like G may also be \\n \\nintroduced. \\n \\n \\n \\n \\n \\n \\n \\n \\nExample : \\n \\nStrings or Words over Alphabet : \\n \\nA string or word over an alphabet   \\nis a finite sequence of concatenated symbols of     \\n. \\n , a, 0, 1, #, \\nwww.indiansbrain.com\\nExample : 0110, 11, 001 are three strings over the binary alphabet { 0, 1 } . \\n \\naab, abcb, b, cc are four strings over the alphabet { a, b, c }.'),\n",
       " Document(metadata={}, page_content='It is not the case that a string over some alphabet should contain all the symbols from the alpha-\\nbet. For example, the string cc over the alphabet { a, b, c } does not contain the symbols a and b. \\nHence, it is true that a string over an alphabet is also a string over any superset of that alphabet. \\n \\nLength of a string : \\nThe number of symbols in a string w is called its length, denoted by |w|. \\n \\nExample : | 011 | = 4, |11| = 2, | b | = 1'),\n",
       " Document(metadata={}, page_content='Convention : We will use small case letters towards the beginning of the English alphabet \\nto denote symbols of an alphabet and small case letters towards the end to \\n \\ndenote strings over an alphabet. That is, \\n (symbols) and \\n \\nare strings. \\n \\nSome String Operations : \\nLet \\nand \\nbe two strings. The concatenation of x and y \\n \\ndenoted by xy, is the string   \\n. That is, the concatenation of x and y \\n \\ndenoted by xy is the string that has a copy of x followed by a copy of y without any intervening'),\n",
       " Document(metadata={}, page_content='space between them. \\n \\nExample : Consider the string 011 over the binary alphabet. All the prefixes, suffixes and \\nsubstrings of this string are listed below. \\n \\nPrefixes:  , 0, 01, 011. \\n \\nSuffixes:  , 1, 11, 011. \\n \\nSubstrings:  , 0, 1, 01, 11, 011. \\n \\nNote that x is a prefix (suffix or substring) to x, for any string x and is a prefix (suffix or \\nsubstring) to any string. \\n \\nA string x is a proper prefix (suffix) of string y if x is a prefix (suffix) of y and x 蝤 y.'),\n",
       " Document(metadata={}, page_content='In the above example, all prefixes except 011 are proper prefixes. \\n \\nPowers of Strings : For any string x and integer \\n, we use \\nto denote the string formed \\nby sequentially concatenating n copies of x. We can also give an inductive \\ndefinition of \\nas follows: \\n= e, if n \\n= 0 ; otherwise \\n \\nwww.indiansbrain.com\\nExample : If x = 011, then  \\n= 011011011,  \\n= 011 and \\n \\nPowers of Alphabets : \\n \\nWe write \\n(for some integer k) to denote the set of strings of length k with symbols \\n \\nfrom \\n. In other words,'),\n",
       " Document(metadata={}, page_content='= { w | w is a string over  \\n and | w | = k}. Hence, for any alphabet,   \\ndenotes the set \\n \\nof all strings of length zero. That is,   \\n= { e }. For the binary alphabet { 0, 1 } we have \\n \\nthe following. \\n \\n \\n \\n \\n \\n \\n \\n \\nThe set of all strings over an alphabet \\n is denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nThe set  \\n contains all the strings that can be generated by iteratively concatenating sym- \\nbols from      \\nany number of times. \\n \\nExample : If \\n= { a, b }, then'),\n",
       " Document(metadata={}, page_content='= {  , a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, …}. \\n \\nPlease note that if \\n, then \\n that is \\n. It may look odd that one can proceed \\nfrom the empty set to a non-empty set by iterated concatenation. But there is a reason for this \\nand we accept this convention \\n \\nThe set of all nonempty strings over an alphabet     \\nis denoted by \\n. That is, \\n \\n \\n \\n \\n \\n \\nNote that  \\nis infinite. It contains no infinite strings but strings of arbitrary lengths. \\n \\nReversal : \\nFor any string'),\n",
       " Document(metadata={}, page_content='the reversal of the string is \\n. \\n \\nAn inductive definition of reversal can be given as follows: \\nwww.indiansbrain.com\\nLanguages : \\nA language over an alphabet is a set of strings over \\nthat alphabet. Therefore, a \\n \\nlanguage L is any subset of \\n. That is, any \\nis a language. \\nExample : \\n \\n \\n1. F is the empty language.  \\n \\n2.\\nis a language for any \\n. \\n \\n3. {e} is a language for any \\n. Note that, \\n. Because the language F does not \\n \\ncontain any string but {e} contains one string of length zero.'),\n",
       " Document(metadata={}, page_content=\"4. The set of all strings over { 0, 1 } containing equal number of 0's and 1's. \\n \\n5. The set of all strings over {a, b, c} that starts with a. \\n \\nConvention : Capital letters A, B, C, L, etc. with or without subscripts are normally used \\nto denote languages. \\n \\nSet operations on languages : Since languages are set of strings we can apply set operations to \\nlanguages. Here are some simple examples (though there is nothing new in it). \\n \\n \\nUnion : A string \\n \\n \\n \\n \\n \\n \\niff \\nor\"),\n",
       " Document(metadata={}, page_content='Example : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 0, 11, 01, 011, 111 } \\nIntersection  : \\nA   string, \\nxϵ  L1 \\n∩ L2 \\niff    x   ϵ  L1    and   x   ϵ  L2    .\\nExample : { 0, 11, 01, 011 } \\n{ 1, 01, 110 } = { 01 } \\n \\nComplement :  Usually, \\nis the universe that a complement is taken with respect to. \\n \\nThus for a language L, the complement is L(bar) = { \\n| \\n}. \\n \\nExample : Let L = { x | |x| is even }. Then its complement is the language { \\n| |x| is \\n \\nodd }.'),\n",
       " Document(metadata={}, page_content='Similarly we can define other usual set operations on languages like relative \\ncom-plement, symmetric difference, etc. \\n \\nReversal of a language : \\nThe reversal of a language L, denoted as \\n, is defined as:  \\n. \\n \\nExample : \\n \\n1.  Let L = { 0, 11, 01, 011 }. Then \\n= { 0, 11, 10, 110 }.\\nwww.indiansbrain.com\\n2.  Let L = { \\n| n is an integer }. Then  \\n=  { \\n| n is an integer }. \\n \\nLanguage concatenation : The concatenation of languages       \\nand \\nis defined as \\n \\n= { xy | \\nand \\n}.'),\n",
       " Document(metadata={}, page_content='Example : { a, ab }{ b, ba } = { ab, aba, abb, abba }. \\n \\nNote that , \\n1. \\n  in general. \\n2. \\n \\n \\n3. \\n \\n \\nIterated concatenation of languages : Since we can concatenate two languages, we also repeat this to \\nconcatenate any number of languages. Or we can concatenate a language with itself any \\n \\nnumber of times. The operation L with itself n times. This is \\ndefined formally as follows: \\n \\n \\n \\n \\n \\n \\nExample :  Let L = { a, ab }. Then according to the definition, \\nwe have \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand so on.'),\n",
       " Document(metadata={}, page_content=\"Kleene's Star operation : The Kleene star operation on a language L, denoted as is defined as \\nfollows : \\n \\n= ( Union n in N ) \\n \\n= \\n \\n \\n= { x | x is the concatenation of zero or more strings from L } \\n \\ndenotes the concatenation of \\nwww.indiansbrain.com\\nThus \\nis the set of all strings derivable by any number of concatenations of strings in \\nL. It is also useful to define \\n \\n=, i.e., all strings derivable by one or more concatenations of strings in L. That is \\n \\n= (Union n in N and n >0) \\n \\n=\"),\n",
       " Document(metadata={}, page_content='Example :  Let L = { a, ab }. Then we have, \\n \\n= \\n \\n= {e} \\n{a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\n= \\n \\n= {a, ab} \\n{aa, aab, aba, abab} \\n… \\n \\nNote : \\nis in  \\n, for every language L, including . \\n \\nThe previously introduced definition of   \\nis an instance of Kleene star. \\n \\n \\n \\n \\n \\n(Generates) \\n(Recognizes) \\nGrammar \\nLanguage \\n  Automata \\n \\nAutomata: A algorithm or program that automatically recognizes if a particular string belongs to \\nthe language or not, by checking the grammar of the string.'),\n",
       " Document(metadata={}, page_content='An automata is an abstract computing device (or machine). There are different varities of such \\nabstract machines (also called models of computation) which can be defined mathematically. \\n \\nEvery Automaton fulfills the three basic requirements. \\n \\n• \\nEvery automaton consists of some essential features as in real computers. It has a mech-\\nanism for reading input. The input is assumed to be a sequence of symbols over a given'),\n",
       " Document(metadata={}, page_content='alphabet and is placed on an input tape(or written on an input file). The simpler automata \\ncan only read the input one symbol at a time from left to right but not change. Powerful \\nversions can both read (from left to right or right to left) and change the input. \\nwww.indiansbrain.com\\n\\uf095 The automaton can produce output of some form. If the output in response to an input \\nstring is binary (say, accept or reject), then it is called an accepter. If it produces an out-'),\n",
       " Document(metadata={}, page_content='put sequence in response to an input sequence, then it is called a transducer(or \\nautomaton with output).\\uf020\\n\\uf020\\n• \\nThe automaton may have a temporary storage, consisting of an unlimited number of \\ncells, each capable of holding a symbol from an alphabet ( whcih may be different from \\nthe input alphabet). The automaton can both read and change the contents of the storage \\ncells in the temporary storage. The accusing capability of this storage varies depending \\non the type of the storage. \\n \\n•'),\n",
       " Document(metadata={}, page_content='The most important feature of the automaton is its control unit, which can be in any \\none of a finite number of interval states at any point. It can change state in some de-\\nfined manner determined by a transition function. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1: The figure above shows a diagrammatic representation of a generic \\nautoma-tion. \\n \\nOperation of the automation is defined as follows. \\n \\nAt any point of time the automaton is in some integral state and is reading a particular symbol'),\n",
       " Document(metadata={}, page_content='from the input tape by using the mechanism for reading input. In the next time step the automa-\\nton then moves to some other integral (or remain in the same state) as defined by the transition \\nfunction. The transition function is based on the current state, input symbol read, and the content \\nof the temporary storage. At the same time the content of the storage may be changed and the \\ninput read may be modifed. The automation may also produce some output during this transition.'),\n",
       " Document(metadata={}, page_content='The internal state, input and the content of storage at any point defines the configuration of the \\nautomaton at that point. The transition from one configuration to the next ( as defined by the \\ntransition function) is called a move. Finite state machine or Finite Automation is the simplest \\ntype of abstract machine we consider. Any system that is at any point of time in one of a finite \\nnumber of interval state and moves among these states in a defined manner in response to some'),\n",
       " Document(metadata={}, page_content='input, can be modeled by a finite automaton. It doesnot have any temporary storage and hence a \\nrestricted model of computation. \\nwww.indiansbrain.com\\nFinite Automata \\n \\nAutomata (singular : automation) are a particularly simple, but useful, model of compu-\\ntation. They were initially proposed as a simple model for the behavior of neurons. \\n \\nStates, Transitions and Finite-State Transition System : \\n \\n \\nLet us first give some intuitive idea about a state of a system and state transitions before'),\n",
       " Document(metadata={}, page_content='describing finite automata. \\n \\nInformally, a state of a system is an instantaneous description of that system which gives all \\nrelevant information necessary to determine how the system can evolve from that point on. \\n \\nTransitions are changes of states that can occur spontaneously or in response to inputs to the \\nstates. Though transitions usually take time, we assume that state transitions are instantaneous \\n(which is an abstraction).'),\n",
       " Document(metadata={}, page_content='Some examples of state transition systems are: digital systems, vending machines, etc. A system \\n \\ncontaining only a finite number of states and transitions among them is \\ncalled a finite-state transition system. \\n \\nFinite-state transition systems can be modeled abstractly by a mathematical model called \\nfinite automation \\n \\nDeterministic Finite (-state) Automata \\n \\nInformally, a DFA (Deterministic Finite State Automaton) is a simple machine that reads an in-'),\n",
       " Document(metadata={}, page_content='put string -- one symbol at a time -- and then, after the input has been completely read, decides \\nwhether to accept or reject the input. As the symbols are read from the tape, the automaton can \\nchange its state, to reflect how it reacts to what it has seen so far. A machine for which a deter-\\nministic code can be formulated, and if there is only one unique way to formulate the code, then \\nthe machine is called deterministic finite automata. \\n \\nThus, a DFA conceptually consists of 3 parts:'),\n",
       " Document(metadata={}, page_content='1. A tape to hold the input string. The tape is divided into a finite number of cells. Each \\ncell holds a symbol from \\n. \\n2. A tape head for reading symbols from the tape \\n3. A control , which itself consists of 3 things: \\n \\no \\nfinite number of states that the machine is allowed to be in (zero or more states \\nare designated as accept or final states), \\n \\no a current state, initially set to a start state, \\nwww.indiansbrain.com\\no a state transition function for changing the current state.'),\n",
       " Document(metadata={}, page_content='An automaton processes a string on the tape by repeating the following actions until the \\ntape head has traversed the entire string: \\n \\n1. The tape head reads the current tape cell and sends the symbol s found there to \\nthe control. Then the tape head moves to the next cell. \\n \\n2. he control takes s and the current state and consults the state transition function to \\nget the next state, which becomes the new current state.'),\n",
       " Document(metadata={}, page_content='Once the entire string has been processed, the state in which the automation enters is examined. \\n \\nIf it is an accept state , the input string is accepted ; otherwise, the string is rejected . Summariz- \\n \\ning all the above we can formulate the following formal definition: \\n \\n \\nDeterministic Finite State Automaton : A Deterministic Finite State Automaton (DFA) is \\n \\na 5-tuple : \\n \\n\\uf095 Q is a finite set of states.\\uf020\\n• \\nis a finite set of input symbols or alphabet \\n \\n\\uf095'),\n",
       " Document(metadata={}, page_content='is the “next state” transition function (which is total ). Intuitively, \\nis\\n a \\nfunction that tells which state to move to in response to an input, i.e., if M is in \\n \\nstate q and sees input a, it moves to state      \\n. \\n \\n\\uf095 \\nis the start state.\\uf020\\n• \\nis the set of accept or final states. \\n \\nAcceptance of Strings : \\n \\nA DFA accepts a string   \\nif there is a sequence of states     \\nin Q \\n \\nsuch that \\n \\n1.  \\nis the start state. \\n2.  \\nfor all \\n. \\n \\n3. \\n \\nLanguage Accepted or Recognized by a DFA :'),\n",
       " Document(metadata={}, page_content='The language accepted or recognized by a DFA M is the set of all strings accepted by M , and \\n \\nis denoted by \\ni.e. \\nThe  notion \\nof \\nacceptance can also be made more precise by extending the transition function \\n.\\n \\nExtended transition function : \\nwww.indiansbrain.com\\nExtend \\n(which is function on symbols) to a function on strings, i.e. . \\n \\n \\n \\nThat is, \\n is the state the automation reaches when it starts from the state q and finish'),\n",
       " Document(metadata={}, page_content='processing the string w. Formally, we can give an inductive definition as follows: \\n \\nThe language of the DFA M is the set of strings that can take the start state to one of \\nthe accepting states i.e. \\n \\n \\nL(M) = { \\n| M accepts w } \\n \\n= {\\n| \\n} \\n \\n \\nExample 1 : \\n \\n \\n \\n \\n \\n \\n \\n \\nis the start state \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is a formal description of a DFA. But it is hard to comprehend. For ex. The language of the \\nDFA is any string over { 0, 1} having at least one 1'),\n",
       " Document(metadata={}, page_content=\"We can describe the same DFA by transition table or state transition diagram as follow-\\ning: \\n \\n \\n \\n \\nTransition Table : \\n \\n0    1 \\nwww.indiansbrain.com\\n \\n \\nIt is easy to comprehend the transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExplanation :  We cannot \\nreach find state \\nw/0 or in the i/p string. There can be any no. \\nof 0's at the beginning. \\n( The self-loop at \\non label 0 indicates it ). Similarly there \\ncan be any no. of 0's & 1's in any order at the end of the string. \\n \\nTransition table :\"),\n",
       " Document(metadata={}, page_content='It is basically a tabular representation of the transition function that takes two arguments (a \\nstate and a symbol) and returns a value (the “next state”). \\n \\n• \\nRows correspond to states, \\n• \\nColumns correspond to input symbols, \\n• \\nEntries correspond to next states \\n• \\nThe start state is marked with an arrow \\n• \\nThe accept states are marked with a star (*). \\n \\n \\n \\n0    1 \\n \\n \\n \\n(State) Transition diagram : \\n \\nA state transition diagram or simply a transition diagram is a directed graph which can'),\n",
       " Document(metadata={}, page_content='be constructed as follows: \\n \\n1.  For each state in Q there is a node. \\n2. There is a directed edge from node q to node p labeled a iff \\n . (If there are \\nseveral input symbols that cause a transition, the edge is labeled by the list of these \\nsymbols.) \\n3. There is an arrow with no source into the start state. \\n4. Accepting states are indicated by double circle. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n5. \\n \\n6. Here is an informal description how a DFA operates. An input to a DFA can be any \\n \\nstring.'),\n",
       " Document(metadata={}, page_content='Put a pointer to the start state q. Read the input string w from left \\n \\nto right, one symbol at a time, moving the pointer according to the transition  \\n \\nfunction, \\n.  If the next symbol of w is a and the pointer is on state p, move the \\n \\npointer to \\n. When the end of the input string w is encountered, the pointer is on \\n \\nsome state, r. The string is said to be accepted by the DFA if \\nand \\n \\nrejected if \\n. Note that there is no formal mechanism for moving the pointer. \\n7. A language'),\n",
       " Document(metadata={}, page_content='is said to be regular if L = L(M) for some DFA M. \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying \\ncertain recursive rules as given below. (In the definition) \\n \\nDefinition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) is a RE \\n \\nii) is a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\n \\nIf'),\n",
       " Document(metadata={}, page_content='and \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\nwww.indiansbrain.com\\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive \\nREs) by a finite no of applications of the recursive step (given in 2). \\n \\nExample : Let \\n = { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this \\nexpression by applying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+'),\n",
       " Document(metadata={}, page_content='Rule 2(i) & Results of Step 1, 2 \\n4 \\n(1+  ) \\nRule 2(iv) & Step 3 \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11 \\n \\nLanguage described by REs : Each describes a language (or a language is associated \\nwith every RE). We will see later that REs are used to attribute regular languages. \\n \\n \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We'),\n",
       " Document(metadata={}, page_content='can define the language L(r) associated with (or described by) a REs as follows. \\n \\n1. is the RE describing the empty language i.e. L( ) = . \\n \\n2. is a RE describing the language {\\n} i.e. L( ) = {\\n} . \\n \\n3. \\n, a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\nwww.indiansbrain.com\\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii)'),\n",
       " Document(metadata={}, page_content='is a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is \\n \\nL(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,. } {0} \\n{1} \\n \\n= {  , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule'),\n",
       " Document(metadata={}, page_content='Consider the RE ab + c. The language described by the RE can be thought of either \\nL(a)L(b+c) or L (ab) L(c) as provided by the rules (of languages described by REs) \\ngiven already. But these two represents two different languages lending to \\nambiguity. To remove this ambiguity we can either \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. \\nLike other algebras mod in mathematics.'),\n",
       " Document(metadata={}, page_content='For REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) \\noperator. \\n \\nii) It is also important to note that concatenation & union (+) operators are \\nassociative and union operation is commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language \\nL(ab) L(c) i.e. it should be grouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For'),\n",
       " Document(metadata={}, page_content='example, the language represented by the RE a(b+c) is L(a)L(b+c). \\nwww.indiansbrain.com\\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language \\nL(a)(L(b))*\\nL(b) \\n \\nExample : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all \\nstrings over {0,1} which are either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an'),\n",
       " Document(metadata={}, page_content=\"even number of 0's followed by an odd number of 1's i.e. \\n \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE \\nrr, \\ndenotes \\nr, and so on. \\n \\nAn arbitrary string over \\n= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of \\nconsecutive 1's} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and \\nwhat goes before is completely arbitrary. Considering these observations we can write\"),\n",
       " Document(metadata={}, page_content=\"the REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE \\n(0+1)*11(0+1)*+(0+1)*00(0+1)* represents the set of string over {0,1} that contains \\nthe substring 11 or 00. \\n \\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the \\nset of strings over {0,1} that contains exactly two 1's. The presence of two 1's in the \\nRE and any no of 0's before, between and after the 1's ensure it.\"),\n",
       " Document(metadata={}, page_content=\"Example : Consider the language of strings over {0,1} containing two or more 1's. \\n \\nSolution : There must be at least two 1's in the RE somewhere and what comes before, \\nbetween, and after is completely arbitrary. Hence we can write the RE as \\n(0+1)*1(0+1)*1(0+1)* . But following two REs also represent the same language, each \\nensuring presence of least two 1's somewhere in the string \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\nwww.indiansbrain.com\\nL(r) = {\"),\n",
       " Document(metadata={}, page_content=\"has no pair of consecutive 1's} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We \\nobserver that, whenever a 1 occurs, it must be immediately followed by a 0. This \\nsubstring may be preceded & followed by any no of 0's. So the final RE must be a \\nrepetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the RE is \\n(0*100*)*. But in this case the strings ending in 1 or consisting of all 0's are not\"),\n",
       " Document(metadata={}, page_content='accounted for. Taking these observations into consideration, the final RE is r = \\n(0*100*)(1+ )+0*(1+\\n). \\n \\nAlternative Solution : \\n \\nThe language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as \\nr = (0+10)*(1+ ).This is a shorter expression but represents the same language. \\n \\nRegular Expression and Regular Language : \\n \\nEquivalence(of REs) with FA : \\n \\nRecall that, language that is accepted by some FAs are known as Regular language.'),\n",
       " Document(metadata={}, page_content='The two concepts : REs and Regular language are essentially same i.e. (for) every \\nregular language can be developed by (there is) a RE, and for every RE there is a \\nRegular Langauge. This fact is rather suprising, because RE approach to describing \\nlanguage is fundamentally differnet from the FA approach. But REs and FA are \\nequivalent in their descriptive power. We can put this fact in the focus of the following \\nTheorem. \\n \\nTheorem : A language is regular iff some RE describes it.'),\n",
       " Document(metadata={}, page_content='This Theorem has two directions, and are stated & proved below as a separate lemma \\n \\n \\nRE to FA : \\n \\nREs denote regular languages : \\n \\nLemma : If L(r) is a language described by the RE r, then it is regular i.e. there is a FA \\nsuch that L(M)\\nL(r). \\n \\nProof : To prove the lemma, we apply structured index on the expression r. First, we \\n \\nshow how to construct FA for the basis elements: \\n, and for any \\n. Then we show \\nhow to combine these Finite Automata into Complex Automata that accept the Union,'),\n",
       " Document(metadata={}, page_content='Concatenation, Kleen Closure of the languages accepted by the original smaller \\nautomata. \\nwww.indiansbrain.com\\nUse of NFAs is helpful in the case i.e. we construct NFAs for every REs which are \\nrepresented by transition diagram only. \\n \\nBasis : \\n \\n\\uf095 Case (i) : \\n. Then \\n. Then \\nand the following NFA N \\nrecognizes L(r). Formally \\nwhere Q = {q} \\nand \\n.\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf095 Case (ii) : \\n. \\n, and the following NFA N accepts L(r). Formally\\uf020\\nwhere \\n.'),\n",
       " Document(metadata={}, page_content='Since the start state is also the accept step, and there is no any transition defined, it \\nwill accept the only string \\nand nothing else. \\n \\n\\uf095 Case (iii) : r = a for some \\n. Then L(r) = {a}, and the following NFA \\nN accepts L(r).\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFormally, \\nwhere \\nfor \\nor \\n \\n \\n \\n \\n \\nInduction : \\nwww.indiansbrain.com\\nAssume that the start of the theorem is true for REs \\nand \\n. Hence we can assume \\nthat we have automata \\nand \\nthat accepts languages denoted by REs \\nand \\n, \\n \\nrespectively i.e. \\nand'),\n",
       " Document(metadata={}, page_content='. The FAs are represented \\nschematically as shown below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEach has an initial state and a final state. There are four cases to consider. \\n \\n\\uf095 Case (i) : Consider the RE \\ndenoting the language \\n. We \\nconstruct FA \\n, from \\nand \\nto accept the language denoted by RE \\nas \\nfollows :\\uf020\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCreate a new (initial) start state \\nand give \\n- transition to the initial state of \\n and \\n.This is the initial state of \\n. \\n \\n\\uf095 Create a final state \\nand give'),\n",
       " Document(metadata={}, page_content='-transition from the two final state of \\nand \\n. \\nis the only final state of \\nand final state of \\nand \\nwill be \\nordinary states in \\n.\\uf020\\n\\uf095 All the state of \\nand \\nare also state of \\n.\\uf020\\nwww.indiansbrain.com\\n\\uf095 All the moves of \\nand \\nare also moves of \\n. [ Formal Construction] \\n \\n \\nIt is easy to prove that \\n \\nProof: To show that \\nwe must show that \\n \\n= \\n \\n \\n= \\nby following transition of \\n \\n \\nStarts at initial state \\nand enters the start state of either \\nor \\nfollwoing the'),\n",
       " Document(metadata={}, page_content='transition i.e. without consuming any input. WLOG, assume that, it enters the start state \\nof \\n. From this point onward it has to follow only the transition of \\nto enter the final \\n \\nstate of \\n, because this is the only way to enter the final state of M by following the e-\\ntransition.(Which is the last transition & no input is taken at hte transition). Hence the \\n \\nwhole input w is considered while traversing from the start state of \\nto the final \\nstate of \\n. Therefore \\nmust accept \\n. \\n \\nSay, \\nor \\n.'),\n",
       " Document(metadata={}, page_content='WLOG, say \\n \\nTherefore when \\nprocess the string w , it starts at the initial state and enters the final \\nstate when w consumed totally, by following its transition. Then \\nalso accepts w, by \\nstarting at state \\nand taking \\n-transition enters the start state of \\n-follows the moves \\n \\nof \\nto enter the final state of \\nconsuming input w thus takes -transition to \\n. Hence proved \\n \\n\\uf095 Case(ii) : Consider the RE \\ndenoting the language \\n. We construct \\nFA \\nfrom \\n& \\nto accept \\nas follows :\\uf020\\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='Create a new start state \\nand a new final state \\n \\n1. Add \\n- transition from \\no \\nto the start state of \\n \\n \\no \\nto \\n \\n \\no final state of \\nto the start state of \\n \\n \\n2. All the states of \\nare also the states of \\n. \\nhas 2 more states than that of \\nnamely \\nand \\n. \\n3. All the moves of \\nare also included in \\n. \\n \\nBy the transition of type (b), \\ncan accept . \\nBy the transition of type (a), \\ncan enters the initial state of \\nw/o any input and then \\nfollow all kinds moves of'),\n",
       " Document(metadata={}, page_content='to enter the final state of \\nand then following \\n-transition \\ncan enter \\n. Hence if any \\nis accepted by \\nthen w is also accepted by \\n. By the \\ntransition of type (b), strings accepted by \\ncan be repeated by any no of times & thus \\naccepted by \\n. Hence \\naccepts \\nand any string accepted by \\nrepeated (i.e. \\n \\nconcatenated) any no of times. Hence \\n \\nCase(iv) : Let \\n=(\\n). Then the FA \\nis also the FA for (\\n), since the use of \\nparentheses does not change the language denoted by the expression'),\n",
       " Document(metadata={}, page_content='Non-Deterministic Finite Automata \\nNondeterminism is an important abstraction in computer science. Importance of \\nnondeterminism is found in the design of algorithms. For examples, there are many \\nproblems with efficient nondeterministic solutions but no known efficient deterministic \\nsolutions. ( Travelling salesman, Hamiltonean cycle, clique, etc). Behaviour of a process \\nis in a distributed system is also a good example of nondeterministic situation. Because \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='the behaviour of a process might depend on some messages from other processes \\nthat might arrive at arbitrary times with arbitrary contents. \\nIt is easy to construct and comprehend an NFA than DFA for a given regular \\nlanguage. The concept of NFA can also be used in proving many theorems and \\nresults. Hence, it plays an important role in this subject. \\nIn the context of FA nondeterminism can be incorporated naturally. That is, an NFA is'),\n",
       " Document(metadata={}, page_content='defined in the same way as the DFA but with the following two exceptions: \\n\\uf095 multiple next state.\\uf020\\n\\uf020\\n\\uf095 \\n- transitions.\\uf020\\n \\nMultiple Next State : \\n \\n\\uf095 In contrast to a DFA, the next state is not necessarily uniquely determined by the \\ncurrent state and input symbol in case of an NFA. (Recall that, in a DFA there is \\nexactly one start state and exactly one transition out of every state for each \\nsymbol in \\n).\\uf020\\n\\uf095 \\nThis means that - in a state q and with input symbol a - there could be one, more'),\n",
       " Document(metadata={}, page_content='than one or zero next state to go, i.e. the value of \\nis a subset of Q. Thus\\uf020\\n \\n= \\nwhich means that any one of \\ncould be the next \\nstate. \\n \\n\\uf095 The zero next state case is a special one giving \\n=\\n, which means that \\nthere is no next state on input symbol when the automata is in state q. In such \\na case, we may think that the automata \"hangs\" and the input will be rejected.\\uf020\\n \\n- transitions : \\n \\nIn an -transition, the tape head doesn\\'t do anything- it doesnot read and it doesnot move.'),\n",
       " Document(metadata={}, page_content='However, the state of the automata can be changed - that is can go to zero, one \\n \\nor more states. This is written formally as \\nimplying that the next \\nstate could by any one of \\nw/o consuming the next input symbol. \\n \\n \\n \\n \\nAcceptance : \\n \\nInformally, an NFA is said to accept its input \\nif it is possible to start in some start state \\nand process \\n, moving according to the transition rules and making choices along the way \\nwhenever the next state is not uniquely defined, such that when'),\n",
       " Document(metadata={}, page_content='is completely processed \\n(i.e. end of \\nis reached), the automata is in an accept state. There may be several \\npossible paths through the automation in response to an input \\nsince the start state is not \\ndetermined and there are choices along the way because of multiple next states. Some of \\nthese paths may lead to accpet states while others may not. The \\nwww.indiansbrain.com\\nautomation is said to accept \\nif at least one computation path on input \\nstarting from at'),\n",
       " Document(metadata={}, page_content='least one start state leads to an accept state- otherwise, the automation rejects input \\n. \\nAlternatively, we can say that, \\nis accepted iff there exists a path with label \\nfrom some \\nstart state to some accept state. Since there is no mechanism for determining which state \\nto start in or which of the possible next moves to take (including the \\n- transitions) in \\nresponse to an input symbol we can think that the automation is having some \"guessing\"'),\n",
       " Document(metadata={}, page_content='power to chose the correct one in case the input is accepted \\n \\nExample 1 : Consider the language L = {\\n {0, 1}* | The 3rd symbol from the right \\nis 1}. The following four-state automation accepts L. \\n \\nThe m/c is not deterministic since there are two transitions from state \\non input 1 \\nand no transition (zero transition) from \\non both 0 & 1. \\n \\nFor any string \\nwhose 3rd symbol from the right is a 1, there exists a sequence of legal \\ntransitions leading from the start state q, to the accept state'),\n",
       " Document(metadata={}, page_content='. But for any string \\nwhere 3rd symbol from the right is 0, there is no possible sequence of legal \\n \\ntranisitons leading from \\nand \\n. Hence m/c accepts L. How does it accept any string \\nL? \\n \\nFormal definition of NFA : \\n \\nFormally, an NFA is a quituple \\nwhere Q, \\n, \\n, and F bear \\nthe same meaning as for a DFA, but \\n, the transition function is redefined as follows: \\n \\n \\n \\n \\nwhere P(Q) is the power set of Q i.e. \\n. \\n \\nThe Langauge of an NFA :'),\n",
       " Document(metadata={}, page_content='From the discussion of the acceptance by an NFA, we can give the formal definition of a \\nlanguage accepted by an NFA as follows : \\n \\nIf \\nis an NFA, then the langauge accepted by N is writtten as L(N) \\nis given by \\n. \\n \\nThat is, L(N) is the set of all strings w in \\nsuch that \\ncontains at least \\none accepting state. \\nwww.indiansbrain.com\\nRemoving ϵ-transition: \\n \\n- transitions do not increase the power of an NFA . That is, any \\n- NFA ( NFA with \\ntransition), we can always construct an equivalent NFA without'),\n",
       " Document(metadata={}, page_content='-transitions. The \\n \\nequivalent NFA must keep track where the \\nNFA goes at every step during \\ncomputation. This can be done by adding extra transitions for removal of every \\n- \\ntransitions from the - NFA as follows. \\n \\nIf we removed the \\n- transition \\nfrom the - NFA , then we need to moves \\n \\nfrom state p to all the state on input symbol \\nwhich are reachable from state q \\n(in the - NFA ) on same input symbol q. This will allow the modified NFA to move'),\n",
       " Document(metadata={}, page_content=\"from state p to all states on some input symbols which were possible in case of \\n-NFA \\non the same input symbol. This process is stated formally in the following theories. \\n \\nTheorem if L is accepted by an - NFA N , then there is some \\nequivalent \\nwithout transitions accepting the same language L \\nProof: \\n \\nLet \\nbe the given \\nwith \\n \\n \\nWe construct \\n \\nWhere, \\nfor all \\nand \\nand \\n \\n \\n \\n \\n \\nOther elements of N' and N \\n \\nWe can show that \\ni.e. N' and N are equivalent. \\n \\nWe need to prove that \\n \\n \\n i.e.\"),\n",
       " Document(metadata={}, page_content='We will show something more, that is, \\nwww.indiansbrain.com\\nWe will show something more, that is, \\n \\n \\nBasis : \\n, then \\n \\n \\nBut \\nby definition of \\n. \\n \\nInduction hypothesis Let the statement hold for all \\nwith \\n. \\n \\n \\nBy definition of extension of \\n \\n \\nBy inductions hypothesis. \\n \\nAssuming that \\n \\n \\n \\n \\nBy definition of \\n \\n \\nSince \\n \\n \\n \\nTo complete the proof we consider the case \\n \\nWhen \\ni.e. \\nthen \\nwww.indiansbrain.com\\n \\nand by the construction of \\nwherever \\nconstrains a state in F. \\n \\nIf \\n(and thus'),\n",
       " Document(metadata={}, page_content=\"is not in F ), then \\nwith \\nleads to an accepting state in N' iff it \\nlead to an accepting state in N ( by the construction of N' and N ). \\n \\nAlso, if (\\n , thus w is accepted by N' iff w is accepted by N (iff \\n) \\n \\nIf \\n(and, thus in M we load \\nin F ), thus is accepted by both N' and N . \\n \\nLet \\n. If w cannot lead to \\nin N , then \\n. (Since can add transitions to get an accept \\nstate). So there is no harm in making \\nan accept state in N'. \\n \\nEx: Consider the following NFA with \\n- transition.\"),\n",
       " Document(metadata={}, page_content=\"Transition Diagram \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTransition diagram for \\n' for the equivalent NFA without - moves \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n          1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSince \\nthe start state q0 must be final state in the equivalent NFA . \\n \\nSince \\nand \\nand \\nwe add moves \\nand \\nin the equivalent NFA . Other moves are also constructed accordingly. \\n \\n-closures: \\n \\nThe concept used in the above construction can be made more formal by defining the\"),\n",
       " Document(metadata={}, page_content=\"-closure for a state (or a set of states). The idea of \\n-closure is that, when moving \\n \\nfrom a state p to a state q (or from a set of states Si to a set of states Sj ) an input \\n, we need to take account of all \\n-moves that could be made after the transition. \\nFormally, for a given state q, \\n \\n \\n-closures: \\n \\nSimilarly, for a given set \\n \\n \\n-closures: \\n \\n \\n \\nSo, in the construction of equivalent NFA N' without -transition from any NFA with \\n \\nmoves. the first rule can now be written as\"),\n",
       " Document(metadata={}, page_content='www.indiansbrain.com\\nEquivalence of NFA and DFA \\n \\nIt is worth noting that a DFA is a special type of NFA and hence the class of languages \\naccepted by DFA s is a subset of the class of languages accepted by NFA s. \\nSurprisingly, these two classes are in fact equal. NFA s appeared to have more power \\nthan DFA s because of generality enjoyed in terms of \\n-transition and multiple next \\nstates. But they are no more powerful than DFA s in terms of the languages they \\naccept. \\n \\nConverting DFA to NFA'),\n",
       " Document(metadata={}, page_content='Theorem: Every DFA has as equivalent NFA \\n \\nProof: A DFA is just a special type of an NFA . In a DFA , the transition functions is \\ndefined from \\nwhereas in case of an NFA it is defined from \\nand \\nbe a DFA . We construct an equivalent NFA \\nas follows. \\n \\n \\n \\n \\ni. e \\n \\nIf \\nand \\n \\nAll other elements of N are as in D. \\n \\nIf \\nthen there is a sequence of states \\nsuch that \\n \\n \\n \\nThen it is clear from the above construction of N that there is a sequence of states (in N) \\nsuch that \\nand \\nand hence'),\n",
       " Document(metadata={}, page_content='Similarly we can show the converse. \\n \\nHence , \\n \\n \\nGiven any NFA we need to construct as equivalent DFA i.e. the DFA need to simulate \\nthe behaviour of the NFA . For this, the DFA have to keep track of all the states where \\nthe NFA could be in at every step during processing a given input string. \\nwww.indiansbrain.com\\nThere are \\npossible subsets of states for any NFA with n states. Every subset \\ncorresponds to one of the possibilities that the equivalent DFA must keep track of. Thus,'),\n",
       " Document(metadata={}, page_content='the equivalent DFA will have \\nstates. \\n \\nThe formal constructions of an equivalent DFA for any NFA is given below. We \\nfirst consider an NFA without \\ntransitions and then we incorporate the affects of \\ntransitions later. \\n \\nFormal construction of an equivalent DFA for a given NFA without transitions. \\n \\nGiven an \\nwithout - moves, we construct an equivalent DFA \\n \\n \\nas follows \\n \\ni.e. \\n \\n \\n \\n \\n \\n(i.e. every subset of Q which as an element in F is considered as a final stat\\nin DFA D ) \\n \\n \\n \\n \\nfor all \\nand'),\n",
       " Document(metadata={}, page_content='where \\n \\n \\nThat is, \\n \\n \\nTo show that this construction works we need to show that L(D)=L(N) i.e. \\n \\n \\n \\n \\n \\n \\nOr,\\n \\n \\nWe will prove the following which is a stranger statement thus required. \\nwww.indiansbrain.com\\n \\n \\nProof : We will show by inductions on \\n \\n \\nBasis If \\n=0, then w =  \\n \\nSo, \\nby definition. \\n \\nInductions hypothesis : Assume inductively that the statement holds \\nof \\nlength less than or equal to n. \\n \\nInductive step \\n \\nLet \\n, then \\nwith \\n \\n \\nNow,'),\n",
       " Document(metadata={}, page_content='Now, given any NFA with -transition, we can first construct an equivalent NFA without \\n-transition and then use the above construction process to construct an equivalent \\nDFA , thus, proving the equivalence of NFA s and DFA s.. \\n \\nIt is also possible to construct an equivalent DFA directly from any given NFA with \\n- transition by integrating the concept of \\n-closure in the above construction. \\n \\nRecall that, for any \\n \\n \\n- closure : \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='In the equivalent DFA , at every step, we need to modify the transition functions \\nto \\nkeep track of all the states where the NFA can go on \\n-transitions. This is done by \\nreplacing \\nby \\n-closure \\n, i.e. we now compute \\nat every step as \\nfollows: \\n \\n \\n \\nBesides this the initial state of the DFA D has to be modified to keep track of all the \\nstates that can be reached from the initial state of NFA on zero or more -transitions. \\nThis can be done by changing the initial state \\nto -closure (\\n ) .'),\n",
       " Document(metadata={}, page_content='It is clear that, at every step in the processing of an input string by the DFA D , it enters \\na state that corresponds to the subset of states that the NFA N could be in at that \\nparticular point. This has been proved in the constructions of an equivalent NFA for any \\n-NFA \\nIf the number of states in the NFA is n , then there are \\nstates in the DFA . That is, \\neach state in the DFA is a subset of state of the NFA . \\n \\nBut, it is important to note that most of these'),\n",
       " Document(metadata={}, page_content='states are inaccessible from the start state \\nand hence can be removed from the DFA without changing the accepted language. Thus, \\nin fact, the number of states in the equivalent DFA would be much less \\n \\nthan \\n. \\nExample : Consider the NFA given below. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n{\\n} \\n \\n \\n \\n \\nSince there are 3 states in the NFA \\nwww.indiansbrain.com\\nThere will be \\nstates (representing all possible subset of states) in the'),\n",
       " Document(metadata={}, page_content='equivalent DFA . The transition table of the DFA constructed by using the subset \\nconstructions process is produced here. \\n \\n0 \\n \\n1 The start state of the DFA is   - closures \\n \\n \\n \\n \\n The final states are all those subsets that contains \\n(since \\nin the NFA). \\n \\n{   } \\nLet us compute one entry, \\n \\n \\n \\n \\n \\n \\n \\n \\n Similarly, all other transitions can be computed \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0 \\n1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCorresponding Transition fig. for DFA.Note that states'),\n",
       " Document(metadata={}, page_content='are not accessible and hence can be removed. \\nThis gives us the following simplified DFA with only 3 states. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt is interesting to note that we can avoid encountering all those inaccessible \\nor unnecessary states in the equivalent DFA by performing the following two \\nsteps inductively. \\n \\n1. If \\nis the start state of the NFA, then make \\n- closure ( \\n) the start state of the \\nequivalent DFA . This is definitely the only accessible state.'),\n",
       " Document(metadata={}, page_content='2. If we have already computed a set \\nof states which are accessible. Then \\n. \\ncompute \\nbecause these set of states will also be accessible. \\n \\nFollowing these steps in the above example, we get the transition table given below \\nwww.indiansbrain.com\\nUNIT-II \\n \\n \\n \\nRegular Expressions: Formal Definition \\n \\nWe construct REs from primitive constituents (basic elements) by repeatedly applying certain recursive rules \\nas given below. (In the definition)'),\n",
       " Document(metadata={}, page_content='Definition : Let S be an alphabet. The regular expressions are defined recursively as follows. \\n \\nBasis : \\n \\ni) \\nis a RE \\n \\nii) \\nis a RE \\n \\niii) \\n, a is RE. \\n \\nThese are called primitive regular expression i.e. Primitive Constituents \\n \\nRecursive Step : \\n \\nIf \\nand \\nare REs over, then so are \\n \\ni) \\n \\n \\nii) \\n \\n \\niii) \\n \\n \\niv) \\n \\n \\n \\n \\n \\nClosure : r is RE over only if it can be obtained from the basis elements (Primitive REs) by a finite no of \\napplications of the recursive step (given in 2).'),\n",
       " Document(metadata={}, page_content='Example : Let \\n= { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this expression by \\napplying the above rules as given in the following step. \\n \\nSteps \\nRE Constructed \\nRule Used \\n1 \\n1 \\nRule 1(iii) \\n2 \\n \\nRule 1(i) \\n3 \\n1+ \\nRule 2(i) & Results of Step 1, 2 \\n \\n \\nwww.indiansbrain.com\\n4 \\n(1+   ) \\nRule 2(iv) & Step 3 \\n \\n \\n5 \\n2 \\n1(iii) \\n6 \\n1 \\n1(iii) \\n7 \\n21 \\n2(ii), 5, 6 \\n8 \\n0 \\n1(iii) \\n9 \\n0+21 \\n2(i), 7, 8 \\n10 \\n(0+21) \\n2(iv), 9 \\n11 \\n(0+21)* \\n2(iii), 10 \\n12 \\n(0+21)* \\n2(ii), 4, 11'),\n",
       " Document(metadata={}, page_content='Language described by REs : Each describes a language (or a language is associated with every RE). \\nWe will see later that REs are used to attribute regular languages. \\n \\nNotation : If r is a RE over some alphabet then L(r) is the language associate with r . We can define the \\nlanguage L(r) associated with (or described by) a REs as follows. \\n \\n1. \\nis the RE describing the empty language i.e. L(\\n) = \\n. \\n \\n2. \\nis a RE describing the language {\\n} i.e. L(\\n) = {\\n} . \\n \\n3.'),\n",
       " Document(metadata={}, page_content=', a is a RE denoting the language {a} i.e . L(a) = {a} . \\n \\n4. If \\nand \\nare REs denoting language L(\\n) and L(\\n) respectively, then \\n \\ni) \\nis a regular expression denoting the language L(\\n) = L(\\n) ∪ L(\\n) \\n \\nii) \\nis a regular expression denoting the language L(\\n)=L(\\n) L(\\n) \\n \\niii) \\nis a regular expression denoting the language \\n \\n \\niv) (\\n) is a regular expression denoting the language L((\\n)) = L(\\n) \\n \\nExample : Consider the RE (0*(0+1)). Thus the language denoted by the RE is'),\n",
       " Document(metadata={}, page_content='L(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) \\n \\n= L(0)*L(0) ∪ L(1) \\n \\n= {\\n , 0,00,000,........} {0} \\n{1} \\n \\n= {\\n , 0,00,000,........} {0,1} \\n \\n= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} \\n \\nPrecedence Rule \\nwww.indiansbrain.com\\nConsider the RE ab + c. The language described by the RE can be thought of either L(a)L(b+c) or \\n \\nL(ab)\\nL(c) as provided by the rules (of languages described by REs) given already. But these two'),\n",
       " Document(metadata={}, page_content='represents two different languages lending to ambiguity. To remove this ambiguity we can either \\n \\n \\n \\n1) Use fully parenthesized expression- (cumbersome) or \\n \\n2) Use a set of precedence rules to evaluate the options of REs in some order. Like other algebras mod in \\nmathematics. \\n \\nFor REs, the order of precedence for the operators is as follows: \\n \\ni) The star operator precedes concatenation and concatenation precedes union (+) operator.'),\n",
       " Document(metadata={}, page_content='ii) It is also important to note that concatenation & union (+) operators are associative and union operation \\nis commutative. \\n \\nUsing these precedence rule, we find that the RE ab+c represents the language L(ab) \\nL(c) i.e. it should be \\ngrouped as ((ab)+c). \\n \\nWe can, of course change the order of precedence by using parentheses. For example, the \\nlanguage represented by the RE a(b+c) is L(a)L(b+c). \\n \\n \\n \\nExample : The RE ab*+b is grouped as ((a(b*))+b) which describes the language L(a)(L(b))*\\nL(b)'),\n",
       " Document(metadata={}, page_content=\"Example : The RE (ab)*+b represents the language (L(a)L(b))* \\nL(b). \\n \\nExample : It is easy to see that the RE (0+1)*(0+11) represents the language of all strings over {0,1} which \\nare either ended with 0 or 11. \\n \\nExample : The regular expression r =(00)*(11)*1 denotes the set of all strings with an even number of 0's \\n \\nfollowed by an odd number of 1's i.e. \\n \\nNote : The notation \\nis used to represent the RE rr*. Similarly, \\nrepresents the RE rr, \\ndenotes \\nr, \\nand so on. \\n \\nAn arbitrary string over\"),\n",
       " Document(metadata={}, page_content=\"= {0,1} is denoted as (0+1)*. \\n \\nExercise : Give a RE r over {0,1} s.t. L(r)={\\n has at least one pair of consecutive 1's} \\n \\nSolution : Every string in L(r) must contain 00 somewhere, but what comes before and what goes before is \\ncompletely arbitrary. Considering these observations we can write the REs as (0+1)*11(0+1)*. \\n \\nExample : Considering the above example it becomes clean that the RE (0+1)*11(0+1)*+(0+1)*00(0+1)* \\nrepresents the set of string over {0,1} that contains the substring 11 or 00.\"),\n",
       " Document(metadata={}, page_content=\"www.indiansbrain.com\\nExample : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the set of strings over {0,1} \\nthat contains exactly two 1's. The presence of two 1's in the RE and any no of 0's before, between and after the \\n1's ensure it. \\n \\nExample : Consider the language of strings over {0,1} containing two or more 1's. \\n \\nSolution : There must be at least two 1's in the RE somewhere and what comes before, between, and after is\"),\n",
       " Document(metadata={}, page_content=\"completely arbitrary. Hence we can write the RE as (0+1)*1(0+1)*1(0+1)*. But following two REs also \\nrepresent the same language, each ensuring presence of least two 1's somewhere in the string \\n \\n \\n \\ni) 0*10*1(0+1)* \\n \\nii) (0+1)*10*10* \\n \\nExample : Consider a RE r over {0,1} such that \\n \\nL(r) = {\\n has no pair of consecutive 1's} \\n \\nSolution : Though it looks similar to ex ……., it is harder to construct to construct. We observer that, whenever\"),\n",
       " Document(metadata={}, page_content=\"a 1 occurs, it must be immediately followed by a 0. This substring may be preceded & followed by any no of \\n0's. So the final RE must be a repetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the \\nRE is (0*100*)*. But in this case the strings ending in 1 or consisting of all 0's are not accounted for. Taking \\nthese observations into consideration, the final RE is  r = (0*100*)(1+ \\n)+0*(1+\\n). \\n \\nAlternative Solution :\"),\n",
       " Document(metadata={}, page_content='The language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as r = (0+10)*(1+\\n).This \\nis a shorter expression but represents the same language. \\n \\nRegular Expression: \\n \\nFA to regular expressions: \\n \\nFA to RE (REs for Regular Languages) : \\n \\nLemma : If a language is regular, then there is a RE to describe it. i.e. if L = L(M) for some DFA M, then there is a \\nRE r such that L = L(r). \\n \\nProof : We need to construct a RE r such that \\n. Since M is a DFA, it has a finite no'),\n",
       " Document(metadata={}, page_content='of states. Let the set of states of M is Q = {1, 2, 3,..., n} for some integer n. [ Note : if the n states of M were \\ndenoted by some other symbols, we can always rename those to indicate as 1, 2, 3,..., n ]. The required RE is \\nconstructed inductively. \\n \\nNotations : \\nis a RE denoting the language which is the set of all strings w such that w is the label of a \\npath from state i to state j \\nin M, and that path has no intermediate state whose number is'),\n",
       " Document(metadata={}, page_content='greater then k. ( i & j (begining and end pts) are not considered to be \"intermediate\" so i and /or j can be \\nwww.indiansbrain.com\\ngreater than k ) \\n \\nWe now construct \\ninductively, for all i, j \\nQ starting at k = 0 and finally reaching k = n. \\n \\nBasis : k = 0, \\ni.e. the paths must not have any intermediate state ( since all states are numbered 1 or \\nabove). There are only two possible paths meeting the above condition : \\n \\n1. A direct transition from state i to state j. \\no'),\n",
       " Document(metadata={}, page_content='= a if then is a transition from state i to state j on symbol the single symbol a. \\n \\no \\n= \\nif there are multiple transitions from state i to state j on symbols \\n \\n. \\no \\n= f if there is no transition at all from state i to state j. \\n \\n2. All paths consisting of only one node i.e. when i = j. This gives the path of length 0 (i.e. the RE \\ndenoting the string \\n) and all self loops. By simply adding Î to various cases above we get \\nthe corresponding REs i.e. \\no \\n='),\n",
       " Document(metadata={}, page_content='+ a if there is a self loop on symbol a in state i . \\n \\no \\n= \\n+ \\nif there are self loops in state i as multiple symbols \\n \\n. \\n \\no \\n= \\nif there is no self loop on state i. \\n \\nInduction : \\n \\nAssume that there exists a path from state i to state j such that there is no intermediate state whose number is \\n \\ngreater than k. The corresponding Re for the label of the path is \\n. There are only two possible cases :'),\n",
       " Document(metadata={}, page_content='1. The path dose not go through the state k at all i.e. number of all the intermediate states are less \\nthan k. So, the label of the path from state i to state j is tha language described by the RE \\n. \\n \\n2. The path goes through the state k at least once. The path may go from i to j and k may appear more \\nthan once. We can break the into pieces as shown in the figure 7. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7'),\n",
       " Document(metadata={}, page_content='1. The first part from the state i to the state k which is the first recurence. In this path, all \\nintermediate states are less than k and it starts at iand ends at k. So the RE \\ndenotes the \\nlanguage of the label of path. \\n \\n2. The last part from the last occurence of the state k in the path to state j. In this path also, no \\nintermediate state is numbered greater than k. Hence the RE \\ndenoting the language of the \\nlabel of the path.'),\n",
       " Document(metadata={}, page_content=\"3. In the middle, for the first occurence of k to the last occurence of k , represents a loop which may be \\ntaken zero times, once or any no of times. And all states between two consecutive k's are \\nnumbered less than k. \\n \\nHence the label of the path of the part is denoted by the RE \\n.The label of the path from state i to state \\nj is the concatenation of these 3 parts which is \\n \\n \\n \\nSince either case 1 or case 2 may happen the labels of all paths from state i to j is denoted by the following RE\"),\n",
       " Document(metadata={}, page_content='We can construct \\nfor all i, j \\n{1,2,..., n} in increasing order of k starting with the basis k = 0 upto k = n since \\ndepends only on expressions with a small superscript (and hence will be available). WLOG, assume \\n \\nthat state 1 is the start state and \\nare the m final states where ji \\n{1, 2, ... , n }, \\nand \\n \\n. According to the convention used, the language of the automatacan be denoted by the RE \\nwww.indiansbrain.com\\n \\n \\nSince'),\n",
       " Document(metadata={}, page_content='is the set of all strings that starts at start state 1 and finishes at final state \\nfollowing the transition of \\nthe FA with any value of the intermediate state (1, 2, ... , n) and hence accepted by the automata. \\n \\nRegular Grammar: \\n \\nA grammar \\nis right-linear if each production has one of the following three forms: \\n \\n\\uf095 \\nA\\ncB ,\\uf020\\n\\uf020\\n\\uf095 \\nA\\nc,\\uf020\\n\\uf095 \\nA\\n\\uf020\\n \\nWhere A, B \\n( with A = B allowed) and \\n. A grammar G is left-linear if each production has once of \\nthe following three forms. \\n \\nA\\nBc , A\\nc, A'),\n",
       " Document(metadata={}, page_content='A right or left-linear grammar is called a regular grammar. \\n \\nRegular grammar and Finite Automata are equivalent as stated in the following theorem. \\n \\nTheorem : A language L is regular iff it has a regular grammar. We use the following two lemmas to prove the \\nabove theorem. \\n \\nLemma 1 : If L is a regular language, then L is generated by some right-linear grammar. \\n \\nProof : Let \\nbe a DFA that accepts L. \\n \\nLet \\nand \\n. \\n \\nWe construct the right-linear grammar \\nby letting \\n \\nN = Q , \\nand \\n \\n[ Note: If'),\n",
       " Document(metadata={}, page_content=', then \\n] \\n \\nLet \\n. For M to accept w, there must be a sequence of states \\nsuch that \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\nBy construction, the grammar G will have one production for each of the above transitions. Therefore, we have \\nthe corresponding derivation. \\n \\n \\n \\n \\nHence w \\nL(g). \\n \\nConversely, if \\n, then the derivation of w in G must have the form as given above. \\nBut, then the construction of G from M implies that \\n \\n, where \\n, completing the proof. \\n \\nLemma 2 : Let'),\n",
       " Document(metadata={}, page_content='be a right-linear grammar. Then L(G) is a regular \\nlanguage. Proof: To prove it, we construct a FA M from G to accept the same language. \\n \\nis constructed as follows: \\n \\n( \\nis a special sumbol not in N ) \\n \\n, \\n \\nFor any \\nand \\nand \\nis defined as \\n \\n \\nif \\n \\nand \\n, if \\n. \\nWe now show that this construction works. \\n \\nLet \\n. Then there is a derivation of w in G of the form \\nwww.indiansbrain.com\\n \\n \\nBy contradiction of M, there must be a sequence of transitions \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimplying that'),\n",
       " Document(metadata={}, page_content='i.e. w is accepted by M. \\n \\nConversely, if \\nis accepted by M, then because \\nis the only accepting state of M, the transitions \\ncausing w to be accepted by M will be of the form given above. These transitions corresponds to a \\n \\nderivationof w in the grammar G. Hence \\n, completing the proof of the lemma. \\n \\nGiven any left-linear grammar G with production of the form \\n, we can construct from it a \\nright-linear grammar \\nby replacing every production of G of the form \\nwith \\n \\n \\nIt is easy to prove that'),\n",
       " Document(metadata={}, page_content='. Since \\nis right-linear, \\nis regular. But then so are \\ni.e. \\nbecause regular languages are closed under reversal. \\n \\nPutting the two lemmas and the discussions in the above paragraph together we get the proof of the theorem- \\n \\nA language L is regular iff it has a regular grammar \\n \\nExample : Consider the grammar \\n \\n \\n \\nIt is easy to see that G generates the language denoted by the regular expression \\n(01)*0. The construction of lemma 2 for this grammar produces the follwoing FA.'),\n",
       " Document(metadata={}, page_content='This FA accepts exactly (01)*1. \\n \\nDecisions Algorithms for CFL \\n \\nIn this section, we examine some questions about CFLs we can answer. A CFL may be represented using a \\nCFG or PDA. But an algorithm that uses one representation can be made to work for the others, since we can \\nconstruct one from the other. \\nwww.indiansbrain.com\\nTesting Emptiness : \\n \\nTheorem : There are algorithms to test emptiness of a CFL.'),\n",
       " Document(metadata={}, page_content='Proof : Given any CFL L, there is a CFG G to generate it. We can determine, using the construction described \\n \\nin the context of elimination of useless symbols, whether the start symbol is useless. If so, then \\n; \\notherwise not. \\n \\nTesting Membership : \\n \\nGiven a CFL L and a string x, the membership, problem is to determine whether \\n? \\n \\nGiven a PDA P for L, simulating the PDA on input string x doesnot quite work, because the PDA can grow \\nits stack indefinitely on'),\n",
       " Document(metadata={}, page_content='input, and the process may never terminate, even if the PDA is deterministic. \\n \\nSo, we assume that a CFG \\nis given such that L = L(G). \\n \\nLet us first present a simple but inefficient algorithm. \\n \\nConvert G to \\nin CNF generating \\n. If the input string \\n, then we need to \\n \\ndetermine whether \\nand it can easily be done using the technique given in the context of elimination of \\n \\n-production. If , \\nthen \\niff \\n. Consider a derivation under a grammar in CNF. At'),\n",
       " Document(metadata={}, page_content='every step, a production in CNF in used, and hence it adds exactly one terminal symbol to the sentential form. \\n \\nHence, if the length of the input string x is n, then it takes exactly n steps to derive x ( provided x is in \\n). \\n \\nLet the maximum number of productions for any nonterminal in \\nis K. So at every step in derivation, there are \\natmost k choices. We may try out all these choices, systematically., to derive the string x in \\n. Since \\n \\nthere are atmost \\ni.e.'),\n",
       " Document(metadata={}, page_content='choices. This algorithms is of exponential time complexity. We now present an \\nefficient (polynomial time) membership algorithm. \\n \\nPumping Lemma: \\n \\nLimitations of Finite Automata and Non regular Languages : \\n \\nThe class of languages recognized by FA s is strictly the regular set. There are certain languages which are \\nnon regular i.e. cannot be recognized by any FA \\n \\nConsider the language \\n \\n \\nIn order to accept is language, we find that, an automaton seems to need to remember when passing the'),\n",
       " Document(metadata={}, page_content=\"center point between a's and b's how many a's it has seen so far. Because it would have to compare that \\nwith the number of b's to either accept (when the two numbers are same) or reject (when they are not same) \\nthe input string. \\nwww.indiansbrain.com\\nBut the number of a's is not limited and may be much larger than the number of states since the string may \\nbe arbitrarily long. So, the amount of information the automaton need to remember is unbounded.\"),\n",
       " Document(metadata={}, page_content='A finite automaton cannot remember this with only finite memory (i.e. finite number of states). The fact that \\nFA s have finite memory imposes some limitations on the structure of the languages recognized. Inductively, we \\ncan say that a language is regular only if in processing any string in this language, the information that has to \\nbe remembered at any point is strictly limited. The argument given above to show that \\nis non regular is'),\n",
       " Document(metadata={}, page_content='informal. We now present a formal method for showing that certain languages such as \\nare non regular \\n \\nProperties of CFL’s \\n \\nClosure properties of CFL: \\n \\nWe consider some important closure properties of CFLs. \\n \\nTheorem : If \\nand \\nare CFLs then so is \\n \\n \\nProof : Let \\nand \\nbe CFGs generating. Without loss of generality, we \\ncan assume that \\n. Let \\nis a nonterminal not in \\nor \\n. We construct the grammar \\n \\nfrom \\nand \\n, where \\n \\n, \\n \\n \\n \\n \\n \\n \\n \\n \\nWe now show that \\n \\n \\nThus proving the theorem. \\n \\nLet'),\n",
       " Document(metadata={}, page_content='. Then \\n. All productions applied in their derivation are also in \\n. Hence \\ni.e. \\n \\n \\n \\nSimilarly, if \\n, then \\n \\n \\nThus \\n. \\nwww.indiansbrain.com\\nConversely, let \\n. Then \\nand the first step in this derivation must be either \\nor \\n. Considering the former case, we have \\n \\n \\nSince \\nand \\nare disjoint, the derivation \\nmust use the productions of \\nonly ( which are also in \\n \\n) Since \\nis the start symbol of \\n. Hence, \\ngiving \\n. \\n \\nUsing similar reasoning, in the latter case, we get \\n. Thus \\n. \\n \\nSo, \\n, as claimed'),\n",
       " Document(metadata={}, page_content='Theorem : If \\nand \\nare CFLs, then so is \\n. \\n \\nProof : Let \\nand \\nbe the CFGs generating \\nand \\nrespectively. \\nAgain, we assume that \\nand \\nare disjoint, and \\nis a nonterminal not in \\nor \\n. we construct the CFG \\nfrom \\nand \\n, where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe claim that \\n \\n \\n \\nTo prove it, we first assume that \\nand \\n. Then \\nand \\n. We can derive the string xy \\nin \\nas shown below. \\n \\n \\n \\n \\n \\nsince \\nand \\n. Hence \\n. \\nwww.indiansbrain.com\\nFor the converse, let \\n. Then the derivation of w in \\nwill be of the form'),\n",
       " Document(metadata={}, page_content='i.e. the first step in the derivation must see the rule \\n. Again, since \\nand \\nare \\ndisjoint and \\nand \\n, some string x will be generated from \\nusing productions in \\n( which \\nare also in \\n) and such that \\n. \\n \\nThus \\n \\nHence \\nand \\n. \\n \\nThis means that w can be divided into two parts x, y such that \\nand \\n. Thus \\n.This \\n \\ncompletes the proof \\nTheorem : If L is a CFL, then so is \\n. \\nProof : Let \\nbe the CFG generating L. Let us construct the CFG \\n \\n \\nwhere \\n. \\n \\nWe now prove that \\n, which prove the theorem.'),\n",
       " Document(metadata={}, page_content=\"can generate \\nin one step by using the production \\nsince \\n, \\n Let \\nfor any n >1 we can \\nwrite \\nwhere \\nfor \\n \\n \\n \\n \\nusing following steps. \\n \\n \\n \\n \\n \\nFirst (n-1)-steps uses the production S\\nSS producing the sentential form of n numbers of S 's. The \\nnonterminal S in the i-th position then generates \\nusing production in P ( which are also in \\n) \\n \\nIt is also easy to see that G can generate the empty string, any string in L and any string \\nfor n >1 \\nand none other. \\n \\nHence\"),\n",
       " Document(metadata={}, page_content='Theorem : CFLs are not closed under intersection \\n \\nProof : We prove it by giving a counter example. Consider the language \\n.The following \\nCFG generates L1 and hence a CFL \\n \\ncan generate any string in L. \\n. w can be generated by \\n \\nfrom G \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\nThe nonterminal X generates strings of the form \\nand C generates strings of the form \\n, \\n. These are the only types of strings generated by X and C. Hence, S generates \\n.'),\n",
       " Document(metadata={}, page_content=\"Using similar reasoning, it can be shown that the following grammar \\nand hence it is \\nalso a CFL. \\n \\n \\n \\n \\n \\n \\n \\nBut, \\nand is already shown to be not context-free. \\n \\nHence proof. \\n \\nTheorem : A CFL's are not closed under complementations \\n \\nProof : Assume, for contradiction, that CFL's are closed under complementation. SInce, CFL's are also closed \\nunder union, the language \\n, where \\nand \\nare CFL's must be CFL. But by DeMorgan's law\"),\n",
       " Document(metadata={}, page_content=\"This contradicts the already proved fact that CFL's are not closed under intersection. \\nBut it can be shown that the CFL's are closed under intersection with a regular set. \\n \\nTheorem : If L is a CFL and R is a regular language, then \\nis a CFL. \\n \\nProof : Let \\nbe a PDA for L and let \\nbe a DFA for \\nR. We construct a PDA M from P and D as follows \\n \\n \\nwhere \\nis defined as \\n \\ncontains \\niff \\nwww.indiansbrain.com\\nand \\ncontains\"),\n",
       " Document(metadata={}, page_content='The idea is that M simulates the moves of P and D parallely on input w, and accepts w iff both P and \\nD accepts. That means, we want to show that \\n \\n \\n \\n \\nWe apply induction on n, the number of moves, to show that \\n \\niff \\n \\nand \\n \\nBasic Case is n=0. Hence \\n, \\nand \\n. For this case it is trivially true \\n \\nInductive hypothesis : Assume that the statement is true for n -1. \\n \\nInductive Step : Let w = xa and \\n \\n \\n \\nLet \\n \\nBy inductive hypothesis, \\nand \\n \\n \\nFrom the definition of'),\n",
       " Document(metadata={}, page_content='and considering the n-th move of the PDA M above, we have \\n \\nand \\n \\n \\nHence \\nand \\n \\n \\nIf \\nand \\n, then \\nand we got that if M accepts w, then both P and D accepts it. \\n \\nWe can show that converse, in a similar way. Hence \\nis a CFL ( since it is accepted by a PDA M ) \\nThis property is useful in showing that certain languages are not context-free. \\n \\nExample : Consider the language \\n \\n \\nIntersecting L with the regular set \\n, we get \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content=\"Which is already known to be not context-free. Hence L is not context-free \\nTheorem : CFL's are closed under reversal. That is if L is a CFL, then so is \\n \\n \\nProof : Let the CFG \\ngenerates L. We construct a CFG \\nwhere \\n \\n. We now show that \\n, thus proving the theorem. \\nWe need to prove that \\niff \\n. \\n \\nThe proof is by induction on n, the number of steps taken by the derivation. We assume, for simplicity (and \\nof course without loss of generality), that G and hence \\nare in CNF.\"),\n",
       " Document(metadata={}, page_content=\"The basis is n=1 in which case it is trivial. Because \\nmust be either \\nor BC with \\n. \\n \\nHence \\niff \\n \\n \\nAssume that it is true for (n-1)-steps. Let \\n. Then the first step must apply a rule of the \\nform \\nand it gives \\n \\nwhere \\nand \\n \\n \\nBy constructing of G', \\n \\nHence \\n \\n \\nThe converse case is exactly similar \\nSubstitution : \\n \\n, let \\nbe a language (over any alphabet). This defines a function S, called substitution, on \\nwhich is \\n \\ndenoted as \\n- for all\"),\n",
       " Document(metadata={}, page_content=\"This definition of substitution can be extended further to apply strings and langauge as well. \\nIf \\n, where \\n, is a string in \\n, then \\n \\n. \\nSimilarly, for any language L, \\n \\nThe following theorem shows that CFLs are closed under substitution. \\n \\nThereom : Let \\nis a CFL, and s is a substitution on \\nsuch that \\nis a CFL for all \\n, \\nthus s(L) is a CFL \\n \\nProof : Let L = L(G) for a CFG \\nand for every \\n, \\nfor some \\n. Without loss of generality, assume that the sets of nonterminals N and \\n's \\nare disjoint.\"),\n",
       " Document(metadata={}, page_content=\"www.indiansbrain.com\\nNow, we construct a grammar \\n, generating s(L), from G and \\n's as follows : \\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095\\uf020\\n \\n\\uf095 \\nconsists of\\uf020\\n\\uf020\\n1. \\nand \\n \\n2. The production of P but with each terminal a in the right hand side of a production replaced \\nby \\neverywhere. \\nWe now want to prove that this construction works i.e. \\niff \\n. \\n \\nIf Part : Let \\nthen according to the definition there is some string \\nand \\nfor \\nsuch that \\n \\nWe will show that \\n. \\n \\nFrom the construction of \\n, we find that, there is a derivation\"),\n",
       " Document(metadata={}, page_content=\"corresponding to the string \\n \\n(since \\ncontains all productions of G but every ai replaced with \\nin the RHS of any \\nproduction). \\n \\nEvery \\nis the start symbol of \\nand all productions of \\nare also included in \\n. \\nHence \\n \\n \\n \\n \\n \\n \\n \\n \\nTherefore, \\n \\n \\n(Only-if Part) Let \\n. Then there must be a derivative as follows : \\n \\n(using the production of G include in \\nas modified by (step 2) of the construction of \\n.) \\n \\nEach \\n(\\n) can only generate a string \\n, since each \\n's and N are disjoin. \\nTherefore, we get\"),\n",
       " Document(metadata={}, page_content=\"since \\n \\nwww.indiansbrain.com\\nsince \\n \\n \\n \\n \\n \\nThe string \\nis formed by substituting strings \\nfor each \\nand hence \\n. \\n \\nTheorem : CFL's are closed under homomorphism \\n \\nProof : Let \\nbe a CFL, and h is a homomorphism on \\ni.e \\nfor some alphabets \\n. consider the \\nfollowing substitution S:Replace each symbol \\nby the language consisting of the only string h(a), i.e. \\n \\nfor all \\n. Then, it is clear that, h(L) = s(L). Hence, CFL's being closed under \\nsubstitution must also be closed under homomorphism.\"),\n",
       " Document(metadata={}, page_content='www.indiansbrain.com\\n \\n \\nUNIT- III \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nGrammar \\n \\nA grammar is a mechanism used for describing languages. This is one of the most simple but yet powerful \\nmechanism. There are other notions to do the same, of course. \\n \\nIn everyday language, like English, we have a set of symbols (alphabet), a set of words constructed from \\nthese symbols, and a set of rules using which we can group the words to construct meaningful sentences. The'),\n",
       " Document(metadata={}, page_content='grammar for English tells us what are the words in it and the rules to construct sentences. It also tells us \\nwhether a particular sentence is well-formed (as per the grammar) or not. But even if one follows the rules of \\nthe english grammar it may lead to some sentences which are not meaningful at all, because of impreciseness \\nand ambiguities involved in the language. In english grammar we use many other higher level constructs like'),\n",
       " Document(metadata={}, page_content='noun-phrase, verb-phrase, article, noun, predicate, verb etc. A typical rule can be defined as \\n \\n< sentence >\\n< noun-phrase > < predicate > \\n \\nmeaning that \"a sentence can be constructed using a \\'noun-phrase\\' followed by a predicate\". \\n \\nSome more rules are as follows: \\n \\n< noun-phrase >\\n< article >< noun > \\n \\n< predicate > \\n< verb > \\n \\nwith similar kind of interpretation given above. \\n \\nIf we take {a, an, the} to be <article>; cow, bird, boy, Ram, pen to be examples of <noun>; and eats, runs,'),\n",
       " Document(metadata={}, page_content='swims, walks, are associated with <verb>, then we can construct the sentence- a cow runs, the boy eats, an \\npen walks- using the above rules. Even though all sentences are well-formed, the last one is not meaningful. \\nWe observe that we start with the higher level construct <sentence> and then reduce it to <noun-phrase>, \\n<article>, <noun>, <verb> successively, eventually leading to a group of words associated with these \\nconstructs.'),\n",
       " Document(metadata={}, page_content=\"These concepts are generalized in formal language leading to formal grammars. The word 'formal' here refers \\nto the fact that the specified rules for the language are explicitly stated in terms of what strings or symbols can \\noccur. There can be no ambiguity in it. \\n \\nFormal definitions of a Grammar \\nwww.indiansbrain.com\\nA grammar G is defined as a quadruple. \\n \\n \\n \\n \\nN is a non-empty finite set of non-terminals or variables, \\n \\nis a non-empty finite set of terminal symbols such that\"),\n",
       " Document(metadata={}, page_content=', is a special non-terminal (or variable) called the start symbol, and \\nis a \\nfinite set of production rules. \\n \\nThe binary relation defined by the set of production rules is denoted by \\n, i.e. \\niff \\n. \\n \\nIn other words, P is a finite set of production rules of the form \\n, where \\nand \\n \\n \\n \\nProduction rules: \\n \\nThe production rules specify how the grammar transforms one string to another. Given a string \\n, we say that \\nthe production rule \\nis applicable to this string, since it is possible to use the rule'),\n",
       " Document(metadata={}, page_content='to rewrite the \\n(in \\n) to \\nobtaining a new string \\n. We say that \\nderives \\nand is denoted as \\n \\n \\n \\n \\nSuccessive strings are dervied by applying the productions rules of the grammar in any arbitrary order. \\nA particular rule can be used if it is applicable, and it can be applied as many times as described. \\n \\nWe write \\nif the string \\ncan be derived from the string \\nin zero or more steps; \\nif \\ncan be \\nderived from \\nin one or more steps.'),\n",
       " Document(metadata={}, page_content='By applying the production rules in arbitrary order, any given grammar can generate many strings of terminal \\nsymbols starting with the special start symbol, S, of the grammar. The set of all such terminal strings is \\ncalled the language generated (or defined) by the grammar. \\n \\nFormaly, for a given grammar \\nthe language generated by G is \\n \\n \\n \\n \\n \\n \\nThat is \\niff \\n. \\nwww.indiansbrain.com\\nIf \\n, we must have for some \\n, \\n, denoted as a \\nderivation sequence of w, The strings'),\n",
       " Document(metadata={}, page_content='are denoted as sentential forms of the \\nderivation.  \\n \\n \\nExample : Consider the grammar \\n \\n, where N = {S},={a, b} and P is the set of the following \\nproduction rules \\n \\n \\n \\n \\n{ S \\nab, SaSb} \\n \\nSome terminal strings generated by this grammar together with their derivation is given below. \\n \\nS \\nab \\n \\nS \\naSb\\naabb \\n \\nS \\naSb\\naaSbb\\naaabbb \\n \\nIt is easy to prove that the language generated by this grammar is \\n \\n \\n \\n \\nBy using the first production, it generates the string ab ( for i =1 ).'),\n",
       " Document(metadata={}, page_content=\"To generate any other string, it needs to start with the production S\\naSb and then the non-terminal S in the RHS can be \\nreplaced either by ab (in which we get the string aabb) or the same production S\\naSb can be used one or more \\ntimes. Every time it adds an 'a' to the left and a 'b' to the right of S, thus giving the sentential \\n \\nform \\n. When the non-terminal is replaced by ab (which is then only possibility for generating \\na terminal string) we get a terminal string of the form \\n.\"),\n",
       " Document(metadata={}, page_content='There is no general rule for finding a grammar for a given language. For many languages we can devise \\ngrammars and there are many languages for which we cannot find any grammar. \\n \\nExample: Find a grammar for the language \\n. \\n \\nIt is possible to find a grammar for L by modifying the previous grammar since we need to generate an extra b \\nat the end of the string \\n. We can do this by adding a production S\\nBb where the non-terminal B \\ngenerates \\nas given in the previous example.'),\n",
       " Document(metadata={}, page_content='Using the above concept we devise the follwoing grammar for L. \\n \\n \\nwhere, N = { S, B }, P = { S\\nBb, B\\nab, B\\naBb } \\n \\nParse Trees: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\nConstruction of a Parse tree: \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nYield of a Parse tree: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAmbiguity in languages and grammars: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\nUNIT-IV \\n \\n \\n \\n \\n \\n \\nPush down automata:'),\n",
       " Document(metadata={}, page_content='Regular language can be charaterized as the language accepted by finite automata. Similarly, we can \\ncharacterize the context-free language as the langauge accepted by a class of machines called \\n\"Pushdown Automata\" (PDA). A pushdown automation is an extension of the NFA. \\n \\nIt is observed that FA have limited capability. (in the sense that the class of languages accepted or characterized by'),\n",
       " Document(metadata={}, page_content='them is small). This is due to the \"finite memory\" (number of states) and \"no external memory\" involved with them. A \\nPDA is simply an NFA augmented with an \"external stack memory\". The addition of a stack provides the PDA with a \\nlast-in, first-out memory management cpapability. This \"Stack\" or \"pushdown store\" can be used to record a \\npotentially unbounded information. It is due to this memory management capability with the help of the stack that a'),\n",
       " Document(metadata={}, page_content='PDA can overcome the memory limitations that prevents a FA to \\n \\naccept many interesting languages like \\n. Although, a PDA can store an unbounded amount of \\ninformation on the stack, its access to the information on the stack is limited. It can push an element onto the \\ntop of the stack and pop off an element from the top of the stack. To read down into the stack the top elements \\nmust be popped off and are lost. Due to this limited access to the information on the stack, a PDA still has'),\n",
       " Document(metadata={}, page_content='some limitations and cannot accept some other interesting languages. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAs shown in figure, a PDA has three components: an input tape with read only head, a finite control and \\na pushdown store. \\n \\nThe input head is read-only and may only move from left to right, one symbol (or cell) at a time. In each step, the \\nPDA pops the top symbol off the stack; based on this symbol, the input symbol it is currently reading, and \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='its present state, it can push a sequence of symbols onto the stack, move its read-only head one cell \\n(or symbol) to the right, and enter a new state, as defined by the transition rules of the PDA. \\n \\nPDA are nondeterministic, by default. That is, \\n- transitions are also allowed in which the PDA can pop and \\npush, and change state without reading the next input symbol or moving its read-only head. Besides this, \\nthere may be multiple options for possible next moves.'),\n",
       " Document(metadata={}, page_content='Formal Definitions : Formally, a PDA M is a 7-tuple M =\\n \\nwhere, \\n \\n\\uf095 \\nis a finite set of states,\\uf020\\n\\uf095 \\nis a finite set of input symbols (input alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a finite set of stack symbols (stack alphabets),\\uf020\\n\\uf020\\n\\uf095 \\nis a transition function from \\nto subset of \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the start state\\uf020\\n\\uf095 \\n, is the initial stack symbol, and\\uf020\\n\\uf020\\n\\uf095 \\n, is the final or accept states.\\uf020\\n \\nExplanation of the transition function, \\n: \\n \\nIf, for any \\n, \\n. This means intitutively that whenever'),\n",
       " Document(metadata={}, page_content='the PDA is in state q reading input symbol a and z on top of the stack, it can nondeterministically for any i, \\n \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack (where \\n) (The usual convention is that if \\n, \\nthen \\nwill be at the top and \\nat the bottom.)\\uf020\\n\\uf095 \\nmove read head right one cell past the current symbol a.\\uf020\\n \\nIf a = \\n, then \\nmeans intitutively that whenver the PDA is in state'),\n",
       " Document(metadata={}, page_content='q with z on the top of the stack regardless of the current input symbol, it can nondeterministically for any \\n \\ni, \\n, \\n \\n\\uf095 \\ngo to state \\n\\uf020\\n\\uf095 \\npop z off the stack\\uf020\\n\\uf020\\n\\uf095 \\npush \\nonto the stack, and\\uf020\\n\\uf095 \\nleave its read-only head where it is.\\uf020\\nwww.indiansbrain.com\\nState transition diagram : A PDA can also be depicted by a state transition diagram. The labels on the arcs \\nindicate both the input and the stack operation. The transition \\n \\nfor \\nand \\nis depicted by'),\n",
       " Document(metadata={}, page_content='Final states are indicated by double circles and the start state is indicated by an arrow to it from nowhere. \\n \\n \\nConfiguration or Instantaneous Description (ID) : \\n \\nA configuration or an instantaneous description (ID) of PDA at any moment during its computation is an \\nelement of \\ndescribing the current state, the portion of the input remaining to be read (i.e. \\nunder and to the right of the read head), and the current stack contents. Only these three elements'),\n",
       " Document(metadata={}, page_content='can affect the computation from that point on and, hence, are parts of the ID. \\n \\nThe start or inital configuartion (or ID) on input \\nis \\n. That is, the PDA always starts in its \\nstart state, \\nwith its read head pointing to the leftmost input symbol and the stack containing only \\nthe start/initial stack symbol, \\n. \\n \\nThe \"next move relation\" one figure describes how the PDA can move from one configuration to \\nanother in one step. \\n \\nFormally, \\n \\n \\n \\n \\niff \\n\\'a\\' may be \\nor an input symbol.'),\n",
       " Document(metadata={}, page_content='Let I, J, K be IDs of a PDA. We define we write I\\nK, if ID I can become K after exactly i moves. The \\nrelations \\nand \\ndefine as follows \\n \\nI \\nK \\n \\nI \\nJ if \\nsuch that I \\nK and K\\n J \\n \\nI \\nJ if \\nsuch that I \\nJ. \\nwww.indiansbrain.com\\nThat is, \\nis the reflexive, transitive closure of \\n. We say that I \\nJ if the ID J follows from the ID I in \\nzero or more moves. \\n \\n( Note : subscript M can be dropped when the particular PDA M is understood. ) \\n \\nLanguage accepted by a PDA M'),\n",
       " Document(metadata={}, page_content='There are two alternative definiton of acceptance as given below. \\n \\n1. Acceptance by final state : \\n \\nConsider the PDA \\n. Informally, the PDA M is said to accept its input \\nby \\nfinal state if it enters any final state in zero or more moves after reading its entire input, starting in the start \\nconfiguration on input \\n. \\n \\nFormally, we define L(M), the language accepted by final state to be \\n \\n{ \\n| \\nfor some \\nand \\n} \\n \\n \\n \\n \\n2. Acceptance by empty stack (or Null stack) : The PDA M accepts its input'),\n",
       " Document(metadata={}, page_content='by empty stack if starting in the \\n \\nstart configuration on input \\n, it ever empties the stack w/o pushing anything back on after reading the \\nentire input. Formally, we define N(M), the language accepted by empty stack, to be \\n \\n{ \\n| \\nfor some \\n} \\n \\nNote that the set of final states, F is irrelevant in this case and we usually let the F to be the empty set i.e. F = \\nQ . \\n \\nExample 1 : Here is a PDA that accepts the language \\n. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n, and \\nconsists of the following transitions'),\n",
       " Document(metadata={}, page_content=\"www.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe PDA can also be described by the adjacent transition diagram. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever the PDA M sees an input a in the start state \\nwith the start symbol z on the top of the stack \\nit pushes a onto the stack and changes state to \\n. (to remember that it has seen the first 'a'). On state \\nif it \\nsees anymore a, it simply pushes it onto the stack. Note that when M is on state \\n, the symbol on the \\n \\ntop of the stack can only be a. On state\"),\n",
       " Document(metadata={}, page_content=\"if it sees the first b with a on the top of the stack, then it needs to \\nstart comparison of numbers of a's and b's, since all the a's at the begining of the input have already been \\npushed onto the stack. It start this process by popping off the a from the top of the stack and enters in state q3 \\n \\n(to remember that the comparison process has begun). On state \\n, it expects only b's in the input (if it sees\"),\n",
       " Document(metadata={}, page_content='any more a in the input thus the input will not be in the proper form of anbn). Hence there is no more on input a \\nwhen it is in state \\n. On state \\nit pops off an a from the top of the stack for every b in the input. When it sees \\nthe last b on state q3 (i.e. when the input is exaushted), then the last a from the stack will be popped off and the \\nstart symbol z is exposed. This is the only possible case when the input (i.e. on \\n-input ) the PDA M \\n \\nwill move to state \\nwhich is an accept state.'),\n",
       " Document(metadata={}, page_content='we can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\nLet the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) \\n \\n( using transition 2 ) \\n \\n( using transition 3 ) \\nwww.indiansbrain.com\\n( using transition 4 ), \\n( using transition 5 ) , \\nis final state. Hence , accept. So'),\n",
       " Document(metadata={}, page_content='the string aabb is rightly accepted by M \\n \\nwe can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nExample 2 : We give an example of a PDA M that accepts the set of balanced strings of parentheses [] by \\nempty stack.'),\n",
       " Document(metadata={}, page_content='The PDA M is given below. \\n \\n \\nwhere \\nis defined as \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInformally, whenever it sees a [, it will push the ] onto the stack. (first two transitions), and whenever it sees a ] \\nand the top of the stack symbol is [, it will pop the symbol [ off the stack. (The third transition). The fourth \\ntransition is used when the input is exhausted in order to pop z off the stack ( to empty the stack) and accept.'),\n",
       " Document(metadata={}, page_content='Note that there is only one state and no final state. The following is a sequence of configurations leading to the \\nacceptance of the string [ [ ] [ ] ] [ ]. \\n \\n \\n \\n \\n \\n \\n \\n \\nEquivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also'),\n",
       " Document(metadata={}, page_content=\"be accepted by final state by some other PDA and vice versa. Hence it doesn't matter which one we use, since \\nwww.indiansbrain.com\\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final \\nstate or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M' from M and the proof of equivalence of M & M' \\nare given below. \\n \\nThere are two cases to be considered.\"),\n",
       " Document(metadata={}, page_content=\"CASE I : PDA M accepts by final state, Let \\nLet qf be a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and M' are equivalent i.e. \\n \\nL(M) = L(\\n) \\n \\nLet \\nL(M) . Then \\nfor some \\nand \\n \\n \\nThen \\n \\n \\nThus \\naccepts \\n \\n \\nConversely, let \\naccepts \\ni.e. \\nL(\\n), then \\nfor \\ninherits all other moves except the last one from M. Hence \\nfor some \\n \\n. \\n \\nThus M accepts \\n. Informally, on any input\"),\n",
       " Document(metadata={}, page_content='simulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE II : PDA M accepts by empty stack. \\n \\nWe will construct \\nfrom M in such a way that \\nsimulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand X\\nand \\ncontains all \\nthe transition of'),\n",
       " Document(metadata={}, page_content=\", as well as the following two transitions. \\n \\n \\nand \\nwww.indiansbrain.com\\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M's stack. From this point onward \\nwill simulate every move of M \\nsince all the transitions of M are also in \\n \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point, \\nwill enter its final state\"),\n",
       " Document(metadata={}, page_content='by using transition rule 2, thereby (correctly) accepting the \\ninput. We will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. Then \\n \\nfor some \\n. But then \\n \\n \\n( by transition rule 1) \\n \\n( Since \\nincludes all the moves of M ) \\n \\n \\n( by transition rule 2 ) \\n \\nHence, \\nalso accepts \\n. Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some \\n \\n \\nEvery move in the sequence, \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e.'),\n",
       " Document(metadata={}, page_content='Equivalence of PDA’s and CFG’s: \\nWe will now show that pushdown automata and context-free grammars are equivalent in expressive power, \\nthat is, the language accepted by PDAs are exactly the context-free languages. To show this, we have to \\nprove each of the following: \\n \\ni) \\nGiven any arbitrary CFG G there exists some PDA M that accepts exactly the same \\nlanguage generated by G. \\n \\nii) \\nGiven any arbitrary PDA M there exists a CFG G that generates exactly the same \\nlanguage accpeted by M.'),\n",
       " Document(metadata={}, page_content='(i) CFA to PDA \\n \\nWe will first prove that the first part i.e. we want to show to convert a given CFG to an equivalent PDA. \\nwww.indiansbrain.com\\nLet the given CFG is \\n. Without loss of generality we can assume that G is in \\nGreibach Normal Form i.e. all productions of G are of the form . \\n \\n where \\nand \\n. \\n \\nFrom the given CFG G we now construct an equivalent PDA M that accepts by empty stack. Note that there \\nis only one state in M. Let \\n \\n, where \\n \\n\\uf095 \\nq is the only state\\uf020\\n\\uf020\\n\\uf095 \\nis the input alphabet,\\uf020'),\n",
       " Document(metadata={}, page_content='\\uf095 \\nN is the stack alphabet ,\\uf020\\n\\uf020\\n\\uf095 \\nq is the start state.\\uf020\\n\\uf095 \\nS is the start/initial stack symbol, and \\n, the transition relation is defined as follows\\uf020\\n \\nFor each production \\n, \\n. We now want to show \\nthat M and G are equivalent i.e. L(G)=N(M). i.e. for any \\n. \\niff \\n. \\n \\nIf \\n, then by definition of L(G), there must be a leftmost derivation starting with S and deriving w. \\n \\ni.e. \\n \\n \\nAgain if \\n, then one sysmbol. Therefore we need to show that for any \\n. \\n \\niff \\n.'),\n",
       " Document(metadata={}, page_content='But we will prove a more general result as given in the following lemma. Replacing A by S (the start \\nsymbol) and \\nby \\ngives the required proof. \\n \\nLemma For any \\n, \\nand \\n, \\nvia a leftmost derivative iff \\n. \\n \\nProof : The proof is by induction on n. \\n \\nBasis : n = 0 \\nwww.indiansbrain.com\\n \\niff \\ni.e. \\nand \\n \\n \\n \\niff \\n \\niff \\n \\n \\nInduction Step : \\n \\nFirst, assume that \\nvia a leftmost derivation. Let the last production applied in their derivation is \\nfor some \\nand \\n. \\n \\nThen, for some \\n, \\n \\n \\n \\n \\n \\n \\nwhere'),\n",
       " Document(metadata={}, page_content='and \\n \\n \\nNow by the indirection hypothesis, we get, \\n \\n.............................................................................(1) \\nAgain by the construction of M, we get \\n \\n \\nso, from (1), we get \\n \\n \\n \\n \\nsince \\nand \\n, we get \\n \\n \\nThat is, if \\n, then \\n. Conversely, assume that \\nand let \\nwww.indiansbrain.com\\nbe the transition used in the last move. Then for some \\n, \\nand \\n \\n \\n \\nwhere \\nand \\n. \\n \\nNow, by the induction hypothesis, we get \\n \\nvia a leftmost derivation. \\n \\nAgain, by the construction of M,'),\n",
       " Document(metadata={}, page_content='must be a production of G. [ Since \\n]. Applying the production to the sentential form \\nwe get \\n \\n \\n \\n \\n \\ni.e. \\n \\n \\nvia a leftmost derivation. \\n \\nHence the proof. \\n \\nExample : Consider the CFG G in GNF \\n \\nS\\naAB \\n \\nA\\na / aA \\nB\\na / bB \\n \\nThe one state PDA M equivalent to G is shown below. For convenience, a production of G and \\nthe corresponding transition in M are marked by the same encircled number. \\n \\n(1) S\\naAB \\n \\n(2) A \\na \\n \\n(3) A\\naA \\n \\n(4) B \\na \\n \\n(5) B \\nbB'),\n",
       " Document(metadata={}, page_content='. We have used the same construction discussed earlier \\n \\nSome Useful Explanations : \\nConsider the moves of M on input aaaba leading to acceptance of the string. \\n \\nSteps \\nwww.indiansbrain.com\\n \\n1. (q, aaaba, s) \\n( q, aaba, AB ) \\n2. \\n( q, aba, AB ) \\n3. \\n( q, ba, B ) \\n4. \\n( q, a, B ) \\n5. \\n( q,   ,   )    Accept by empty stack. \\n \\nNote : encircled numbers here shows the transitions rule applied at every step.'),\n",
       " Document(metadata={}, page_content='Now consider the derivation of the same string under grammar G. Once again, the production used at \\nevery step is shown with encircled number. \\n \\nS \\naAB \\naaAB \\naaaB \\naaabB \\naaaba \\n \\nSteps \\n1 \\n2 \\n3 \\n4 \\n5\\n \\nObservations: \\n\\uf095 \\nThere is an one-to-one correspondence of the sequence of moves of the PDA M and the derivation\\uf020\\n\\uf020\\nsequence under the CFG G for the same input string in the sense that - number of steps in both'),\n",
       " Document(metadata={}, page_content='the cases are same and transition rule corresponding to the same production is used at every step \\n(as shown by encircled number). \\n\\uf020\\n\\uf095 \\nconsidering the moves of the PDA and derivation under G together, it is also observed that at \\nevery step the input read so far and the stack content together is exactly identical to the \\ncorresponding sentential form i.e.\\uf020\\n\\uf020\\n<what is Read><stack> = <sentential form> \\n \\nSay, at step 2, Read so far = \\na stack = AB \\nSentential form = aAB From this property we claim that'),\n",
       " Document(metadata={}, page_content='iff \\n. If the claim is \\n \\ntrue, then apply with \\nand we get \\niff \\n or \\niff \\n( \\nby definition ) \\n \\nThus N(M) = L(G) as desired. Note that we have already proved a more general version of the \\nclaim PDA and CFG: \\nWe now want to show that for every PDA M that accpets by empty stack, there is a CFG G such that L(G) = \\nN(M) \\n \\nwe first see whether the \"reverse of the construction\" that was used in part (i) can be used here to construct \\nan equivalent CFG from any PDA M.'),\n",
       " Document(metadata={}, page_content='It can be show that this reverse construction works only for single state PDAs. \\nwww.indiansbrain.com\\n\\uf095 \\nThat is, for every one-state PDA M there is CFG G such that L(G) = N(M). For every move of the \\nPDA M \\nwe introduce a production \\nin the \\ngrammar \\nwhere N = T and \\n.\\uf020\\n \\nwe can now apply the proof in part (i) in the reverse direction to show that L(G) = N(M). \\n \\nBut the reverse construction does not work for PDAs with more than one state. For example, consider the PDA'),\n",
       " Document(metadata={}, page_content='M produced here to accept the langauge \\n \\n \\n \\n \\n \\nNow let us construct CFG \\nusing the \"reverse\" construction. \\n \\n( Note \\n). \\n \\nTransitions in M \\nCorresponding Production in G \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWe can drive strings like aabaa which is in the language. \\n \\n \\n \\n \\nBut under this grammar we can also derive some strings which are not in the language. e.g \\n \\n \\n \\n \\nand \\n. But \\n \\n \\nTherefore, to complete the proof of part (ii) we need to prove the following claim also.'),\n",
       " Document(metadata={}, page_content='Claim: For every PDA M there is some one-state PDA \\nsuch that \\n. \\n \\nIt is quite possible to prove the above claim. But here we will adopt a different approach. We start with \\nany arbitrary PDA M that accepts by empty stack and directly construct an equivalent CFG G. \\nwww.indiansbrain.com\\nPDA to CFG \\n \\nWe want to construct a CFG G to simulate any arbitrary PDA M with one or more states. Without loss \\nof generality we can assume that the PDA M accepts by empty stack.'),\n",
       " Document(metadata={}, page_content='The idea is to use nonterminal of the form <PAq> whenever PDA M in state P with A on top of the stack goes \\n \\nto state \\n. That is, for example, for a given transition of the PDA corresponding production in the grammar as \\nshown below, \\nAnd, we would like to show, in general, that \\niff the PDA M, when started from state P with A on \\n \\nthe top of the stack will finish processing \\n, arrive at state q and remove A from the stack.'),\n",
       " Document(metadata={}, page_content='we are now ready to give the construction of an equivalent CFG G from a given PDA M. we need to introduce \\ntwo kinds of producitons in the grammar as given below. The reason for introduction of the first kind of \\nproduction will be justified at a later point. Introduction of the second type of production has been justified in \\nthe above discussion. \\n \\nLet \\nbe a PDA. We construct from M a equivalent CFG \\n \\n \\nWhere \\n \\n\\uf095 \\nN is the set of nonterminals of the form <PAq> for \\nand \\nand P contains the follwoing\\uf020'),\n",
       " Document(metadata={}, page_content=\"two kind of production \\n \\n1.  \\n \\n \\n2. If \\n, then for every choice of the sequence \\n,\\n \\n, \\n. \\n \\n \\nInclude the follwoing production \\n \\n \\n \\n \\nIf n = 0, then the production is \\n.For the whole exercise to be meaningful we want \\nmeans there is a sequence of transitions ( for PDA M ), starting in state q, ending in \\n, \\n \\nduring which the PDA M consumes the input string \\nand removes A from the stack (and, of course, all \\nother symbols pushed onto stack in A's place, and so on.) \\n \\nThat is we want to claim that\"),\n",
       " Document(metadata={}, page_content='iff \\n \\n \\nIf this claim is true, then let \\nto get \\niff \\nfor some \\n \\n. But for all \\nwe have \\nas production in G. Therefore, \\nwww.indiansbrain.com\\niff \\ni.e. \\niff PDA M accepts w by empty stack or L(G) = N(M) \\n \\nNow, to show that the above construction of CFG G from any PDA M works, we need to prove the \\nproposed claim. \\n \\nNote: At this point, the justification for introduction of the first type of production (of the form \\n) in'),\n",
       " Document(metadata={}, page_content=\"the CFG G, is quite clear. This helps use deriving a string from the start symbol of the grammar. \\n \\nProof : Of the claim \\niff \\nfor some \\n, \\nand \\n \\n \\nThe proof is by induction on the number of steps in a derivation of G (which of course is equal to the number \\nof moves taken by M). Let the number of steps taken is n. \\n \\nThe proof consists of two parts: ' if ' part and ' only if ' part. First, consider the ' if ' part \\n \\nIf \\nthen \\n. \\n \\nBasis is n =1 \\n \\nThen \\n. In this case, it is clear that\"),\n",
       " Document(metadata={}, page_content='. Hence, by construction \\nis a production of G. \\n \\nThen \\n \\nInductive Hypothesis : \\n \\n \\n \\n \\n \\nInductive Step : \\n \\n \\nFor n >1, let w = ax for some \\nand \\nconsider the first move of the PDA M which uses \\nthe general transition \\n= \\n \\n. Now M must remove \\nfrom stack \\nwhile consuming x in the remaining n-1 moves. \\n \\nLet \\n, where \\nis the prefix of x that M has consumed when \\nfirst appears at top \\nof the stack. Then there must exist a sequence of states in M (as per construction) \\n(with \\n \\n), such that'),\n",
       " Document(metadata={}, page_content='www.indiansbrain.com\\n \\n [ This step implies \\n] \\n [ This step implies \\n] \\n \\n... \\n \\n \\n=\\n \\n \\n[ Note: Each step takes less than or equal to n -1 moves because the total number of moves required \\nassumed to be n-1.] \\n \\nThat is, in general \\n \\n \\n, \\n. \\n \\nSo, applying inductive hypothesis we get \\n \\n, \\n. But corresponding to the original move \\n \\nin M we have added the following production in G. \\n \\nWe can show the computation of the PDA on a given input using the IDs and next move relations. For example,'),\n",
       " Document(metadata={}, page_content='following are the computation on two input strings. \\n \\ni) Let the input be aabb. we start with the start configuration and proceed to the subsequent IDs using \\nthe transition function defined \\n \\n( using transition 1 ) , \\n( using transition 2 ) \\n \\n( using transition 3 ), \\n( using transition 4 ) \\n \\n( using transition 5 ) , \\nis final state. Hence, accept. \\n \\nSo the string aabb is rightly accepted by M.'),\n",
       " Document(metadata={}, page_content='we can show the computation of the PDA on a given input using the IDs and next move relations. For example, \\nfollowing are the computation on two input strings. \\n \\ni) Let the input be aabab. \\nwww.indiansbrain.com\\n \\n \\nNo further move is defined at this point. \\n \\nHence the PDA gets stuck and the string aabab is not accepted. \\n \\nThe following is a sequence of configurations leading to the acceptance of the string [ [ ] [ ] ] [ ].'),\n",
       " Document(metadata={}, page_content=\"Equivalence of acceptance by final state and empty stack. \\n \\nIt turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty \\nstack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also \\nbe accepted by final state by some other PDA and vice versa. Hence it doesn't matter which one we use, since \\neach kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final\"),\n",
       " Document(metadata={}, page_content=\"state or empty stack, we can always construct an equivalent PDA M with a single final state that accpets \\nexactly the same language L. The construction process of M' from M and the proof of equivalence of M & M' \\nare given below \\n \\nThere are two cases to be considered. \\n \\nCASE 1 : PDA M accepts by final state, Let \\n. Let \\nbe a new state not in Q. \\nConsider the PDA \\nwhere \\nas well as the following transition. \\n \\ncontains \\nand \\n. It is easy to show that M and \\nare equivalent i.e. \\n \\n \\n. \\n \\nLet \\n. Then\"),\n",
       " Document(metadata={}, page_content='for some \\nand \\n \\n \\nThen \\n. \\n \\nThus \\naccepts \\n. \\nwww.indiansbrain.com\\nConversely, let \\naccepts \\ni.e. \\n, then \\nfor some \\n. \\ninherits all other moves except the last one from M. Hence \\nfor some \\n. \\n \\nThus M accepts \\n. Informally, on any input \\nsimulate all the moves of M and enters in its own final \\nstate \\nwhenever M enters in any one of its final status in F. Thus \\naccepts a string \\niff M accepts it. \\n \\nCASE 2 : PDA M accepts by empty stack. \\n \\nwe will construct \\nfrom M in such a way that'),\n",
       " Document(metadata={}, page_content=\"simulates M and detects when M empties its stack. \\n \\nenters its final state \\nwhen and only when M empties its stack.Thus \\nwill accept a string \\niff M \\naccepts. \\n \\nLet \\nwhere \\nand \\nand \\ncontains all \\nthe transition of \\n, as well as the following two transitions. \\n \\nand \\n \\n \\n \\n \\n \\nTransitions 1 causes \\nto enter the initial configuration of M except that \\nwill have its own bottom-of-stack \\nmarker X which is below the symbols of M's stack. From this point onward M' will simulate every move of M\"),\n",
       " Document(metadata={}, page_content='since all the transitions of M are also in \\n. \\n \\nIf M ever empties its stack, then \\nwhen simulating M will empty its stack except the symbol X at the bottom. \\n \\nAt this point\\n, will enter its final state \\nby using transition rule 2, thereby (correctly) accepting the \\ninput. we will prove that M and \\nare equivalent. \\n \\nLet M accepts \\n. \\n \\nThen \\n \\nfor some \\n. But then, \\n \\n \\n( by transition rule 1 ) \\n \\n( since \\ninclude all the moves of M ) \\nwww.indiansbrain.com\\n \\n( by transition rule 2 ) \\n \\nHence,'),\n",
       " Document(metadata={}, page_content='also accepts \\n.Conversely, let \\naccepts \\n. \\n \\nThen \\nfor some Q . \\n \\nEvery move in the sequence \\n \\nwere taken from M. \\n \\nHence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. \\n \\n \\n. \\n \\nDeterministic PDA: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRegular Languages and DPDA’s The DPDA’s accepts a class of languages that is in between the regular \\nlanguages and CFL’s. \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='Deterministic Pushdown Automata (DPDA) and Deterministic Context-free Languages (DCFLs) \\n \\nPushdown automata that we have already defined and discussed are nondeterministic by default, that is , there may be two or \\nmore moves involving the same combinations of state, input symbol, and top of the stock, and again, for some state and \\ntop of the stock the machine may either read and input symbol or make an \\n- transition (without consuming any input).'),\n",
       " Document(metadata={}, page_content='In deterministic PDA , there is never a choice of move in any situation. This is handled by preventing the above mentioned two \\ncases as described in the definition below. \\n \\nDefnition : Let \\nbe a PDA . Then M is deterministic if and only if both the following conditions are \\nsatisfied. \\n \\n1. \\nhas at most one element for any \\nand \\n(this condition prevents multiple choice f \\nany combination of \\n) \\n2. \\nIf \\nand \\nfor every'),\n",
       " Document(metadata={}, page_content='(This condition prevents the possibility of a choice between a move with or without an input symbol). \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEmpty Production Removal \\n \\nThe productions of context-free grammars can be coerced into a variety of forms without \\naffecting the expressive power of the grammars. If the empty string does not belong to a language, \\nthen there is a way to eliminate the productions of the form A→ λ from the grammar.'),\n",
       " Document(metadata={}, page_content='If the empty string belongs to a language, then we can eliminate λ from all productions \\n \\nsave for the single production S → λ. In this case we can also eliminate any occurrences of S \\nfrom the right-hand side of productions. \\n \\nProcedure to find CFG with out empty Productions \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUnit production removal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLeft Recursion Removal \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='NORMAL FORMS \\n \\nTwo kinds of normal forms viz., Chomsky Normal Form and Greibach Normal Form (GNF) \\nare considered here. \\n \\nChomsky Normal Form (CNF) \\n \\nAny context-free language L without any λ-production is generated by a grammar is \\nwhich productions are of the form A → BC or A→ a, where A, B ∈VN , and a ∈ V Τ. \\n \\nProcedure to find Equivalent Grammar in CNF \\n \\n(i) Eliminate the unit productions, and λ-productions if any, \\n \\n(ii) Eliminate the terminals on the right hand side of length two or more.'),\n",
       " Document(metadata={}, page_content='(iii) Restrict the number of variables on the right hand side of productions to two. \\nProof: \\n \\nFor Step (i): Apply the following theorem: “Every context free language can be generated by \\na grammar with no useless symbols and no unit productions”. \\n \\nAt the end of this step the RHS of any production has a single terminal or two or more symbols. \\n \\nLet us assume the equivalent resulting grammar as G = (VN ,VT ,P ,S ). \\nFor Step (ii): Consider any production of the form \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='Example \\n \\nObtain a grammar in Chomsky Normal Form (CNF) equivalent to the grammar G \\nwith productions P given \\n \\n \\n \\n \\n \\n \\nSolution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\nPumping Lemma for CFG \\nA “Pumping Lemma” is a theorem used to show that, if certain strings belong to a \\n \\nlanguage, then certain other strings must also belong to the language. Let us discuss a Pumping'),\n",
       " Document(metadata={}, page_content='Lemma for CFL. We will show that , if L is a context-free language, then strings of L that are at \\nleast ‘m’ symbols long can be “pumped” to produce additional strings in L. The value of ‘m’ \\ndepends on the particular language. Let L be an infinite context-free language. Then there is some \\npositive integer ‘m’ such that, if S is a string of L of Length at least ‘m’, then \\n \\n(i) S = uvwxy (for some u, v, w, x, y) \\n \\n(ii) | vwx| \\uf064 m \\n(iii) | vx| \\uf0651 \\n(iv) uv iwx i y∈L. \\n \\nfor all non-negative values of i.'),\n",
       " Document(metadata={}, page_content='It should be understood that \\n \\n(i) If S is sufficiently long string, then there are two substrings, v and x, somewhere in \\nS. There is stuff (u) before v, stuff (w) between v and x, and stuff (y), after x. \\n \\n(ii) The stuff between v and x won’t be too long, because | vwx | can’t be larger than m. \\n(iii) Substrings v and x won’t both be empty, though either one could be. \\n \\n(iv) If we duplicate substring v, some number (i) of times, and duplicate x the same'),\n",
       " Document(metadata={}, page_content='number of times, the resultant string will also be in L. \\n \\nDefinitions \\nA variable is useful if it occurs in the derivation of some string. This requires that \\n \\n(a) the variable occurs in some sentential form (you can get to the variable if you start from S), and \\n \\n(b) a string of terminals can be derived from the sentential form (the variable is not a “dead end”). \\nA variable is “recursive” if it can generate a string containing itself. For example, variable A is \\nrecursive if'),\n",
       " Document(metadata={}, page_content='Proof of Pumping Lemma \\n \\n(a) Suppose we have a CFL given by L. Then there is some context-free Grammar G that \\ngenerates L. Suppose \\n(i) L is infinite, hence there is no proper upper bound on the length of strings belonging to L. \\n \\n(ii) L does not contain l. \\n(iii) G has no productions or l-productions. \\nwww.indiansbrain.com\\nThere are only a finite number of variables in a grammar and the productions for each'),\n",
       " Document(metadata={}, page_content='variable have finite lengths. The only way that a grammar can generate arbitrarily long strings is if \\none or more variables is both useful and recursive. Suppose no variable is recursive. Since the start \\nsymbol is non recursive, it must be defined only in terms of terminals and other variables. Then \\nsince those variables are non recursive, they have to be defined in terms of terminals and still other \\nvariables and so on.'),\n",
       " Document(metadata={}, page_content='After a while we run out of “other variables” while the generated string is still finite. Therefore \\nthere is an upper bond on the length of the string which can be generated from the start \\nsymbol. This contradicts our statement that the language is finite. \\nHence, our assumption that no variable is recursive must be incorrect. \\n \\n(b) Let us consider a string X belonging to L. If X is sufficiently long, then the derivation of X'),\n",
       " Document(metadata={}, page_content='must have involved recursive use of some variable A. Since A was used in the derivation, the \\nderivation should have started as \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUsage of Pumping Lemma \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHence our original assumption, that L is context free should be false. Hence the language L is not \\ncon text-free. \\n \\nExample'),\n",
       " Document(metadata={}, page_content='Check whether the language given by L \\uf03d {a mbmcn : m \\uf064 n \\uf064 2m} is a CFL or not. \\nSolution \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nClosure properties of CFL – Substitution \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nApplications of substitution theorem \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReversal \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInverse Homomorphism: \\nwww.indiansbrain.com\\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='UNIT-V \\nTuring machine: \\n \\nInformal Definition: \\n \\nWe consider here a basic model of TM which is deterministic and have one-tape. There are many variations, \\nall are equally powerfull. \\n \\nThe basic model of TM has a finite set of states, a semi-infinite tape that has a leftmost cell but is infinite to \\nthe right and a tape head that can move left and right over the tape, reading and writing symbols.'),\n",
       " Document(metadata={}, page_content='For any input w with |w|=n, initially it is written on the n leftmost (continguous) tape cells. The infinitely many \\ncells to the right of the input all contain a blank symbol, B whcih is a special tape symbol that is not an input \\nsymbol. The machine starts in its start state with its head scanning the leftmost symbol of the input w. De-\\npending upon the symbol scanned by the tape head and the current state the machine makes a move which \\nconsists of the following: \\n \\n\\uf095'),\n",
       " Document(metadata={}, page_content='writes a new symbol on that tape cell,  \\uf095\\uf020\\n\\uf020\\nmoves its head one cell either to the left or to the right and \\n\\uf095 \\n(possibly) enters a new state.\\uf020\\n \\nThe action it takes in each step is determined by a transition functions. The machine continues computing (i.e. \\nmaking moves) until \\n \\n\\uf095 \\nit decides to \"accept\" its input by entering a special state called accept or final state or\\uf020\\n\\uf095 \\nhalts without accepting i.e. rejecting the input when there is no move defined.\\uf020'),\n",
       " Document(metadata={}, page_content='On some inputs the TM many keep on computing forever without ever accepting or rejecting the input, in \\nwhich case it is said to \"loop\" on that input \\n \\nFormal Definition : \\n \\nFormally, a deterministic turing machine (DTM) is a 7-tuple \\n, where \\n \\n\\uf095 \\nQ is a finite nonempty set of states.\\uf020\\n\\uf095 \\nis a finite non-empty set of tape symbols, callled the tape alphabet of M.\\uf020\\n\\uf020\\n\\uf095 \\nis a finite non-empty set of input symbols, called the input alphabet of M.\\uf020\\n\\uf095 \\nis the transition function of M,\\uf020\\nwww.indiansbrain.com\\n\\uf095'),\n",
       " Document(metadata={}, page_content='is the initial or start state.\\uf020\\n\\uf095 \\nis the blank symbol\\uf020\\n\\uf020\\n\\uf095 \\nis the set of final state.\\uf020\\n \\nSo, given the current state and tape symbol being read, the transition function describes the next state, symbol \\nto be written on the tape, and the direction in which to move the tape head ( L and R denote left and right, \\nrespectively ). \\n \\nTransition function :\\n \\n \\n\\uf095 \\nThe heart of the TM is the transition function, \\nbecause it tells us how the machine gets one step \\nto the next.\\uf020\\n\\uf020\\n\\uf095'),\n",
       " Document(metadata={}, page_content='when the machine is in a certain state q\\nQ and the head is currently scanning the tape symbol \\n, and if \\n, then the machine\\uf020\\n \\n1. replaces the symbol X by Y on the tape \\n \\n2. goes to state p, and \\n3. the tape head moves one cell ( i.e. one tape symbol ) to the left ( or right ) if D is L ( or R ). \\n \\nThe ID (instantaneous description) of a TM capture what is going out at any moment i.e. it contains all the \\ninformation to exactly capture the \"current state of the computations\".'),\n",
       " Document(metadata={}, page_content='It contains the following: \\n \\n\\uf095 \\nThe current state, q\\uf020\\n\\uf095 \\nThe position of the tape head,\\uf020\\n\\uf020\\n\\uf095 \\nThe constants of the tape up to the rightmost nonblank symbol or the symbol to the left of the head, \\nwhichever is rightmost.\\uf020\\n \\nNote that, although there is no limit on how far right the head may move and write nonblank symbols on the \\ntape, at any finite \\n \\ntime, the TM has visited only a finite prefix of the infinite tape. \\n \\nAn ID (or configuration) of a TM M is denoted by \\nwhere \\nand \\n \\n\\uf095'),\n",
       " Document(metadata={}, page_content='is the tape contents to the left of the head\\uf020\\n\\uf095 \\nq is the current state.\\uf020\\n\\uf020\\n\\uf095 \\nis the tape contents at or to the right of the tape head\\uf020\\n \\nThat is, the tape head is currently scanning the leftmost tape symbol of \\n. ( Note that if \\n, then the \\ntape head is scanning a blank symbol) \\n \\nIf \\nis the start state and w is the input to a TM M then the starting or initial configuration of M is onviously \\ndenoted by \\n \\nwww.indiansbrain.com\\nMoves of Turing Machines \\n \\nTo indicate one move we use the symbol'),\n",
       " Document(metadata={}, page_content='. Similarly, zero, one, or more moves will be represented by \\n. A \\nmove of a TM \\n \\nM is defined as follows. \\n \\nLet \\nbe an ID of M where \\n, \\nand \\n. \\n \\nLet there exists a transition \\nof M. \\n \\nThen we write \\nmeaning that ID \\nyields \\n \\n \\n\\uf095 \\nAlternatively \\n, \\nif \\nis \\na \\ntransition \\nof \\nM, \\nthen \\nwe \\nwrite \\nwhich means that the ID \\nyields \\n\\uf020\\n\\uf020\\n\\uf095 \\nIn other words, when two IDs are related by the relation \\n, we say that the first one yields the \\nsecond ( or the second is the result of the first) by one move.\\uf020\\n\\uf095'),\n",
       " Document(metadata={}, page_content='If IDj results from IDi by zero, one or more (finite) moves then we write \\n( If the TM M is understand, \\nthen the subscript M can be dropped from \\nor \\n)\\uf020\\n \\nSpecial Boundary Cases \\n \\n\\uf095 \\nLet \\nbe an ID and \\nbe an transition of M. Then \\n. That is, the head is not \\nallowed to fall off the left end of the tape.\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure (Note that \\nis equivalent to \\n)\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n\\uf020\\n\\uf095 \\nLet \\nbe an ID and \\nthen figure\\uf020\\n \\nThe language accepted by a TM \\n, denoted as L(M) is'),\n",
       " Document(metadata={}, page_content='L(M) = { w | \\nand figure for some p\\nF and \\n} \\n \\nIn other words the TM M accepts a string \\nthat cause M to enter a final or accepting state when started \\nin its initial ID (i.e. \\n). That is a TM M accepts the string \\nif a sequence of IDs, \\n \\nexists such that \\n \\n\\uf095 \\nis the initial or starting ID of M\\uf020\\n\\uf095 \\n; \\n\\uf020\\nwww.indiansbrain.com\\n\\uf095 \\nThe representation of IDk contains an accepting state.\\uf020\\n \\nThe set of strings that M accepts is the language of M, denoted L(M), as defined'),\n",
       " Document(metadata={}, page_content='above More about configuration and acceptance \\n \\n\\uf095 \\nAn ID \\nof M is called an accepting (or final) ID if \\n\\uf020\\n\\uf020\\n\\uf095 \\nAn ID \\nis called a blocking (or halting) ID if \\nis undefined i.e. the TM has no move at this \\npoint.\\uf020\\n\\uf095 \\nis called reactable from \\nif \\n\\uf020\\n\\uf020\\n\\uf095 \\nis the initial (or starting) ID if \\nis the input to the TM and \\nis the initial (or start) state \\nof M.\\uf020\\n \\nOn any input string \\n \\n \\neither \\n \\n\\uf095 \\nM halts on w if there exists a blocking (configuration) ID, \\nsuch that \\n\\uf020\\n \\nThere are two cases to be considered'),\n",
       " Document(metadata={}, page_content='\\uf095 \\nM accepts w if I is an accepting ID. The set of all \\naccepted by M is denoted as L(M) as\\uf020\\nalready defined \\n \\n\\uf095 \\nM rejects w if \\nis a blocking configuration. Denote by reject (M), the set of all \\nrejected by M.\\uf020\\n \\nor \\n \\n\\uf095 \\nM loops on w if it does not halt on w.\\uf020\\n \\nLet loop(M) be the set of all \\non which M loops for. \\n \\nIt is quite clear that \\n \\n \\n \\n \\nThat is, we assume that a TM M halts \\n \\n\\uf095 \\nWhen it enters an accepting \\nor\\uf020\\n\\uf095 \\nWhen it enters a blocking \\ni.e. when there is no next move.\\uf020'),\n",
       " Document(metadata={}, page_content='However, on some input string, , \\n, it is possible that the TM M loops for ever i.e. it never halts \\nwww.indiansbrain.com\\n \\nThe Halting Problem \\n \\nThe input to a Turing machine is a string. Turing machines themselves can be written as strings. \\nSince these strings can be used as input to other Turing machines. A “Universal Turing \\nmachine” is one whose input consists of a description M of some arbitrary Turing machine, and'),\n",
       " Document(metadata={}, page_content='some input w to which machine M is to be applied, we write this combined input as M + w. \\nThis produces the same output that would be produced by M. This is written as \\n \\nUniversal Turing Machine (M + w) = M (w). \\n \\nAs a Turing machine can be represented as a string, it is fully possible to supply a Turing \\n \\nmachine as input to itself, for example M (M). This is not even a particularly bizarre thing to do for'),\n",
       " Document(metadata={}, page_content='example, suppose you have written a C pretty printer in C, then used the Pretty printer on itself. \\nAnother common usage is Bootstrapping—where some convenient languages used to write a \\nminimal compiler for some new language L, then used this minimal compiler for L to write a new, \\nimproved compiler for language L. Each time a new feature is added to language L, you can \\nrecompile and use this new feature in the next version of the compiler. Turing machines sometimes'),\n",
       " Document(metadata={}, page_content='halt, and sometimes they enter an infinite loop. \\n \\nA Turing machine might halt for one input string, but go into an infinite loop when given \\nsome other string. The halting problem asks: “It is possible to tell, in general, whether a given \\n \\nmachine will halt for some given input?” If it is possible, then there is an effective procedure to look \\nat a Turing machine and its input and determine whether the machine will halt with that input. If'),\n",
       " Document(metadata={}, page_content='there is an effective procedure, then we can build a Turing machine to implement it. Suppose we \\nhave a Turing machine “WillHalt” which, given an input string M + w, will halt and accept the string \\nif Turing machine M halts on input w and will halt and reject the string if Turing machine M does \\nnot halt on input w. When viewed as a Boolean function, “WillHalt (M, w)” halts and returns \\n“TRUE” in the first case, and (halts and) returns “FALSE” in the second. \\n \\nTheorem'),\n",
       " Document(metadata={}, page_content='Turing Machine “WillHalt (M, w)” does not exist. \\n \\nProof: This theorem is proved by contradiction. Suppose we could build a machine “WillHalt”. \\nThen we can certainly build a second machine, “LoopIfHalts”, that will go into an infinite loop \\nif and only if “WillHalt” accepts its input: \\n Function LoopIfHalts (M, \\nw): if WillHalt (M, w) then \\nwhile true do { } \\nelse \\nreturn false; \\n \\nWe will also define a machine “LoopIfHaltOnItSelf” that, for any given input M, representing a'),\n",
       " Document(metadata={}, page_content='Turing machine, will determine what will happen if M is applied to itself, and loops if M will halt \\nin this case. \\n Function LoopIfHaltsOnItself (M): \\nreturn LoopIfHalts (M, M): \\n \\nFinally, we ask what happens if we try: \\nFunc tion Impos sible: \\nreturn LoopIfHaltsOnItself (LoopIfHaltsOnItself): \\n \\nThis machine, when applied to itself, goes into an infinite loop if and only if it halts \\nwhen applied to itself. This is impossible. Hence the theorem is proved. \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='Implications of Halting Problem \\n \\nProgramming \\nThe Theorem of “Halting Problem” does not say that we can never determine whether or not \\n \\na given program halts on a given input. Most of the times, for practical reasons, we could \\neliminate infinite loops from programs. Sometimes a “meta-program” is used to check another \\nprogram for potential infinite loops, and get this meta-program to work most of the time.'),\n",
       " Document(metadata={}, page_content='The theorem says that we cannot ever write such a meta-program and have it work all of the \\ntime. This result is also used to demonstrate that certain other programs are also impossible. \\nThe basic outline is as follows: \\n \\n(i) If we could solve a problem X, we could solve the Halting problem \\n(ii) We cannot solve the Halting Problem \\n(iii) Therefore, we cannot solve problem X \\n \\n \\n \\n \\n \\n \\nA Turing machine can be \"programmed,\" in much the same manner as a computer is'),\n",
       " Document(metadata={}, page_content='programmed. When one specifies the function which we usually call δ for a Tm, he is really \\nwriting a program for the Tm. \\n \\n1. Storage in finite Control \\n \\nThe finite control can be used to hold a finite amount of information. To do so, the state is written \\nas a pair of elements, one exercising control and the other storing a symbol. It should be \\n \\nemphasized that this arrangement is for conceptual purposes only. No modification in the definition \\nof the Turing machine has been made. \\n \\nExample'),\n",
       " Document(metadata={}, page_content='Consider the Turing machine \\nSolution \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2. Multiple Tracks \\n \\nWe can imagine that the tape of the Turing machine is divided into k tracks, for any finite k. This \\narrangement is shown in Fig., with k = 3. What is actually done is that the symbols on the tape \\nare considered as k-tuples. One component for each track. \\n \\nExample'),\n",
       " Document(metadata={}, page_content='The tape in Fig. can be imagined to be that of a Turing machine which takes a binary input \\n \\ngreater than 2, written on the first track, and determines if it is a prime. The input is surrounded by \\n¢ and $ on the first track. \\n \\nThus, the allowable input symbols are [¢, B, B], [0, B, B ], [1, B, B ], and [$, B, B]. These \\n \\nsymbols can be identified with ¢, 0, 1, and $, respectively, when viewed as input symbols. The blank \\nwww.indiansbrain.com\\nsymbol can be represented by [B, B, B ]'),\n",
       " Document(metadata={}, page_content='To test if its input is a prime, the Tm first writes the number two in binary on the second track \\n \\nand copies the first track onto the third track. Then, the second track is subtracted, as many times \\nas possible, from the third track, effectively dividing the third track by the second and leaving the \\nremainder. If the remainder is zero, the number on the first track is not a prime. If the remainder is \\nnonzero, increase the number on the second track by one.'),\n",
       " Document(metadata={}, page_content='If now the second track equals the first, the number on the first track is a prime, because it cannot \\nbe divided by any number between one and itself. If the second is less than the first, the whole \\noperation is repeated for the new number on the second track. In Fig., the Tm is testing to determine \\nif 47 is a prime. The Tm is dividing by 5; already 5 has been subtracted twice, so 37 appears on the \\nthird track. \\n \\n3. Subroutines \\nwww.indiansbrain.com\\n \\nUNDECIDABILITY'),\n",
       " Document(metadata={}, page_content='Design a Turing machine to add two given integers. \\n \\nSolution: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSome unsolvable Problems are as follows: \\n(i) Does a given Turing machine M halts on all input? \\n(ii) Does Turing machine M halt for any input? \\n(iii) Is the language L(M) finite? \\n \\n(iv) Does L(M) contain a string of length k, for some given k? \\n \\n(v) Do two Turing machines M1 and M2 accept the same language?'),\n",
       " Document(metadata={}, page_content='It is very obvious that if there is no algorithm that decides, for an arbitrary given Turing machine \\nM and input string w, whether or not M accepts w. These problems for which no algorithms exist \\nare called “UNDECIDABLE” or “UNSOLVABLE”. \\n \\nCode for Turing Machine: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\nDiagonalization language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis table represents language acceptable by Turing machine \\nwww.indiansbrain.com'),\n",
       " Document(metadata={}, page_content='Proof that Ld is not recursively enumerable: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRecursive Languages: \\nwww.indiansbrain.com\\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUniversal \\n \\nLanguage: \\nwww.indiansbrain.com\\n \\nUndecidability of Universal Language: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nProblem -Reduction : \\nIf P1 reduced to P2,'),\n",
       " Document(metadata={}, page_content=\"Then P2 is at least as hard as P1. \\nTheorem: If P1 reduces to P2 then, \\n\\uf095 If P1 is undecidable the so is P2.\\uf020\\n\\uf095 If P1 is Non-RE then so is P2.\\uf020\\nwww.indiansbrain.com\\nPost's Correspondence Problem (PCP) \\n \\nA post correspondence system consists of a finite set of ordered pairs \\nwhere \\nfor some alphabet \\n. \\n \\nAny sequence of numbers \\n \\n \\nis called a solution to a Post Correspondence System. \\n \\nThe Post's Correspondence Problem is the problem of determining whether\"),\n",
       " Document(metadata={}, page_content='a Post Correspondence system has a solutions. \\n \\nExample 1 : Consider the post correspondence system \\n \\n The list 1,2,1,3 is a solution to it. \\n \\nBecause \\n \\n \\n \\n \\n \\n \\n \\ni \\n             xi                                 yi \\n \\n1 \\n \\n2 \\n \\n3 \\n \\n \\n(A post correspondence system is also denoted as an instance of the \\nPCP) Example 2 : The following PCP instance has no solution \\ni \\n          xi                          yi \\n \\n1 \\n \\n2 \\n \\nThis can be proved as follows.'),\n",
       " Document(metadata={}, page_content='cannot be chosen at the start, since than the LHS and RHS would \\ndiffer in the first symbol ( \\nin LHS and \\nin RHS). So, we must start with \\n. The next pair must be \\nso that the 3 rd symbol in the RHS becomes identical to that of the LHS, which is a . After this step, \\nLHS and RHS are not matching. If \\nis selected next, then would be mismatched in the 7 th symbol \\nwww.indiansbrain.com\\n( \\nin LHS and \\nin RHS). If \\nis selected, instead, there will not be any choice to match the both side \\nin the next step.'),\n",
       " Document(metadata={}, page_content='Example3 : The list 1,3,2,3 is a solution to the following PCP instance. \\n \\n \\ni \\n   \\nx\\ni \\n   \\ny\\ni \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n1 \\n \\n1 \\n \\n101 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n2 \\n \\n10 \\n \\n00 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\n3 \\n011 \\n11 \\n \\n \\n \\n   \\n \\n  \\n  \\n \\n \\nThe following properties can easily be proved. \\n \\nProposition The Post Correspondence System \\n \\n has solutions if and only if \\n \\n \\n \\n \\n \\n \\nCorollary : PCP over one-letter alphabet is decidable. \\n \\nProposition Any PCP instance over an alphabet \\nwith'),\n",
       " Document(metadata={}, page_content='is equivalent to a PCP instance over \\nan alphabet \\nwith \\n \\n \\nProof : Let \\n \\n \\nConsider \\nWe can now encode every \\nas \\nany PCP instance over \\nwill now \\nhave only two symbols, 0 and 1 and, hence, is equivalent to a PCP instance over \\n \\n \\nTheorem : PCP is undecidable. That is, there is no algorithm that determines whether an arbitrary Post \\nCorrespondence System has a solution. \\n \\nProof: The halting problem of turning machine can be reduced to PCP to show the undecidability of PCP. Since'),\n",
       " Document(metadata={}, page_content='halting problem of TM is undecidable (already proved), This reduction shows that PCP is also undecidable. \\nThe proof is little bit lengthy and left as an exercise. \\n \\nSome undecidable problem in context-free languages \\n \\nWe can use the undecidability of PCP to show that many problem concerning the context-free languages \\nare undecidable. To prove this we reduce the PCP to each of these problem. The following discussion \\nmakes it clear how PCP can be used to serve this purpose. \\nwww.indiansbrain.com\\nLet'),\n",
       " Document(metadata={}, page_content=\"be a Post Correspondence System over the alphabet \\n. We \\nconstruct two CFG's Gx and Gy from the ordered pairs x,y respectively as follows. \\n \\n and \\n \\n where \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand \\n \\n \\nit is clear that the grammar \\ngenerates the strings that can appear in the LHS of a sequence while solving the \\nPCP followed by a sequence of numbers. The sequence of number at the end records the sequence of \\n \\nstrings from the PCP instance (in reverse order) that generates the string. Similarly,\"),\n",
       " Document(metadata={}, page_content='generates the \\nstrings that can be obtained from the RHS of a sequence and the corresponding sequence of numbers (in \\nreverse order). \\n \\nNow, if the Post Correspondence System has a solution, then there must be a sequence \\n \\n \\n \\n \\n \\n \\n \\n \\nAccording to the construction of \\nand \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn this case \\nwww.indiansbrain.com\\n \\n \\nHence , \\nand \\nimplying \\n \\n \\n \\n \\n \\nConversely, let \\n \\n \\nHence, w must be in the form w1w2 where \\nand w2 in a sequence \\n(since, only that kind'),\n",
       " Document(metadata={}, page_content='of strings can be generated by each of \\nand \\n). \\n \\nNow, the string \\nis a solution to the Post Correspondence System. \\n \\nIt is interesting to note that we have here reduced PCP to the language of pairs of CFG,s whose intersection is \\nnonempty. The following result is a direct conclusion of the above. \\n \\nTheorem : Given any two CFG\\'s G1 and G2 the question \"Is \\n\" is undecidable. \\n \\nProof: Assume for contradiction that there exists an algorithm A to decide this question. This would imply'),\n",
       " Document(metadata={}, page_content=\"that PCP is decidable as shown below. \\n \\nFor any Post Correspondence System, P construct grammars \\nand \\nby using the constructions \\nelaborated already. We can now use the algorithm A to decide whether and \\nThus, PCP is decidable, a contradiction. So, such an algorithm does not exist. \\n \\nIf \\nand \\nare CFG's constructed from any arbitrary Post Correspondence System, than it is not difficult to \\n \\nshow that \\nand \\nare also context-free, even though the class of context-free languages are\"),\n",
       " Document(metadata={}, page_content='not closed under complementation. \\n \\nand their complements can be used in various ways to show that many other questions \\nrelated to CFL\\'s are undecidable. We prove here some of those. \\n \\nTheorem : Foe any two arbitrary CFG\\'s \\nthe following questions are undecidable \\n \\ni. \\nIs \\n \\n \\nii. \\nIs \\n \\nwww.indiansbrain.com\\niii. Is \\n \\nProof : \\ni. \\nIf \\nthen, \\n \\n \\nHence, it suffice to show that the question “Is \\n\" is undecidable. \\n \\nSince, \\nand \\nare CFl\\'s and CFL\\'s are closed under union, \\nis also context-'),\n",
       " Document(metadata={}, page_content=\"free. By DeMorgan's theorem, \\n \\n \\nIf there is an algorithm to decide whether \\nwe can use it to decide whether \\nor not. But this problem has already been proved to be undecidable. \\n \\n \\nHence there is no such algorithm to decide or not. \\nii. \\n \\nLet P be any arbitrary Post correspondence system and \\nand \\nare CFg's constructed from the pairs of \\nstrings. \\n \\nmust be a CFL and let G1generates L1. That is, \\n \\n \\n \\n \\n \\nby De Morgan's theorem, as shown already, any string, \\nrepresents a solution to the \\nPCP. Hence,\"),\n",
       " Document(metadata={}, page_content='contains all but those strings representing the solution to the PCP. \\n \\nLet \\nfor same CFG G2. \\n \\nIt is now obvious that \\nif and only if the PCP has no solutions, which is already proved to be \\nundecidable. Hence, the question “Is \\n?\" is undecidable. \\n \\niii. \\nwww.indiansbrain.com\\nLet \\nbe a CFG generating the language \\nand G2 be a CFG generating \\nwhere \\nand \\nare CFG.s constructed from same arbitrary instance of PCP. \\n \\niff \\n \\n \\ni.e. iff the PCP instance has no solutions as discussed in part (ii).'),\n",
       " Document(metadata={}, page_content=\"Hence the proof. \\n \\nTheorem : It is undecidable whether an arbitrary CFG is ambiguous. \\n \\nProof : Consider an arbitrary instance of PCP and construct the CFG's \\nand \\nfrom the ordered pairs \\nof strings. \\n \\nWe construct a new grammar G from \\nand \\nas follows. \\n \\n where \\n \\n \\n \\n \\n \\nis same as that of \\nand \\n. \\n \\n \\n \\n \\n \\nThis constructions gives a reduction of PCP to the -------- of whether a CFG is ambiguous, thus leading to\"),\n",
       " Document(metadata={}, page_content='the undecidability of the given problem. That is, we will now show that the PCP has a solution if and only if G \\nis ambiguous. (where G is constructed from an arbitrary instance of PCP). \\n \\nOnly if Assume that \\nis a solution sequence to this instance of PCP. \\n \\nConsider the following two derivation in \\n. \\nwww.indiansbrain.com\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBut , \\n \\n \\n \\n \\nis a solution to the PCP. Hence the same string of terminals \\nhas two derivations. Both these'),\n",
       " Document(metadata={}, page_content='derivations are, clearly, leftmost. Hence G is ambiguous. \\n \\nIf It is important to note that any string of terminals cannot have more than one derivation in \\nand \\n \\nBecause, every terminal string which are derivable under these grammars ends with a sequence of integers \\nThis sequence uniquely determines which productions must be used at every step of the derivation. \\n \\nHence, if a terminal string, \\n, has two leftmost derivations, then one of them must begin with \\nthe step.'),\n",
       " Document(metadata={}, page_content='then continues with derivations under \\n \\n \\nIn both derivations the resulting string must end with a sequence \\nfor same \\nThe reverse \\nof this sequence must be a solution to the PCP, because the string that precede in one case is \\n \\nand \\nin the other case. Since the string derived in both cases are identical, the \\n \\nsequence \\n \\nmust be a solution to the PCP. \\n \\nHence the proof \\nwww.indiansbrain.com\\nClass p-problem solvable in polynomial time: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNon deterministic polynomial time:'),\n",
       " Document(metadata={}, page_content='A nondeterministic TM that never makes more than p(n) moves in any sequence of choices for \\nsome polynomial p is said to be non polynomial time NTM. \\n\\uf095 NP is the set of languags that are accepted by polynomial time NTM’s\\uf020\\n\\uf020\\n\\uf095 Many problems are in NP but appear not to be in p.\\uf020\\n\\uf095 One of the great mathematical questions of our age: is there anything in NP that is not in p?\\uf020\\nNP-complete problems: \\n\\uf020\\nIf We cannot resolve the “p=np question, we can at least demonstrate that certain problems in NP'),\n",
       " Document(metadata={}, page_content='are the hardest , in the sense that if any one of them were in P , then P=NP. \\n\\uf020\\n\\uf095 These are called NP-complete.\\uf020\\n\\uf020\\n\\uf095 Intellectual leverage: Each NP-complete problem’s apparent difficulty reinforces the belief \\nthat they are all hard.\\uf020\\n \\nMethods for proving NP-Complete problems: \\n \\n\\uf095 Polynomial time reduction (PTR): Take time that is some polynomial in the input size to \\nconvert instances of one problem to instances of another.\\uf020\\n\\uf020\\n\\uf095 If P1 PTR to P2 and P2 is in P1 the so is P1.\\uf020'),\n",
       " Document(metadata={}, page_content='\\uf095 Start by showing every problem in NP has a PTR to Satisfiability of Boolean formula.\\uf020\\n\\uf020\\n\\uf095 Then, more problems can be proven NP complete by showing that SAT PTRs to them \\ndirectly or indirectly.\\uf020\\nwww.indiansbrain.com')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the content into documents\n",
    "from langchain.docstore.document import Document\n",
    "# Create a Document object from the content\n",
    "document=Document(page_content=combined_content)\n",
    "# Split the content into documents\n",
    "documents = text_splitter.split_documents([document])\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automata Tutorial | Theory of Computation - JavatpointTutorials×PythonPython Django Numpy Pandas Tkinter Pytorch Flask OpenCVAI, ML and Data ScienceArtificial Intelligence Machine Learning Data Science Deep Learning TensorFlow Artificial Neural Network Matplotlib Python ScipyJavaJava Servlet JSP Spring Boot Spring Framework Hibernate JavaFX Java Web ServicesB.Tech and MCADBMS Data Structures Operating System Computer Network DAA Computer Organization Software Engineering Data MiningWeb TechnologyHTML CSS'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embedding \n",
    "* initialize the tokenizer and model for hugging face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model for Hugging Face embeddings\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForQuestionAnswering, pipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# Initialize the HuggingFaceEmbeddings class with a model name\n",
    "# Initialize the HuggingFaceEmbeddings class with a model name\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Chroma collection and add documents with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for each document\n",
    "texts = [doc.page_content for doc in documents]\n",
    "embeddings = embedding_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma collection\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_docs \u001b[38;5;241m=\u001b[39m documents[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m      5\u001b[0m batch_texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m batch_docs]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mvectordb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "# Add documents to Chroma in smaller batches\n",
    "batch_size = 166\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_docs = documents[i:i + batch_size]\n",
    "    batch_texts = [doc.page_content for doc in batch_docs]\n",
    "    vectordb.add_texts(texts=batch_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Batch size 435 exceeds maximum batch size 166",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Chroma vector database from documents\u001b[39;00m\n\u001b[0;32m      2\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m vectordb\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    879\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    880\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    881\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    882\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    883\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    884\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    885\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    886\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    887\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    889\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:842\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    837\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    838\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    839\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    840\u001b[0m         )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:320\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m         embeddings_without_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             [embeddings[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m empty_ids] \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m         )\n\u001b[0;32m    319\u001b[0m         ids_without_metadatas \u001b[38;5;241m=\u001b[39m [ids[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m empty_ids]\n\u001b[1;32m--> 320\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_without_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_without_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_without_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    327\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    328\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    329\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:302\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    292\u001b[0m (\n\u001b[0;32m    293\u001b[0m     ids,\n\u001b[0;32m    294\u001b[0m     embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m     ids, embeddings, metadatas, documents, images, uris\n\u001b[0;32m    300\u001b[0m )\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\chromadb\\api\\segment.py:452\u001b[0m, in \u001b[0;36mSegmentAPI._upsert\u001b[1;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[0;32m    450\u001b[0m coll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collection(collection_id)\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mhint_use_collection(collection_id, t\u001b[38;5;241m.\u001b[39mOperation\u001b[38;5;241m.\u001b[39mUPSERT)\n\u001b[1;32m--> 452\u001b[0m \u001b[43mvalidate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_max_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m records_to_submit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    457\u001b[0m     _records(\n\u001b[0;32m    458\u001b[0m         t\u001b[38;5;241m.\u001b[39mOperation\u001b[38;5;241m.\u001b[39mUPSERT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m     )\n\u001b[0;32m    465\u001b[0m )\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record_set(coll, records_to_submit)\n",
      "File \u001b[1;32mc:\\Users\\ACER 5\\Desktop\\ASHWINI KAKDE\\Langchain\\langchain\\lib\\site-packages\\chromadb\\api\\types.py:571\u001b[0m, in \u001b[0;36mvalidate_batch\u001b[1;34m(batch, limits)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_batch\u001b[39m(\n\u001b[0;32m    561\u001b[0m     batch: Tuple[\n\u001b[0;32m    562\u001b[0m         IDs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m     limits: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    569\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m limits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds maximum batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Batch size 435 exceeds maximum batch size 166"
     ]
    }
   ],
   "source": [
    "# Add documents in smaller batches\n",
    "batch_size = 100\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_docs = documents[i:i + batch_size]\n",
    "    batch_texts = [doc.page_content for doc in batch_docs]\n",
    "    vectordb.add_texts(\n",
    "        texts=batch_texts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tokenizer and model for question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model for question-answering\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pipeline for question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=model_qa, tokenizer=tokenizer_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    # Use the vector DB to find the most relevant document\n",
    "    query_embedding = embeddings.embed_documents([question])[0]\n",
    "    results = vectordb.similarity_search(question, k=1)\n",
    "\n",
    "    # Get the most relevant document\n",
    "    most_relevant_doc = results[0].page_content\n",
    "\n",
    "    # Use the QA model to find the answer\n",
    "    answer = qa_pipeline(question=question, context=most_relevant_doc)\n",
    "    return answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get an answer to a query\n",
    "question = \"What is Regular Expression  in Theory of computation?\"\n",
    "answer = get_answer(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using another model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
