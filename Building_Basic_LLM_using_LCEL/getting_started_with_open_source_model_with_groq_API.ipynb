{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gettting started with GROQ_API \n",
    "* GROQ_API is a free api you can use to call your LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Groq API key\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model used is The Gemma 2-9B IT \n",
    "* model is a large language model developed by Google and supported by Groq1\n",
    "* Here are some key details about it:\n",
    "* Developer: Google1\n",
    "* Model Size: 9 billion parameters1\n",
    "* Context Window: 8,192 tokens2\n",
    "* Use Cases: Text generation, question answering, summarization, and reasoning1\n",
    "* Performance: Designed for high performance and efficiency, making it suitable for both online and offline applications3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000027A51A7A3B0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027A51AA4130>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "# Initialize Groq API client with the new API key\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building simple LLM ,Prompt and StroutputCHains with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    SystemMessage(content=\"You are helpful assistant. PLease provide me clar and concise answer for question asked\"),\n",
    "    HumanMessage(content=\"What is Langchain?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are helpful assistant. PLease provide me clar and concise answer for question asked', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is Langchain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langchain is a framework designed to help developers build applications powered by large language models (LLMs). \\n\\nThink of it as a toolbox filled with components that make it easier to:\\n\\n* **Connect LLMs to other data sources:**  It lets you pull information from databases, APIs, and files to give your LLM more context.\\n* **Chain together different LLMs and tools:**  You can create complex workflows by combining multiple LLMs and other tools in a sequence.\\n* **Manage and improve LLM performance:** It offers techniques for prompting, memory management, and evaluation to enhance the quality and reliability of your LLM applications.\\n\\n\\nEssentially, Langchain simplifies the process of building sophisticated and useful applications that leverage the power of LLMs. \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 30, 'total_tokens': 188, 'completion_time': 0.287272727, 'prompt_time': 0.000183989, 'queue_time': 0.013290498999999999, 'total_time': 0.287456716}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e271390b-af58-440e-8ebd-3276b4be9c48-0', usage_metadata={'input_tokens': 30, 'output_tokens': 158, 'total_tokens': 188})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langchain is a framework that simplifies building applications with large language models (LLMs). \\n\\nThink of it like a toolbox for developers who want to use LLMs to do more than just generate text. \\n\\n**Here's what Langchain does:**\\n\\n* **Connects LLMs to other tools and data:** It lets you integrate LLMs with APIs, databases, and files, allowing them to access and process information beyond their training data.\\n* **Chains together different LLMs and components:** You can create complex workflows by linking multiple LLMs or other tools in a sequence.\\n* **Provides building blocks for common LLM tasks:** It offers pre-built modules for tasks like question answering, summarization, and code generation, making development faster.\\n\\n**In short, Langchain empowers developers to build powerful and versatile applications using the capabilities of LLMs.**\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 30, 'total_tokens': 209, 'completion_time': 0.325454545, 'prompt_time': 0.00015035, 'queue_time': 0.014011108, 'total_time': 0.325604895}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-01f39f41-a778-4cf1-8f10-fc014f460f7b-0', usage_metadata={'input_tokens': 30, 'output_tokens': 179, 'total_tokens': 209})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=model.invoke(messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Langchain is a framework that simplifies building applications with large language models (LLMs). \\n\\nThink of it like a toolbox for developers who want to use LLMs to do more than just generate text. \\n\\n**Here's what Langchain does:**\\n\\n* **Connects LLMs to other tools and data:** It lets you integrate LLMs with APIs, databases, and files, allowing them to access and process information beyond their training data.\\n* **Chains together different LLMs and components:** You can create complex workflows by linking multiple LLMs or other tools in a sequence.\\n* **Provides building blocks for common LLM tasks:** It offers pre-built modules for tasks like question answering, summarization, and code generation, making development faster.\\n\\n**In short, Langchain empowers developers to build powerful and versatile applications using the capabilities of LLMs.**\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By using output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant. Please respond to questions clearly and concisely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ask question from user \n",
    "def ask_question(question):\n",
    "    human_message=HumanMessage(content=f\"question:{question}\")\n",
    "    message=[system_message,human_message]\n",
    "    response=model.invoke(message)\n",
    "    response = response.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a framework for developing applications powered by large language models (LLMs). \\n\\nIt provides tools and components for:\\n\\n* **Connecting LLMs to external data sources:**  Enabling LLMs to access and process information beyond their training data.\\n* **Building complex LLM workflows:**  Chaining together multiple LLMs and other tools to perform sophisticated tasks.\\n* **Fine-tuning LLMs for specific applications:**  Adapting pre-trained LLMs to perform better on particular tasks. \\n\\n\\nEssentially, Langchain simplifies the process of building powerful and practical applications with LLMs.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What is Langchain?\"\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Turing Machine is a theoretical model of computation that forms the foundation of computer science. \\n\\n**Here\\'s a detailed explanation:**\\n\\n**Core Components:**\\n\\n* **Tape:** An infinitely long strip divided into cells, each capable of holding a single symbol from a finite alphabet.\\n* **Head:** A read/write device that can move left or right along the tape, reading and writing symbols in the cells.\\n* **State Register:**  Holds the current state of the machine, chosen from a finite set of states.\\n* **Transition Function:** The \"brain\" of the Turing Machine. It defines how the machine behaves based on its current state and the symbol it reads. This function dictates:\\n    * The next state the machine will enter.\\n    * The symbol to write on the tape.\\n    * The direction the head will move (left or right).\\n\\n**How it Works:**\\n\\n1. The Turing Machine starts in a designated initial state with an input string on the tape.\\n2. The head reads the first symbol on the tape.\\n3. Based on the current state and the read symbol, the transition function determines the next state, the symbol to write, and the head\\'s movement direction.\\n4. These actions are performed, and the process repeats, with the head moving along the tape, reading, writing, and changing states according to the transition function.\\n5. This continues until a halting state is reached, indicating the computation is complete.\\n\\n**The Power of Turing Machines:**\\n\\n* **Universal Computation:**  A Turing Machine can simulate any computer algorithm.  This means they can theoretically solve any problem that can be solved algorithmically.\\n* **Theoretical Significance:** Turing Machines provide a formal framework for understanding the limits of computation. They help us define what problems are solvable and unsolvable.\\n\\n**Limitations:**\\n\\n* **Abstract Model:** Turing Machines are theoretical constructs and not physical machines. They don\\'t directly translate to real-world computers.\\n* **Infinite Tape:** The concept of an infinite tape is impractical for physical implementation.\\n\\n**Key Takeaways:**\\n\\nTuring Machines are powerful theoretical models that:\\n\\n* Form the basis of our understanding of computation.\\n* Demonstrate the limits of what can be computed.\\n* Show us the fundamental principles behind how computers work.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What is Turing Machine please explain in detail?\"\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    # Create a human message\n",
    "    human_message = HumanMessage(content=f\"Question: {question}\")\n",
    "    \n",
    "    # Combine the messages\n",
    "    messages = [system_message, human_message]\n",
    "    \n",
    "    # Generate response using the model\n",
    "    try:\n",
    "        response = model.invoke(messages)\n",
    "        # Extract and return the content, removing extra newline characters\n",
    "        cleaned_response = response.content.replace(\"\\n\", \" \").strip()\n",
    "        return cleaned_response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Turing Machine is a theoretical model of computation.    **Imagine a simple machine with:**  * **Tape:** An infinitely long strip divided into cells, each holding a symbol from a finite alphabet. * **Head:**  A device that can read and write symbols on the tape. It can also move left or right along the tape. * **State Register:** Holds the current state of the machine, chosen from a finite set of states. * **Transition Function:** This is the heart of the Turing Machine. It dictates the machine's actions based on the current state and the symbol it reads.  The function defines:     * The next state the machine enters.     * The symbol to write on the tape.     * Whether the head moves left or right.  **How it Works:**  1. The machine starts in a designated initial state with an initial tape configuration. 2. It reads the symbol under the head. 3. Using the transition function, it determines its next state, the symbol to write, and the head's movement. 4. It performs these actions and repeats steps 2-3 until it reaches a designated halting state.  **Essentially, a Turing Machine manipulates symbols on a tape according to a set of rules. Its power lies in its ability to:**  * **Store and process information:** The tape acts as memory. * **Follow complex instructions:** The transition function allows for intricate computations. * **Be arbitrarily complex:** The size of the alphabet, states, and transition rules can be vast.  **Why is it Important?**  * **Theoretical Foundation:** Turing Machines provide a formal model for understanding the limits of computation. They demonstrate what problems can be solved algorithmically. * **Church-Turing Thesis:** This powerful idea states that any problem solvable by an algorithm can also be solved by a Turing Machine.  **Limitations:**  * **Theoretical:** Turing Machines are abstract models and not physical machines. * **Infinite Tape:**  The infinite tape assumption is impractical for real-world implementation.  **In Conclusion:**  The Turing Machine is a fundamental concept in computer science, demonstrating the power and limitations of computation. Though theoretical, it provides a powerful framework for understanding the nature of algorithms and the problems they can solve.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What is Turing Machine please explain in detail?\"\n",
    "ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Legacy Langchain Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are an expert! please reposnd to my question with clear and consise answer', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is DFA in TOC?', additional_kwargs={}, response_metadata={})]\n",
      "Question asked to model is: What is DFA in TOC?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"DFA stands for **Deterministic Finite Automata** in the context of Theory of Computation (TOC). \\n\\nIt's a fundamental model of computation used to describe and analyze **finite state machines**. \\n\\nThink of it as a simple computer program with a limited set of states and transitions that can only accept or reject input strings based on those states. \\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are an expert! please reposnd to my question with clear and consise answer\"),\n",
    "    HumanMessage(content=\"What is DFA in TOC?\")\n",
    "]\n",
    "print(messages)\n",
    "result=model.invoke(messages)\n",
    "human_message=HumanMessage(content=\"What is DFA in TOC?\")\n",
    "# Print only the question from the instance\n",
    "print(f\"Question asked to model is: {human_message.content}\")\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LCEL chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "promt = ChatPromptTemplate.from_template(\"Tell me interesting facts about:{technology}\")\n",
    "chain = promt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['technology'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['technology'], input_types={}, partial_variables={}, template='Tell me interesting facts about:{technology}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000027A51A7A3B0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027A51AA4130>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's dive into some interesting facts about Amazon RDS:\\n\\n**What is it?**\\n\\n* Amazon RDS (Relational Database Service) is a fully managed relational database service in the cloud. Think of it as a hassle-free way to set up, operate, and scale your databases without the headaches of managing the underlying infrastructure.\\n\\n**Why is it special?**\\n\\n* **Focus on the Data, Not the Server:** RDS takes care of all the heavy lifting: provisioning, patching, backups, replication, and more. This allows you to concentrate entirely on developing and optimizing your applications.\\n* **Variety is the Spice of Life:** RDS supports popular database engines like MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. Whether you prefer open-source or proprietary solutions, RDS has you covered.\\n* **Scalability on Demand:** Need more power? Just a few clicks to increase your database capacity. RDS scales both vertically (more resources on your existing instance) and horizontally (adding more instances for high availability and performance).\\n* **Security First:** RDS is built with security in mind. It offers multi-factor authentication, encryption at rest and in transit, and integrates seamlessly with AWS Identity and Access Management (IAM) for fine-grained control over access.\\n* **Cost-Effective Powerhouse:**  You only pay for what you use.  RDS offers flexible pricing models, including pay-as-you-go and reserved instances, to fit your budget.\\n\\n\\n**Interesting tidbits:**\\n\\n* **Automated Backups:** RDS automatically backs up your data, so you don't have to worry about data loss. You can restore from these backups with ease.\\n* **Multi-AZ Deployments:** For maximum availability, deploy your RDS instances in multiple Availability Zones (AZs). This ensures that your database remains operational even if one AZ experiences an outage.\\n* **Read Replicas:** Improve read performance and scalability by creating read replicas of your primary database. These replicas can handle read-only queries, freeing up your primary database for write operations.\\n* **Integration with Other AWS Services:** RDS integrates seamlessly with other AWS services like EC2, Lambda, and DynamoDB. This allows you to build powerful and scalable applications that leverage the full breadth of the AWS ecosystem.\\n\\n\\n\\nLet me know if you have any more specific questions about RDS or want to explore a particular aspect in more detail!\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chain.invoke({\"technology\":\"AMAZON RDS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
