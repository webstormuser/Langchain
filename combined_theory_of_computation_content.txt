Automata Tutorial | Theory of Computation - JavatpointTutorials×PythonPython Django Numpy Pandas Tkinter Pytorch Flask OpenCVAI, ML and Data ScienceArtificial Intelligence Machine Learning Data Science Deep Learning TensorFlow Artificial Neural Network Matplotlib Python ScipyJavaJava Servlet JSP Spring Boot Spring Framework Hibernate JavaFX Java Web ServicesB.Tech and MCADBMS Data Structures Operating System Computer Network DAA Computer Organization Software Engineering Data MiningWeb TechnologyHTML CSS JavaScript Jquery Angular-8 React JS React Native Node JSSoftware TestingSoftware Testing Selenium JIRA JMeter Postman TestNG SoapUI CucumberInterview×Technical InterviewC C++ Php Java Python JavaScript TypeScriptJava InterviewJDBC Servlet Maven Jenkins Spring Spring Boot JDB Hibernate JSFWeb InterviewHTML CSS JavaScript Jquery Angular Node-JS AJAXDatabase InterviewDBMS SQL PL/SQL Oracle MySQL MongoDB Redis MariaDBCompany InterviewsIBM Adobe Microsoft Amazon TCS HCL Wipro DXC Accenture Capgemini Space X Ericsson Infosy IGate EXL IndiaMART SapientCompilerPythonJavaPhpCC++RHtmlJavascriptTypescriptSwiftHome Python Java JavaScriptHTML SQL PHP C# C++ DS Aptitude Reasoning Selenium DBMS C Andriod Interview QAutomata TutorialAutomata TutorialTheory of AutomataFinite AutomataTransition DiagramTransition TableDFAExamples of DFANFAExamples of NFAEliminating ε TransitionsConversion from NFA to DFAConversion from NFA with ε to DFAMinimization of DFARegular ExpressionRegular ExpressionExamples of Regular ExpressionConversion of RE to FAArden's TheoremMoore MachineMealy MachineConversion from Mealy machine to Moore machineConversion from Moore machine to Mealy machineCFGContext-free GrammarDerivationDerivation TreeAmbiguity in GrammarUnambiguous GrammarSimplification of CFGChomsky's Normal Form (CNF)Greibach Normal Form (GNF)PDAPushdown AutomataPDA AcceptanceNon-deterministic Pushdown AutomataCFG to PDA ConversionTuring MachineApplication of Different Automata | Theory of ComputationIntroduction to Computational Complexity TheoryAutomata and Game TheoryRecursive Descent Parsernext →Automata TutorialTheory of automata is a theoretical branch of computer science and mathematical. It is the study of abstract machines and the computation problems that can be solved using these machines. The abstract machine is called the automata. An automaton with a finite number of states is called a Finite automaton.In this tutorial, we are going to learn how to construct deterministic finite automata, non-deterministic finite automata, Regular expression, context-free grammar, context-free language, Push down automata, Turning machines, etc.PrerequisiteBefore learning Automata, you should have a basic understanding of string, language, alphabets, symbols.AudienceOur Automata Tutorial is designed to help beginners and professionals.ProblemsWe assure that you will not find any problem in this Automata Tutorial. But if there is any mistake, please post the problem in contact form.Next TopicTheory of Automatanext →Latest CoursesWe provides tutorials and interview questions of all technology like java tutorial, android, java frameworksContact info G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India[email protected].Follow usLatest PostPRIVACY POLICYTutorialsJava Data Structures C Programming C++ Tutorial C# Tutorial PHP Tutorial HTML Tutorial JavaScript Tutorial jQuery Tutorial Spring TutorialInterview QuestionsTcs Intuit Wipro Adobe Infosys Amazon Accenture Cognizant Capgemini MicrosoftOnline CompilerC R C++ Php Java Html Swift Python JavaScript TypeScript© Copyright 2024 Javatpoint. All Rights Reserved.




























Introduction of Theory of Computation - GeeksforGeeks



























































Skip to content









CoursesDSA to DevelopmentNewly Launched!Android with KotlinGenerative AI & ChatGPTMaster Django FrameworkBecome AWS CertifiedFor Working ProfessionalsInterview 101: DSA & System DesignData Science Training ProgramJAVA Backend Development (Live)DevOps Engineering (LIVE)Software Testing & Automation (Live)Data Structures & Algorithms in PythonFor StudentsPlacement Preparation CourseData Science (Live)Data Structure & Algorithm-Self Paced (C++/JAVA)Master Competitive Programming (Live)Full Stack Development with React & Node JS (Live)GATE Exam CoursesGATE CS & IT (Self-Paced)GATE DS & AI (Self-Paced)All CoursesTutorialsData Structures & AlgorithmsDSA for BeginnersData StructuresArraysMatrixStringsLinked ListStackQueueTreeGeneric TreeBinary TreeBinary Search TreeAVL TreeB TreeB+ TreeRed Black TreeTree Data Structure TutorialHeapHashingGraphSet Data StructureMap Data StructureAdvanced Data StructureData Structures TutorialAlgorithmsAnalysis of AlgorithmsSearching AlgorithmsLinear SearchBinary SearchSearching Algorithms TutorialSorting AlgorithmsSelection SortBubble SortInsertion SortMerge SortQuick SortHeap SortCounting SortRadix SortBucket SortSorting Algorithms TutorialGreedy AlgorithmsDynamic ProgrammingGraph AlgorithmsPattern SearchingRecursionBacktrackingDivide and ConquerMathematical AlgorithmsGeometric AlgorithmsBitwise AlgorithmsRandomized AlgorithmsBranch and BoundAlgorithms TutorialComplete DSA TutorialCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE SheetNetflix SDE SheetGoogle SDE SheetWipro Coding SheetInfosys Coding SheetTCS Coding SheetCognizant Coding SheetHCL Coding SheetDSA Cheat SheetsDSA Sheet for BeginnersSDE SheetsFAANG Coding SheetLove Babbaar SheetMass Recruiter SheetProduct-Based Coding SheetCompany-Wise Preparation SheetTop 100 DSA Interview Questions Topic-wise100 Days of CodePythonPython TutorialPython ExercisesPython List ExercisePython String ExercisePython Tuple ExercisePython Dictionary ExercisePython Set ExercisePython Excercises Topic wisePython QuizPython ProgramsAdvanced Python TutorialPython API TutorialPython Database TutorialPython JSONPython Cheat SheetPython ProjectsPython Interview QuestionsML & Data ScienceMachine LearningMachine Learning TutorialMaths for MLML Projects100 Days of Machine LearningData Science TutorialData Science PackagesPandas TutorialNumPy TutorialData VisualizationData Visualization with PythonData Visualization with RTableauPower BIData AnalysisData Analysis with PythonData Analysis with R100 Days of Data AnalyticsDeep LearningNLP TutorialOpenCV TutorialInterview QuestionsMachine Learning Interview QuestionsDeep Learning Interview QuestionsR Interview QuestionsSystem DesignSystem Design TutorialSoftware Design PatternsSystem Design RoadmapTop 10 System Design Interview QuestionsInterview CornerCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsPuzzles for InterviewsLanguagesCC++JavaPythonR TutorialC#SQLScalaPerlGo LanguageWeb DevelopmentHTMLHTML TutorialFree HTML CourseHTML Cheat SheetCSSCSS TutorialFree CSS CourseCSS Cheat SheetJavaScriptJavaScript TutorialJavaScript QuestionsJavaScript Cheat SheetDSA using JavaScriptFree JavaScript CourseJavaScript A to Z Complete GuideTypeScriptReactJSReactJS TutorialFree ReactJS CourseReactJS Cheat SheetNextJSNode.jsPHPAngularJSjQueryWeb Development Using PythonDjangoFlaskSeleniumPostmanGithubWeb Design100 Days of Web DevelopmentCS SubjectsOperating SystemDBMSComputer NetworksEngineering MathematicsComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOps And LinuxDevOps TutorialGITAWSKubernetesDockerMicrosoft Azure TutorialGoogle Cloud PlatformDevOps RoadmapDevOps Interview QuestionsLinuxLinux TutorialLinux Commands A-ZLinux Commands CheatsheetFile Permissions in LinuxLinux System AdministrationLinux Shell ScriptingLinux NetworkingLinux Interview QuestionsSchool LearningClass 8 Study MaterialClass 9 Study MaterialClass 10 Study MaterialClass 11Study MaterialClass 12 Study MaterialEnglish GrammarGfG SchoolCommerceGATEGATE Computer Science NotesLast Minute NotesGATE CS Solved PapersGATE CS Original Papers and Official KeysGATE CS 2025 SyllabusGATE DA 2025 SyllabusOther CS ExamsISROUGC NETGeeksforGeeks VideosJobsGet Hired: Apply for JobsCorporate Hiring SolutionsFiltered JobsJobs for FreshersJobs for ExperiencedAll JobsPracticePractice Coding ProblemsAll DSA ProblemsProblem of the DayCompany Wise Coding PracticeAmazonMicrosoftFlipkartExplore AllGfG SDE SheetPractice Problems Difficulty WiseBasicEasyMediumHardLanguage Wise Coding PracticeCPPJavaPythonCurated DSA ListsBeginner's DSA SheetLove Babbar SheetTop 50 Array ProblemsTop 50 String ProblemsTop 50 DP ProblemsTop 50 Graph ProblemsTop 50 Tree ProblemsContestsJob-A-Thon Hiring ChallengeGfG Weekly [Rated Contest]All Contests and Events





 



























Notifications




All





 

View All









Notifications



Mark all as read






All


Unread


Read





                                        You're all caught up!!
                                    































AptitudeEngineering MathematicsDiscrete MathematicsOperating SystemDBMSComputer NetworksDigital Logic and DesignC ProgrammingData StructuresAlgorithmsTheory of ComputationCompiler DesignComputer Org and Architecture 




▲












Open In App












Go PremiumShare Your ExperiencesAutomata TutorialAutomata _ IntroductionIntroduction of Theory of ComputationChomsky Hierarchy in Theory of ComputationApplications of various AutomataRegular Expression and Finite AutomataIntroduction of Finite AutomataArden's Theorem in Theory of ComputationArden's Theorem and Challenging Applications | Set 2L-graphs and what they represent in TOCHypothesis (language regularity) and algorithm (L-graph to NFA) in TOCRegular Expressions, Regular Grammar and Regular LanguagesHow to identify if a language is regular or notDesigning Finite Automata from Regular Expression (Set 1)Star Height of Regular Expression and Regular LanguageGenerating regular expression from Finite AutomataDesigning Deterministic Finite Automata (Set 1)Designing Deterministic Finite Automata (Set 2)DFA for Strings not ending with "THE"DFA of a string with at least two 0’s and at least two 1’sDFA for accepting the language L = {  anbm | n+m=even }DFA machines accepting odd number of 0’s or/and even number of 1’sDFA of a string in which 2nd symbol from RHS is 'a'Union process in DFAConcatenation process in DFADFA in LEX code which accepts even number of zeros and even number of onesConversion from NFA to DFAMinimization of DFAReversing Deterministic Finite AutomataComplementation process in DFAKleene's Theorem in TOC | Part-1Mealy and Moore Machines in TOCDifference Between Mealy Machine and Moore MachineCFGRelationship between grammar and language in Theory of ComputationSimplifying Context Free GrammarsClosure Properties of Context Free LanguagesUnion and Intersection of Regular languages with CFLConverting Context Free Grammar to Chomsky Normal FormConverting Context Free Grammar to Greibach Normal FormPumping Lemma in Theory of ComputationCheck if the language is Context Free or NotAmbiguity in Context free Grammar and Context free LanguagesOperator grammar and precedence parser in TOCContext-sensitive Grammar (CSG) and Language (CSL)PDA (Pushdown Automata)Introduction of Pushdown AutomataPushdown Automata Acceptance by Final StateConstruct Pushdown Automata for given languagesConstruct Pushdown Automata for all length palindromeDetailed Study of PushDown AutomataNPDA for accepting the language  L = {an bm cn | m,n>=1}NPDA for accepting the language L = {an bn cm | m,n>=1}NPDA for accepting the language  L = {an bn | n>=1}NPDA for accepting the language  L = {am b(2m) | m>=1}NPDA for accepting the language  L = {am bn cp dq | m+n=p+q ; m,n,p,q>=1}Construct Pushdown automata for L = {0n1m2m3n | m,n ≥ 0}Construct Pushdown automata for L = {0n1m2(n+m) | m,n ≥ 0}NPDA for accepting the language L = {ambnc(m+n) | m,n ≥ 1}NPDA for accepting the language L = {amb(m+n)cn | m,n ≥ 1}NPDA for accepting the language L = {a2mb3m | m ≥ 1}NPDA for accepting the language L = {amb(2m+1) | m ≥ 1}NPDA for accepting the language L = {aibjckdl | i==k or j==l,i>=1,j>=1}Construct Pushdown automata for L = {a(2*m)c(4*n)dnbm | m,n ≥ 0}NPDA for L =  {0i1j2k | i==j or j==k ; i , j , k >= 1}NPDA for accepting the language L = {anb(2n) | n>=1} U {anbn | n>=1}NPDA for the language L ={w∈ {a,b}*| w contains equal no. of a's and b's}Turing MachineRecursive and Recursive Enumerable Languages in TOCTuring Machine in TOCTuring Machine for additionTuring machine for subtraction | Set 1Turing machine for multiplicationTuring machine for copying dataConstruct a Turing Machine for language L = {0n1n2n | n≥1}Construct a Turing Machine for language L = {wwr | w ∈ {0, 1}}Construct a Turing Machine for language L = {ww | w ∈ {0,1}}Construct Turing machine for L = {an bm a(n+m) | n,m≥1}Construct a Turing machine for L = {aibjck | i*j = k; i, j, k ≥ 1}Turing machine for 1's and 2’s complementRecursive and Recursive Enumerable Languages in TOCTuring Machine for subtraction | Set 2Halting Problem in Theory of ComputationTuring Machine as ComparatorDecidabilityDecidable and Undecidable Problems in Theory of ComputationUndecidability and Reducibility in TOCComputable and non-computable problems in TOCTOC Interview preparationLast Minute Notes - Theory of ComputationTOC  Quiz and PYQ's in TOCTheory of Computation - GATE CSE Previous Year QuestionsRegular languages and finite automataContext free languages and Push-down automataRecursively enumerable sets and Turing machinesUndecidabilityDSA to Development Course 













Introduction of Theory of Computation



Last Updated : 
27 Sep, 2024






Summarize






Comments







Improve



























Suggest changes


 


Like Article



Like








Save








Share







Report







Follow





Automata theory (also referred to as the Theory Of Computation) is a branch of Computer Science and Mathematics that studies how machines compute functions and solve problems. This field is mainly focused on mathematical structures called automata and is crucial for the purpose of studying processes occurring in discrete systems.
What is Automata Theory?
In automata theory, scientists and engineers can predict the behavior of computing systems thereby improving problem-solving approaches. Originally developed to describe and explain the dynamics of systems, automata theory is the theoretical base of the formal languages theory, grammar, and computational complexity.
Basic Terminologies of Theory of Computation
Now, let’s understand the basic terminologies, which are important and frequently used in the Theory of Computation. 
Symbol
A symbol (often also called a character) is the smallest building block, which can be any alphabet, letter, or picture. 

Alphabets (Σ)
Alphabets are a set of symbols, which are always finite. 

String 
A string is a finite sequence of symbols from some alphabet. A string is generally denoted as w and the length of a string is denoted as |w|. 
Empty string is the string with zero occurrence of symbols, represented as ε.
Number of Strings (of length 2) that can be generated over the alphabet {a, b}:                     -   -                     a   a                     a   b                     b   a                     b   bLength of String |w| = 2Number of Strings = 4Conclusion:For alphabet {a, b} with length n, number of strings can be generated = 2n.
__mask-blockquote__index=1__

The Theory of Computation explores automata, languages, and complexity. If you want to dive deeper into this subject for GATE, the GATE CS Self-Paced Course covers it extensively.

Closure Representation in TOC
L+: It is a Positive Closure that represents a set of all strings except Null or ε-strings.
L*: It is “Kleene Closure“, that represents the occurrence of certain alphabets for given language alphabets from zero to the infinite number of times. In which ε-string is also included.
From the above two statements, it can be concluded that:
L* = εL+
Example:(a) Regular expression for language accepting all combination of g's over Σ={g}:                                         R = g*                               R={ε,g,gg,ggg,gggg,ggggg,...}(b) Regular Expression for language accepting all combination of g's over Σ={g} : R = g+                               R={g,gg,ggg,gggg,ggggg,gggggg,...}
Note: Σ* is a set of all possible strings(often power set(need not be unique here or we can say multiset) of string) So this implies that language is a subset of Σ*.This is also called a “Kleene Star”.
Kleene Star is also called a “Kleene Operator” or “Kleene Closure”. Engineers and IT professionals make use of Kleene Star to achieve all set of strings which is to be included from a given set of characters or symbols. It is one kind of Unary operator. In Kleene Star methodology all individual elements of a given string must be present but additional elements or combinations of these alphabets can be included to any extent.
Example:Input String: "GFG".Σ* = { ε,"GFG","GGFG","GGFG","GFGGGGGGGG","GGGGGGGGFFFFFFFFFGGGGGGGG",...}  (Kleene Star is an infinite set but if we provide any grammar rules then it can work as a finite set.Please note that we can include ε string also in given Kleene star representation.)
Language
A language is a set of strings, chosen from some Σ* or we can say- ‘A language is a subset of Σ* ‘. A language that can be formed over ‘ Σ ‘ can be Finite or Infinite.
Example of Finite Language:           L1 = { set of string of 2 }         L1 = { xy, yx, xx, yy }Example of Infinite Language:         L1 = { set of all strings starts with 'b' }         L1 = { babb, baa, ba, bbb, baab, ....... }
Conclusion
It is an important branch of computation that is concerned with formal languages, and automata theory in particular. It provides a basis for other courses such as Turing machines and computational complexity that are very important in computer science.
Introduction of Theory of Computation – FAQs
What is the relevance of the automata theory in computer science?

Automata theory is used in modeling computational problems hence enhancing the understanding and design of systems such as compilers, interpreters among others.

what is the purpose of using Kleene Star in the study of formal languages?

The Kleene Star extends symbols from a given alphabet where one is able to create infinite strings from it or even the null string.

Is it possible to implement automata theory into real life?

Of course, automata theory has found its use in certain areas like compiler design, artificial intelligence, network security and natural language processing.















 

abhishek1 





 Follow 









 







Improve








Previous Article



Automata Tutorial




Next Article




Chomsky Hierarchy in Theory of Computation






Read More



Similar Reads



Introduction to Computation Complex Theory
Broad Overview : Complexity theory, in a nutshell, a complexity word is a quite fancy word, literally, it sounds complex, but it is not an intimidating topic. What it really means is analyzing the program or we can say analyzing the efficiency of the program, figuring out whether the program is correct, figuring out whether one program is better th



4 min read




Introduction To Grammar in Theory of Computation
In the context of the Theory of Computation, grammar refers to a formal system that defines how strings in a language are structured. It plays a crucial role in determining the syntactic correctness of languages and serves as a foundation for parsing and interpreting programming languages, natural languages, and various formal systems. This article



5 min read




Theory of Computation - GATE CSE Previous Year Questions
Solving GATE Previous Year's Questions (PYQs) not only clears the concepts but also helps to gain flexibility, speed, accuracy, and understanding of the level of questions generally asked in the GATE exam, and that eventually helps you to gain good marks in the examination. Previous Year Questions help a candidate practice and revise for GATE, whic



5 min read





Relationship between grammar and language in Theory of Computation
A grammar is a set of production rules which are used to generate strings of a language. In this article, we have discussed how to find the language generated by a grammar and vice versa as well. Language generated by a grammar - Given a grammar G, its corresponding language L(G) represents the set of all strings generated from G. Consider the foll



4 min read




Theory of Computation | Regular languages and finite automata | Question 2
What is the complement of the language accepted by the NFA shown below? (A) A (B) B (C) C (D) D Answer: (B) Explanation: Quiz of this QuestionPlease comment below if you find anything wrong in the above post



1 min read




Arden's Theorem in Theory of Computation
Arden's theorem state that: "If P and Q are two regular expressions over "∑", and if P does not contain "∈" , then the following equation in R given by R = Q + RP has a unique solution i.e., R = QP*." That means, whenever we get any equation in the form of R = Q + RP, then we can directly replace it with R = QP*. So, here we will first prove that R



4 min read




Decidability Table in Theory of Computation
Prerequisite - Undecidability, Decidable and undecidable problems Identifying languages (or problems*) as decidable, undecidable or partially decidable is a very common question in GATE. With correct knowledge and ample experience, this question becomes very easy to solve. A language is undecidable if it is not decidable. An undecidable language ma



2 min read





Pumping Lemma in Theory of Computation
There are two Pumping Lemmas, which are defined for 1. Regular Languages, and 2. Context - Free Languages Pumping Lemma for Regular Languages For any regular language L, there exists an integer n, such that for all x ? L with |x| ? n, there exists u, v, w ? ?*, such that x = uvw, and (1) |uv| ? n (2) |v| ? 1 (3) for all i ? 0: uviw ? L In simple te



4 min read




Decidable and Undecidable Problems in Theory of Computation
In the Theory of Computation, problems can be classified into decidable and undecidable categories based on whether they can be solved using an algorithm. A decidable problem is one for which a solution can be found in a finite amount of time, meaning there exists an algorithm that can always provide a correct answer. While an undecidable problem i



6 min read




Halting Problem in Theory of Computation
To understand better the halting problem, we must know Decidability , Undecidability and Turing machine , decision problems and also a theory named as Computability theory and Computational complexity theory. Some important terms: Computability theory - The branch of theory of computation that studies which problems are computationally solvable usi



4 min read




Chomsky Hierarchy in Theory of Computation
According to Chomsky hierarchy, grammar is divided into 4 types as follows: Type 0 is known as unrestricted grammar.Type 1 is known as context-sensitive grammar.Type 2 is known as a context-free grammar.Type 3 Regular Grammar.Type 0: Unrestricted Grammar: Type-0 grammars include all formal grammar. Type 0 grammar languages are recognized by turing



2 min read




Automata Theory | Set 3
Following questions have been asked in GATE CS 2011 exam. 1) The lexical analysis for a modern language such as Java needs the power of which one of the following machine models in a necessary and sufficient sense? (A) Finite state automata (B) Deterministic pushdown automata (C) Non-deterministic pushdown automata (D) Turing machine Answer (A) Lex



2 min read




Automata Theory | Set 4
Following questions have been asked in GATE CS 2011 exam. 1) Let P be a regular language and Q be context-free language such that Q ⊆ P. (For example, let P be the language represented by the regular expression p*q* and Q be {pnqn|n ∈ N}). Then which of the following is ALWAYS regular? (A) P ∩ Q (B) P - Q (C) ∑* - P (D) ∑* - Q



2 min read





Automata Theory | Set 5
Following questions have been asked in GATE CS 2009 exam. 1) S --> aSa| bSb| a| b ;The language generated by the above grammar over the alphabet {a,b} is the set of (A) All palindromes. (B) All odd length palindromes. (C) Strings that begin and end with the same symbol (D) All even length palindromes. Answer (B) The strings accepted by language are



3 min read




Automata Theory | Set 7
These questions for practice purpose for GATE CS Exam. Ques-1: Consider L= {(TM) | TM is the Turing machine that halts on all input and L(TM)= L' for some undecidable language L'}. Here, (TM) is the encoding of a Turing machine as a string over alphabet {0, 1} then L is: (A) decidable and recursively enumerable (B) decidable and recursive (C) decid



3 min read




Automata Theory | Set 8
These questions for practice purpose for GATE CS Exam. Ques-1: Which one of the following language is Regular? (A) {wxwR | w,x ∈ (a+b)+} (B) {wxwR | w ∈ (a+b)*, x ∈ {a,b}} (C) {wwRx | w,x ∈ (a+b)+} (D) {wwR | w ∈ (a+b)*} Explanation: (A) It is correct, since this language can form regular expression which is {{ a(a + b)+a } + {b(a + b)+b}}, i.e., s



2 min read




Automata Theory | Set 9
These questions for practice purpose for GATE CS Exam. Ques-1: Consider the following two statements with respect to Countability: Statement-1: If X union of 'Y' is uncountable, then both set 'X' and set 'Y' must be uncountable. Statement-2: The Cartesian product of two countable sets 'X' and 'Y' is countable. Which of the following option is true



3 min read





Automata Theory | Set 10
These questions for practice purpose of GATE CS Exam. Ques-1: Consider the following statements: X: For any language either a language L or its complement L' must be finite.Y: DFA for language which contains epsilon must have initial state as final state.Z: Non-deterministic finite automata is more powerful than deterministic finite automata. Which



3 min read




Regular Graph in Graph Theory
Prerequisite: Graph Theory Basics – Set 1, Set 2 Regular Graph: A graph is called regular graph if degree of each vertex is equal. A graph is called K regular if degree of each vertex in the graph is K. Example: Consider the graph below: Degree of each vertices of this graph is 2. So, the graph is 2 Regular. Similarly, below graphs are 3 Regular an



2 min read




5 Color Theorem in Graph Theory
The graph is a data structure that is used extensively in real-life. Planar Graph: If a graph can be drawn on the plane without crossing, it is said to be planar. Coloring of a simple graph is the assignment of color to each vertex of the graph so that no two adjacent vertices are assigned the same color. Bi-Partite Graphs: A bipartite graph, also



4 min read




Mathematics | Graph Theory Basics - Set 1
A graph is a data structure that is defined by two components : A node or a vertex.An edge E or ordered pair is a connection between two nodes u,v that is identified by unique pair(u,v). The pair (u,v) is ordered because (u,v) is not same as (v,u) in case of directed graph.The edge may have a weight or is set to one in case of unweighted graph.Cons



4 min read




Mathematics | Graph theory practice questions
Problem 1 - There are 25 telephones in Geeksland. Is it possible to connect them with wires so that each telephone is connected with exactly 7 others. Solution - Let us suppose that such an arrangement is possible. This can be viewed as a graph in which telephones are represented using vertices and wires using the edges. Now we have 25 vertices in



4 min read




Set Theory Operations in Relational Algebra
Relational Algebra in DBMS These Set Theory operations are the standard mathematical operations on set. These operations are Binary operations that are, operated on 2 relations unlike PROJECT, SELECT and RENAME operations. These operations are used to merge 2 sets in various ways. The set operation is mainly categorized into the following: Union op



3 min read





Applications of Group Theory
Group theory is the branch of mathematics that includes the study of elements in a group. Group is the fundamental concept of algebraic structure like other algebraic structures like rings and fields. Group: A non-empty set G with * as operation, (G, *) is called a group if it follows the closure, associativity, identity, and inverse properties. Pr



4 min read




Quotient Group in Group Theory
We can say that "o" is the binary operation on set G if: G is a non-empty set & G * G = { (a,b): a, b∈ G } and o: G * G --> G. Here, aob denotes the image of ordered pair (a,b) under the function/operation o.Example - "+" is called a binary operation on G (any non-empty set ) if & only if: a+b ∈G; ∀ a,b ∈G and a+b give the same result ev



12 min read




Types of Sets in Set Theory
In mathematics, a Set is a fundamental concept representing a collection of well-defined objects or elements. Sets are typically denoted by capital letters, and the individual elements within a set are listed in curly braces, separated by commas. For example, A={1,2,3,4,5} represents a set A with elements 1, 2, 3, 4, and 5. The order of elements wi



7 min read




Mathematics | Graph Theory Basics - Set 2
Graph theory is a basic branch of discrete mathematics that mainly focuses on the relationship between objects. These objects are called vertices and these vertices are joined by edges. Graphs are common in computer science, network analysis, and many other everyday uses because they provide a good representation of connection, relationship, and pr



10 min read





Group in Maths: Group Theory
Group theory is one of the most important branches of abstract algebra which is concerned with the concept of the group. A group consists of a set equipped with a binary operation that satisfies four key properties: specifically, it includes property of closure, associativity, the existence of an identity element, and the existence of inverse eleme



13 min read




Matching (Graph Theory)
Matching (Graph Theory): In graph theory, matching is a fundamental concept used to describe a set of edges without common vertices. Matchings are used in various applications such as network design, job assignments, and scheduling. Understanding matchings is essential for solving problems involving optimal pairings and resource allocation. Table o



4 min read




Automata Theory | Set 2
Questions Asked in the GATE CS 2012 Exam1) What is the complement of the language accepted by the NFA shown below? Assume ∑ = {a} and ε is the empty string  (A) Φ (B) ε (C) a (D) {a, ε} Answer (B) Explanation: The given alphabet ∑ contains only one symbol {a} and the given NFA accepts all strings with any number of occurrences of 'a'. In other word



3 min read






Article Tags : 


GATE CS


Theory of Computation
 






Like





















  Please Login to comment...













































76k+ interested Geeks 



Core Computer Science Subject for Interview Preparation 




Explore
















28k+ interested Geeks 



GATE Computer Science & Information Technology - 2025 




Explore
















27k+ interested Geeks 



GATE Data Science and Artificial Intelligence 2025 




Explore






 




Explore More



























































                      Corporate & Communications Address:- A-143, 9th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305) | Registered Address:- K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                    



























CompanyAbout UsLegalCareersIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramExploreJob-A-Thon Hiring ChallengeHack-A-ThonGfG Weekly ContestOffline Classes (Delhi/NCR)DSA in JAVA/C++Master System DesignMaster CPGeeksforGeeks VideosGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonData Science For BeginnerMachine LearningML MathsData VisualisationPandasNumPyNLPDeep LearningWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython Programming ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceAccountancyBusiness StudiesEconomicsManagementHR ManagementFinanceIncome TaxDatabasesSQLMYSQLPostgreSQLPL/SQLMongoDBPreparation CornerCompany-Wise Recruitment ProcessResume TemplatesAptitude PreparationPuzzlesCompany-Wise PreparationCompaniesCollegesCompetitive ExamsJEE AdvancedUGC NETUPSCSSC CGLSBI POSBI ClerkIBPS POIBPS ClerkMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsRecent ArticlesFree Online ToolsTyping TestImage EditorCode FormattersCode ConvertersCurrency ConverterRandom Number GeneratorRandom Password GeneratorWrite & EarnWrite an ArticleImprove an ArticlePick Topics to WriteShare your ExperiencesInternships 






@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved










        We use cookies to ensure you have the best browsing experience on our website. By using our site, you
        acknowledge that you have read and understood our
        Cookie Policy &
        Privacy Policy


        Got It !
    














Improvement





Please go through our recently updated Improvement Guidelines before submitting any improvements.
This improvement is locked by another user right now. You can suggest the changes for now and it will be under 'My Suggestions' Tab on Write.
You will be notified via email once the article is available for improvement.
                        Thank you for your valuable feedback!
                    

Suggest changes



Please go through our recently updated Improvement Guidelines before submitting any improvements.


Suggest Changes
Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.







Create Improvement
Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.














Suggest Changes







min 4 words, max CharLimit:2000
















Create Improvement

































What kind of Experience do you want to share?









Interview Experiences







Admission Experiences







Career Journeys







Work Experiences







Campus Experiences







Competitive Exam Experiences






                        Can't choose a topic to write? click here for suggested topics
                    



                       Write and publish your own Article
                    



























Theory Of Computation Notes PDF, Syllabus ✅ [2021] B Tech
















































































































































































Skip to content










HomeBest Courses

Google Professional Certificates
Human Resource

Human Resource Human Resource Management Human Resource Planning Organizational Culture Organization Development Organizational Behavior
Learning DealsAll Blog PostManagement

Business Statistics Lean Six Sigma Management Operation Management Research Methodology Operations Research Procurement Management Production Management Supply Chain Strategic Management
Marketing

Economics Brand Management Business Business Communication Business Law Entrepreneurship Consumer Behaviour Marketing Essentials Marketing Management Sales Management Shark Tank India
Business Tech

Project Management Business Analytics Management Information System Enterprise Resource Planning Technologies Cloud Computing
About

About Us Cookie Policy DMCA Policy Disclaimer Contact Us
Toggle website search









All Category
Close






Home
Best Courses

Google Professional Certificates


Human Resource

Human Resource
Human Resource Management
Human Resource Planning
Organizational Culture
Organization Development
Organizational Behavior


Learning Deals
All Blog Post
Management

Business Statistics
Lean Six Sigma
Management
Operation Management
Research Methodology
Operations Research
Procurement Management
Production Management
Supply Chain
Strategic Management


Marketing

Economics
Brand Management
Business
Business Communication
Business Law
Entrepreneurship
Consumer Behaviour
Marketing Essentials
Marketing Management
Sales Management
Shark Tank India


Business Tech

Project Management
Business Analytics
Management Information System
Enterprise Resource Planning
Technologies
Cloud Computing


About

About Us
Cookie Policy
DMCA Policy
Disclaimer
Contact Us








 








Theory of Computation Notes | PDF, Syllabus | B Tech 2021
Home>B Tech Study Material>Theory of Computation Notes | PDF, Syllabus | B Tech 2021







Theory of Computation Notes | PDF, Syllabus | B Tech 2021


Post last modified:30 March 2021
Reading time:27 mins read
Post category:B Tech Study Material












Download Theory of Computation Notes PDF, syllabus for B Tech, BCA, MCA 2021. We provide a complete theory of computation pdf. Theory of Computation lecture notes includes a theory of computation notes, theory of computation book, theory of computation courses, theory of computation syllabus, theory of computation question paper, MCQ, case study, theory of computation interview questions and available in theory of computation pdf form.
Theory of Computation Notes
Theory of Computation subject is included in B Tech CSE, BCA, MCA, M Tech. So, students can able to download theory of computation notes pdf.

Table of Content1 Theory of Computation Syllabus2 Theory of Computation PDF3 Theory of Computation Notes3.1 What is Theory of Computation?3.2 Theory of Computation Handwritten Notes4 Theory of Computation Interview Questions5 Theory of Computation Question Paper6 Theory of Computation Book

Theory of Computation Notes can be downloaded in theory of computation pdf from the below article

Theory of Computation Syllabus
A detailed theory of computation syllabus as prescribed by various Universities and colleges in India are as under. You can download the syllabus in the theory of computation pdf form.
Unit I






Introduction to Automata: The Methods Introduction to Finite Automata, Structural Representations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, Proof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets Strings, Languages, Applications of Automata Theory. 
Finite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, Extending the Transition Function to Strings, The Language of a DFA 
Nondeterministic Finite Automata: An Informal View. The Extended Transition Function, The Languages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite Automata With Epsilon-Transitions: Uses of Î-Transitions, The Formal Notation for an Î-NFA, Epsilon-Closures, Extended Transitions and Languages for Î-NFA’s, Eliminating Î- Transitions.
 Unit II
Regular Expressions and Languages: Regular Expressions: The Operators of regular Expressions, Building Regular Expressions, Precedence of Regular-Expression Operators, Precedence of Regular-Expression Operators Finite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions, Converting DFA’s to Regular Expressions by Eliminating States, Converting Regular Expressions to Automata. 
Algebraic Laws for Regular Expressions: Properties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of the Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular Languages, Equivalence and Minimization of Automata, 
Context-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations Using a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, Parse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and Parse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive Inferences, Applications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: Ambiguous Grammars, Removing Ambiguity.





Unit III 
Pushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical Notation for PDA’s, Instantaneous Descriptions of a PDA, 
Languages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack to Final State, From Final State to Empty Stack Equivalence of PDA’s and CFG’s: From Grammars to Pushdown Automata, From PDA’s to Grammars 
Deterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and Deterministic PDA’s, DPDA’s and Context-Free Languages, DPDA’s and Ambiguous Grammars 
Properties of Context-Free Languages: Normal Forms for Context-Free Grammars, The Pumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, Decision Properties of CFL’s
Unit IV
Introduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for Turing Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, Turing Machines and Halting Programming Techniques for Turing Machines, Extensions to the Basic Turing Machine, Restricted Turing Machines, Turing Machines and Computers
UNIT V
Recursive And Recursively Enumerable Languages: Properties of recursive and recursively enumerable languages, Universal Turing machine, The Halting problem, Undecidable problems about TMs. Context-sensitive language and linear bounded automata (LBA), Chomsky hierarchy, Decidability, Post’s correspondence problem (PCP), undecidability of PCP.

Theory of Computation PDF





 Theory of Computation Notes PDF(How to download) Theory of Computation Notes Download  Theory of Computation Book Download Theory of Computation Syllabus Download  Theory of Computation Question Paper Download  Theory of Computation Interview Questions Download 

Theory of Computation Notes
What is Theory of Computation?





 Download PDF






Theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory.
Theory of Computation Handwritten Notes



 Download PDF

Theory of Computation Interview Questions
Some of the theory of computation interview questions are mentioned below. You can download the QnA in theory of computation pdf form.
What is TOC?What is Automata Theory in TOC?What is Regular Language in TOC?What is Grammer and Language in TOC?What is Null String in TOC?What is Grammer and Language in TOC?What is Regular Expression in TOC?What is Linear Bound Automata in TOC?What is Context-Free Language(CFL) in TOC?What is Recursive Language in TOC?What is the use of Lexical Analysis in TOC?What is Chomsky Classification of Languages in TOC?Define Kleene Star Closure in TOC?What is the Productions in TOC? Explain Production Rules.

Theory of Computation Question Paper
If you have already studied the theory of computation notes, now it’s time to move ahead and go through previous year theory of computation question paper. 
 Download PDF Fill Before Download
It will help you to understand question paper pattern and type of theory of computation questions and answers asked in B Tech, BCA, MCA, M Tech theory of computation exam. You can download the syllabus in theory of computation pdf form.

Theory of Computation Book
Below is the list of theory of computation book recommended by the top university in India.
Introduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, R.Motwani & J.D.Ullman (3rd Edition) – Pearson EducationTheory of Computer Science (Automata Language & Computations), by K.L.Mishra & N. Chandrashekhar, PHI

Download B Tech (CS) Study Material



Computer Networks Notes ✅ [2020] PDF – Download 

Computer Networks Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Networks Notes) 



Computer Graphics Notes ✅ [2020] PDF – Download 

Computer Graphics Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Computer Graphics Notes)



Operating System Notes ✅ [2020] PDF – Download  

Operating System Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operating System Notes)



Compiler Design Notes ✅ [2020] PDF – Download

Compiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper(Download Compiler Design Notes)



Data Structures Notes ✅ [2020] PDF – Download 

Data Structures Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Structures Notes)



Digital Image Processing Notes ✅ [2020] PDF – Download 

Digital Image Processing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Digital Image Processing Notes) 



Theory of Computation Notes ✅ [2020] PDF – Download 

Theory of Computation Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Theory of Computation Notes) 



Computer Organization and Architecture Notes ✅ [2020] PDF – Download 

Computer Organization and Architecture Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Computer Organization and Architecture Notes) 



Cloud Computing Notes ✅ [2020] PDF – Download 

Cloud Computing Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Cloud Computing Notes) 



Data Communication and Networking Notes ✅ [2020] PDF – Download 

Data Communication and Networking Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Data Communication and Networking Notes) 



 Software Engineering Notes ✅ [2020] PDF – Download 

Software Engineering Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Software Engineering Notes) 



Web Technologies Notes ✅ [2020] PDF – Download 

Web Technologies Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Web Technologies Notes) 



Microprocessor and Microcontrollers Notes ✅ [2020] PDF – Download 

Microprocessor and Microcontrollers Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Microprocessor and Microcontrollers Notes) 



Design and Analysis of Algorithm Notes ✅ [2020] PDF – Download 

Design and Analysis of Algorithm Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Design and Analysis of Algorithm Notes) 



Operation Research Notes ✅ [2020] PDF – Download 

Operation Research Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Operation Research Notes) 



Database Management Systems Notes ✅ [2020] PDF – Download 

Database Management Systems Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Database Management Systems Notes)



Compiler Design Notes ✅ [2020] PDF – Download 

Compiler Design Notes [2020] PDF, Syllabus, PPT, Book, Interview questions, Question Paper (Download Compiler Design Notes)





In the above article, a student can download theory of computation notes for B Tech, BCA, MCA, M Tech. Theory of Computation lecture notes and study material includes theory of computation notes, theory of computation books, theory of computation syllabus, theory of computation question paper, theory of computation case study, theory of computation interview questions, theory of computation courses in theory of computation pdf form.

Go On, Share & Help your Friend
 Did we miss something in B.Tech Computer Science Notes or You want something More? Come on! Tell us what you think about our post on Theory of Computation Notes | PDF, Syllabus, Book | B Tech 2020 in the comments section and Share this post with your friends.











Read more articles
 Previous PostWeb Technologies Notes | PDF, Syllabus, Book | B Tech 2021 Next PostDigital Image Processing Notes | PDF, Syllabus | B Tech 2021


Tags: B Tech, B Tech CS Books, B Tech CS Syllabus, B Tech CSE Books, B Tech CSE Notes, B Tech CSE Syllabus, B Tech Notes, B Tech Study Material, Computer Science Notes PDF, CS Notes, CSE Notes, Theory of Computation Book, Theory of Computation Course, Theory of Computation Interview Questions, Theory of Computation Notes, Theory of Computation PDF, Theory of Computation PPT, Theory of Computation Question Paper, Theory of Computation Syllabus, Theory of Computation Tutorial


You Might Also Like




Java Programming Notes PDF, Syllabus | B Tech 2021

22 November 2020



Signals and Systems Notes | PDF, Syllabus, Book | B Tech (2024)

3 July 2020



Electromagnetic Theory PDF | Notes, Syllabus | B Tech 2021

3 July 2020




 


Computer Networks Notes | PDF, Syllabus, Books | B Tech (2024)

21 March 2020




 


Digital Image Processing Notes | PDF, Syllabus | B Tech 2021

22 March 2020




 


Data Structures Notes | PDF, Book, Syllabus | B Tech [2021]

4 March 2020



Business Intelligence Notes | PDF, Syllabus | B Tech 2021

3 July 2020



Wireless Communication PDF | Notes, Syllabus | B Tech (2024)

3 July 2020




 


Design and Analysis of Algorithm Notes PDF | B Tech (2024)

20 March 2020



Network Programming Notes | PDF, Syllabus | B Tech 2021

3 July 2020



How to Download Notes on Geektonight

27 June 2020



Digital Communication Notes | PDF, Syllabus | B Tech 2021

3 July 2020





Leave a Reply Cancel replyYou must be logged in to post a comment. 






All CategoryAll Category
Select Category
Accounting
B Tech Study Material
BBA Study Material
BCOM Study Material
Best Online Course
Best Online Deal
Best Software Review
Brand Management
Business
Business Analytics
Business Communication
Business Ethics
Business Law
Business Statistics
Certification Answers
Cloud Computing
Computer Network
Consumer Behaviour
Corporate Finance
Corporate Social Responsibility
Customer Relationship Management
Digital Marketing
E-Business
Economics
Enterprise Resource Planning
Entrepreneurship
Finance
Financial Accounting
Financial Institutions & Markets
Financial Management
Google Career Certificates
Google Certification
Hotel Management
HubSpot Certification
Human Resource
Human Resource Development
Human Resource Management
Human Resource Planning
Insurance and Risk Management
International Banking
Lean Six Sigma
LinkedIn Certification
Management
Management Accounting
Management Information System
Marketing Essentials
Marketing Management
MBA Study Material
MCOM Study Material
Microsoft Certification
Operation Management
Operations Research
Organization Development
Organizational Behavior
Organizational Culture
Performance Management
Portfolio Management
Procurement Management
Production Management
Project Management
Puzzle
Research Methodology
Sales Management
SEMrush Certification
Service Operations Management
Shark Tank India
Shipping and Insurance
Software Engineering
Strategic Management
Strategy Tools
Supply Chain
Technologies
Treasury Management in Banking
Twitter Certification
Uncategorized













 




 





World's Best Online Courses at One Place 



We’ve spent the time in finding, so you can spend your time in learning 











Digital Marketing 



Google Ads CourseFacebook Ads CourseSEO CourseInstagram Marketing CourseSEM CourseSocial Media CourseEmail Marketing CoursePinterest CourseChatbot CourseBlogging CourseContent Marketing CourseWooCommerce CourseClickbank Affiliate Marketing CourseAffiliate Marketing CourseAmazon Affiliate Marketing CourseShopify, eCommerce & Dropshipping CourseExcel Data Analysis CourseWordPress CourseGoogle Tag Manager CourseGoogle Analytics CourseDigital Marketing CourseYoutube Marketing CourseBing Ads CourseSocial Media Analytics Course 







Business 



Product Strategy CourseSales CourseBrand Strategy CourseBusiness Law CourseStrategic Management CourseMarketing Analytics CourseBusiness Strategy CourseMarketing Management CourseHuman Resource CourseProduct Management CourseProduct Marketing CourseB2B Marketing CourseGrowth Hacking CoursePeople HR Analytics CourseEntrepreneurship CourseBusiness Statistics CourseProject Management CourseNegotiation CourseTime Management CourseLeadership CourseCareer Development CourseStress Management CourseAnxiety Management CourseDesign Thinking CourseEmotional Intelligence CourseTeam Building CourseBusiness Analytics CourseDigital Transformation Course 







Personal Growth 



English Grammar CourseVocabulary CourseSoft Skills CoursePublic Speaking CoursePhotography CourseBody Language CourseCommunication Skills CourseInterview Preparation CourseProductivity CourseMindfulness CourseMemory CourseSelf DisciplineSpeed ReadingAcademic WritingCopywriting CourseScientific Writing CourseNovel Writing CourseAcademic Writing CourseTravel Writer CourseCreative Writing CourseInterior Design CourseGraphic Design CourseDrawing CourseDigital Art CourseUI UX Designer Course 












 












 











Finance 



Mutual Fund CourseFinancial Analysis CoursePersonal Finance CourseCost Accounting CourseAudit CourseFintech CourseValue Investing CourseTrading CourseFinancial Modeling CourseInvestment CourseProject Finance CourseStock Trading CourseFinancial & Capital Markets CourseAccounting CourseFinancial Engineering Course 







FinTech 



NFT CourseMongoDB CoursejQuery CourseBlockchain CourseCryptocurrency CourseSwift CourseAWS CourseRedux CourseGo CourseDeFi CourseSolidity CourseMetaverse CourseDjango CourseJIRA CourseConversion Rate Optimization (CRO) CourseAnalytics CourseCustomer Loyalty Course 







Language 



English SpeakingKorean LanguageGerman LanguageSpanish LanguageFrench Language Italian Language Russian Language Japanese Language Arabic LanguageSwedish LanguageHindi LanguagePortuguese LanguageDutch LanguageLatin LanguageTurkish LanguageHungarian LanguageVietnameseAmerican AccentPronunciationSpelling Courses 











 











 











Tech 



Data Science CourseR Programming CourseBig Data CourseSQL CourseData Analytics CourseMachine Learning CoursePython CourseSQL Data Science CourseArtificial Intelligence CourseCloud Computing CourseData Warehouse CourseNLP Course 







Development 



React JS CoursesFront End Development CourseFull Stack Web Developer CourseC++ CourseData Engineering CourseHTML & CSS3 CourseMicrosoft SQL CourseMySQL CourseJava CourseJavaScript CourseTypeScript CourseBack End Development CourseDatabase CourseGraphQL Course 







Exam Prep 



GRE PrepGMAT PrepMCAT PrepIELTS PrepDAT PrepPSAT PrepCFA PrepOAT PrepACT PrepLSAT PrepFRM PrepSSAT PrepCPA PrepTESOL PrepSAT PrepSSAT Prep 











 











 











Python 



Python CourseDeep Learning Python CoursePython Data Science CoursePython for Marketing CoursePython for Finance CoursePython Pandas CoursePython Data Visualization CoursePython Machine Learning CoursePython Data Processing CoursePython Scripting CoursePython for Data Analysis CoursePython Data Structure CourseNLP Python CourseMatplotlib CourseData Cleaning CourseStatistical Modeling CourseKeras CoursePytorch CourseMachine Learning Finance Course 







Tech 



SCADA CourseASP.net CourseScrum CourseSpring Boot and MVC CourseIT Support & Help Desk CourseRuby on Rails CourseKubernetes CourseDocker CourseNodeJs CourseAngular CoursePHP CourseAPI CourseAlteryx CoursePower BI CourseTableau CourseData Visualization CourseDAX CourseData Streaming CourseRegex CourseQlik Sense CoursePlotly Dash CourseData Modeling Course 







Development 



Android CourseiOS Development CourseFlutter CourseKotlin CourseIonic CourseXamarin CourseVirtual Reality CourseMatlab CourseGit & GitHub CourseSelenium CourseShell Scripting CourseARKit CourseGame Design CourseUnity CourseUnreal Engine CourseGame Development CourseBlender CourseDreamweaver CourseVisual Studio CourseC# (C-Sharp) CourseBootstrap Course 











 











Child Care 



Child Nutrition CourseBaby Massage CourseChildcare & Early Education CourseBaby Sign Language CourseKids Art & Drawing CourseKids Coding CourseChild Development Course 



















 












 Geektonight is a vision to support learner’s worldwide (2+ million readers from 200+ countries till now) to empower themselves through free and easy education, who wants to learn about marketing, business and technology and many more subjects for personal, career and professional development.

 

Connect With Us

Opens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tabOpens in a new tab 

 

MoreAbout UsOpens in a new tabDisclaimerOpens in a new tabCookie PolicyOpens in a new tabPrivacy PolicyOpens in a new tabDMCA PolicyOpens in a new tab 

CategoriesCategories
Select Category
Accounting
B Tech Study Material
BBA Study Material
BCOM Study Material
Best Online Course
Best Online Deal
Best Software Review
Brand Management
Business
Business Analytics
Business Communication
Business Ethics
Business Law
Business Statistics
Certification Answers
Cloud Computing
Computer Network
Consumer Behaviour
Corporate Finance
Corporate Social Responsibility
Customer Relationship Management
Digital Marketing
E-Business
Economics
Enterprise Resource Planning
Entrepreneurship
Finance
Financial Accounting
Financial Institutions & Markets
Financial Management
Google Career Certificates
Google Certification
Hotel Management
HubSpot Certification
Human Resource
Human Resource Development
Human Resource Management
Human Resource Planning
Insurance and Risk Management
International Banking
Lean Six Sigma
LinkedIn Certification
Management
Management Accounting
Management Information System
Marketing Essentials
Marketing Management
MBA Study Material
MCOM Study Material
Microsoft Certification
Operation Management
Operations Research
Organization Development
Organizational Behavior
Organizational Culture
Performance Management
Portfolio Management
Procurement Management
Production Management
Project Management
Puzzle
Research Methodology
Sales Management
SEMrush Certification
Service Operations Management
Shark Tank India
Shipping and Insurance
Software Engineering
Strategic Management
Strategy Tools
Supply Chain
Technologies
Treasury Management in Banking
Twitter Certification
Uncategorized


 





Copyright 2023 Geektonight  











Search this website

Type then hit enter to search























































MBA NOTES 
THEORY OF COMPUTATION
BCS601T 
THEORY OF COMPUTATION 
Unit – I 
 
(12 hours) 
Introduction to Automata: The Methods Introduction to Finite Automata, Structural
Representations, Automata and Complexity. Proving Equivalences about Sets, The Contrapositive, 
Proof by Contradiction, Inductive Proofs: General Concepts of Automata Theory: Alphabets 
Strings, Languages, Applications of Automata Theory. 
 
Finite Automata: The Ground Rules, The Protocol, Deterministic Finite Automata: Definition of 
a Deterministic Finite Automata, How a DFA Processes Strings, Simpler Notations for DFA’s, 
Extending the Transition Function to Strings, The Language of a DFA 
 
Nondeterministic Finite Automata: An Informal View. The Extended Transition Function, The 
Languages of an NFA, Equivalence of Deterministic and Nondeterministic Finite Automata. Finite 
Automata With Epsilon-Transitions: Uses of -Transitions, The Formal Notation for an 
 
-NFA, Epsilon-Closures, Extended Transitions and Languages for -NFA’s, Eliminating -
Transitions. 
 
Unit – II 
(12 hours) 
Regular  Expressions
and  Languages:  Regular  Expressions:  The  Operators  of  regular
Expressions, Building 
Regular  Expressions,  Precedence  of  Regular-Expression  Operators,
 
Precedence of Regular-Expression Operators 
 
Finite Automata and Regular Expressions: From DFA’s to Regular Expressions, Converting 
DFA’s to Regular Expressions, Converting DFA’s to R egular Expressions by Eliminating States, 
Converting Regular Expressions to Automata. 
 
Algebraic Laws for Regular Expressions: 
 
Properties of Regular Languages: The Pumping Lemma for Regular Languages, Applications of 
the Pumping Lemma Closure Properties of Regular Languages, Decision Properties of Regular 
Languages, Equivalence and Minimization of Automata, 
 
Unit – III 
(12 hours) 
Grammar : Types of Grammar 
Context-Free Grammars and Languages: Definition of Context-Free Grammars, Derivations 
Using a Grammars Leftmost and Rightmost Derivations, The Languages of a Grammar, 
 
Parse Trees: Constructing Parse Trees, The Yield of a Parse Tree, Inference Derivations, and 
Parse Trees, From Inferences to Trees, From Trees to Derivations, From Derivation to Recursive 
Inferences, 
 
Applications of Context-Free Grammars: Parsers, Ambiguity in Grammars and Languages: 
Ambiguous Grammars, Removing Ambiguity From Grammars, Leftmost Derivations as a Way to 
Express Ambiguity, Inherent Anbiguity 
www.indiansbrain.com
 
Unit – IV 
 
(12 hours) 
 
Pushdown Automata: Definition Formal Definition of Pushdown Automata, A Graphical 
Notation for PDA’s, Instantaneous Descriptions of a PDA, 
 
Languages of PDA: Acceptance by Final State, Acceptance by Empty Stack, From Empty Stack 
to Final State, From Final State to Empty Stack 
 
Equivalence of PDA’s and CFG’s: From Grammars to Pu shdown Automata, From PDA’s to 
Grammars 
 
Deterministic Pushdown Automata: Definition of a Deterministic PDA, Regular Languages and 
Deterministic PDA’s, DPDA’s and Context-Free La nguages, DPDA’s and Ambiguous Grammars 
 
Properties of Context-Free Languages: Normal Forms for Context-Free Grammars, The 
Pumping Lemma for Context-Free Languages, Closure Properties of Context-Free Languages, 
Decision Properties of CFL’s 
 
Unit – V 
 
(12 hours) 
Introduction to Turing Machines: The Turing Machine: The Instantaneous Descriptions for 
Turing Machines, Transition Diagrams for Turing Machines, The Language of a Turing Machine, 
Turing Machines and Halting 
 
Programming Techniques for Turing Machines, Extensions to the Basic Turing Machine, 
Restricted Turing Machines, Turing Machines and Computers, 
 
Undecidability: A Language That is Not Recursively Enumerable, Enumerating the Binary 
Strings, Codes for Turing Machines, The Diagonalization Language 
 
An Undecidable Problem That Is RE: Recursive Languages, Complements of Recursive and RE 
languages, The Universal Languages, Undecidability of the Universal Language 
 
Undecidable Problems About Turing Machines: Reductions, Turing Machines That Accept the 
Empty Language. Post’s Correspondence Problem: Definition of Post’s Correspondence Problem, 
The “Modified” PCP, Other Undecidable Prob lems: Undecidability of Ambiguity for CFG’s 
 
 
 
 
Text Book: 
 
1. Introduction to Automata Theory Languages, and Computation, by J.E.Hopcroft, 
R.Motwani & J.D.Ullman (3rd Edition) – Pearson Education 
 
www.indiansbrain.com
 
UNIT - I 
 
 
 
 
 
 
 
 
 
What is TOC? 
 
In theoretical computer science, the theory of computation is the branch that deals with 
whether and how efficiently problems can be solved on a model of computation, using an 
algorithm. The field is divided into three major branches: automata theory, computability theory 
and computational complexity theory. 
 
In order to perform a rigorous study of computation, computer scientists work with a 
mathematical abstraction of computers called a model of computation. There are several 
models in use, but the most commonly examined is the Turing machine. 
 
Automata theory 
 
In theoretical computer science, automata theory is the study of abstract machines (or more 
appropriately, abstract 'mathematical' machines or systems) and the computational problems that 
can be solved using these machines. These abstract machines are called automata. 
 
This automaton consists of 
 
 states (represented in the figure by circles),
 and transitions (represented by arrows).
 
As the automaton sees a symbol of input, it makes a transition (or jump) to another state, 
according to its transition function (which takes the current state and the recent symbol as 
its inputs). 
Uses of Automata: compiler design and parsing. 
 
 
 
 
 
 
 
 
 
 
 
Introduction to formal proof: 
Basic Symbols used : 
U – Union 
∩- Conjunction 
 
ϵ - Empty String 
Φ – NULL set 
7- negation 
 
‘ – compliment 
= > implies 
www.indiansbrain.com
Additive inverse: a+(-a)=0 
Multiplicative inverse: a*1/a=1 
Universal set U={1,2,3,4,5} 
Subset A={1,3} 
A’ ={2,4,5} 
Absorption law: AU(A ∩B) = A, A∩(AUB) = A 
 
De Morgan’s Law: 
 
(AUB)’ =A’ ∩ B’ 
(A∩B)’ = A’ U B’ 
Double compliment 
(A’)’ =A 
 
A ∩ A’ = Φ 
 
Logic relations: 
a b = > 7a U b 
7(a∩b)=7a U 7b 
 
Relations: 
 
Let a and b be two sets a relation R contains aXb. 
Relations used in TOC: 
Reflexive: a = a 
Symmetric: aRb = > bRa 
Transition: aRb, bRc = > aRc 
 
If a given relation is reflexive, symmentric and transitive then the relation is called equivalence 
relation. 
 
Deductive proof: Consists of sequence of statements whose truth lead us from some 
initial statement called the hypothesis or the give statement to a conclusion statement. 
 
 
 
 
Additional forms of proof: 
Proof of sets 
Proof by contradiction 
Proof by counter example 
 
Direct proof (AKA) Constructive proof: 
If p is true then q is true 
 
Eg: if a and b are odd numbers then product is also an odd 
number. Odd number can be represented as 2n+1 
 
a=2x+1, b=2y+1 
product of a X b = (2x+1) X (2y+1) 
= 2(2xy+x+y)+1 = 2z+1 (odd number) 
www.indiansbrain.com
 
 
 
Proof by contrapositive: 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
Proof by Contradiction: 
 
H and not C implies falsehood. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Be regarded as an observation than a theorem. 
 
 
 
 
 
 
 
 
For any sets a,b,c if a∩b = Φ and c is a subset of b the prove that a∩c 
=Φ Given : a∩b=Φ and c subset b 
 
Assume: a∩c  Φ 
 
Then 
 
 
= > a∩b Φ = > a∩c=Φ(i.e., the assumption is wrong) 
www.indiansbrain.com
Proof by mathematical Induction: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Languages : 
 
The languages we consider for our discussion is an abstraction of natural languages. That is, 
our focus here is on formal languages that need precise and formal definitions. Programming 
languages belong to this category. 
 
Symbols : 
 
Symbols are indivisible objects or entity that cannot be defined. That is, symbols are the atoms 
 
of the world of languages. A symbol is any single object such as begin, or do. 
 
Alphabets : 
 
An alphabet is a finite, nonempty set of symbols. The alphabet of a language is 
normally denoted by 
. When more than one alphabets are considered for discussion, 
then 
subscripts may be used (e.g.    
etc) or sometimes other symbol like G may also be 
 
introduced. 
 
 
 
 
 
 
 
 
Example : 
 
Strings or Words over Alphabet : 
 
A string or word over an alphabet   
is a finite sequence of concatenated symbols of     
. 
 , a, 0, 1, #, 
www.indiansbrain.com
Example : 0110, 11, 001 are three strings over the binary alphabet { 0, 1 } . 
 
aab, abcb, b, cc are four strings over the alphabet { a, b, c }. 
 
It is not the case that a string over some alphabet should contain all the symbols from the alpha-
bet. For example, the string cc over the alphabet { a, b, c } does not contain the symbols a and b. 
Hence, it is true that a string over an alphabet is also a string over any superset of that alphabet. 
 
Length of a string : 
The number of symbols in a string w is called its length, denoted by |w|. 
 
Example : | 011 | = 4, |11| = 2, | b | = 1 
 
Convention : We will use small case letters towards the beginning of the English alphabet 
to denote symbols of an alphabet and small case letters towards the end to 
 
denote strings over an alphabet. That is, 
 (symbols) and 
 
are strings. 
 
Some String Operations : 
Let 
and 
be two strings. The concatenation of x and y 
 
denoted by xy, is the string   
. That is, the concatenation of x and y 
 
denoted by xy is the string that has a copy of x followed by a copy of y without any intervening 
space between them. 
 
Example : Consider the string 011 over the binary alphabet. All the prefixes, suffixes and 
substrings of this string are listed below. 
 
Prefixes:  , 0, 01, 011. 
 
Suffixes:  , 1, 11, 011. 
 
Substrings:  , 0, 1, 01, 11, 011. 
 
Note that x is a prefix (suffix or substring) to x, for any string x and is a prefix (suffix or 
substring) to any string. 
 
A string x is a proper prefix (suffix) of string y if x is a prefix (suffix) of y and x 蝤 y. 
 
In the above example, all prefixes except 011 are proper prefixes. 
 
Powers of Strings : For any string x and integer 
, we use 
to denote the string formed 
by sequentially concatenating n copies of x. We can also give an inductive 
definition of 
as follows: 
= e, if n 
= 0 ; otherwise 
 
www.indiansbrain.com
Example : If x = 011, then  
= 011011011,  
= 011 and 
 
Powers of Alphabets : 
 
We write 
(for some integer k) to denote the set of strings of length k with symbols 
 
from 
. In other words, 
 
= { w | w is a string over  
 and | w | = k}. Hence, for any alphabet,   
denotes the set 
 
of all strings of length zero. That is,   
= { e }. For the binary alphabet { 0, 1 } we have 
 
the following. 
 
 
 
 
 
 
 
 
The set of all strings over an alphabet 
 is denoted by 
. That is, 
 
 
 
 
 
 
The set  
 contains all the strings that can be generated by iteratively concatenating sym- 
bols from      
any number of times. 
 
Example : If 
= { a, b }, then 
= {  , a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, …}. 
 
Please note that if 
, then 
 that is 
. It may look odd that one can proceed 
from the empty set to a non-empty set by iterated concatenation. But there is a reason for this 
and we accept this convention 
 
The set of all nonempty strings over an alphabet     
is denoted by 
. That is, 
 
 
 
 
 
 
Note that  
is infinite. It contains no infinite strings but strings of arbitrary lengths. 
 
Reversal : 
For any string  
the reversal of the string is 
. 
 
An inductive definition of reversal can be given as follows: 
www.indiansbrain.com
Languages : 
A language over an alphabet is a set of strings over 
that alphabet. Therefore, a 
 
language L is any subset of 
. That is, any 
is a language. 
Example : 
 
 
1. F is the empty language.  
 
2.
is a language for any 
. 
 
3. {e} is a language for any 
. Note that, 
. Because the language F does not 
 
contain any string but {e} contains one string of length zero. 
4. The set of all strings over { 0, 1 } containing equal number of 0's and 1's. 
 
5. The set of all strings over {a, b, c} that starts with a. 
 
Convention : Capital letters A, B, C, L, etc. with or without subscripts are normally used 
to denote languages. 
 
Set operations on languages : Since languages are set of strings we can apply set operations to 
languages. Here are some simple examples (though there is nothing new in it). 
 
 
Union : A string 
 
 
 
 
 
 
iff 
or 
 
Example : { 0, 11, 01, 011 } 
{ 1, 01, 110 } = { 0, 11, 01, 011, 111 } 
Intersection  : 
A   string, 
xϵ  L1 
∩ L2 
iff    x   ϵ  L1    and   x   ϵ  L2    .
Example : { 0, 11, 01, 011 } 
{ 1, 01, 110 } = { 01 } 
 
Complement :  Usually, 
is the universe that a complement is taken with respect to. 
 
Thus for a language L, the complement is L(bar) = { 
| 
}. 
 
Example : Let L = { x | |x| is even }. Then its complement is the language { 
| |x| is 
 
odd }. 
 
Similarly we can define other usual set operations on languages like relative 
com-plement, symmetric difference, etc. 
 
Reversal of a language : 
The reversal of a language L, denoted as 
, is defined as:  
. 
 
Example : 
 
1.  Let L = { 0, 11, 01, 011 }. Then 
= { 0, 11, 10, 110 }.
www.indiansbrain.com
2.  Let L = { 
| n is an integer }. Then  
=  { 
| n is an integer }. 
 
Language concatenation : The concatenation of languages       
and 
is defined as 
 
= { xy | 
and 
}. 
 
Example : { a, ab }{ b, ba } = { ab, aba, abb, abba }. 
 
Note that , 
1. 
  in general. 
2. 
 
 
3. 
 
 
Iterated concatenation of languages : Since we can concatenate two languages, we also repeat this to 
concatenate any number of languages. Or we can concatenate a language with itself any 
 
number of times. The operation L with itself n times. This is 
defined formally as follows: 
 
 
 
 
 
 
Example :  Let L = { a, ab }. Then according to the definition, 
we have 
 
 
 
 
 
 
 
 
 
 
 
and so on. 
 
 
 
Kleene's Star operation : The Kleene star operation on a language L, denoted as is defined as 
follows : 
 
= ( Union n in N ) 
 
= 
 
 
= { x | x is the concatenation of zero or more strings from L } 
 
denotes the concatenation of 
www.indiansbrain.com
Thus 
is the set of all strings derivable by any number of concatenations of strings in 
L. It is also useful to define 
 
=, i.e., all strings derivable by one or more concatenations of strings in L. That is 
 
= (Union n in N and n >0) 
 
= 
 
Example :  Let L = { a, ab }. Then we have, 
 
= 
 
= {e} 
{a, ab} 
{aa, aab, aba, abab} 
… 
 
= 
 
= {a, ab} 
{aa, aab, aba, abab} 
… 
 
Note : 
is in  
, for every language L, including . 
 
The previously introduced definition of   
is an instance of Kleene star. 
 
 
 
 
 
(Generates) 
(Recognizes) 
Grammar 
Language 
  Automata 
 
Automata: A algorithm or program that automatically recognizes if a particular string belongs to 
the language or not, by checking the grammar of the string. 
 
An automata is an abstract computing device (or machine). There are different varities of such 
abstract machines (also called models of computation) which can be defined mathematically. 
 
Every Automaton fulfills the three basic requirements. 
 
• 
Every automaton consists of some essential features as in real computers. It has a mech-
anism for reading input. The input is assumed to be a sequence of symbols over a given 
alphabet and is placed on an input tape(or written on an input file). The simpler automata 
can only read the input one symbol at a time from left to right but not change. Powerful 
versions can both read (from left to right or right to left) and change the input. 
www.indiansbrain.com
 The automaton can produce output of some form. If the output in response to an input 
string is binary (say, accept or reject), then it is called an accepter. If it produces an out-
put sequence in response to an input sequence, then it is called a transducer(or 
automaton with output).

• 
The automaton may have a temporary storage, consisting of an unlimited number of 
cells, each capable of holding a symbol from an alphabet ( whcih may be different from 
the input alphabet). The automaton can both read and change the contents of the storage 
cells in the temporary storage. The accusing capability of this storage varies depending 
on the type of the storage. 
 
• 
The most important feature of the automaton is its control unit, which can be in any 
one of a finite number of interval states at any point. It can change state in some de-
fined manner determined by a transition function. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: The figure above shows a diagrammatic representation of a generic 
automa-tion. 
 
Operation of the automation is defined as follows. 
 
At any point of time the automaton is in some integral state and is reading a particular symbol 
from the input tape by using the mechanism for reading input. In the next time step the automa-
ton then moves to some other integral (or remain in the same state) as defined by the transition 
function. The transition function is based on the current state, input symbol read, and the content 
of the temporary storage. At the same time the content of the storage may be changed and the 
input read may be modifed. The automation may also produce some output during this transition. 
The internal state, input and the content of storage at any point defines the configuration of the 
automaton at that point. The transition from one configuration to the next ( as defined by the 
transition function) is called a move. Finite state machine or Finite Automation is the simplest 
type of abstract machine we consider. Any system that is at any point of time in one of a finite 
number of interval state and moves among these states in a defined manner in response to some 
input, can be modeled by a finite automaton. It doesnot have any temporary storage and hence a 
restricted model of computation. 
www.indiansbrain.com
Finite Automata 
 
Automata (singular : automation) are a particularly simple, but useful, model of compu-
tation. They were initially proposed as a simple model for the behavior of neurons. 
 
States, Transitions and Finite-State Transition System : 
 
 
Let us first give some intuitive idea about a state of a system and state transitions before 
describing finite automata. 
 
Informally, a state of a system is an instantaneous description of that system which gives all 
relevant information necessary to determine how the system can evolve from that point on. 
 
Transitions are changes of states that can occur spontaneously or in response to inputs to the 
states. Though transitions usually take time, we assume that state transitions are instantaneous 
(which is an abstraction). 
 
Some examples of state transition systems are: digital systems, vending machines, etc. A system 
 
containing only a finite number of states and transitions among them is 
called a finite-state transition system. 
 
Finite-state transition systems can be modeled abstractly by a mathematical model called 
finite automation 
 
Deterministic Finite (-state) Automata 
 
Informally, a DFA (Deterministic Finite State Automaton) is a simple machine that reads an in-
put string -- one symbol at a time -- and then, after the input has been completely read, decides 
whether to accept or reject the input. As the symbols are read from the tape, the automaton can 
change its state, to reflect how it reacts to what it has seen so far. A machine for which a deter-
ministic code can be formulated, and if there is only one unique way to formulate the code, then 
the machine is called deterministic finite automata. 
 
Thus, a DFA conceptually consists of 3 parts: 
 
 
 
 
1. A tape to hold the input string. The tape is divided into a finite number of cells. Each 
cell holds a symbol from 
. 
2. A tape head for reading symbols from the tape 
3. A control , which itself consists of 3 things: 
 
o 
finite number of states that the machine is allowed to be in (zero or more states 
are designated as accept or final states), 
 
o a current state, initially set to a start state, 
www.indiansbrain.com
o a state transition function for changing the current state. 
 
An automaton processes a string on the tape by repeating the following actions until the 
tape head has traversed the entire string: 
 
1. The tape head reads the current tape cell and sends the symbol s found there to 
the control. Then the tape head moves to the next cell. 
 
2. he control takes s and the current state and consults the state transition function to 
get the next state, which becomes the new current state. 
 
Once the entire string has been processed, the state in which the automation enters is examined. 
 
If it is an accept state , the input string is accepted ; otherwise, the string is rejected . Summariz- 
 
ing all the above we can formulate the following formal definition: 
 
 
Deterministic Finite State Automaton : A Deterministic Finite State Automaton (DFA) is 
 
a 5-tuple : 
 
 Q is a finite set of states.
• 
is a finite set of input symbols or alphabet 
 
 
is the “next state” transition function (which is total ). Intuitively, 
is
 a 
function that tells which state to move to in response to an input, i.e., if M is in 
 
state q and sees input a, it moves to state      
. 
 
 
is the start state.
• 
is the set of accept or final states. 
 
Acceptance of Strings : 
 
A DFA accepts a string   
if there is a sequence of states     
in Q 
 
such that 
 
1.  
is the start state. 
2.  
for all 
. 
 
3. 
 
Language Accepted or Recognized by a DFA : 
 
The language accepted or recognized by a DFA M is the set of all strings accepted by M , and 
 
is denoted by 
i.e. 
The  notion 
of 
acceptance can also be made more precise by extending the transition function 
.
 
Extended transition function : 
www.indiansbrain.com
Extend 
(which is function on symbols) to a function on strings, i.e. . 
 
 
 
That is, 
 is the state the automation reaches when it starts from the state q and finish 
processing the string w. Formally, we can give an inductive definition as follows: 
 
The language of the DFA M is the set of strings that can take the start state to one of 
the accepting states i.e. 
 
 
L(M) = { 
| M accepts w } 
 
= {
| 
} 
 
 
Example 1 : 
 
 
 
 
 
 
 
 
is the start state 
 
 
 
 
 
 
 
 
 
 
 
It is a formal description of a DFA. But it is hard to comprehend. For ex. The language of the 
DFA is any string over { 0, 1} having at least one 1 
 
We can describe the same DFA by transition table or state transition diagram as follow-
ing: 
 
 
 
 
Transition Table : 
 
0    1 
www.indiansbrain.com
 
 
It is easy to comprehend the transition diagram. 
 
 
 
 
 
 
 
 
 
 
 
Explanation :  We cannot 
reach find state 
w/0 or in the i/p string. There can be any no. 
of 0's at the beginning. 
( The self-loop at 
on label 0 indicates it ). Similarly there 
can be any no. of 0's & 1's in any order at the end of the string. 
 
Transition table : 
 
It is basically a tabular representation of the transition function that takes two arguments (a 
state and a symbol) and returns a value (the “next state”). 
 
• 
Rows correspond to states, 
• 
Columns correspond to input symbols, 
• 
Entries correspond to next states 
• 
The start state is marked with an arrow 
• 
The accept states are marked with a star (*). 
 
 
 
0    1 
 
 
 
(State) Transition diagram : 
 
A state transition diagram or simply a transition diagram is a directed graph which can 
be constructed as follows: 
 
1.  For each state in Q there is a node. 
2. There is a directed edge from node q to node p labeled a iff 
 . (If there are 
several input symbols that cause a transition, the edge is labeled by the list of these 
symbols.) 
3. There is an arrow with no source into the start state. 
4. Accepting states are indicated by double circle. 
www.indiansbrain.com
 
 
 
 
 
 
 
 
5. 
 
6. Here is an informal description how a DFA operates. An input to a DFA can be any 
 
string. 
Put a pointer to the start state q. Read the input string w from left 
 
to right, one symbol at a time, moving the pointer according to the transition  
 
function, 
.  If the next symbol of w is a and the pointer is on state p, move the 
 
pointer to 
. When the end of the input string w is encountered, the pointer is on 
 
some state, r. The string is said to be accepted by the DFA if 
and 
 
rejected if 
. Note that there is no formal mechanism for moving the pointer. 
7. A language 
is said to be regular if L = L(M) for some DFA M. 
 
 
 
Regular Expressions: Formal Definition 
 
We construct REs from primitive constituents (basic elements) by repeatedly applying 
certain recursive rules as given below. (In the definition) 
 
Definition : Let S be an alphabet. The regular expressions are defined recursively as follows. 
 
Basis : 
 
i) is a RE 
 
ii) is a RE 
 
iii) 
, a is RE. 
 
These are called primitive regular expression i.e. Primitive Constituents 
 
Recursive Step : 
 
 
If 
 
and 
are REs over, then so are 
 
i) 
 
 
ii) 
 
www.indiansbrain.com
iii) 
 
 
iv) 
 
 
 
 
 
Closure : r is RE over only if it can be obtained from the basis elements (Primitive 
REs) by a finite no of applications of the recursive step (given in 2). 
 
Example : Let 
 = { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this 
expression by applying the above rules as given in the following step. 
 
Steps 
RE Constructed 
Rule Used 
1 
1 
Rule 1(iii) 
2 
 
Rule 1(i) 
3 
1+ 
Rule 2(i) & Results of Step 1, 2 
4 
(1+  ) 
Rule 2(iv) & Step 3 
5 
2 
1(iii) 
6 
1 
1(iii) 
7 
21 
2(ii), 5, 6 
8 
0 
1(iii) 
9 
0+21 
2(i), 7, 8 
10 
(0+21) 
2(iv), 9 
11 
(0+21)* 
2(iii), 10 
12 
(0+21)* 
2(ii), 4, 11 
 
Language described by REs : Each describes a language (or a language is associated 
with every RE). We will see later that REs are used to attribute regular languages. 
 
 
 
Notation : If r is a RE over some alphabet then L(r) is the language associate with r . We 
can define the language L(r) associated with (or described by) a REs as follows. 
 
1. is the RE describing the empty language i.e. L( ) = . 
 
2. is a RE describing the language {
} i.e. L( ) = {
} . 
 
3. 
, a is a RE denoting the language {a} i.e . L(a) = {a} . 
 
4. If 
and 
are REs denoting language L(
) and L(
) respectively, then 
 
i) 
is a regular expression denoting the language L(
) = L(
) ∪ L(
) 
www.indiansbrain.com
ii) 
is a regular expression denoting the language L(
)=L(
) L(
) 
 
iii) 
is a regular expression denoting the language 
 
 
iv) (
) is a regular expression denoting the language L((
)) = L(
) 
 
Example : Consider the RE (0*(0+1)). Thus the language denoted by the RE is 
 
L(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) 
 
= L(0)*L(0) ∪ L(1) 
 
= {
 , 0,00,000,. } {0} 
{1} 
 
= {  , 0,00,000,........} {0,1} 
 
= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} 
 
Precedence Rule 
Consider the RE ab + c. The language described by the RE can be thought of either 
L(a)L(b+c) or L (ab) L(c) as provided by the rules (of languages described by REs) 
given already. But these two represents two different languages lending to 
ambiguity. To remove this ambiguity we can either 
 
1) Use fully parenthesized expression- (cumbersome) or 
 
2) Use a set of precedence rules to evaluate the options of REs in some order. 
Like other algebras mod in mathematics. 
 
For REs, the order of precedence for the operators is as follows: 
 
i) The star operator precedes concatenation and concatenation precedes union (+) 
operator. 
 
ii) It is also important to note that concatenation & union (+) operators are 
associative and union operation is commutative. 
 
Using these precedence rule, we find that the RE ab+c represents the language 
L(ab) L(c) i.e. it should be grouped as ((ab)+c). 
 
We can, of course change the order of precedence by using parentheses. For 
example, the language represented by the RE a(b+c) is L(a)L(b+c). 
www.indiansbrain.com
Example : The RE ab*+b is grouped as ((a(b*))+b) which describes the language 
L(a)(L(b))*
L(b) 
 
Example : The RE (ab)*+b represents the language (L(a)L(b))* 
L(b). 
 
Example : It is easy to see that the RE (0+1)*(0+11) represents the language of all 
strings over {0,1} which are either ended with 0 or 11. 
 
Example : The regular expression r =(00)*(11)*1 denotes the set of all strings with an 
even number of 0's followed by an odd number of 1's i.e. 
 
 
Note : The notation 
is used to represent the RE rr*. Similarly, 
represents the RE 
rr, 
denotes 
r, and so on. 
 
An arbitrary string over 
= {0,1} is denoted as (0+1)*. 
 
Exercise : Give a RE r over {0,1} s.t. L(r)={
 has at least one pair of 
consecutive 1's} 
 
Solution : Every string in L(r) must contain 00 somewhere, but what comes before and 
what goes before is completely arbitrary. Considering these observations we can write 
the REs as (0+1)*11(0+1)*. 
 
Example : Considering the above example it becomes clean that the RE 
(0+1)*11(0+1)*+(0+1)*00(0+1)* represents the set of string over {0,1} that contains 
the substring 11 or 00. 
 
Example : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the 
set of strings over {0,1} that contains exactly two 1's. The presence of two 1's in the 
RE and any no of 0's before, between and after the 1's ensure it. 
 
Example : Consider the language of strings over {0,1} containing two or more 1's. 
 
Solution : There must be at least two 1's in the RE somewhere and what comes before, 
between, and after is completely arbitrary. Hence we can write the RE as 
(0+1)*1(0+1)*1(0+1)* . But following two REs also represent the same language, each 
ensuring presence of least two 1's somewhere in the string 
 
i) 0*10*1(0+1)* 
 
ii) (0+1)*10*10* 
 
Example : Consider a RE r over {0,1} such that 
www.indiansbrain.com
L(r) = {
 has no pair of consecutive 1's} 
 
Solution : Though it looks similar to ex ……., it is harder to construct to construct. We 
observer that, whenever a 1 occurs, it must be immediately followed by a 0. This 
substring may be preceded & followed by any no of 0's. So the final RE must be a 
repetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the RE is 
(0*100*)*. But in this case the strings ending in 1 or consisting of all 0's are not 
accounted for. Taking these observations into consideration, the final RE is r = 
(0*100*)(1+ )+0*(1+
). 
 
Alternative Solution : 
 
The language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as 
r = (0+10)*(1+ ).This is a shorter expression but represents the same language. 
 
Regular Expression and Regular Language : 
 
Equivalence(of REs) with FA : 
 
Recall that, language that is accepted by some FAs are known as Regular language. 
The two concepts : REs and Regular language are essentially same i.e. (for) every 
regular language can be developed by (there is) a RE, and for every RE there is a 
Regular Langauge. This fact is rather suprising, because RE approach to describing 
language is fundamentally differnet from the FA approach. But REs and FA are 
equivalent in their descriptive power. We can put this fact in the focus of the following 
Theorem. 
 
Theorem : A language is regular iff some RE describes it. 
 
This Theorem has two directions, and are stated & proved below as a separate lemma 
 
 
RE to FA : 
 
REs denote regular languages : 
 
Lemma : If L(r) is a language described by the RE r, then it is regular i.e. there is a FA 
such that L(M)
L(r). 
 
Proof : To prove the lemma, we apply structured index on the expression r. First, we 
 
show how to construct FA for the basis elements: 
, and for any 
. Then we show 
how to combine these Finite Automata into Complex Automata that accept the Union, 
Concatenation, Kleen Closure of the languages accepted by the original smaller 
automata. 
www.indiansbrain.com
Use of NFAs is helpful in the case i.e. we construct NFAs for every REs which are 
represented by transition diagram only. 
 
Basis : 
 
 Case (i) : 
. Then 
. Then 
and the following NFA N 
recognizes L(r). Formally 
where Q = {q} 
and 
.
 
 
 
 
 
 
 
 
 
 
 Case (ii) : 
. 
, and the following NFA N accepts L(r). Formally
where 
. 
 
 
 
 
 
 
 
 
 
 
Since the start state is also the accept step, and there is no any transition defined, it 
will accept the only string 
and nothing else. 
 
 Case (iii) : r = a for some 
. Then L(r) = {a}, and the following NFA 
N accepts L(r).
 
 
 
 
 
 
 
 
 
 
Formally, 
where 
for 
or 
 
 
 
 
 
Induction : 
www.indiansbrain.com
Assume that the start of the theorem is true for REs 
and 
. Hence we can assume 
that we have automata 
and 
that accepts languages denoted by REs 
and 
, 
 
respectively i.e. 
and 
. The FAs are represented 
schematically as shown below. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Each has an initial state and a final state. There are four cases to consider. 
 
 Case (i) : Consider the RE 
denoting the language 
. We 
construct FA 
, from 
and 
to accept the language denoted by RE 
as 
follows :
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Create a new (initial) start state 
and give 
- transition to the initial state of 
 and 
.This is the initial state of 
. 
 
 Create a final state 
and give 
-transition from the two final state of 
and 
. 
is the only final state of 
and final state of 
and 
will be 
ordinary states in 
.
 All the state of 
and 
are also state of 
.
www.indiansbrain.com
 All the moves of 
and 
are also moves of 
. [ Formal Construction] 
 
 
It is easy to prove that 
 
Proof: To show that 
we must show that 
 
= 
 
 
= 
by following transition of 
 
 
Starts at initial state 
and enters the start state of either 
or 
follwoing the 
transition i.e. without consuming any input. WLOG, assume that, it enters the start state 
of 
. From this point onward it has to follow only the transition of 
to enter the final 
 
state of 
, because this is the only way to enter the final state of M by following the e-
transition.(Which is the last transition & no input is taken at hte transition). Hence the 
 
whole input w is considered while traversing from the start state of 
to the final 
state of 
. Therefore 
must accept 
. 
 
Say, 
or 
. 
 
 
WLOG, say 
 
Therefore when 
process the string w , it starts at the initial state and enters the final 
state when w consumed totally, by following its transition. Then 
also accepts w, by 
starting at state 
and taking 
-transition enters the start state of 
-follows the moves 
 
of 
to enter the final state of 
consuming input w thus takes -transition to 
. Hence proved 
 
 Case(ii) : Consider the RE 
denoting the language 
. We construct 
FA 
from 
& 
to accept 
as follows :
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Create a new start state 
and a new final state 
 
1. Add 
- transition from 
o 
to the start state of 
 
 
o 
to 
 
 
o final state of 
to the start state of 
 
 
2. All the states of 
are also the states of 
. 
has 2 more states than that of 
namely 
and 
. 
3. All the moves of 
are also included in 
. 
 
By the transition of type (b), 
can accept . 
By the transition of type (a), 
can enters the initial state of 
w/o any input and then 
follow all kinds moves of 
to enter the final state of 
and then following 
-transition 
can enter 
. Hence if any 
is accepted by 
then w is also accepted by 
. By the 
transition of type (b), strings accepted by 
can be repeated by any no of times & thus 
accepted by 
. Hence 
accepts 
and any string accepted by 
repeated (i.e. 
 
concatenated) any no of times. Hence 
 
Case(iv) : Let 
=(
). Then the FA 
is also the FA for (
), since the use of 
parentheses does not change the language denoted by the expression 
 
Non-Deterministic Finite Automata 
Nondeterminism is an important abstraction in computer science. Importance of 
nondeterminism is found in the design of algorithms. For examples, there are many 
problems with efficient nondeterministic solutions but no known efficient deterministic 
solutions. ( Travelling salesman, Hamiltonean cycle, clique, etc). Behaviour of a process 
is in a distributed system is also a good example of nondeterministic situation. Because 
www.indiansbrain.com
the behaviour of a process might depend on some messages from other processes 
that might arrive at arbitrary times with arbitrary contents. 
It is easy to construct and comprehend an NFA than DFA for a given regular 
language. The concept of NFA can also be used in proving many theorems and 
results. Hence, it plays an important role in this subject. 
In the context of FA nondeterminism can be incorporated naturally. That is, an NFA is 
defined in the same way as the DFA but with the following two exceptions: 
 multiple next state.

 
- transitions.
 
Multiple Next State : 
 
 In contrast to a DFA, the next state is not necessarily uniquely determined by the 
current state and input symbol in case of an NFA. (Recall that, in a DFA there is 
exactly one start state and exactly one transition out of every state for each 
symbol in 
).
 
This means that - in a state q and with input symbol a - there could be one, more 
than one or zero next state to go, i.e. the value of 
is a subset of Q. Thus
 
= 
which means that any one of 
could be the next 
state. 
 
 The zero next state case is a special one giving 
=
, which means that 
there is no next state on input symbol when the automata is in state q. In such 
a case, we may think that the automata "hangs" and the input will be rejected.
 
- transitions : 
 
In an -transition, the tape head doesn't do anything- it doesnot read and it doesnot move. 
However, the state of the automata can be changed - that is can go to zero, one 
 
or more states. This is written formally as 
implying that the next 
state could by any one of 
w/o consuming the next input symbol. 
 
 
 
 
Acceptance : 
 
Informally, an NFA is said to accept its input 
if it is possible to start in some start state 
and process 
, moving according to the transition rules and making choices along the way 
whenever the next state is not uniquely defined, such that when 
is completely processed 
(i.e. end of 
is reached), the automata is in an accept state. There may be several 
possible paths through the automation in response to an input 
since the start state is not 
determined and there are choices along the way because of multiple next states. Some of 
these paths may lead to accpet states while others may not. The 
www.indiansbrain.com
automation is said to accept 
if at least one computation path on input 
starting from at 
least one start state leads to an accept state- otherwise, the automation rejects input 
. 
Alternatively, we can say that, 
is accepted iff there exists a path with label 
from some 
start state to some accept state. Since there is no mechanism for determining which state 
to start in or which of the possible next moves to take (including the 
- transitions) in 
response to an input symbol we can think that the automation is having some "guessing" 
power to chose the correct one in case the input is accepted 
 
Example 1 : Consider the language L = {
 {0, 1}* | The 3rd symbol from the right 
is 1}. The following four-state automation accepts L. 
 
The m/c is not deterministic since there are two transitions from state 
on input 1 
and no transition (zero transition) from 
on both 0 & 1. 
 
For any string 
whose 3rd symbol from the right is a 1, there exists a sequence of legal 
transitions leading from the start state q, to the accept state 
. But for any string 
where 3rd symbol from the right is 0, there is no possible sequence of legal 
 
tranisitons leading from 
and 
. Hence m/c accepts L. How does it accept any string 
L? 
 
Formal definition of NFA : 
 
Formally, an NFA is a quituple 
where Q, 
, 
, and F bear 
the same meaning as for a DFA, but 
, the transition function is redefined as follows: 
 
 
 
 
where P(Q) is the power set of Q i.e. 
. 
 
The Langauge of an NFA : 
 
From the discussion of the acceptance by an NFA, we can give the formal definition of a 
language accepted by an NFA as follows : 
 
If 
is an NFA, then the langauge accepted by N is writtten as L(N) 
is given by 
. 
 
That is, L(N) is the set of all strings w in 
such that 
contains at least 
one accepting state. 
www.indiansbrain.com
Removing ϵ-transition: 
 
- transitions do not increase the power of an NFA . That is, any 
- NFA ( NFA with 
transition), we can always construct an equivalent NFA without 
-transitions. The 
 
equivalent NFA must keep track where the 
NFA goes at every step during 
computation. This can be done by adding extra transitions for removal of every 
- 
transitions from the - NFA as follows. 
 
If we removed the 
- transition 
from the - NFA , then we need to moves 
 
from state p to all the state on input symbol 
which are reachable from state q 
(in the - NFA ) on same input symbol q. This will allow the modified NFA to move 
from state p to all states on some input symbols which were possible in case of 
-NFA 
on the same input symbol. This process is stated formally in the following theories. 
 
Theorem if L is accepted by an - NFA N , then there is some 
equivalent 
without transitions accepting the same language L 
Proof: 
 
Let 
be the given 
with 
 
 
We construct 
 
Where, 
for all 
and 
and 
 
 
 
 
 
Other elements of N' and N 
 
We can show that 
i.e. N' and N are equivalent. 
 
We need to prove that 
 
 
 i.e. 
 
 
 
 
 
We will show something more, that is, 
www.indiansbrain.com
We will show something more, that is, 
 
 
Basis : 
, then 
 
 
But 
by definition of 
. 
 
Induction hypothesis Let the statement hold for all 
with 
. 
 
 
By definition of extension of 
 
 
By inductions hypothesis. 
 
Assuming that 
 
 
 
 
By definition of 
 
 
Since 
 
 
 
To complete the proof we consider the case 
 
When 
i.e. 
then 
www.indiansbrain.com
 
and by the construction of 
wherever 
constrains a state in F. 
 
If 
(and thus 
is not in F ), then 
with 
leads to an accepting state in N' iff it 
lead to an accepting state in N ( by the construction of N' and N ). 
 
Also, if (
 , thus w is accepted by N' iff w is accepted by N (iff 
) 
 
If 
(and, thus in M we load 
in F ), thus is accepted by both N' and N . 
 
Let 
. If w cannot lead to 
in N , then 
. (Since can add transitions to get an accept 
state). So there is no harm in making 
an accept state in N'. 
 
Ex: Consider the following NFA with 
- transition. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Transition Diagram 
 
 
0 
1 
 
 
 
 
 
 
 
 
 
 
Transition diagram for 
' for the equivalent NFA without - moves 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
0 
          1 
 
 
 
 
 
 
 
 
 
 
Since 
the start state q0 must be final state in the equivalent NFA . 
 
Since 
and 
and 
we add moves 
and 
in the equivalent NFA . Other moves are also constructed accordingly. 
 
-closures: 
 
The concept used in the above construction can be made more formal by defining the 
-closure for a state (or a set of states). The idea of 
-closure is that, when moving 
 
from a state p to a state q (or from a set of states Si to a set of states Sj ) an input 
, we need to take account of all 
-moves that could be made after the transition. 
Formally, for a given state q, 
 
 
-closures: 
 
Similarly, for a given set 
 
 
-closures: 
 
 
 
So, in the construction of equivalent NFA N' without -transition from any NFA with 
 
moves. the first rule can now be written as 
www.indiansbrain.com
Equivalence of NFA and DFA 
 
It is worth noting that a DFA is a special type of NFA and hence the class of languages 
accepted by DFA s is a subset of the class of languages accepted by NFA s. 
Surprisingly, these two classes are in fact equal. NFA s appeared to have more power 
than DFA s because of generality enjoyed in terms of 
-transition and multiple next 
states. But they are no more powerful than DFA s in terms of the languages they 
accept. 
 
Converting DFA to NFA 
 
 
 
 
Theorem: Every DFA has as equivalent NFA 
 
Proof: A DFA is just a special type of an NFA . In a DFA , the transition functions is 
defined from 
whereas in case of an NFA it is defined from 
and 
be a DFA . We construct an equivalent NFA 
as follows. 
 
 
 
 
i. e 
 
If 
and 
 
All other elements of N are as in D. 
 
If 
then there is a sequence of states 
such that 
 
 
 
Then it is clear from the above construction of N that there is a sequence of states (in N) 
such that 
and 
and hence 
 
 
Similarly we can show the converse. 
 
Hence , 
 
 
Given any NFA we need to construct as equivalent DFA i.e. the DFA need to simulate 
the behaviour of the NFA . For this, the DFA have to keep track of all the states where 
the NFA could be in at every step during processing a given input string. 
www.indiansbrain.com
There are 
possible subsets of states for any NFA with n states. Every subset 
corresponds to one of the possibilities that the equivalent DFA must keep track of. Thus, 
 
the equivalent DFA will have 
states. 
 
The formal constructions of an equivalent DFA for any NFA is given below. We 
first consider an NFA without 
transitions and then we incorporate the affects of 
transitions later. 
 
Formal construction of an equivalent DFA for a given NFA without transitions. 
 
Given an 
without - moves, we construct an equivalent DFA 
 
 
as follows 
 
i.e. 
 
 
 
 
 
(i.e. every subset of Q which as an element in F is considered as a final stat
in DFA D ) 
 
 
 
 
for all 
and 
 
 
where 
 
 
That is, 
 
 
To show that this construction works we need to show that L(D)=L(N) i.e. 
 
 
 
 
 
 
Or,
 
 
We will prove the following which is a stranger statement thus required. 
www.indiansbrain.com
 
 
Proof : We will show by inductions on 
 
 
Basis If 
=0, then w =  
 
So, 
by definition. 
 
Inductions hypothesis : Assume inductively that the statement holds 
of 
length less than or equal to n. 
 
Inductive step 
 
Let 
, then 
with 
 
 
Now, 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Now, given any NFA with -transition, we can first construct an equivalent NFA without 
-transition and then use the above construction process to construct an equivalent 
DFA , thus, proving the equivalence of NFA s and DFA s.. 
 
It is also possible to construct an equivalent DFA directly from any given NFA with 
- transition by integrating the concept of 
-closure in the above construction. 
 
Recall that, for any 
 
 
- closure : 
www.indiansbrain.com
In the equivalent DFA , at every step, we need to modify the transition functions 
to 
keep track of all the states where the NFA can go on 
-transitions. This is done by 
replacing 
by 
-closure 
, i.e. we now compute 
at every step as 
follows: 
 
 
 
Besides this the initial state of the DFA D has to be modified to keep track of all the 
states that can be reached from the initial state of NFA on zero or more -transitions. 
This can be done by changing the initial state 
to -closure (
 ) . 
 
It is clear that, at every step in the processing of an input string by the DFA D , it enters 
a state that corresponds to the subset of states that the NFA N could be in at that 
particular point. This has been proved in the constructions of an equivalent NFA for any 
-NFA 
If the number of states in the NFA is n , then there are 
states in the DFA . That is, 
each state in the DFA is a subset of state of the NFA . 
 
But, it is important to note that most of these 
states are inaccessible from the start state 
and hence can be removed from the DFA without changing the accepted language. Thus, 
in fact, the number of states in the equivalent DFA would be much less 
 
than 
. 
Example : Consider the NFA given below. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0 
1 
 
 
{
} 
 
 
 
 
Since there are 3 states in the NFA 
www.indiansbrain.com
There will be 
states (representing all possible subset of states) in the 
equivalent DFA . The transition table of the DFA constructed by using the subset 
constructions process is produced here. 
 
0 
 
1 The start state of the DFA is   - closures 
 
 
 
 
 The final states are all those subsets that contains 
(since 
in the NFA). 
 
{   } 
Let us compute one entry, 
 
 
 
 
 
 
 
 
 Similarly, all other transitions can be computed 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
0 
1 
 
 
 
 
 
 
 
 
 
 
 
 
Corresponding Transition fig. for DFA.Note that states 
 
are not accessible and hence can be removed. 
This gives us the following simplified DFA with only 3 states. 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
It is interesting to note that we can avoid encountering all those inaccessible 
or unnecessary states in the equivalent DFA by performing the following two 
steps inductively. 
 
1. If 
is the start state of the NFA, then make 
- closure ( 
) the start state of the 
equivalent DFA . This is definitely the only accessible state. 
 
2. If we have already computed a set 
of states which are accessible. Then 
. 
compute 
because these set of states will also be accessible. 
 
Following these steps in the above example, we get the transition table given below 
www.indiansbrain.com
UNIT-II 
 
 
 
Regular Expressions: Formal Definition 
 
We construct REs from primitive constituents (basic elements) by repeatedly applying certain recursive rules 
as given below. (In the definition) 
 
Definition : Let S be an alphabet. The regular expressions are defined recursively as follows. 
 
Basis : 
 
i) 
is a RE 
 
ii) 
is a RE 
 
iii) 
, a is RE. 
 
These are called primitive regular expression i.e. Primitive Constituents 
 
Recursive Step : 
 
If 
and 
are REs over, then so are 
 
i) 
 
 
ii) 
 
 
iii) 
 
 
iv) 
 
 
 
 
 
Closure : r is RE over only if it can be obtained from the basis elements (Primitive REs) by a finite no of 
applications of the recursive step (given in 2). 
 
Example : Let 
= { 0,1,2 }. Then (0+21)*(1+ F ) is a RE, because we can construct this expression by 
applying the above rules as given in the following step. 
 
Steps 
RE Constructed 
Rule Used 
1 
1 
Rule 1(iii) 
2 
 
Rule 1(i) 
3 
1+ 
Rule 2(i) & Results of Step 1, 2 
 
 
www.indiansbrain.com
4 
(1+   ) 
Rule 2(iv) & Step 3 
 
 
5 
2 
1(iii) 
6 
1 
1(iii) 
7 
21 
2(ii), 5, 6 
8 
0 
1(iii) 
9 
0+21 
2(i), 7, 8 
10 
(0+21) 
2(iv), 9 
11 
(0+21)* 
2(iii), 10 
12 
(0+21)* 
2(ii), 4, 11 
 
Language described by REs : Each describes a language (or a language is associated with every RE). 
We will see later that REs are used to attribute regular languages. 
 
Notation : If r is a RE over some alphabet then L(r) is the language associate with r . We can define the 
language L(r) associated with (or described by) a REs as follows. 
 
1. 
is the RE describing the empty language i.e. L(
) = 
. 
 
2. 
is a RE describing the language {
} i.e. L(
) = {
} . 
 
3. 
, a is a RE denoting the language {a} i.e . L(a) = {a} . 
 
4. If 
and 
are REs denoting language L(
) and L(
) respectively, then 
 
i) 
is a regular expression denoting the language L(
) = L(
) ∪ L(
) 
 
ii) 
is a regular expression denoting the language L(
)=L(
) L(
) 
 
iii) 
is a regular expression denoting the language 
 
 
iv) (
) is a regular expression denoting the language L((
)) = L(
) 
 
Example : Consider the RE (0*(0+1)). Thus the language denoted by the RE is 
 
L(0*(0+1)) = L(0*) L(0+1) .......................by 4(ii) 
 
= L(0)*L(0) ∪ L(1) 
 
= {
 , 0,00,000,........} {0} 
{1} 
 
= {
 , 0,00,000,........} {0,1} 
 
= {0, 00, 000, 0000,..........,1, 01, 001, 0001,...............} 
 
Precedence Rule 
www.indiansbrain.com
Consider the RE ab + c. The language described by the RE can be thought of either L(a)L(b+c) or 
 
L(ab)
L(c) as provided by the rules (of languages described by REs) given already. But these two 
represents two different languages lending to ambiguity. To remove this ambiguity we can either 
 
 
 
1) Use fully parenthesized expression- (cumbersome) or 
 
2) Use a set of precedence rules to evaluate the options of REs in some order. Like other algebras mod in 
mathematics. 
 
For REs, the order of precedence for the operators is as follows: 
 
i) The star operator precedes concatenation and concatenation precedes union (+) operator. 
 
ii) It is also important to note that concatenation & union (+) operators are associative and union operation 
is commutative. 
 
Using these precedence rule, we find that the RE ab+c represents the language L(ab) 
L(c) i.e. it should be 
grouped as ((ab)+c). 
 
We can, of course change the order of precedence by using parentheses. For example, the 
language represented by the RE a(b+c) is L(a)L(b+c). 
 
 
 
Example : The RE ab*+b is grouped as ((a(b*))+b) which describes the language L(a)(L(b))*
L(b) 
 
Example : The RE (ab)*+b represents the language (L(a)L(b))* 
L(b). 
 
Example : It is easy to see that the RE (0+1)*(0+11) represents the language of all strings over {0,1} which 
are either ended with 0 or 11. 
 
Example : The regular expression r =(00)*(11)*1 denotes the set of all strings with an even number of 0's 
 
followed by an odd number of 1's i.e. 
 
Note : The notation 
is used to represent the RE rr*. Similarly, 
represents the RE rr, 
denotes 
r, 
and so on. 
 
An arbitrary string over 
= {0,1} is denoted as (0+1)*. 
 
Exercise : Give a RE r over {0,1} s.t. L(r)={
 has at least one pair of consecutive 1's} 
 
Solution : Every string in L(r) must contain 00 somewhere, but what comes before and what goes before is 
completely arbitrary. Considering these observations we can write the REs as (0+1)*11(0+1)*. 
 
Example : Considering the above example it becomes clean that the RE (0+1)*11(0+1)*+(0+1)*00(0+1)* 
represents the set of string over {0,1} that contains the substring 11 or 00. 
www.indiansbrain.com
Example : Consider the RE 0*10*10*. It is not difficult to see that this RE describes the set of strings over {0,1} 
that contains exactly two 1's. The presence of two 1's in the RE and any no of 0's before, between and after the 
1's ensure it. 
 
Example : Consider the language of strings over {0,1} containing two or more 1's. 
 
Solution : There must be at least two 1's in the RE somewhere and what comes before, between, and after is 
completely arbitrary. Hence we can write the RE as (0+1)*1(0+1)*1(0+1)*. But following two REs also 
represent the same language, each ensuring presence of least two 1's somewhere in the string 
 
 
 
i) 0*10*1(0+1)* 
 
ii) (0+1)*10*10* 
 
Example : Consider a RE r over {0,1} such that 
 
L(r) = {
 has no pair of consecutive 1's} 
 
Solution : Though it looks similar to ex ……., it is harder to construct to construct. We observer that, whenever 
a 1 occurs, it must be immediately followed by a 0. This substring may be preceded & followed by any no of 
0's. So the final RE must be a repetition of strings of the form: 00…0100….00 i.e. 0*100*. So it looks like the 
RE is (0*100*)*. But in this case the strings ending in 1 or consisting of all 0's are not accounted for. Taking 
these observations into consideration, the final RE is  r = (0*100*)(1+ 
)+0*(1+
). 
 
Alternative Solution : 
The language can be viewed as repetitions of the strings 0 and 01. Hence get the RE as r = (0+10)*(1+
).This 
is a shorter expression but represents the same language. 
 
Regular Expression: 
 
FA to regular expressions: 
 
FA to RE (REs for Regular Languages) : 
 
Lemma : If a language is regular, then there is a RE to describe it. i.e. if L = L(M) for some DFA M, then there is a 
RE r such that L = L(r). 
 
Proof : We need to construct a RE r such that 
. Since M is a DFA, it has a finite no 
of states. Let the set of states of M is Q = {1, 2, 3,..., n} for some integer n. [ Note : if the n states of M were 
denoted by some other symbols, we can always rename those to indicate as 1, 2, 3,..., n ]. The required RE is 
constructed inductively. 
 
Notations : 
is a RE denoting the language which is the set of all strings w such that w is the label of a 
path from state i to state j 
in M, and that path has no intermediate state whose number is 
greater then k. ( i & j (begining and end pts) are not considered to be "intermediate" so i and /or j can be 
www.indiansbrain.com
greater than k ) 
 
We now construct 
inductively, for all i, j 
Q starting at k = 0 and finally reaching k = n. 
 
Basis : k = 0, 
i.e. the paths must not have any intermediate state ( since all states are numbered 1 or 
above). There are only two possible paths meeting the above condition : 
 
1. A direct transition from state i to state j. 
o 
= a if then is a transition from state i to state j on symbol the single symbol a. 
 
o 
= 
if there are multiple transitions from state i to state j on symbols 
 
. 
o 
= f if there is no transition at all from state i to state j. 
 
2. All paths consisting of only one node i.e. when i = j. This gives the path of length 0 (i.e. the RE 
denoting the string 
) and all self loops. By simply adding Î to various cases above we get 
the corresponding REs i.e. 
o 
= 
+ a if there is a self loop on symbol a in state i . 
 
o 
= 
+ 
if there are self loops in state i as multiple symbols 
 
. 
 
o 
= 
if there is no self loop on state i. 
 
Induction : 
 
Assume that there exists a path from state i to state j such that there is no intermediate state whose number is 
 
greater than k. The corresponding Re for the label of the path is 
. There are only two possible cases : 
 
1. The path dose not go through the state k at all i.e. number of all the intermediate states are less 
than k. So, the label of the path from state i to state j is tha language described by the RE 
. 
 
2. The path goes through the state k at least once. The path may go from i to j and k may appear more 
than once. We can break the into pieces as shown in the figure 7. 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7 
 
1. The first part from the state i to the state k which is the first recurence. In this path, all 
intermediate states are less than k and it starts at iand ends at k. So the RE 
denotes the 
language of the label of path. 
 
2. The last part from the last occurence of the state k in the path to state j. In this path also, no 
intermediate state is numbered greater than k. Hence the RE 
denoting the language of the 
label of the path. 
 
3. In the middle, for the first occurence of k to the last occurence of k , represents a loop which may be 
taken zero times, once or any no of times. And all states between two consecutive k's are 
numbered less than k. 
 
Hence the label of the path of the part is denoted by the RE 
.The label of the path from state i to state 
j is the concatenation of these 3 parts which is 
 
 
 
Since either case 1 or case 2 may happen the labels of all paths from state i to j is denoted by the following RE 
 
 
 
 
We can construct 
for all i, j 
{1,2,..., n} in increasing order of k starting with the basis k = 0 upto k = n since 
depends only on expressions with a small superscript (and hence will be available). WLOG, assume 
 
that state 1 is the start state and 
are the m final states where ji 
{1, 2, ... , n }, 
and 
 
. According to the convention used, the language of the automatacan be denoted by the RE 
www.indiansbrain.com
 
 
Since 
is the set of all strings that starts at start state 1 and finishes at final state 
following the transition of 
the FA with any value of the intermediate state (1, 2, ... , n) and hence accepted by the automata. 
 
Regular Grammar: 
 
A grammar 
is right-linear if each production has one of the following three forms: 
 
 
A
cB ,

 
A
c,
 
A

 
Where A, B 
( with A = B allowed) and 
. A grammar G is left-linear if each production has once of 
the following three forms. 
 
A
Bc , A
c, A
 
 
A right or left-linear grammar is called a regular grammar. 
 
Regular grammar and Finite Automata are equivalent as stated in the following theorem. 
 
Theorem : A language L is regular iff it has a regular grammar. We use the following two lemmas to prove the 
above theorem. 
 
Lemma 1 : If L is a regular language, then L is generated by some right-linear grammar. 
 
Proof : Let 
be a DFA that accepts L. 
 
Let 
and 
. 
 
We construct the right-linear grammar 
by letting 
 
N = Q , 
and 
 
[ Note: If 
, then 
] 
 
Let 
. For M to accept w, there must be a sequence of states 
such that 
www.indiansbrain.com
 
 
 
 
 
 
 
 
and 
 
By construction, the grammar G will have one production for each of the above transitions. Therefore, we have 
the corresponding derivation. 
 
 
 
 
Hence w 
L(g). 
 
Conversely, if 
, then the derivation of w in G must have the form as given above. 
But, then the construction of G from M implies that 
 
, where 
, completing the proof. 
 
Lemma 2 : Let 
be a right-linear grammar. Then L(G) is a regular 
language. Proof: To prove it, we construct a FA M from G to accept the same language. 
 
is constructed as follows: 
 
( 
is a special sumbol not in N ) 
 
, 
 
For any 
and 
and 
is defined as 
 
 
if 
 
and 
, if 
. 
We now show that this construction works. 
 
Let 
. Then there is a derivation of w in G of the form 
www.indiansbrain.com
 
 
By contradiction of M, there must be a sequence of transitions 
 
 
 
 
 
 
 
 
 
 
implying that 
i.e. w is accepted by M. 
 
Conversely, if 
is accepted by M, then because 
is the only accepting state of M, the transitions 
causing w to be accepted by M will be of the form given above. These transitions corresponds to a 
 
derivationof w in the grammar G. Hence 
, completing the proof of the lemma. 
 
Given any left-linear grammar G with production of the form 
, we can construct from it a 
right-linear grammar 
by replacing every production of G of the form 
with 
 
 
It is easy to prove that 
. Since 
is right-linear, 
is regular. But then so are 
i.e. 
because regular languages are closed under reversal. 
 
Putting the two lemmas and the discussions in the above paragraph together we get the proof of the theorem- 
 
A language L is regular iff it has a regular grammar 
 
Example : Consider the grammar 
 
 
 
It is easy to see that G generates the language denoted by the regular expression 
(01)*0. The construction of lemma 2 for this grammar produces the follwoing FA. 
 
This FA accepts exactly (01)*1. 
 
Decisions Algorithms for CFL 
 
In this section, we examine some questions about CFLs we can answer. A CFL may be represented using a 
CFG or PDA. But an algorithm that uses one representation can be made to work for the others, since we can 
construct one from the other. 
www.indiansbrain.com
Testing Emptiness : 
 
Theorem : There are algorithms to test emptiness of a CFL. 
 
Proof : Given any CFL L, there is a CFG G to generate it. We can determine, using the construction described 
 
in the context of elimination of useless symbols, whether the start symbol is useless. If so, then 
; 
otherwise not. 
 
Testing Membership : 
 
Given a CFL L and a string x, the membership, problem is to determine whether 
? 
 
Given a PDA P for L, simulating the PDA on input string x doesnot quite work, because the PDA can grow 
its stack indefinitely on 
input, and the process may never terminate, even if the PDA is deterministic. 
 
So, we assume that a CFG 
is given such that L = L(G). 
 
Let us first present a simple but inefficient algorithm. 
 
Convert G to 
in CNF generating 
. If the input string 
, then we need to 
 
determine whether 
and it can easily be done using the technique given in the context of elimination of 
 
-production. If , 
then 
iff 
. Consider a derivation under a grammar in CNF. At 
every step, a production in CNF in used, and hence it adds exactly one terminal symbol to the sentential form. 
 
Hence, if the length of the input string x is n, then it takes exactly n steps to derive x ( provided x is in 
). 
 
Let the maximum number of productions for any nonterminal in 
is K. So at every step in derivation, there are 
atmost k choices. We may try out all these choices, systematically., to derive the string x in 
. Since 
 
there are atmost 
i.e. 
choices. This algorithms is of exponential time complexity. We now present an 
efficient (polynomial time) membership algorithm. 
 
Pumping Lemma: 
 
Limitations of Finite Automata and Non regular Languages : 
 
The class of languages recognized by FA s is strictly the regular set. There are certain languages which are 
non regular i.e. cannot be recognized by any FA 
 
Consider the language 
 
 
In order to accept is language, we find that, an automaton seems to need to remember when passing the 
center point between a's and b's how many a's it has seen so far. Because it would have to compare that 
with the number of b's to either accept (when the two numbers are same) or reject (when they are not same) 
the input string. 
www.indiansbrain.com
But the number of a's is not limited and may be much larger than the number of states since the string may 
be arbitrarily long. So, the amount of information the automaton need to remember is unbounded. 
 
A finite automaton cannot remember this with only finite memory (i.e. finite number of states). The fact that 
FA s have finite memory imposes some limitations on the structure of the languages recognized. Inductively, we 
can say that a language is regular only if in processing any string in this language, the information that has to 
be remembered at any point is strictly limited. The argument given above to show that 
is non regular is 
informal. We now present a formal method for showing that certain languages such as 
are non regular 
 
Properties of CFL’s 
 
Closure properties of CFL: 
 
We consider some important closure properties of CFLs. 
 
Theorem : If 
and 
are CFLs then so is 
 
 
Proof : Let 
and 
be CFGs generating. Without loss of generality, we 
can assume that 
. Let 
is a nonterminal not in 
or 
. We construct the grammar 
 
from 
and 
, where 
 
, 
 
 
 
 
 
 
 
 
We now show that 
 
 
Thus proving the theorem. 
 
Let 
. Then 
. All productions applied in their derivation are also in 
. Hence 
i.e. 
 
 
 
Similarly, if 
, then 
 
 
Thus 
. 
www.indiansbrain.com
Conversely, let 
. Then 
and the first step in this derivation must be either 
or 
. Considering the former case, we have 
 
 
Since 
and 
are disjoint, the derivation 
must use the productions of 
only ( which are also in 
 
) Since 
is the start symbol of 
. Hence, 
giving 
. 
 
Using similar reasoning, in the latter case, we get 
. Thus 
. 
 
So, 
, as claimed 
 
 
Theorem : If 
and 
are CFLs, then so is 
. 
 
Proof : Let 
and 
be the CFGs generating 
and 
respectively. 
Again, we assume that 
and 
are disjoint, and 
is a nonterminal not in 
or 
. we construct the CFG 
from 
and 
, where 
 
 
 
 
 
 
 
 
 
 
 
 
We claim that 
 
 
 
To prove it, we first assume that 
and 
. Then 
and 
. We can derive the string xy 
in 
as shown below. 
 
 
 
 
 
since 
and 
. Hence 
. 
www.indiansbrain.com
For the converse, let 
. Then the derivation of w in 
will be of the form 
 
i.e. the first step in the derivation must see the rule 
. Again, since 
and 
are 
disjoint and 
and 
, some string x will be generated from 
using productions in 
( which 
are also in 
) and such that 
. 
 
Thus 
 
Hence 
and 
. 
 
This means that w can be divided into two parts x, y such that 
and 
. Thus 
.This 
 
completes the proof 
Theorem : If L is a CFL, then so is 
. 
Proof : Let 
be the CFG generating L. Let us construct the CFG 
 
 
where 
. 
 
We now prove that 
, which prove the theorem. 
 
can generate 
in one step by using the production 
since 
, 
 Let 
for any n >1 we can 
write 
where 
for 
 
 
 
 
using following steps. 
 
 
 
 
 
First (n-1)-steps uses the production S
SS producing the sentential form of n numbers of S 's. The 
nonterminal S in the i-th position then generates 
using production in P ( which are also in 
) 
 
It is also easy to see that G can generate the empty string, any string in L and any string 
for n >1 
and none other. 
 
Hence 
 
 
Theorem : CFLs are not closed under intersection 
 
Proof : We prove it by giving a counter example. Consider the language 
.The following 
CFG generates L1 and hence a CFL 
 
can generate any string in L. 
. w can be generated by 
 
from G 
www.indiansbrain.com
 
 
 
 
 
The nonterminal X generates strings of the form 
and C generates strings of the form 
, 
. These are the only types of strings generated by X and C. Hence, S generates 
. 
 
Using similar reasoning, it can be shown that the following grammar 
and hence it is 
also a CFL. 
 
 
 
 
 
 
 
But, 
and is already shown to be not context-free. 
 
Hence proof. 
 
Theorem : A CFL's are not closed under complementations 
 
Proof : Assume, for contradiction, that CFL's are closed under complementation. SInce, CFL's are also closed 
under union, the language 
, where 
and 
are CFL's must be CFL. But by DeMorgan's law 
 
 
 
 
This contradicts the already proved fact that CFL's are not closed under intersection. 
But it can be shown that the CFL's are closed under intersection with a regular set. 
 
Theorem : If L is a CFL and R is a regular language, then 
is a CFL. 
 
Proof : Let 
be a PDA for L and let 
be a DFA for 
R. We construct a PDA M from P and D as follows 
 
 
where 
is defined as 
 
contains 
iff 
www.indiansbrain.com
and 
contains 
 
 
The idea is that M simulates the moves of P and D parallely on input w, and accepts w iff both P and 
D accepts. That means, we want to show that 
 
 
 
 
We apply induction on n, the number of moves, to show that 
 
iff 
 
and 
 
Basic Case is n=0. Hence 
, 
and 
. For this case it is trivially true 
 
Inductive hypothesis : Assume that the statement is true for n -1. 
 
Inductive Step : Let w = xa and 
 
 
 
Let 
 
By inductive hypothesis, 
and 
 
 
From the definition of 
and considering the n-th move of the PDA M above, we have 
 
and 
 
 
Hence 
and 
 
 
If 
and 
, then 
and we got that if M accepts w, then both P and D accepts it. 
 
We can show that converse, in a similar way. Hence 
is a CFL ( since it is accepted by a PDA M ) 
This property is useful in showing that certain languages are not context-free. 
 
Example : Consider the language 
 
 
Intersecting L with the regular set 
, we get 
www.indiansbrain.com
 
 
 
Which is already known to be not context-free. Hence L is not context-free 
Theorem : CFL's are closed under reversal. That is if L is a CFL, then so is 
 
 
Proof : Let the CFG 
generates L. We construct a CFG 
where 
 
. We now show that 
, thus proving the theorem. 
We need to prove that 
iff 
. 
 
The proof is by induction on n, the number of steps taken by the derivation. We assume, for simplicity (and 
of course without loss of generality), that G and hence 
are in CNF. 
 
The basis is n=1 in which case it is trivial. Because 
must be either 
or BC with 
. 
 
Hence 
iff 
 
 
Assume that it is true for (n-1)-steps. Let 
. Then the first step must apply a rule of the 
form 
and it gives 
 
where 
and 
 
 
By constructing of G', 
 
Hence 
 
 
The converse case is exactly similar 
Substitution : 
 
, let 
be a language (over any alphabet). This defines a function S, called substitution, on 
which is 
 
denoted as 
- for all 
 
 
This definition of substitution can be extended further to apply strings and langauge as well. 
If 
, where 
, is a string in 
, then 
 
. 
Similarly, for any language L, 
 
The following theorem shows that CFLs are closed under substitution. 
 
Thereom : Let 
is a CFL, and s is a substitution on 
such that 
is a CFL for all 
, 
thus s(L) is a CFL 
 
Proof : Let L = L(G) for a CFG 
and for every 
, 
for some 
. Without loss of generality, assume that the sets of nonterminals N and 
's 
are disjoint. 
www.indiansbrain.com
Now, we construct a grammar 
, generating s(L), from G and 
's as follows : 
 

 

 

 
 
consists of

1. 
and 
 
2. The production of P but with each terminal a in the right hand side of a production replaced 
by 
everywhere. 
We now want to prove that this construction works i.e. 
iff 
. 
 
If Part : Let 
then according to the definition there is some string 
and 
for 
such that 
 
We will show that 
. 
 
From the construction of 
, we find that, there is a derivation 
corresponding to the string 
 
(since 
contains all productions of G but every ai replaced with 
in the RHS of any 
production). 
 
Every 
is the start symbol of 
and all productions of 
are also included in 
. 
Hence 
 
 
 
 
 
 
 
 
Therefore, 
 
 
(Only-if Part) Let 
. Then there must be a derivative as follows : 
 
(using the production of G include in 
as modified by (step 2) of the construction of 
.) 
 
Each 
(
) can only generate a string 
, since each 
's and N are disjoin. 
Therefore, we get 
 
 
since 
 
www.indiansbrain.com
since 
 
 
 
 
 
The string 
is formed by substituting strings 
for each 
and hence 
. 
 
Theorem : CFL's are closed under homomorphism 
 
Proof : Let 
be a CFL, and h is a homomorphism on 
i.e 
for some alphabets 
. consider the 
following substitution S:Replace each symbol 
by the language consisting of the only string h(a), i.e. 
 
for all 
. Then, it is clear that, h(L) = s(L). Hence, CFL's being closed under 
substitution must also be closed under homomorphism. 
www.indiansbrain.com
 
 
UNIT- III 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Grammar 
 
A grammar is a mechanism used for describing languages. This is one of the most simple but yet powerful 
mechanism. There are other notions to do the same, of course. 
 
In everyday language, like English, we have a set of symbols (alphabet), a set of words constructed from 
these symbols, and a set of rules using which we can group the words to construct meaningful sentences. The 
grammar for English tells us what are the words in it and the rules to construct sentences. It also tells us 
whether a particular sentence is well-formed (as per the grammar) or not. But even if one follows the rules of 
the english grammar it may lead to some sentences which are not meaningful at all, because of impreciseness 
and ambiguities involved in the language. In english grammar we use many other higher level constructs like 
noun-phrase, verb-phrase, article, noun, predicate, verb etc. A typical rule can be defined as 
 
< sentence >
< noun-phrase > < predicate > 
 
meaning that "a sentence can be constructed using a 'noun-phrase' followed by a predicate". 
 
Some more rules are as follows: 
 
< noun-phrase >
< article >< noun > 
 
< predicate > 
< verb > 
 
with similar kind of interpretation given above. 
 
If we take {a, an, the} to be <article>; cow, bird, boy, Ram, pen to be examples of <noun>; and eats, runs, 
swims, walks, are associated with <verb>, then we can construct the sentence- a cow runs, the boy eats, an 
pen walks- using the above rules. Even though all sentences are well-formed, the last one is not meaningful. 
We observe that we start with the higher level construct <sentence> and then reduce it to <noun-phrase>, 
<article>, <noun>, <verb> successively, eventually leading to a group of words associated with these 
constructs. 
 
These concepts are generalized in formal language leading to formal grammars. The word 'formal' here refers 
to the fact that the specified rules for the language are explicitly stated in terms of what strings or symbols can 
occur. There can be no ambiguity in it. 
 
Formal definitions of a Grammar 
www.indiansbrain.com
A grammar G is defined as a quadruple. 
 
 
 
 
N is a non-empty finite set of non-terminals or variables, 
 
is a non-empty finite set of terminal symbols such that 
 
 
, is a special non-terminal (or variable) called the start symbol, and 
is a 
finite set of production rules. 
 
The binary relation defined by the set of production rules is denoted by 
, i.e. 
iff 
. 
 
In other words, P is a finite set of production rules of the form 
, where 
and 
 
 
 
Production rules: 
 
The production rules specify how the grammar transforms one string to another. Given a string 
, we say that 
the production rule 
is applicable to this string, since it is possible to use the rule 
to rewrite the 
(in 
) to 
obtaining a new string 
. We say that 
derives 
and is denoted as 
 
 
 
 
Successive strings are dervied by applying the productions rules of the grammar in any arbitrary order. 
A particular rule can be used if it is applicable, and it can be applied as many times as described. 
 
We write 
if the string 
can be derived from the string 
in zero or more steps; 
if 
can be 
derived from 
in one or more steps. 
 
By applying the production rules in arbitrary order, any given grammar can generate many strings of terminal 
symbols starting with the special start symbol, S, of the grammar. The set of all such terminal strings is 
called the language generated (or defined) by the grammar. 
 
Formaly, for a given grammar 
the language generated by G is 
 
 
 
 
 
 
That is 
iff 
. 
www.indiansbrain.com
If 
, we must have for some 
, 
, denoted as a 
derivation sequence of w, The strings 
 
are denoted as sentential forms of the 
derivation.  
 
 
Example : Consider the grammar 
 
, where N = {S},={a, b} and P is the set of the following 
production rules 
 
 
 
 
{ S 
ab, SaSb} 
 
Some terminal strings generated by this grammar together with their derivation is given below. 
 
S 
ab 
 
S 
aSb
aabb 
 
S 
aSb
aaSbb
aaabbb 
 
It is easy to prove that the language generated by this grammar is 
 
 
 
 
By using the first production, it generates the string ab ( for i =1 ). 
 
To generate any other string, it needs to start with the production S
aSb and then the non-terminal S in the RHS can be 
replaced either by ab (in which we get the string aabb) or the same production S
aSb can be used one or more 
times. Every time it adds an 'a' to the left and a 'b' to the right of S, thus giving the sentential 
 
form 
. When the non-terminal is replaced by ab (which is then only possibility for generating 
a terminal string) we get a terminal string of the form 
. 
 
There is no general rule for finding a grammar for a given language. For many languages we can devise 
grammars and there are many languages for which we cannot find any grammar. 
 
Example: Find a grammar for the language 
. 
 
It is possible to find a grammar for L by modifying the previous grammar since we need to generate an extra b 
at the end of the string 
. We can do this by adding a production S
Bb where the non-terminal B 
generates 
as given in the previous example. 
 
Using the above concept we devise the follwoing grammar for L. 
 
 
where, N = { S, B }, P = { S
Bb, B
ab, B
aBb } 
 
Parse Trees: 
www.indiansbrain.com
 
 
 
 
 
 
Construction of a Parse tree: 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Yield of a Parse tree: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Ambiguity in languages and grammars: 
www.indiansbrain.com
www.indiansbrain.com
 
 
 
 
UNIT-IV 
 
 
 
 
 
 
Push down automata: 
 
Regular language can be charaterized as the language accepted by finite automata. Similarly, we can 
characterize the context-free language as the langauge accepted by a class of machines called 
"Pushdown Automata" (PDA). A pushdown automation is an extension of the NFA. 
 
It is observed that FA have limited capability. (in the sense that the class of languages accepted or characterized by 
them is small). This is due to the "finite memory" (number of states) and "no external memory" involved with them. A 
PDA is simply an NFA augmented with an "external stack memory". The addition of a stack provides the PDA with a 
last-in, first-out memory management cpapability. This "Stack" or "pushdown store" can be used to record a 
potentially unbounded information. It is due to this memory management capability with the help of the stack that a 
PDA can overcome the memory limitations that prevents a FA to 
 
accept many interesting languages like 
. Although, a PDA can store an unbounded amount of 
information on the stack, its access to the information on the stack is limited. It can push an element onto the 
top of the stack and pop off an element from the top of the stack. To read down into the stack the top elements 
must be popped off and are lost. Due to this limited access to the information on the stack, a PDA still has 
some limitations and cannot accept some other interesting languages. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
As shown in figure, a PDA has three components: an input tape with read only head, a finite control and 
a pushdown store. 
 
The input head is read-only and may only move from left to right, one symbol (or cell) at a time. In each step, the 
PDA pops the top symbol off the stack; based on this symbol, the input symbol it is currently reading, and 
www.indiansbrain.com
its present state, it can push a sequence of symbols onto the stack, move its read-only head one cell 
(or symbol) to the right, and enter a new state, as defined by the transition rules of the PDA. 
 
PDA are nondeterministic, by default. That is, 
- transitions are also allowed in which the PDA can pop and 
push, and change state without reading the next input symbol or moving its read-only head. Besides this, 
there may be multiple options for possible next moves. 
 
Formal Definitions : Formally, a PDA M is a 7-tuple M =
 
where, 
 
 
is a finite set of states,
 
is a finite set of input symbols (input alphabets),

 
is a finite set of stack symbols (stack alphabets),

 
is a transition function from 
to subset of 


 
is the start state
 
, is the initial stack symbol, and

 
, is the final or accept states.
 
Explanation of the transition function, 
: 
 
If, for any 
, 
. This means intitutively that whenever 
the PDA is in state q reading input symbol a and z on top of the stack, it can nondeterministically for any i, 
 
 
 
go to state 


 
pop z off the stack

 
push 
onto the stack (where 
) (The usual convention is that if 
, 
then 
will be at the top and 
at the bottom.)
 
move read head right one cell past the current symbol a.
 
If a = 
, then 
means intitutively that whenver the PDA is in state 
q with z on the top of the stack regardless of the current input symbol, it can nondeterministically for any 
 
i, 
, 
 
 
go to state 

 
pop z off the stack

 
push 
onto the stack, and
 
leave its read-only head where it is.
www.indiansbrain.com
State transition diagram : A PDA can also be depicted by a state transition diagram. The labels on the arcs 
indicate both the input and the stack operation. The transition 
 
for 
and 
is depicted by 
 
 
 
 
 
 
 
Final states are indicated by double circles and the start state is indicated by an arrow to it from nowhere. 
 
 
Configuration or Instantaneous Description (ID) : 
 
A configuration or an instantaneous description (ID) of PDA at any moment during its computation is an 
element of 
describing the current state, the portion of the input remaining to be read (i.e. 
under and to the right of the read head), and the current stack contents. Only these three elements 
can affect the computation from that point on and, hence, are parts of the ID. 
 
The start or inital configuartion (or ID) on input 
is 
. That is, the PDA always starts in its 
start state, 
with its read head pointing to the leftmost input symbol and the stack containing only 
the start/initial stack symbol, 
. 
 
The "next move relation" one figure describes how the PDA can move from one configuration to 
another in one step. 
 
Formally, 
 
 
 
 
iff 
'a' may be 
or an input symbol. 
 
Let I, J, K be IDs of a PDA. We define we write I
K, if ID I can become K after exactly i moves. The 
relations 
and 
define as follows 
 
I 
K 
 
I 
J if 
such that I 
K and K
 J 
 
I 
J if 
such that I 
J. 
www.indiansbrain.com
That is, 
is the reflexive, transitive closure of 
. We say that I 
J if the ID J follows from the ID I in 
zero or more moves. 
 
( Note : subscript M can be dropped when the particular PDA M is understood. ) 
 
Language accepted by a PDA M 
 
There are two alternative definiton of acceptance as given below. 
 
1. Acceptance by final state : 
 
Consider the PDA 
. Informally, the PDA M is said to accept its input 
by 
final state if it enters any final state in zero or more moves after reading its entire input, starting in the start 
configuration on input 
. 
 
Formally, we define L(M), the language accepted by final state to be 
 
{ 
| 
for some 
and 
} 
 
 
 
 
2. Acceptance by empty stack (or Null stack) : The PDA M accepts its input 
by empty stack if starting in the 
 
start configuration on input 
, it ever empties the stack w/o pushing anything back on after reading the 
entire input. Formally, we define N(M), the language accepted by empty stack, to be 
 
{ 
| 
for some 
} 
 
Note that the set of final states, F is irrelevant in this case and we usually let the F to be the empty set i.e. F = 
Q . 
 
Example 1 : Here is a PDA that accepts the language 
. 
 
 
 
 
 
 
 
 
 
, and 
consists of the following transitions 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
The PDA can also be described by the adjacent transition diagram. 
 
 
 
 
 
 
 
 
 
 
 
 
Informally, whenever the PDA M sees an input a in the start state 
with the start symbol z on the top of the stack 
it pushes a onto the stack and changes state to 
. (to remember that it has seen the first 'a'). On state 
if it 
sees anymore a, it simply pushes it onto the stack. Note that when M is on state 
, the symbol on the 
 
top of the stack can only be a. On state 
if it sees the first b with a on the top of the stack, then it needs to 
start comparison of numbers of a's and b's, since all the a's at the begining of the input have already been 
pushed onto the stack. It start this process by popping off the a from the top of the stack and enters in state q3 
 
(to remember that the comparison process has begun). On state 
, it expects only b's in the input (if it sees 
any more a in the input thus the input will not be in the proper form of anbn). Hence there is no more on input a 
when it is in state 
. On state 
it pops off an a from the top of the stack for every b in the input. When it sees 
the last b on state q3 (i.e. when the input is exaushted), then the last a from the stack will be popped off and the 
start symbol z is exposed. This is the only possible case when the input (i.e. on 
-input ) the PDA M 
 
will move to state 
which is an accept state. 
 
we can show the computation of the PDA on a given input using the IDs and next move relations. For example, 
following are the computation on two input strings. 
 
Let the input be aabb. we start with the start configuration and proceed to the subsequent IDs using 
the transition function defined 
 
( using transition 1 ) 
 
( using transition 2 ) 
 
( using transition 3 ) 
www.indiansbrain.com
( using transition 4 ), 
( using transition 5 ) , 
is final state. Hence , accept. So 
the string aabb is rightly accepted by M 
 
we can show the computation of the PDA on a given input using the IDs and next move relations. For example, 
following are the computation on two input strings. 
 
i) Let the input be aabab. 
 
 
 
 
 
 
 
 
 
 
 
No further move is defined at this point. 
 
Hence the PDA gets stuck and the string aabab is not accepted. 
 
Example 2 : We give an example of a PDA M that accepts the set of balanced strings of parentheses [] by 
empty stack. 
The PDA M is given below. 
 
 
where 
is defined as 
 
 
 
 
 
 
 
 
 
 
 
Informally, whenever it sees a [, it will push the ] onto the stack. (first two transitions), and whenever it sees a ] 
and the top of the stack symbol is [, it will pop the symbol [ off the stack. (The third transition). The fourth 
transition is used when the input is exhausted in order to pop z off the stack ( to empty the stack) and accept. 
Note that there is only one state and no final state. The following is a sequence of configurations leading to the 
acceptance of the string [ [ ] [ ] ] [ ]. 
 
 
 
 
 
 
 
 
Equivalence of acceptance by final state and empty stack. 
 
It turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty 
stack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also 
be accepted by final state by some other PDA and vice versa. Hence it doesn't matter which one we use, since 
www.indiansbrain.com
each kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final 
state or empty stack, we can always construct an equivalent PDA M with a single final state that accpets 
exactly the same language L. The construction process of M' from M and the proof of equivalence of M & M' 
are given below. 
 
There are two cases to be considered. 
 
CASE I : PDA M accepts by final state, Let 
Let qf be a new state not in Q. 
Consider the PDA 
where 
as well as the following transition. 
 
contains 
and 
. It is easy to show that M and M' are equivalent i.e. 
 
L(M) = L(
) 
 
Let 
L(M) . Then 
for some 
and 
 
 
Then 
 
 
Thus 
accepts 
 
 
Conversely, let 
accepts 
i.e. 
L(
), then 
for 
inherits all other moves except the last one from M. Hence 
for some 
 
. 
 
Thus M accepts 
. Informally, on any input 
simulate all the moves of M and enters in its own final 
state 
whenever M enters in any one of its final status in F. Thus 
accepts a string 
iff M accepts it. 
 
CASE II : PDA M accepts by empty stack. 
 
We will construct 
from M in such a way that 
simulates M and detects when M empties its stack. 
 
enters its final state 
when and only when M empties its stack.Thus 
will accept a string 
iff M 
accepts. 
 
Let 
where 
and X
and 
contains all 
the transition of 
, as well as the following two transitions. 
 
 
and 
www.indiansbrain.com
Transitions 1 causes 
to enter the initial configuration of M except that 
will have its own bottom-of-stack 
marker X which is below the symbols of M's stack. From this point onward 
will simulate every move of M 
since all the transitions of M are also in 
 
 
If M ever empties its stack, then 
when simulating M will empty its stack except the symbol X at the bottom. 
 
At this point, 
will enter its final state 
by using transition rule 2, thereby (correctly) accepting the 
input. We will prove that M and 
are equivalent. 
 
Let M accepts 
. Then 
 
for some 
. But then 
 
 
( by transition rule 1) 
 
( Since 
includes all the moves of M ) 
 
 
( by transition rule 2 ) 
 
Hence, 
also accepts 
. Conversely, let 
accepts 
. 
 
Then 
for some 
 
 
Every move in the sequence, 
were taken from M. 
 
Hence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. 
 
 
 
 
Equivalence of PDA’s and CFG’s: 
We will now show that pushdown automata and context-free grammars are equivalent in expressive power, 
that is, the language accepted by PDAs are exactly the context-free languages. To show this, we have to 
prove each of the following: 
 
i) 
Given any arbitrary CFG G there exists some PDA M that accepts exactly the same 
language generated by G. 
 
ii) 
Given any arbitrary PDA M there exists a CFG G that generates exactly the same 
language accpeted by M. 
 
(i) CFA to PDA 
 
We will first prove that the first part i.e. we want to show to convert a given CFG to an equivalent PDA. 
www.indiansbrain.com
Let the given CFG is 
. Without loss of generality we can assume that G is in 
Greibach Normal Form i.e. all productions of G are of the form . 
 
 where 
and 
. 
 
From the given CFG G we now construct an equivalent PDA M that accepts by empty stack. Note that there 
is only one state in M. Let 
 
, where 
 
 
q is the only state

 
is the input alphabet,
 
N is the stack alphabet ,

 
q is the start state.
 
S is the start/initial stack symbol, and 
, the transition relation is defined as follows
 
For each production 
, 
. We now want to show 
that M and G are equivalent i.e. L(G)=N(M). i.e. for any 
. 
iff 
. 
 
If 
, then by definition of L(G), there must be a leftmost derivation starting with S and deriving w. 
 
i.e. 
 
 
Again if 
, then one sysmbol. Therefore we need to show that for any 
. 
 
iff 
. 
 
But we will prove a more general result as given in the following lemma. Replacing A by S (the start 
symbol) and 
by 
gives the required proof. 
 
Lemma For any 
, 
and 
, 
via a leftmost derivative iff 
. 
 
Proof : The proof is by induction on n. 
 
Basis : n = 0 
www.indiansbrain.com
 
iff 
i.e. 
and 
 
 
 
iff 
 
iff 
 
 
Induction Step : 
 
First, assume that 
via a leftmost derivation. Let the last production applied in their derivation is 
for some 
and 
. 
 
Then, for some 
, 
 
 
 
 
 
 
where 
and 
 
 
Now by the indirection hypothesis, we get, 
 
.............................................................................(1) 
Again by the construction of M, we get 
 
 
so, from (1), we get 
 
 
 
 
since 
and 
, we get 
 
 
That is, if 
, then 
. Conversely, assume that 
and let 
www.indiansbrain.com
be the transition used in the last move. Then for some 
, 
and 
 
 
 
where 
and 
. 
 
Now, by the induction hypothesis, we get 
 
via a leftmost derivation. 
 
Again, by the construction of M, 
must be a production of G. [ Since 
]. Applying the production to the sentential form 
we get 
 
 
 
 
 
i.e. 
 
 
via a leftmost derivation. 
 
Hence the proof. 
 
Example : Consider the CFG G in GNF 
 
S
aAB 
 
A
a / aA 
B
a / bB 
 
The one state PDA M equivalent to G is shown below. For convenience, a production of G and 
the corresponding transition in M are marked by the same encircled number. 
 
(1) S
aAB 
 
(2) A 
a 
 
(3) A
aA 
 
(4) B 
a 
 
(5) B 
bB 
 
 
. We have used the same construction discussed earlier 
 
Some Useful Explanations : 
Consider the moves of M on input aaaba leading to acceptance of the string. 
 
Steps 
www.indiansbrain.com
 
1. (q, aaaba, s) 
( q, aaba, AB ) 
2. 
( q, aba, AB ) 
3. 
( q, ba, B ) 
4. 
( q, a, B ) 
5. 
( q,   ,   )    Accept by empty stack. 
 
Note : encircled numbers here shows the transitions rule applied at every step. 
 
Now consider the derivation of the same string under grammar G. Once again, the production used at 
every step is shown with encircled number. 
 
S 
aAB 
aaAB 
aaaB 
aaabB 
aaaba 
 
Steps 
1 
2 
3 
4 
5
 
Observations: 
 
There is an one-to-one correspondence of the sequence of moves of the PDA M and the derivation

sequence under the CFG G for the same input string in the sense that - number of steps in both 
the cases are same and transition rule corresponding to the same production is used at every step 
(as shown by encircled number). 

 
considering the moves of the PDA and derivation under G together, it is also observed that at 
every step the input read so far and the stack content together is exactly identical to the 
corresponding sentential form i.e.

<what is Read><stack> = <sentential form> 
 
Say, at step 2, Read so far = 
a stack = AB 
Sentential form = aAB From this property we claim that 
iff 
. If the claim is 
 
true, then apply with 
and we get 
iff 
 or 
iff 
( 
by definition ) 
 
Thus N(M) = L(G) as desired. Note that we have already proved a more general version of the 
claim PDA and CFG: 
We now want to show that for every PDA M that accpets by empty stack, there is a CFG G such that L(G) = 
N(M) 
 
we first see whether the "reverse of the construction" that was used in part (i) can be used here to construct 
an equivalent CFG from any PDA M. 
 
It can be show that this reverse construction works only for single state PDAs. 
www.indiansbrain.com
 
That is, for every one-state PDA M there is CFG G such that L(G) = N(M). For every move of the 
PDA M 
we introduce a production 
in the 
grammar 
where N = T and 
.
 
we can now apply the proof in part (i) in the reverse direction to show that L(G) = N(M). 
 
But the reverse construction does not work for PDAs with more than one state. For example, consider the PDA 
M produced here to accept the langauge 
 
 
 
 
 
Now let us construct CFG 
using the "reverse" construction. 
 
( Note 
). 
 
Transitions in M 
Corresponding Production in G 
 
 
 
 
 
 
 
 
 
 
 
We can drive strings like aabaa which is in the language. 
 
 
 
 
But under this grammar we can also derive some strings which are not in the language. e.g 
 
 
 
 
and 
. But 
 
 
Therefore, to complete the proof of part (ii) we need to prove the following claim also. 
 
Claim: For every PDA M there is some one-state PDA 
such that 
. 
 
It is quite possible to prove the above claim. But here we will adopt a different approach. We start with 
any arbitrary PDA M that accepts by empty stack and directly construct an equivalent CFG G. 
www.indiansbrain.com
PDA to CFG 
 
We want to construct a CFG G to simulate any arbitrary PDA M with one or more states. Without loss 
of generality we can assume that the PDA M accepts by empty stack. 
 
The idea is to use nonterminal of the form <PAq> whenever PDA M in state P with A on top of the stack goes 
 
to state 
. That is, for example, for a given transition of the PDA corresponding production in the grammar as 
shown below, 
And, we would like to show, in general, that 
iff the PDA M, when started from state P with A on 
 
the top of the stack will finish processing 
, arrive at state q and remove A from the stack. 
 
we are now ready to give the construction of an equivalent CFG G from a given PDA M. we need to introduce 
two kinds of producitons in the grammar as given below. The reason for introduction of the first kind of 
production will be justified at a later point. Introduction of the second type of production has been justified in 
the above discussion. 
 
Let 
be a PDA. We construct from M a equivalent CFG 
 
 
Where 
 
 
N is the set of nonterminals of the form <PAq> for 
and 
and P contains the follwoing
 
two kind of production 
 
1.  
 
 
2. If 
, then for every choice of the sequence 
,
 
, 
. 
 
 
Include the follwoing production 
 
 
 
 
If n = 0, then the production is 
.For the whole exercise to be meaningful we want 
means there is a sequence of transitions ( for PDA M ), starting in state q, ending in 
, 
 
during which the PDA M consumes the input string 
and removes A from the stack (and, of course, all 
other symbols pushed onto stack in A's place, and so on.) 
 
That is we want to claim that 
 
iff 
 
 
If this claim is true, then let 
to get 
iff 
for some 
 
. But for all 
we have 
as production in G. Therefore, 
www.indiansbrain.com
iff 
i.e. 
iff PDA M accepts w by empty stack or L(G) = N(M) 
 
Now, to show that the above construction of CFG G from any PDA M works, we need to prove the 
proposed claim. 
 
Note: At this point, the justification for introduction of the first type of production (of the form 
) in 
the CFG G, is quite clear. This helps use deriving a string from the start symbol of the grammar. 
 
Proof : Of the claim 
iff 
for some 
, 
and 
 
 
The proof is by induction on the number of steps in a derivation of G (which of course is equal to the number 
of moves taken by M). Let the number of steps taken is n. 
 
The proof consists of two parts: ' if ' part and ' only if ' part. First, consider the ' if ' part 
 
If 
then 
. 
 
Basis is n =1 
 
Then 
. In this case, it is clear that 
. Hence, by construction 
is a production of G. 
 
Then 
 
Inductive Hypothesis : 
 
 
 
 
 
Inductive Step : 
 
 
For n >1, let w = ax for some 
and 
consider the first move of the PDA M which uses 
the general transition 
= 
 
. Now M must remove 
from stack 
while consuming x in the remaining n-1 moves. 
 
Let 
, where 
is the prefix of x that M has consumed when 
first appears at top 
of the stack. Then there must exist a sequence of states in M (as per construction) 
(with 
 
), such that 
www.indiansbrain.com
 
 [ This step implies 
] 
 [ This step implies 
] 
 
... 
 
 
=
 
 
[ Note: Each step takes less than or equal to n -1 moves because the total number of moves required 
assumed to be n-1.] 
 
That is, in general 
 
 
, 
. 
 
So, applying inductive hypothesis we get 
 
, 
. But corresponding to the original move 
 
in M we have added the following production in G. 
 
We can show the computation of the PDA on a given input using the IDs and next move relations. For example, 
following are the computation on two input strings. 
 
i) Let the input be aabb. we start with the start configuration and proceed to the subsequent IDs using 
the transition function defined 
 
( using transition 1 ) , 
( using transition 2 ) 
 
( using transition 3 ), 
( using transition 4 ) 
 
( using transition 5 ) , 
is final state. Hence, accept. 
 
So the string aabb is rightly accepted by M. 
 
we can show the computation of the PDA on a given input using the IDs and next move relations. For example, 
following are the computation on two input strings. 
 
i) Let the input be aabab. 
www.indiansbrain.com
 
 
No further move is defined at this point. 
 
Hence the PDA gets stuck and the string aabab is not accepted. 
 
The following is a sequence of configurations leading to the acceptance of the string [ [ ] [ ] ] [ ]. 
 
 
 
 
 
 
 
 
 
 
 
Equivalence of acceptance by final state and empty stack. 
 
It turns out that the two definitions of acceptance of a language by a PDA - accpetance by final state and empty 
stack- are equivalent in the sense that if a language can be accepted by empty stack by some PDA, it can also 
be accepted by final state by some other PDA and vice versa. Hence it doesn't matter which one we use, since 
each kind of machine can simulate the other.Given any arbitrary PDA M that accpets the language L by final 
 
state or empty stack, we can always construct an equivalent PDA M with a single final state that accpets 
exactly the same language L. The construction process of M' from M and the proof of equivalence of M & M' 
are given below 
 
There are two cases to be considered. 
 
CASE 1 : PDA M accepts by final state, Let 
. Let 
be a new state not in Q. 
Consider the PDA 
where 
as well as the following transition. 
 
contains 
and 
. It is easy to show that M and 
are equivalent i.e. 
 
 
. 
 
Let 
. Then 
for some 
and 
 
 
Then 
. 
 
Thus 
accepts 
. 
www.indiansbrain.com
Conversely, let 
accepts 
i.e. 
, then 
for some 
. 
inherits all other moves except the last one from M. Hence 
for some 
. 
 
Thus M accepts 
. Informally, on any input 
simulate all the moves of M and enters in its own final 
state 
whenever M enters in any one of its final status in F. Thus 
accepts a string 
iff M accepts it. 
 
CASE 2 : PDA M accepts by empty stack. 
 
we will construct 
from M in such a way that 
simulates M and detects when M empties its stack. 
 
enters its final state 
when and only when M empties its stack.Thus 
will accept a string 
iff M 
accepts. 
 
Let 
where 
and 
and 
contains all 
the transition of 
, as well as the following two transitions. 
 
and 
 
 
 
 
 
Transitions 1 causes 
to enter the initial configuration of M except that 
will have its own bottom-of-stack 
marker X which is below the symbols of M's stack. From this point onward M' will simulate every move of M 
 
since all the transitions of M are also in 
. 
 
If M ever empties its stack, then 
when simulating M will empty its stack except the symbol X at the bottom. 
 
At this point
, will enter its final state 
by using transition rule 2, thereby (correctly) accepting the 
input. we will prove that M and 
are equivalent. 
 
Let M accepts 
. 
 
Then 
 
for some 
. But then, 
 
 
( by transition rule 1 ) 
 
( since 
include all the moves of M ) 
www.indiansbrain.com
 
( by transition rule 2 ) 
 
Hence, 
also accepts 
.Conversely, let 
accepts 
. 
 
Then 
for some Q . 
 
Every move in the sequence 
 
were taken from M. 
 
Hence, M starting with its initial configuration will eventually empty its stack and accept the input i.e. 
 
 
. 
 
Deterministic PDA: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Regular Languages and DPDA’s The DPDA’s accepts a class of languages that is in between the regular 
languages and CFL’s. 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Deterministic Pushdown Automata (DPDA) and Deterministic Context-free Languages (DCFLs) 
 
Pushdown automata that we have already defined and discussed are nondeterministic by default, that is , there may be two or 
more moves involving the same combinations of state, input symbol, and top of the stock, and again, for some state and 
top of the stock the machine may either read and input symbol or make an 
- transition (without consuming any input). 
 
In deterministic PDA , there is never a choice of move in any situation. This is handled by preventing the above mentioned two 
cases as described in the definition below. 
 
Defnition : Let 
be a PDA . Then M is deterministic if and only if both the following conditions are 
satisfied. 
 
1. 
has at most one element for any 
and 
(this condition prevents multiple choice f 
any combination of 
) 
2. 
If 
and 
for every 
 
 
(This condition prevents the possibility of a choice between a move with or without an input symbol). 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Empty Production Removal 
 
The productions of context-free grammars can be coerced into a variety of forms without 
affecting the expressive power of the grammars. If the empty string does not belong to a language, 
then there is a way to eliminate the productions of the form A→ λ from the grammar. 
 
If the empty string belongs to a language, then we can eliminate λ from all productions 
 
save for the single production S → λ. In this case we can also eliminate any occurrences of S 
from the right-hand side of productions. 
 
Procedure to find CFG with out empty Productions 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
Unit production removal 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Left Recursion Removal 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
NORMAL FORMS 
 
Two kinds of normal forms viz., Chomsky Normal Form and Greibach Normal Form (GNF) 
are considered here. 
 
Chomsky Normal Form (CNF) 
 
Any context-free language L without any λ-production is generated by a grammar is 
which productions are of the form A → BC or A→ a, where A, B ∈VN , and a ∈ V Τ. 
 
Procedure to find Equivalent Grammar in CNF 
 
(i) Eliminate the unit productions, and λ-productions if any, 
 
(ii) Eliminate the terminals on the right hand side of length two or more. 
 
(iii) Restrict the number of variables on the right hand side of productions to two. 
Proof: 
 
For Step (i): Apply the following theorem: “Every context free language can be generated by 
a grammar with no useless symbols and no unit productions”. 
 
At the end of this step the RHS of any production has a single terminal or two or more symbols. 
 
Let us assume the equivalent resulting grammar as G = (VN ,VT ,P ,S ). 
For Step (ii): Consider any production of the form 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Example 
 
Obtain a grammar in Chomsky Normal Form (CNF) equivalent to the grammar G 
with productions P given 
 
 
 
 
 
 
Solution 
www.indiansbrain.com
www.indiansbrain.com
Pumping Lemma for CFG 
A “Pumping Lemma” is a theorem used to show that, if certain strings belong to a 
 
language, then certain other strings must also belong to the language. Let us discuss a Pumping 
Lemma for CFL. We will show that , if L is a context-free language, then strings of L that are at 
least ‘m’ symbols long can be “pumped” to produce additional strings in L. The value of ‘m’ 
depends on the particular language. Let L be an infinite context-free language. Then there is some 
positive integer ‘m’ such that, if S is a string of L of Length at least ‘m’, then 
 
(i) S = uvwxy (for some u, v, w, x, y) 
 
(ii) | vwx|  m 
(iii) | vx| 1 
(iv) uv iwx i y∈L. 
 
for all non-negative values of i. 
It should be understood that 
 
(i) If S is sufficiently long string, then there are two substrings, v and x, somewhere in 
S. There is stuff (u) before v, stuff (w) between v and x, and stuff (y), after x. 
 
(ii) The stuff between v and x won’t be too long, because | vwx | can’t be larger than m. 
(iii) Substrings v and x won’t both be empty, though either one could be. 
 
(iv) If we duplicate substring v, some number (i) of times, and duplicate x the same 
number of times, the resultant string will also be in L. 
 
Definitions 
A variable is useful if it occurs in the derivation of some string. This requires that 
 
(a) the variable occurs in some sentential form (you can get to the variable if you start from S), and 
 
(b) a string of terminals can be derived from the sentential form (the variable is not a “dead end”). 
A variable is “recursive” if it can generate a string containing itself. For example, variable A is 
recursive if 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proof of Pumping Lemma 
 
(a) Suppose we have a CFL given by L. Then there is some context-free Grammar G that 
generates L. Suppose 
(i) L is infinite, hence there is no proper upper bound on the length of strings belonging to L. 
 
(ii) L does not contain l. 
(iii) G has no productions or l-productions. 
www.indiansbrain.com
There are only a finite number of variables in a grammar and the productions for each 
 
variable have finite lengths. The only way that a grammar can generate arbitrarily long strings is if 
one or more variables is both useful and recursive. Suppose no variable is recursive. Since the start 
symbol is non recursive, it must be defined only in terms of terminals and other variables. Then 
since those variables are non recursive, they have to be defined in terms of terminals and still other 
variables and so on. 
 
After a while we run out of “other variables” while the generated string is still finite. Therefore 
there is an upper bond on the length of the string which can be generated from the start 
symbol. This contradicts our statement that the language is finite. 
Hence, our assumption that no variable is recursive must be incorrect. 
 
(b) Let us consider a string X belonging to L. If X is sufficiently long, then the derivation of X 
must have involved recursive use of some variable A. Since A was used in the derivation, the 
derivation should have started as 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Usage of Pumping Lemma 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Hence our original assumption, that L is context free should be false. Hence the language L is not 
con text-free. 
 
Example 
 
Check whether the language given by L  {a mbmcn : m  n  2m} is a CFL or not. 
Solution 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Closure properties of CFL – Substitution 
www.indiansbrain.com
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Applications of substitution theorem 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Reversal 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Inverse Homomorphism: 
www.indiansbrain.com
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
UNIT-V 
Turing machine: 
 
Informal Definition: 
 
We consider here a basic model of TM which is deterministic and have one-tape. There are many variations, 
all are equally powerfull. 
 
The basic model of TM has a finite set of states, a semi-infinite tape that has a leftmost cell but is infinite to 
the right and a tape head that can move left and right over the tape, reading and writing symbols. 
 
For any input w with |w|=n, initially it is written on the n leftmost (continguous) tape cells. The infinitely many 
cells to the right of the input all contain a blank symbol, B whcih is a special tape symbol that is not an input 
symbol. The machine starts in its start state with its head scanning the leftmost symbol of the input w. De-
pending upon the symbol scanned by the tape head and the current state the machine makes a move which 
consists of the following: 
 
 
writes a new symbol on that tape cell,  

moves its head one cell either to the left or to the right and 
 
(possibly) enters a new state.
 
The action it takes in each step is determined by a transition functions. The machine continues computing (i.e. 
making moves) until 
 
 
it decides to "accept" its input by entering a special state called accept or final state or
 
halts without accepting i.e. rejecting the input when there is no move defined.
 
On some inputs the TM many keep on computing forever without ever accepting or rejecting the input, in 
which case it is said to "loop" on that input 
 
Formal Definition : 
 
Formally, a deterministic turing machine (DTM) is a 7-tuple 
, where 
 
 
Q is a finite nonempty set of states.
 
is a finite non-empty set of tape symbols, callled the tape alphabet of M.

 
is a finite non-empty set of input symbols, called the input alphabet of M.
 
is the transition function of M,
www.indiansbrain.com
 
is the initial or start state.
 
is the blank symbol

 
is the set of final state.
 
So, given the current state and tape symbol being read, the transition function describes the next state, symbol 
to be written on the tape, and the direction in which to move the tape head ( L and R denote left and right, 
respectively ). 
 
Transition function :
 
 
 
The heart of the TM is the transition function, 
because it tells us how the machine gets one step 
to the next.

 
when the machine is in a certain state q
Q and the head is currently scanning the tape symbol 
, and if 
, then the machine
 
1. replaces the symbol X by Y on the tape 
 
2. goes to state p, and 
3. the tape head moves one cell ( i.e. one tape symbol ) to the left ( or right ) if D is L ( or R ). 
 
The ID (instantaneous description) of a TM capture what is going out at any moment i.e. it contains all the 
information to exactly capture the "current state of the computations". 
 
It contains the following: 
 
 
The current state, q
 
The position of the tape head,

 
The constants of the tape up to the rightmost nonblank symbol or the symbol to the left of the head, 
whichever is rightmost.
 
Note that, although there is no limit on how far right the head may move and write nonblank symbols on the 
tape, at any finite 
 
time, the TM has visited only a finite prefix of the infinite tape. 
 
An ID (or configuration) of a TM M is denoted by 
where 
and 
 
 
is the tape contents to the left of the head
 
q is the current state.

 
is the tape contents at or to the right of the tape head
 
That is, the tape head is currently scanning the leftmost tape symbol of 
. ( Note that if 
, then the 
tape head is scanning a blank symbol) 
 
If 
is the start state and w is the input to a TM M then the starting or initial configuration of M is onviously 
denoted by 
 
www.indiansbrain.com
Moves of Turing Machines 
 
To indicate one move we use the symbol 
. Similarly, zero, one, or more moves will be represented by 
. A 
move of a TM 
 
M is defined as follows. 
 
Let 
be an ID of M where 
, 
and 
. 
 
Let there exists a transition 
of M. 
 
Then we write 
meaning that ID 
yields 
 
 
 
Alternatively 
, 
if 
is 
a 
transition 
of 
M, 
then 
we 
write 
which means that the ID 
yields 


 
In other words, when two IDs are related by the relation 
, we say that the first one yields the 
second ( or the second is the result of the first) by one move.
 
If IDj results from IDi by zero, one or more (finite) moves then we write 
( If the TM M is understand, 
then the subscript M can be dropped from 
or 
)
 
Special Boundary Cases 
 
 
Let 
be an ID and 
be an transition of M. Then 
. That is, the head is not 
allowed to fall off the left end of the tape.
 
Let 
be an ID and 
then figure (Note that 
is equivalent to 
)

 
Let 
be an ID and 
then figure

 
Let 
be an ID and 
then figure
 
The language accepted by a TM 
, denoted as L(M) is 
 
L(M) = { w | 
and figure for some p
F and 
} 
 
In other words the TM M accepts a string 
that cause M to enter a final or accepting state when started 
in its initial ID (i.e. 
). That is a TM M accepts the string 
if a sequence of IDs, 
 
exists such that 
 
 
is the initial or starting ID of M
 
; 

www.indiansbrain.com
 
The representation of IDk contains an accepting state.
 
The set of strings that M accepts is the language of M, denoted L(M), as defined 
above More about configuration and acceptance 
 
 
An ID 
of M is called an accepting (or final) ID if 


 
An ID 
is called a blocking (or halting) ID if 
is undefined i.e. the TM has no move at this 
point.
 
is called reactable from 
if 


 
is the initial (or starting) ID if 
is the input to the TM and 
is the initial (or start) state 
of M.
 
On any input string 
 
 
either 
 
 
M halts on w if there exists a blocking (configuration) ID, 
such that 

 
There are two cases to be considered 
 
 
M accepts w if I is an accepting ID. The set of all 
accepted by M is denoted as L(M) as
already defined 
 
 
M rejects w if 
is a blocking configuration. Denote by reject (M), the set of all 
rejected by M.
 
or 
 
 
M loops on w if it does not halt on w.
 
Let loop(M) be the set of all 
on which M loops for. 
 
It is quite clear that 
 
 
 
 
That is, we assume that a TM M halts 
 
 
When it enters an accepting 
or
 
When it enters a blocking 
i.e. when there is no next move.
 
However, on some input string, , 
, it is possible that the TM M loops for ever i.e. it never halts 
www.indiansbrain.com
 
The Halting Problem 
 
The input to a Turing machine is a string. Turing machines themselves can be written as strings. 
Since these strings can be used as input to other Turing machines. A “Universal Turing 
machine” is one whose input consists of a description M of some arbitrary Turing machine, and 
some input w to which machine M is to be applied, we write this combined input as M + w. 
This produces the same output that would be produced by M. This is written as 
 
Universal Turing Machine (M + w) = M (w). 
 
As a Turing machine can be represented as a string, it is fully possible to supply a Turing 
 
machine as input to itself, for example M (M). This is not even a particularly bizarre thing to do for 
example, suppose you have written a C pretty printer in C, then used the Pretty printer on itself. 
Another common usage is Bootstrapping—where some convenient languages used to write a 
minimal compiler for some new language L, then used this minimal compiler for L to write a new, 
improved compiler for language L. Each time a new feature is added to language L, you can 
recompile and use this new feature in the next version of the compiler. Turing machines sometimes 
halt, and sometimes they enter an infinite loop. 
 
A Turing machine might halt for one input string, but go into an infinite loop when given 
some other string. The halting problem asks: “It is possible to tell, in general, whether a given 
 
machine will halt for some given input?” If it is possible, then there is an effective procedure to look 
at a Turing machine and its input and determine whether the machine will halt with that input. If 
there is an effective procedure, then we can build a Turing machine to implement it. Suppose we 
have a Turing machine “WillHalt” which, given an input string M + w, will halt and accept the string 
if Turing machine M halts on input w and will halt and reject the string if Turing machine M does 
not halt on input w. When viewed as a Boolean function, “WillHalt (M, w)” halts and returns 
“TRUE” in the first case, and (halts and) returns “FALSE” in the second. 
 
Theorem 
Turing Machine “WillHalt (M, w)” does not exist. 
 
Proof: This theorem is proved by contradiction. Suppose we could build a machine “WillHalt”. 
Then we can certainly build a second machine, “LoopIfHalts”, that will go into an infinite loop 
if and only if “WillHalt” accepts its input: 
 Function LoopIfHalts (M, 
w): if WillHalt (M, w) then 
while true do { } 
else 
return false; 
 
We will also define a machine “LoopIfHaltOnItSelf” that, for any given input M, representing a 
Turing machine, will determine what will happen if M is applied to itself, and loops if M will halt 
in this case. 
 Function LoopIfHaltsOnItself (M): 
return LoopIfHalts (M, M): 
 
Finally, we ask what happens if we try: 
Func tion Impos sible: 
return LoopIfHaltsOnItself (LoopIfHaltsOnItself): 
 
This machine, when applied to itself, goes into an infinite loop if and only if it halts 
when applied to itself. This is impossible. Hence the theorem is proved. 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Implications of Halting Problem 
 
Programming 
The Theorem of “Halting Problem” does not say that we can never determine whether or not 
 
a given program halts on a given input. Most of the times, for practical reasons, we could 
eliminate infinite loops from programs. Sometimes a “meta-program” is used to check another 
program for potential infinite loops, and get this meta-program to work most of the time. 
 
The theorem says that we cannot ever write such a meta-program and have it work all of the 
time. This result is also used to demonstrate that certain other programs are also impossible. 
The basic outline is as follows: 
 
(i) If we could solve a problem X, we could solve the Halting problem 
(ii) We cannot solve the Halting Problem 
(iii) Therefore, we cannot solve problem X 
 
 
 
 
 
 
A Turing machine can be "programmed," in much the same manner as a computer is 
 
programmed. When one specifies the function which we usually call δ for a Tm, he is really 
writing a program for the Tm. 
 
1. Storage in finite Control 
 
The finite control can be used to hold a finite amount of information. To do so, the state is written 
as a pair of elements, one exercising control and the other storing a symbol. It should be 
 
emphasized that this arrangement is for conceptual purposes only. No modification in the definition 
of the Turing machine has been made. 
 
Example 
Consider the Turing machine 
Solution 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2. Multiple Tracks 
 
We can imagine that the tape of the Turing machine is divided into k tracks, for any finite k. This 
arrangement is shown in Fig., with k = 3. What is actually done is that the symbols on the tape 
are considered as k-tuples. One component for each track. 
 
Example 
The tape in Fig. can be imagined to be that of a Turing machine which takes a binary input 
 
greater than 2, written on the first track, and determines if it is a prime. The input is surrounded by 
¢ and $ on the first track. 
 
Thus, the allowable input symbols are [¢, B, B], [0, B, B ], [1, B, B ], and [$, B, B]. These 
 
symbols can be identified with ¢, 0, 1, and $, respectively, when viewed as input symbols. The blank 
www.indiansbrain.com
symbol can be represented by [B, B, B ] 
 
To test if its input is a prime, the Tm first writes the number two in binary on the second track 
 
and copies the first track onto the third track. Then, the second track is subtracted, as many times 
as possible, from the third track, effectively dividing the third track by the second and leaving the 
remainder. If the remainder is zero, the number on the first track is not a prime. If the remainder is 
nonzero, increase the number on the second track by one. 
 
If now the second track equals the first, the number on the first track is a prime, because it cannot 
be divided by any number between one and itself. If the second is less than the first, the whole 
operation is repeated for the new number on the second track. In Fig., the Tm is testing to determine 
if 47 is a prime. The Tm is dividing by 5; already 5 has been subtracted twice, so 37 appears on the 
third track. 
 
3. Subroutines 
www.indiansbrain.com
 
UNDECIDABILITY 
 
 
Design a Turing machine to add two given integers. 
 
Solution: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Some unsolvable Problems are as follows: 
(i) Does a given Turing machine M halts on all input? 
(ii) Does Turing machine M halt for any input? 
(iii) Is the language L(M) finite? 
 
(iv) Does L(M) contain a string of length k, for some given k? 
 
(v) Do two Turing machines M1 and M2 accept the same language? 
 
It is very obvious that if there is no algorithm that decides, for an arbitrary given Turing machine 
M and input string w, whether or not M accepts w. These problems for which no algorithms exist 
are called “UNDECIDABLE” or “UNSOLVABLE”. 
 
Code for Turing Machine: 
www.indiansbrain.com
www.indiansbrain.com
 
Diagonalization language: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This table represents language acceptable by Turing machine 
www.indiansbrain.com
Proof that Ld is not recursively enumerable: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Recursive Languages: 
www.indiansbrain.com
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Universal 
 
Language: 
www.indiansbrain.com
 
Undecidability of Universal Language: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Problem -Reduction : 
If P1 reduced to P2, 
 
Then P2 is at least as hard as P1. 
Theorem: If P1 reduces to P2 then, 
 If P1 is undecidable the so is P2.
 If P1 is Non-RE then so is P2.
www.indiansbrain.com
Post's Correspondence Problem (PCP) 
 
A post correspondence system consists of a finite set of ordered pairs 
where 
for some alphabet 
. 
 
Any sequence of numbers 
 
 
is called a solution to a Post Correspondence System. 
 
The Post's Correspondence Problem is the problem of determining whether 
a Post Correspondence system has a solutions. 
 
Example 1 : Consider the post correspondence system 
 
 The list 1,2,1,3 is a solution to it. 
 
Because 
 
 
 
 
 
 
 
i 
             xi                                 yi 
 
1 
 
2 
 
3 
 
 
(A post correspondence system is also denoted as an instance of the 
PCP) Example 2 : The following PCP instance has no solution 
i 
          xi                          yi 
 
1 
 
2 
 
This can be proved as follows. 
cannot be chosen at the start, since than the LHS and RHS would 
differ in the first symbol ( 
in LHS and 
in RHS). So, we must start with 
. The next pair must be 
so that the 3 rd symbol in the RHS becomes identical to that of the LHS, which is a . After this step, 
LHS and RHS are not matching. If 
is selected next, then would be mismatched in the 7 th symbol 
www.indiansbrain.com
( 
in LHS and 
in RHS). If 
is selected, instead, there will not be any choice to match the both side 
in the next step. 
 
Example3 : The list 1,3,2,3 is a solution to the following PCP instance. 
 
 
i 
   
x
i 
   
y
i 
 
 
 
   
 
  
  
 
 
1 
 
1 
 
101 
 
 
 
   
 
  
  
 
 
2 
 
10 
 
00 
 
 
 
   
 
  
  
 
 
3 
011 
11 
 
 
 
   
 
  
  
 
 
The following properties can easily be proved. 
 
Proposition The Post Correspondence System 
 
 has solutions if and only if 
 
 
 
 
 
 
Corollary : PCP over one-letter alphabet is decidable. 
 
Proposition Any PCP instance over an alphabet 
with 
is equivalent to a PCP instance over 
an alphabet 
with 
 
 
Proof : Let 
 
 
Consider 
We can now encode every 
as 
any PCP instance over 
will now 
have only two symbols, 0 and 1 and, hence, is equivalent to a PCP instance over 
 
 
Theorem : PCP is undecidable. That is, there is no algorithm that determines whether an arbitrary Post 
Correspondence System has a solution. 
 
Proof: The halting problem of turning machine can be reduced to PCP to show the undecidability of PCP. Since 
halting problem of TM is undecidable (already proved), This reduction shows that PCP is also undecidable. 
The proof is little bit lengthy and left as an exercise. 
 
Some undecidable problem in context-free languages 
 
We can use the undecidability of PCP to show that many problem concerning the context-free languages 
are undecidable. To prove this we reduce the PCP to each of these problem. The following discussion 
makes it clear how PCP can be used to serve this purpose. 
www.indiansbrain.com
Let 
be a Post Correspondence System over the alphabet 
. We 
construct two CFG's Gx and Gy from the ordered pairs x,y respectively as follows. 
 
 and 
 
 where 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and 
 
 
it is clear that the grammar 
generates the strings that can appear in the LHS of a sequence while solving the 
PCP followed by a sequence of numbers. The sequence of number at the end records the sequence of 
 
strings from the PCP instance (in reverse order) that generates the string. Similarly, 
generates the 
strings that can be obtained from the RHS of a sequence and the corresponding sequence of numbers (in 
reverse order). 
 
Now, if the Post Correspondence System has a solution, then there must be a sequence 
 
 
 
 
 
 
 
 
According to the construction of 
and 
 
 
 
 
 
 
 
 
 
 
 
In this case 
www.indiansbrain.com
 
 
Hence , 
and 
implying 
 
 
 
 
 
Conversely, let 
 
 
Hence, w must be in the form w1w2 where 
and w2 in a sequence 
(since, only that kind 
of strings can be generated by each of 
and 
). 
 
Now, the string 
is a solution to the Post Correspondence System. 
 
It is interesting to note that we have here reduced PCP to the language of pairs of CFG,s whose intersection is 
nonempty. The following result is a direct conclusion of the above. 
 
Theorem : Given any two CFG's G1 and G2 the question "Is 
" is undecidable. 
 
Proof: Assume for contradiction that there exists an algorithm A to decide this question. This would imply 
that PCP is decidable as shown below. 
 
For any Post Correspondence System, P construct grammars 
and 
by using the constructions 
elaborated already. We can now use the algorithm A to decide whether and 
Thus, PCP is decidable, a contradiction. So, such an algorithm does not exist. 
 
If 
and 
are CFG's constructed from any arbitrary Post Correspondence System, than it is not difficult to 
 
show that 
and 
are also context-free, even though the class of context-free languages are 
not closed under complementation. 
 
and their complements can be used in various ways to show that many other questions 
related to CFL's are undecidable. We prove here some of those. 
 
Theorem : Foe any two arbitrary CFG's 
the following questions are undecidable 
 
i. 
Is 
 
 
ii. 
Is 
 
www.indiansbrain.com
iii. Is 
 
Proof : 
i. 
If 
then, 
 
 
Hence, it suffice to show that the question “Is 
" is undecidable. 
 
Since, 
and 
are CFl's and CFL's are closed under union, 
is also context-
free. By DeMorgan's theorem, 
 
 
If there is an algorithm to decide whether 
we can use it to decide whether 
or not. But this problem has already been proved to be undecidable. 
 
 
Hence there is no such algorithm to decide or not. 
ii. 
 
Let P be any arbitrary Post correspondence system and 
and 
are CFg's constructed from the pairs of 
strings. 
 
must be a CFL and let G1generates L1. That is, 
 
 
 
 
 
by De Morgan's theorem, as shown already, any string, 
represents a solution to the 
PCP. Hence, 
contains all but those strings representing the solution to the PCP. 
 
Let 
for same CFG G2. 
 
It is now obvious that 
if and only if the PCP has no solutions, which is already proved to be 
undecidable. Hence, the question “Is 
?" is undecidable. 
 
iii. 
www.indiansbrain.com
Let 
be a CFG generating the language 
and G2 be a CFG generating 
where 
and 
are CFG.s constructed from same arbitrary instance of PCP. 
 
iff 
 
 
i.e. iff the PCP instance has no solutions as discussed in part (ii). 
 
Hence the proof. 
 
Theorem : It is undecidable whether an arbitrary CFG is ambiguous. 
 
Proof : Consider an arbitrary instance of PCP and construct the CFG's 
and 
from the ordered pairs 
of strings. 
 
We construct a new grammar G from 
and 
as follows. 
 
 where 
 
 
 
 
 
is same as that of 
and 
. 
 
 
 
 
 
This constructions gives a reduction of PCP to the -------- of whether a CFG is ambiguous, thus leading to 
the undecidability of the given problem. That is, we will now show that the PCP has a solution if and only if G 
is ambiguous. (where G is constructed from an arbitrary instance of PCP). 
 
Only if Assume that 
is a solution sequence to this instance of PCP. 
 
Consider the following two derivation in 
. 
www.indiansbrain.com
 
 
 
 
 
 
 
 
 
But , 
 
 
 
 
is a solution to the PCP. Hence the same string of terminals 
has two derivations. Both these 
derivations are, clearly, leftmost. Hence G is ambiguous. 
 
If It is important to note that any string of terminals cannot have more than one derivation in 
and 
 
Because, every terminal string which are derivable under these grammars ends with a sequence of integers 
This sequence uniquely determines which productions must be used at every step of the derivation. 
 
Hence, if a terminal string, 
, has two leftmost derivations, then one of them must begin with 
the step. 
 
then continues with derivations under 
 
 
In both derivations the resulting string must end with a sequence 
for same 
The reverse 
of this sequence must be a solution to the PCP, because the string that precede in one case is 
 
and 
in the other case. Since the string derived in both cases are identical, the 
 
sequence 
 
must be a solution to the PCP. 
 
Hence the proof 
www.indiansbrain.com
Class p-problem solvable in polynomial time: 
 
 
 
 
 
 
 
 
 
 
 
Non deterministic polynomial time: 
 
A nondeterministic TM that never makes more than p(n) moves in any sequence of choices for 
some polynomial p is said to be non polynomial time NTM. 
 NP is the set of languags that are accepted by polynomial time NTM’s

 Many problems are in NP but appear not to be in p.
 One of the great mathematical questions of our age: is there anything in NP that is not in p?
NP-complete problems: 

If We cannot resolve the “p=np question, we can at least demonstrate that certain problems in NP 
are the hardest , in the sense that if any one of them were in P , then P=NP. 

 These are called NP-complete.

 Intellectual leverage: Each NP-complete problem’s apparent difficulty reinforces the belief 
that they are all hard.
 
Methods for proving NP-Complete problems: 
 
 Polynomial time reduction (PTR): Take time that is some polynomial in the input size to 
convert instances of one problem to instances of another.

 If P1 PTR to P2 and P2 is in P1 the so is P1.
 Start by showing every problem in NP has a PTR to Satisfiability of Boolean formula.

 Then, more problems can be proven NP complete by showing that SAT PTRs to them 
directly or indirectly.
www.indiansbrain.com


http://engineeringbooks.net
THEORY OF COMPUTER SCIENCE
Automata, Languages and Computation
THIRD EDITION
K.l.P. MISHRA
Formerly Professor
Department of Electrical and Electronics Engineering
and Principal/ Regional Engineering College
Tiruchirapal/i
N. CHANDRASEKARAN
Professor
Department of Mathematics
St. Joseph/s College
Tiruchirapalli
Prentice'Hall of India [P[?lmGJD@ LsOWJov8d]
New Delhi - 110 '001
2008
http://engineeringbooks.net
Preface
Notations
Contents
ix
Xl
1.
PROPOSITIONS AND PREDICATES
1-35
1.1
Propositions (or Statements)
1
1.1.1
Connectives (Propositional Connectives
or Logical Connectives)
2
1.1.2
Well-formed Formulas
6
1.1.3
Truth Table for a Well-formed Formula
7
1.1.4 Equivalence of Well-formed Formulas
9
1.1.5
Logical Identities
9
1.2
Normal Forms of Well-formed Formulas
11
1.2.1
Construction to Obtain a Disjunctive Normal
Form of a Given Formula
II
1.2.2
Construction to Obtain the Principal
Disjunctive Normal Form of a Given Formula
12
1.3
Rules of Inference for Propositional Calculus
(Statement Calculus)
15
1.4
Predicate Calculus
19
1.4.1
Predicates
19
1.4.2
Well-formed Formulas of Predicate Calculus
21
1.5
Rules of Inference for Predicate Calculus
23
1.6
Supplementary Examples
26
Se(f-Test
31
Exercises
32
iii
http://engineeringbooks.net
iv
J;J
Contents
2.
MATHEMATICAL PRELIMINARIES
2.1
Sets, Relations and Functions
36
2.1.1
Sets and Subsets
36
2.1.2
Sets with One Binary Operation
37
2.1.3
Sets with Two Binary Operations
39
2.1.4
Relations
40
2.1.5
Closure of Relations
43
2.1.6
Functions
45
2.2
Graphs and Trees
47
2.2.1
Graphs
47
2.2.2
Trees
49
2.3
Strings and Their Properties
54
2.3.1
Operations on Strings
54
2.3.2
Terminal and Nonterrninal Symbols
56
2.4
Principle of Induction
57
2.4.1
Method of Proof by Induction
57
2.4.2
Modified Method of Induction
58
2.4.3
Simultaneous Induction
60
2.5
Proof by Contradiction
61
2.6
Supplementary Examples
62
Self-Test
66
Exercises
67
36-70
3.
THE THEORY OF AUTOMATA
71-106
3.1
Definition of an Automaton
7]
3.2
Description of a Finite Automaton
73
3.3
Transition Systems
74
3.4
Propeliies of Transition Functions
75
3.5
Acceptability of a String by a Finite Automaton
77
3.6
Nondeterministic Finite State Machines
78
3.7
The Equivalence of DFA and NDFA
80
3.8
Mealy and Moore Models
84
3.8.1
Finite Automata with Outputs
84
3.8.2
Procedure for Transforming a Mealy Machine
into a Moore Machine
85
3.8.3
Procedure for Transforming a Moore Machine
into a Mealy Machine
87
3.9
Minimization of Finite Automata
91
3.9.1
Construction of Minimum Automaton
92
3.10 Supplementary Examples
97
Self-Test
103
Exercises
]04
http://engineeringbooks.net
Contents
!O!l
v
4.
FORMAL LANGUAGES
4.1
Basic Definitions and Examples
107
4.1.1
Definition of a Grammar
109
4.1.2
Derivations and the Language Generated by a
Grammar
110
4.2
Chomsky Classification of Languages
120
4.3
Languages and Their Relation
123
4.4
Recursive and Recursively Enumerable Sets
124
4.5
Operations on Languages
126
4.6
Languages and Automata
128
4.7
Supplementary Examples
129
Self-Test
132
Exercises
134
107-135
5.
REGULAR SETS
A~TJ) REGULAR GRAMMARS
136-179
5.1
Regular Expressions
136
5.1.1
Identities for Regular Expressions
138
5.2
Finite Automata and Regular Expressions
140
5.2.1
Transition System Containing A-moves
140
5.2.2
NDFAs with A-moves and Regular Expressions
142
5.2.3
Conversion of Nondeterministic Systems to
Deterministic Systems
146
5.2.4
Algebraic Method Using Arden's Theorem
148
5.2.5
Construction of Finite Automata Equivalent
to a Regular Expression
153
5.2.6
Equivalence of Two Finite Automata
157
5.2.7
Equivalence of Two Regular Expressions
160
5.3
Pumping Lemma for Regular Sets
162
5.4
Application of Pumping Lemma
163
5.5
Closure Properties of Regular Sets
165
5.6
Regular Sets and Regular Grammars
167
5.6.1
Construction of a Regular Grammar Generating
~
T(M) for a Given DFA M
168
5.6.2
Construction of a Transition System M Accepting
L(G) for a Given Regular Grammar G
169
5.7
Supplementary Examples
170
Self-Test
175
Exercises
176
6.
CONTEXT·FREE LANGUAGES
6.1
Context-free Languages and Derivation Trees
180
6.1.1
Derivation Trees
181
6.2
Ambiguity in Context-free Grammars
188
18G-226
http://engineeringbooks.net
vi
~
Contents
189
190
196
199
201
6.3
Simplification of Context-free Grammars
6.3.1
Construction of Reduced Grammars
6.3.2
Elimination of Null Productions
6.3.3
Elimination of Unit Productions
6.4
Normal Forms for Context-free Grammars
6.4.1
Chomsky Normal Form
201
6.4.2
Greibach Normal Form
206
6.5
Pumping Lemma for Context-free Languages
6.6
Decision Algorithms for Context-free Languages
6.7
Supplementary Examples
218
Self-Test
223
Exercises
224
213
217
7.
PUSHDOWN AUTOMATA
227-266
7.1
Basic Definitions
227
7.2
Acceptance by pda
233
7.3
Pushdown Automata and Context-free Languages
240
7.4
Parsing and Pushdown Automata
251
7.4.1
Top-down Parsing
252
7.4.2
Top-down Parsing Using Deterministic pda's
256
7.4.3
Bottom-up Parsing
258
7.5
Supplementary Examples
260
Sell Test
264
Exercises
265
8.
LR(k) GRAMMARS
8.1
LR(k) Grammars
267
8.2
Properties of LR(k) Grammars
8.3
Closure Properties of Languages
8.4
Supplementary Examples
272
Self-Test
273
Erercises
274
270
272
267-276
9.
TURING MACHINES AND LINEAR BOUNDED
AUTOMATA
277-308
9.1
Turing Machine Model
278
9.2
Representation of Turing Machines
279
9.2.1
Representation by Instantaneous Descriptions
279
9.2.2
Representation by Transition Table
280
9.2.3
Representation by Transition Diagram
281
9.3
Language Acceptability by Turing Machines
283
9.4
Design of Turing Machines
284
9.5
Description of Turing Machines
289
http://engineeringbooks.net
Contents
~
vii
9.6
Techniques for TM Construction
289
9.6.1
Turing Machine with Stationary Head
289
9.6.2
Storage in the State
290
9.6.3
Multiple Track Turing Machine
290
9.6.4
Subroutines
290
9.7
Variants of Turing Machines
292
9.7.1
Multitape Turing Machines
292
9.7.2
Nondeterministic Turing Machines
295
9.8
The Model of Linear Bounded Automaton
297
9.8.1
Relation Between LBA and Context-sensitive
Languages
299
9.9
Turing Machines and Type 0 Grammars
299
9.9.1
Construction of a Grammar Corresponding to TM
299
9.10 Linear Bounded Automata and Languages
301
9.11 Supplementary Examples
303
Self-Test
307
Exercises
308
10. DECIDABILITY AJ\i'D RECURSIVELY El\TU1\fERABLE
LANGUAGES
309-321
10.1
The Definition of an Algorithm
309
10.2
Decidability
310
10.3
Decidable Languages
311
10.4
Undecidable Languages
313
10.5
Halting Problem of Turing Machine
314
10.6
The Post Correspondence Problem
315
10.7
Supplementary Examples
317
Self-Test
319
Exercises
319
322-345
332
325
327
333
11. COMPUTABILITY
11.1
Introduction and Basic Concepts
322
11.2
Primitive Recursive Functions
323
11.2.1
Initial Functions
323
11.2.2
Primitive Recursive Functions Over N
11.2.3
Primitive Recursive Functions Over {a. b}
11.3
Recursive Functions
329
11.4
Partial Recursive Functions and Turing Machines
11.4.1
Computability
332
11.4.2
A Turing Model for Computation
11.4.3
Turing-computable Functions
333
11.4.4
Construction of the Turing Machine That
Can Compute the Zero Function Z
334
11.4.5
Construction of the TUling Machine for Computing-
The Successor Function
335
http://engineeringbooks.net
viii
J;;;l
Contents
11.4.6
Construction of the Turing Machine for Computing
the Projection Vi"
336
11.4.7
Construction of the Turing Machine That Can
Perform Composition
338
11.4.8
Construction of the Turing Machine That Can
Perform Recursion
339
11.4.9
Construction of the Turing Machine That Can Perform
Minimization
340
11.5
Supplementary Examples
340
Self-Test
342
Exercises
343
12. COMPLEXITY
12.1
Growth Rate of Functions
346
12.2
The Classes P and NP
349
12.3
Polynomial Time Reduction and NP-completeness
12.4
Importance of NP-complete Problems
352
12.5
SAT is NP-complete
353
12.5.1
Boolean Expressions
353
12.5.2
Coding a Boolean Expression
353
12.5.3
Cook's Theorem
354
12.6
Other NP-complete Problems
359
12.7
Use of NP-completeness
360
12.8
Quantum Computation
360
12.8.1
Quantum Computers
361
12.8.2
Church-Turing Thesis
362
12.8.3
Power of Quantum Computation
363
12.8.4
Conclusion
364
12.9
Supplementary Examples
365
Self-Test
369
Exercises
370
Answers to Self-Tests
Solutions (or Hints) to Chapter-end Exercises
Further Reading
Index
346-371
351
373-374
375-415
417-418
419-422
http://engineeringbooks.net
Preface
The enlarged third edition of Thea/}' of Computer Science is the result of the
enthusiastic reception given to earlier editions of this book and the feedback
received from the students and teachers who used the second edition for
several years,
The new edition deals with all aspects of theoretical computer science,
namely automata, formal languages, computability and complexity, Very
few books combine all these theories and give/adequate examples. This book
provides numerous examples that illustrate the basic concepts. It is profusely
illustrated with diagrams. While dealing with theorems and algorithms, the
emphasis is on constructions. Each construction is immediately followed by an
example and only then the formal proof is given so that the student can master
the technique involved in the construction before taking up the formal proof.
The key feature of the book that sets it apart from other books is the
provision of detailed solutions (at the end of the book) to chapter-end
exercises.
The chapter on Propositions and Predicates (Chapter 10 of the second
edition) is now the first chapter in the new edition. The changes in other
chapters have been made without affecting the structure of the second edition.
The chapter on Turing machines (Chapter 7 of the second edition) has
undergone major changes.
A novel feature of the third edition is the addition of objective type
questions in each chapter under the heading Self-Test. This provides an
opportunity to the student to test whether he has fully grasped the fundamental
concepts. Besides, a total number of 83 additional solved examples have been
added as Supplementary Examples which enhance the variety of problems
dealt with in the book.
ix
http://engineeringbooks.net
x
);!
Preface
The sections on pigeonhole principle and the principle of induction (both
in Chapter 2) have been expanded. In Chapter 5, a rigorous proof of Kleene's
theorem has been included. The chapter on LR(k) grammars remains the same
Chapter 8 as in the second edition.
Chapter 9 focuses on the treatment of Turing machines (TMs). A new
section on high-level description of TM has been added and this is used in later
examples and proofs. Some techniques for the construction of TMs have been
added in Section 9.6. The multitape Turing machine and the nondeterministic
Turing machine are discussed in Section 9.7.
A new chapter (Chapter 10) on decidability and recursively enumerable
languages is included in this third edition. In the previous edition only a
sketchy introduction to these concepts was given. Some examples of
recursively enumerable languages are given in Section 10.3 and undecidable
languages are discussed in Section lOA. The halting problem of TM is
discussed in Section 10.5. Chapter 11 on computability is Chapter 9 of the
previous edition without changes.
Chapter 12 is a new chapter on complexity theory and NP-complete
problems. Cook's theorem is proved in detail. A section on Quantum
Computation is added as the last section in this chapter. Although this topic
does not fall under the purview of theoretical computer science, this section
is added with a view to indicating how the success of Quantum Computers will
lead to dramatic changes in complexity theory in the future.
The book fulfils the curriculum needs of undergraduate and postgraduate
students of computer science and engineering as well as those of MCA courses.
Though designed for a one-year course, the book can be used as a one-
semester text by a judicious choice of the topics presented.
Special thanks go to all the teachers and students who patronized this book
over the years and offered helpful suggestions that have led to this new
edition. In particular, the critical comments of Prof. M. Umaparvathi,
Professor of Mathematics, Seethalakshmi College, Tiruchirapalli are gratefully
acknowledged.
Finally. the receipt of suggestions, comments and error reports for further
improvement of the book would be welcomed and duly acknowledged.
K.L.P. Mishra
N. Chandrasekran
http://engineeringbooks.net
Symbol
T
F
v
T
F
Ar;;;B
o
AuB
AuE
AxB
Notations
Meaning
Truth value
False value
The logical connective NOT
The logical connective AND
The logical connective OR
The logical connective IF ... THEN
The logical connective If and Only If
Any tautology
Any contradiction
For every
There exists
Equivalence of predicate fonnulas
The element a belongs to the set A.
The set A is a subset of set B
The null set
The union of the sets A and B
The intersection of the sets A and B
The complement of B in A
The complement of A
The power set of A.
The cartesian product of A and B
xi
Section in \'vhich the
srmbol appears first
and is explained
1.1
1.1
1.1
1.1
1.1
1.1
1.1
1.1
1.1
1.4
1.4
1.4
2.1.1
21.1
2.1.1
2.1.1
2.].1
2.1.1
2.1.1
2.1.1
2.1.1
http://engineeringbooks.net
xii
~
Notations
Symbol
Meaning
Section in which the
symbol appears first
and is explained
n
UA;
i:::::!
*, 0
xRy
xR'y
i =j
modulo n
Cn
R+
R*
R] 0 Rz
f: X -7 Y
f(x)
rxl
L*
Ixl
(Q, L, 0, qo, F)
q:
$
(Q, L, 0. Qo, F)
(Q, L, fl, 0, )" qo)
Irk
(~v, L, P, S)
a=?f3
G
*
a=?f3
G
a~f3
G
L(G)
,io
The union of the sets AI> Az, ..., An
Binary operations
x is related to y under the relation
x is not related to y under the relation R
i is congruent to j modulo n
The equivalence class containing a
The transitive closure of R
The reflexive-transitive closure of R
The composite of the relations R1 and Rz
Map/function from a set X to a set Y
The image of x under f
The smallest integer;::; x
The set of all strings over the alphabet set L
The empty string
The set of all nonempty strings over L
The lertgthof the string x
A finite automaton
Left endmarker in input tape
Right endmarker in input tape
A transition system
A MealyIMoore machine
Partition corresponding to equivalence of states
Partition corresponding to k-equivalence of states
A grammar
a directly derives f3 in grammar G
a derives f3 in grammar G
a derives f3 in n steps in grammar G
The language generated by G
The family of type 0 languages
The family of context-sensitive languages
2.1.2
2.1.2, 2.1.3
2.1.4
2.1.4
2.1.4
2.1.4
2.1.5
2.1.5
2.1.5
2.1.6
2.1.6
2.2.2
2.3
2.3
2.3
2.3
3.2
3.2
3.2
3.3
3.8
3.9
3.9
4.1.1
4.1.2
4.1.2
4.1.2
4.1.2
4.3
4.3
The family of context-free languages
4.3
The family of regular languages
4.3
The union of regular expressions R] and Rz
5.1
The concatenationof regular expressions R] and Rz 5.1
The iteration (closure) of R
5.1
http://engineeringbooks.net
Symbol
Meaning
Notations
l;l,
xiii
Section in which the
symbol appears first
and is explained
a
IV"I
(Q,
~, r, 8,
qQ, Zo, F)
ID
~
A
(Q, ~, r, 8, qQ, b, F)
~
nn)
ADFA
AU-G
ACSG
An-!
HALTrM
Z(x)
Sex)
nn
L'j
nil (x)
cons a(x)
cons b(x)
P(x)
id
fJ
X4
O(g(n»
O(nk)
P and NP
1111 >
PSPACE
EXP
l'tl:'P
BQP
l'i'PI
The regular expression corresponding to {a}
The number of elements in Vv
A pushdown automaton
An instantaneous description of a pda
A move relation in a pda A
A Turing machine
The move relation in a TM
Maximum of the running time of M
Sets
Sets
Sets
Sets
Sets
The image of x under zero function
The image of x under successor function
The projection function
The image of x under nil function
The concatenation of a and x
---~----/
The concatenation of b and x
The image of x under predecessor function
The proper subtraction function
The absolute value of x
Identity function
The minimization function
The characteristic function of the set A
The order of g(n)
The order of r/
Classes
quantum bit (qubit)
Classes
Classes
Classes
Classes
Classes
5.1
6.3
7.1
7.1
7.1
9.1
9.2.2
9.7.1
10.3
10.3
10.3
lOA
10.5
11.2
11.2
11.2
11.2
11.2
11.2
11.2
Example 11.8
11.2
11.2
11.3
Exercise 11.8
12.1
12.1
12.2
12.8.1
12.8.3
12.8.3
12.8.3
12.8.3
12.8.3
http://engineeringbooks.net
Propositions and
Predicates
Mathematical logic is the foundation on which the proofs and arguments rest.
Propositions are statements used in mathematical logic, which are either true
or false but not both and we can definitely say whether a proposition is true
or false.
In this chapter we introduce propositions and logical connectives. Normal
forms for well-formed formulas are given. Predicates are introduced. Finally,
\ve discuss the rules of inference for propositional calculus and predicate calculus.
1.1
PROPOSITIONS (OR STATEMENTS)
A proposition (or a statement) is classified as a declarative sentence to which
only one of the truth values. i.e. true or false. can be assigned. When a
proposition is true, we say that its truth value is T. When it is false, we say
that its truth value is F.
Consider. for example. the following sentences in English:
1. New Delhi is the capital of India.
'1
The square of 4 is 16.
3. The square of 5 is 27.
4. Every college will have a computer by 2010 A.D.
S. Mathematical logic is a difficult subject.
6. Chennai is a beautiful city.
7. Bring me coffee.
8. No. thank you.
9. This statement is false.
The sentences 1-3 are propositions. The sentences 1 and 2 have the truth value
T. The sentence 3 has the truth value F. Although we cannot know the truth
value of 4 at present. we definitely know that it is true or false, but not both.
1
http://engineeringbooks.net
2
~
Theory ofComputer Science
So the sentence 4 is a proposition. For the same reason, the sentences 5 and
6 are propositions. To sentences 7 and 8, we cannot assign truth values as they
are not declarative sentences. The sentence 9 looks like a proposition.
However, if we assign the truth value T to sentence 9, then the sentence asserts
that it is false. If we assign the truth value F to sentence 9, then the sentence
asserts that it is true. Thus the sentence 9 has either both the truth values (or
none of the two truth values), Therefore, the sentence 9 is not a proposition,
We use capital letters to denote propositions,
1.1.1
CONNECTIVES (PROPOSITIONAL CONNECTIVES
OR LOGICAL CONNECTIVES)
Just as we form new sentences from the given sentences using words like
'and', 'but', 'if', we can get new propositions from the given propositions
using 'connectives'. But a new sentence obtained from the given propositions
using connectives will be a proposition only when the new sentence has a truth
value either T or F (but not both). The truth value of the new sentence
depends on the (logical) connectives used and the truth value of the given
propositions.
We
now
define
the
following
connectives.
There
are
five
basic
connectives.
(i) Negation (NOT)
(ii) Conjunction (AND)
(iii) Disjunction (OR)
(iv) Implication (IF
THEN
,:~/
(v) If and Only If.
Negation (NOT)
If P is a proposition then the negation P or NOT P (read as 'not PO) is a
proposition (denoted by -, P) whose truth value is T if P has the truth value
F, and whose truth value is F if P has the truth value T. Usually, the truth
values of a proposition defined using a connective are listed in a table called
the truth table for that connective (Table 1.1),
TABLE 1.1
Truth Table for Negation
p
T
F
,p
F
T
Conjunction (AND)
If P and Q are two propositions, then the conjunction of P and Q (read as 'P
and Q') is a proposition (denoted by P 1\ Q) whose truth values are as given
in Table 1.2.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
!!!!
3
TABLE 1.2
Truth Table for Conjunction
P
T
T
F
F
Q
T
F
T
F
PI\Q
T
F
F
F
Disjunction (OR)
If P and Q are two propositions, then the disjunction of P and Q (read as 'P
or Q ') is a proposition (denoted by P v Q) whose truth values are as given
in Table 1.3.
TABLE 1.3
Truth Table for Disjunction
P
T
T
F
F
Q
T
F
T
F
PvQ
T
T
T
F
It should be noted that P v Q is true if P is true or Q is true or both are
true. This OR is known as inclusive OR, i.e. either P is true or Q is true or
both are true. Here we have defined OR in the inclusive sense. We will define
another connective called exclusive OR (either P is true or Q is true, but not
both, i.e. where OR is used in the exclusive sense) in the Exercises at the end
of this chapter.
EXAMPLE 1.1
If P represents 'This book is good' and Q represents 'This book is cheap',
write the following sentences in symbolic form:
(a) This book is good and cheap.
(b) This book is not good but cheap.
(c) This book is costly but good.
(d) This book is neither good nor cheap.
(e) This book is either good or cheap.
Solution
(a) P /\ Q
(b) -,P /\ Q
(c) -, Q /\ P
(d) -, P /\ -,Q
(e) P v Q
Note:
The truth tables for P /\ Q and Q /\ P coincide. So P /\ Q and
Q
1\ P are equivalent (for the ddinition,
see Section
1.1.4). But in
natural languages this need not happen. For example. the two sentences,
http://engineeringbooks.net
4
g,
Theory ofComputer Science
namely 'I went to the railway station and boarded the train', and 'I boarded
the train and went to the railway station', have different meanings. Obviously,
we cannot write the second sentence in place of the first sentence.
Implication (IF ... THEN ...)
If P and Q are two propositions, then 'IF P THEN Q' is a propoSltlOn
(denoted by P => Q) whose truth values are given Table 1.4. We also read
P => Q as 'P implies Q'.
TABLE 1.4
Truth Table for Implication
P
T
T
F
F
Q
T
F
T
F
P=}Q
T
F
T
T
We can note that P => Q assumes the truth value F only if P has the truth
value T and Q has the truth value F. In all the other cases, P => Q assumes
the truth value T. In the case of natural languages, we are concerned about the
truth values of the sentence 'IF P THEN Q' only when P is true. When P
is false, we are not concerned about the truth value of 'IF P THEN Q'. But
in the case of mathematical logic, we have to definitely specify the truth value
of P => Q in all cases. So the truth value of P => Q is defined as T when
P has the truth value F (irrespective of the truth value of Q).
EXAMPLE 1.2
Find the truth values of the following propositions:
(a) If 2 is not an integer, then 1/2 is an integer.
(b) If 2 is an integer, then 1/2 is an integer.
Solution
Let P and Q be '2 is an integer', '1/2 is an integer', respectively. Then the
proposition (a) is true (as P is false and Q is false) and the proposition (b)
is false (as P is true and Q is false).
The above example illustrates the following: 'We can prove anything if
we start with a false assumption.' We use P => Q whenever we want to
'translate' anyone of the following: 'P only if Q', 'P is a sufficient condition
for Q', 'Q is a necessary condition for p', 'Q follows from P', 'Q whenever
P', .Q provided P'.
:f and Only If
If P and Q are two statements, then 'P if and only if Q' is a statement
(denoted by P ¢:::} Q) whose truth value is T when the truth values of P and
Q are the same and whose truth value is F when the statements differ. The
truth values of P
¢:::} Q are given in Table 1.5.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
5
TABLE 1.5
Truth Table for If and Only If
P
T
T
F
F
Q
P¢=;Q
T
T
F
F
T
F
F
T
Table 1.6 summarizes the representation and meaning of the five logical
connectives discussed above.
TABLE 1.6
Five Basic Logical Connectives
Connective
Negation -,
Conjunction /\
Disjunction v
Implication ~
If and only if
¢=;
Resulting proposition
-,P
P"
Q
PvQ
P~Q
P¢=;Q
Read as
Not P
P and Q
P or Q
(or both)
If P then Q
(P implies Q)
P if and only if Q
EXAMPLE 1.3
Translate the following sentences into propositional forms:
(a) If it is not raining and I have the time. then I will go to a movie.
(b) It is raining and I will not go to a movie..
(c) It is not raining.
./
(d) I will not go to a movie.
(e) I will go to a movie only if it is not raining.
Solution
Let P be the proposition 'It is raining'.
Let Q be the proposition 'I have the time'.
Let R be the proposition '1 will go to a movie'.
Then
(a) (-, P 1\ Q)
=:} R
(b) P
1\
---, R
(c)
---, P
(d)
---, R
(e) R
=:}
---, P
EXAMPLE 1.4
If P, Q, R are the propositions as given in Example 1.3, write the sentences
in English corresponding to the following propositional forms:
http://engineeringbooks.net
6
Q
Theory ofComputer Science
(a) (-, P /\ Q)
¢::} R
(b) (Q ::::} R) /\ (R ::::} Q)
(c) -, (Q v R)
(d) R ::::} -, P /\ Q
Solution
(a) I will go to a movie if and only if it is not raining and I have the
time.
(b) I will go to a movie if and only if I have the time.
(c) It is not the case that I have the time or I will go to a movie.
(d) I will go to a movie, only if it is not raining or I have the time.
1.1 .2
WELL-FORMED FORMULAS
Consider the propositions P /\ Q and Q /\ P. The truth tables of these two
propositions are identical irrespective of any proposition in place of P and any
proposition in place of Q. SO we can develop the concept of a propositional
variable
(corresponding
to
propositions)
and
well-formed
formulas
(corresponding to propositions involving connectives).
Definition 1.1
A propositional variable is a symbol representing any
proposition. We note that usually a real variable is represented by the symbol
x. This means that x is not a real number but can take a real value. Similarly,
a propositional variable is not a proposition but can be replaced by a
proposition.
Usually a mathematical object can be defined in terms of the property/
condition satisfied by the mathematical object. Another way of defining a
mathematical object is by recursion. Initially some objects are declared to
follow the definition. The process by which more objects can be constructed
is specified. This way of defining a mathematical object is called a recursive
definition. This corresponds to a function calling itself in a programming
language.
The factorial n! can be defined as n(n -
1) ... 2.1. The recursive
definition of n! is as follows:
O! = 1,
n! = n(n -
I)!
Definition 1.2
A well-formed formula (wff) is defined recursively as
follows:
(i) If P is a propositional variable, then it is a wff.
(ii) If ex is a wff, then -, ex is a wff.
(iii) If ex and f3 are well-formed formulas, then (ex v /3), (ex /\ /3), (ex::::} /3),
and (ex ¢::} /3) are well-formed formulas.
(iv) A string of symbols is a wff if and only if it is obtained by a finite
number of applications of (i)-(iii).
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
7
Notes: (1) A wff is not a proposition, but if we substitute a proposition in
place of a propositional variable, we get a proposition. For example:
(i) -, (P v Q) 1\ (-, Q 1\ R) ~ Q is a wff.
(ii) (-, P 1\ Q)
¢:::> Q is a wff.
(2) We can drop parentheses when there is no ambiguity. For example, in
propositions we can remove the outermost parentheses. We can also specify the
hierarchy of connectives and avoid parentheses.
For the sake of convenience, we can refer to a wff as a formula.
1.1.3
TRUTH TABLE FOR A WELL-FORMED FORMULA
If we replace the propositional variables in a formula ex by propositions, we
get a proposition involving connectives. The table giving the truth values of
such a proposition obtained by replacing the propositional variables by
arbitrary propositions is called the truth table of ex.
If ex involves n propositional constants, then we have 2" possible
combinations of truth values of propositions replacing the variables.
EXAMPLE 1.5
Obtain the truth table for ex = (P v Q)
1\ (P ~ Q)
1\ (Q ~ P).
Solution
The truth values of the given wff are shown i~; 1.7.
TABLE 1.7
Truth Table of Example 1.5
P
Q
PvQ
P=:oQ
(P v Q) /\ (P
=:0 Q)
(Q
=:0 P)
ex
T
T
T
T
T
T
T
T
F
T
F
F
T
F
F
T
T
T
T
F
F
F
F
F
T
F
T
F
EXAMPLE 1.6
Construct the truth table for ex = (P v Q) ~ ((P v R) ~ (R v Q».
Solution
The truth values of the given formula are shown in Table 1.8.
http://engineeringbooks.net
8
~
Theory ofComputer Science
TABLE 1.8
Truth Table of Example 1.6
P
Q
R
PvR
RvQ
(P v R) => (R v Q)
(P v Q)
a
T
T
T
T
T
T
T
T
T
T
F
T
T
T
T
T
T
F
T
T
T
T
T
T
T
F
F
T
F
F
T
F
F
T
T
T
T
T
T
T
F
T
F
F
T
T
T
T
F
F
T
T
T
T
F
T
F
F
F
F
F
T
F
T
Some formulas have the truth value T for all possible assignments of truth
values to the propositional variables. For example. P v --, P has the truth value
T irrespective of the truth value of P. Such formulas are called tautologies.
Definition 1.3
A tautology or a universally true formula is a well-fonned
formula whose truth value is T for all possible assignments of truth values to
the propositional variables.
For example. P v --, P, (P ;\ Q) ==> P. and ((P ==> Q) ;\ (Q ==> R» ==>
(P ==> R) are tautologies.
Note: When it is not clear whether a given formula is a tautology. we can
construct the truth table and verify that the truth value is T for all combinations
of truth values of the propositional variables appearing in the given formula.
EXAMPLE 1.7
Show that ex = (P ==> (Q ==> R)
==> ((P ==> Q) ==>
(P--~R)
is a tautology.
Solution
We give the truth values of ex in Table 1.9.
TABLE 1.9
Truth Table of Example 1.7
P
Q
R
Q=>R
P => (Q => R)
P=>Q
P=>R
(P => Q) => (P => R)
a
T
T
T
T
T
T
T
T
T
T
T
F
F
F
T
F
F
T
T
F
T
T
T
F
T
T
T
T
F
F
T
T
F
F
T
T
F
T
T
T
T
T
T
T
T
F
T
F
F
T
T
T
T
T
F
F
T
T
T
T
T
T
T
F
F
F
T
T
T
T
T
T
Defmition 1.4
A contradiction (or absurdity) is a wff whose truth value is
F for all possible assignments of truth values to the propositional variables.
For example.
and
(P ;\ Q) 1\ --, Q
are contradictions.
Note: ex is a contradiction if and only jf --, ex is a tautology.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
9
1.1.4
EQUIVALENCE OF WELL-FORMED FORMULAS
Definition 1.5
Two wffs 0: and {3 in propositional variables P b P2, ' , ., P"
are equivalent (or logically equivalent) if the fOTI11ula 0: q
{3 is a tautology.
When 0: and {3 are equivalent, we write 0: == {3.
Notes: (1) The wffs 0: and {3 are equivalent if the truth tables for 0: and {3 are
the same. For example.
and
(2) It is important to note the difference between
0: q
{3 and
0: == {3.
0: q
{3 is a fOTI11ula. whereas 0: == {3 is not a fOTI11ula but it denotes the relation
between
[f. and {3.
EXAMPLE 1.8
Show that (P ==> (Q v R)
== ((P ==> Q) v (P ==> R).
Solution
Let 0: = (P ==> (Q v R») and {3 = ((P ==> Q) v (P ==> R). We construct the
truth values of [f. and {3 for all assignments of truth values to the variables P,
Q and R. The truth values of 0: and {3 are given in Table 1.10.
TABLE 1.10
Truth Table of Example 1.8
P
Q
R
QvR
P => (Q v
R)
P=>Q
P=>R
(P=> Q) v
(P => R)
~
T
T
T
T
T
T
T
T
T
T
F
T
T
T
F
T
T
F
T
T
T
F
T
T
T
F
F
F
F
F
F
F
F
T
T
T
T
T
T
T
F
T
F
T
T
T
T
T
F
F
T
T
T
T
T
T
F
F
F
F
T
T
T
T
I
As the columns corresponding to
[f. and {3 coincide.
[f. == {3.
As the truth value of a tautology is T, irrespective of the truth values of
the propositional variables. we denote any tautology by T. Similarly, we
denote any contradiction by F.
1.1.5
LOGICAL IDENTITIES
Some equivalences are useful for deducing other equivalences. We call them
Identities and give a list of such identities in Table 1.11.
The identities 11-1 12 can be used to simplify fOTI11ulas. If a fOTI11ula {3 is
pmi of another fOTI11ula
[f., and {3 is equivalent to {3'. then we can replace {3
by {3' in 0: and the resulting wff is equivalent to a.
http://engineeringbooks.net
10
!!!!
Theory ofComputer Science
TABLE 1.11
Logical Identities
11
Idempotent laws:
P v P '" P,
P ;\ P '" P
12
Commutative laws:
P v Q '" Q v
P,
p;\ Q '" Q
1\ P
13
Associative
laws:
P v (Q v R) '" (P v Q) v R,
14
Distributive laws:
P
1\ (Q
1\ R) '" (P ;\ Q)
1\ R
P 1\ (P
V
Q) '" P
by using the distributive law (i.e. 14)
by using Is
by using 19
by using 112
by using the commutative law (i.e. 12)
by using the distributive law (i.e. 14)
by using the DeMorgan's law (i.e. 16)
by using the commutative law (i.e. 12)
,by using 1]2
P v (Q
1\ R) '" (P v Q)
1\ (P v R),
P ;\ (Q v R) '" (P ;\ Q) v (P
1\ R)
----_._----_._~-~-_
.._--------_ -
-_._,
,.__._._
_ - ~ ~ - - -
Is
Absorption
laws:
P v (P 1\ Q) ",p.
Is
DeMorgan's laws:
---, (P v
Q) '" ---, P
1\
---, Q,
---, (P
1\ Q) '" ---, P v
---, Q
17
Double negation:
P '" ---, (-, P)
18
P V
---, P '" T,
P
1\
---, P '" F
19
P v T '" T,
P 1\ T '" P,
P v F '" P,
P 1\ F '" F
~~I__~ Q)
1\ (P =}
---, Q) =-
~____
~
~~
~~
~~_._...
..
_
111
Contrapositive:
P=}Q"'---,Q=}---,P
112
P =} Q '" (-, P v
Q)
EXAMPLE 1.9
Show that (P 1\ Q)
V (P 1\ --, Q) == P.
Solution
L.H.S. = (P 1\ Q)
V (P 1\ --, Q)
== P 1\ (Q
V
--, Q)
==PI\T
== P
= R.H.S.
EXAMPLE 1.10
Show that (P ~ Q) 1\ (R ~ Q) == (P v R) ~ Q
Solution
L.H.S. = (P ~ Q)
1\ (R ~ Q)
== (--, P v Q) 1\ (--, R v Q)
== (Q v --, P)
1\ (Q
V
--, R)
== Q v
(--, P
1\ --, R)
== Q v (--, (P v R))
== (--, (P v R)) v Q
== (P v R) ~ Q
= R.H.S.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
!Oil
11
1.2
NORMAL FORMS OF WELL-FORMED FORMULAS
We have seen various well-fonned fonnulas in tenns of two propositional
variables, say, P and Q. We also know that two such fonnulas are equivalent
if and only if they have the same truth table. The number of distinct truth
tables for fonnulas in P and Q is 24. (As the possible combinations of truth
values of P and Q are IT, TF, FT, FF, the truth table of any fonnula in P
and Q has four rows. So the number of distinct truth tables is 24.) Thus there
are only 16 distinct (nonequivalent) fonnulas, and any fonnula in P and Q is
equivalent to one of these 16 fonnulas.
In this section we give a method of reducing a given fonnula to an
equivalent fonn called the 'nonnal fonn'. We also use 'sum' for disjunction,
'product' for conjunction, and 'literal' either for P or for -, P, where P is any
propositional variable.
DefInition 1.6
An elementary product is a product of literals. An elementary
sum is a sum of literals. For example, P 1\ -, Q, -, P 1\ -, Q, P 1\ Q, -, P 1\ Q
are elementary products. And P v -, Q, P v -, R are elementary sums.
DefInition 1.7
A fonnula is in disjunctive nonnal fonn if it is a sum of
elementary products. For example, P v (Q 1\ R) and P v (-, Q 1\ R) are in
disjunctive nonnal fonn. P
1\ (Q v R) is not in disjunctive nonnal fonn.
1.2.1
CONSTRUCTION TO OBTAIN A ~CTIVE NORMAL
FORM OF A GIVEN FORMULA
~
Step 1
Eliminate ~ and
¢:::} using logical identities. (We can use I 1e, l.e.
P ~ Q == (-, P v Q).)
Step 2
Use DeMorgan's laws (/6) to eliminate -, before sums or products.
The resulting fonnula has -, only before the propositional variables, i.e. it
involves sum, product and literals.
Step 3
Apply distributive laws (/4) repeatedly to eliminate the product of
sums. The resulting fonnula will be a sum of products of literals, i.e. sum of
elementary products.
EXAMPLE 1.11
Obtain a disjunctive nonnal fonn of
P v (-,P ~ (Q v (Q ~ -,R»)
Solution
P v (--, P ~ (Q v (Q ~ -, R»)
== P v (--, P ~ (Q v (--, Q v -, R»)
== P v (P v (Q v (--, Q v -, R»)
(step 1 using In)
(step 1 using 112 and h)
http://engineeringbooks.net
12
~
Theory ofComputer Science
== P v P v Q v -, Q v -, R
by using 13
== P v Q v -, Q v -, R
by using Ii
Thus, P v Q v -, Q v -, R is a disjunctive normal form of the given formula.
EXAMPLE 1.12
Obtain the disjunctive normal form of
(P ;\ -, (Q ;\ R)) v (P =} Q)
Solution
(P ;\ -, (Q ;\ R)) v
(P
=} Q)
== (P ;\ -, (Q ;\ R) v (---, P v Q)
(step 1 using 1d
== (P ;\ (-, Q v -, R)) v (---, P v Q)
(step 2 using 17)
== (P ;\ -, Q) v (P ;\ -, R) v -, P v Q
(step 3 using 14 and 13)
Therefore, (P ;\ -, Q) v (P ;\ -, R) v -, P v Q is a disjunctive normal form
of the given formula.
For the same formula, we may get different disjunctive normal forms. For
example, (P ;\ Q ;\ R) v (P ;\ Q ;\ -, R) and P ;\ Q are disjunctive normal
forms of P ;\ Q. SO. we introduce one more normal form, called the principal
disjunctive nomwl form or the sum-of-products canonical form in the next
definition. The advantages of constructing the principal disjunctive normal
form are:
(i) For a given formula, its principal disjunctive normal form is unique.
(ii) Two formulas are equivalent if and only if their principal disjunctive
normal forms coincide.
Definition 1.8
A minterm in n propositional variables p], .,', P/1 is
QI ;\ Q2 ' "
;\ Q/l' where each Qi is either Pi or -, Pi'
For example. the minterms in PI and P2 are Pi ;\ P 2, -, p] ;\ P 2,
p] ;\ -, P'J -, PI ;\ -, P2, The number of minterms in n variables is 2/1.
Definition 1.9
A formula ex is in principal disjunctive normal form if ex is
a sum of minterms.
1.2.2
CONSTRUCTION TO OBTAIN THE PRINCIPAL
DISJUNCTIVE NORMAL FORM OF A GIVEN FORMULA
Step 1
Obtain a disjunctive normal form.
Step 2
Drop the elementary products which are contradictions (such as
P ;\ -, P),
Step 3
If Pi and -, Pi are missing in an elementary product ex, replace ex by
(ex;\ P) v (ex;\ -,PJ
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
g,
13
Step 4
Repeat step 3 until all the elementary products are reduced to sum
of minterms. Use the idempotent laws to avoid repetition of minterms.
EXAMPLE 1.13
Obtain the canonical sum-of-products form (i.e. the principal disjunctive
normal form) of
ex = P v (-, P ;\ -, Q ;\ R)
Solution
Here ex is already in disjunctive normal form. There are no contradictions. So
we have to introduce the missing variables (step 3). -, P ;\ -, Q ;\ R in ex is
already a minterm. Now,
P == (P ;\ Q) v (P ;\ -, Q)
== ((P ;\ Q ;\ R) v (P ;\ Q ;\ -, R)) v (P ;\ -, Q ;\ R) v (P ;\ -, Q ;\ -, R)
== ((P ;\ Q ;\ R) v (P ;\ Q ;\ -, R)) v ((P ;\ -, Q ;\ R) v (P ;\ -, Q ;\ -, R))
Therefore. the canonical sum-of-products form of ex is
if;\Q;\mvif;\Q;\-,mvif;\-,Q;\m
v (P ;\ -, Q ;\ -, R) v (-, P ;\ -, Q ;\ R)
EXAMPLE 1.14
Obtain the principal disjunctive normal form of
ex = (-, P v -, Q) ::::} (-, P ;\ R)
Solution
ex = (-, P v -, Q) ::::} (-, P ;\ R)
== hh P v -, Q)) v h
P ;\ R)
by using 1\2
== (P ;\ Q) v h
P ;\ R)
by using DeMorgan's law
== ((P ;\ Q ;\ R) v (P ;\ Q ;\ -, R)) v (h P ;\ R ;\ Q) v h P ;\ R ;\ -, Q))
==if;\Q;\mvif;\Q;\-,mvhP;\Q;\mv(-,p;\-,Q;\m
So, the principal disjunctive normal form of ex is
if;\Q;\mvif;\Q;\-,mvhP;\Q;\mVhP;\-,QAm
A minterm of the form Ql ;\ Q2
A ...
A Qn can be represented by
(11(12 ..• (I", where (Ii = 0 if Qi = -, Pi and (Ii = 1 if Qi = Pi' So the principal
disjunctive normal form can be represented by a 'sum' of binary strings. For
L;.ample, (P ;\ Q ;\ R) v (P ;\ Q A -, R) v (-, P ;\ -, Q ;\ R) can be represented
by 111 v 110 v 001.
The minterms in the two variables P and Q are 00, 01, 10, and 11, Each
wff is equivalent to its principal disjunctive normal form. Every principal
disjunctive normal form corresponds to the minterms in it, and hence to a
http://engineeringbooks.net
14
~
Theory ofComputer Science
subset of {OO, 01, 10, 11}. As the number of subsets is 24, the number of
distinct formulas is 16. (Refer to the remarks made at the beginning of this
section.)
The truth table and the principal disjunctive normal form of a are closely
related. Each minterm corresponds to a particular assignment of truth values
to the variables yielding the truth value T to a. For example, P 1\ Q 1\ --, R
corresponds to the assignment of T, T, F to P, Q and R, respectively. So, if
the truth table of a is given. then the minterms are those which correspond
to the assignments yielding the truth value T to ex.
EXAMPLE 1.1 5
For a given formula a, the truth values are given in Table 1.12. Find the
principal disjunctive normal form.
TABLE 1.12
Truth Table of Example 1.15
P
Q
R
a
T
T
T
T
T
T
F
F
T
F
T
F
T
F
F
T
F
T
T
T
F
T
F
F
F
F
T
F
F
F
F
T
Solution
We have T in the a-column corresponding to the rows 1, 4, 5 and 8. The
minterm corresponding to the first row is P 1\ Q 1\ R.
Similarly, the mintem1S corresponding to rows 4, 5 and 8 are respectively
P 1\ --, Q 1\ ---, R, --, P 1\ Q 1\ Rand --, P 1\ ---, Q 1\ --, R. Therefore, the principal
disjunctive normal form of ex is
ifI\Ql\mvifl\--,QI\--,mvbPI\Ql\mvbPI\--,QI\--,m
We can form the 'dual' of the disjunctive normal form which is termed the
conjunctive normal form.
DefInition 1.10
A formula is in conjunctive normal form if it is a product
of elementary sums.
If a is in disjunctive normal form, then --, a is in conjunctive normal
form. (This can be seen by applying the DeMorgan's laws.) So to obtain the
conjunctive normal form of a, we construct the disjunctive normal form of
--, a and use negation.
Deimition 1.11
A maxterm in n propositional variables PI, P2, ••. , Pn is
Ql V Q2 V ... V QII' where each Qi is either Pi or --, Pi'
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
J;;I,
15
DefInition 1.12
A formula ex is in principal conjunctive normal form if ex
is a product of maxterms. For obtaining the principal conjunctive normal form
of ex, we can construct the principal disjunctive normal form of -, ex and apply
negation.
EXAMPLE 1.16
Find the principal conjunctive normal form of ex = P v (Q :::::} R).
Solution
-, ex= -,(P v (Q:::::} R))
== -, (P v (-, Q v R))
by using 112
== -, P 1\ (-, (-, Q v R))
by using DeMorgan'slaw
== -, P
1\ (Q 1\ -, R)
by using DeMorgan's law and 17
-, P /\ Q 1\ -, R is the principal disjunctive normal form of -, ex. Hence,
the principal conjunctive normal form of ex is
-, (-, P
1\ Q 1\ -, R) = P v -, Q v R
The logical identities given in Table 1.11 and the normal forms of well-formed
formulas bear a close resemblance to identities in Boolean algebras and normal
forms of Boolean functions. Actually, the propositions under v, 1\ and -, form
a Boolean algebra if the equivalent propositions are identified. T and F act as
bounds (i.e. 0 and 1 of a Boolean algebra). Also, the statement formulas form
a Boolean algebra under v, 1\ and -, if the equivalent formulas are identified.
The normal forms of \vell-formed formulas correspond to normal forms
of Boolean functions and we can 'minimize' a formula in a similar manner.
1.3
RULES OF INFERENCE FOR PROPOSITIONAL
CALCULUS (STATEMENT CALCULUS)
In logical reasoning. a certain number of propositions are assumed to be true.
and based on that assumption some other propositions are derived (deduced or
inferred). In this section we give some important rules of logical reasoning or
rules of inference. The propositions that are assumed to be true are called
h)potheses or premises. The proposition derived by using the rules of inference
is called a conclusion. The process of deriving conclusions based on the
assumption of premises is called a valid argument. So in a valid argument we
/
are concerned with the process of arriving at the conclusion rather ~
obtaining the conclusion.
The rules of inference are simply tautologies in the form of implication
(i.e. P :::::} Q). For example. P :::::} (P v Q) is such a tautology, and it is a rule
P
of inference. We write this in the form
Q . Here P denotes a premise.
.
".Pv
The proposition below the line. i.e. P v Q is the conclusion.
http://engineeringbooks.net
16
J;;i
Theory ofComputer Science
We give in Table 1.13 some of the important rules of inference. Of course,
we can derive more rules of inference and use them in valid arguments.
For valid arguments,
we can use the rules of inference given in
Table 1.13. As the logical identities given in Table 1.11 are two-way
implications. we can also use them as rules of inference.
TABLE 1.13
Rules of Inference
Rule of inference
Implication
form
RI1: Addition
P
:. PvQ
Rh Conjunction
p
Q
:. P A Q
Rh Simplification
PAQ
P
Rh: Modus ponens
P
P=>Q
~
F~I5: Modus tollens
-,Q
P=>Q
:. ~P
RIs: Disjunctive syllogism
-,P
PvQ
~
RI7: Hypothetical syllogism
P=>Q
Q=>R
:. P => R
RIa: Constructive dilemma
(P => Q)
/"
(R => 8)
PvR
:. Qv S
RIg: Destructive dilemma
CJ => Q)
1\ (R => 8)
~Qv--,S
:. P v R
P => (P v
Q)
(P
A
Q) => P
(P
1\ (P => Q)) => Q
(-, Q 1\ (P => Q)) => -, Q
(-, P
1\ (P
V Q)) => Q
((P => Q)
1\ (Q => R)) => (P => R)
((P => Q)
1\ (R => 8)
1\ (P v
R)) => (Q v 8)
((P => Q)
1\ (R => 8)
1\
(-, Q v -,8)) => (--, P v -, R)
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
17
EXAMPLE 1.17
Can we conclude S from the following premises?
(i) P
=} Q
(ii) P =} R
(iii) -,( Q /\ R)
(iv) S \j P
Solution
The valid argument for deducing S from the given four premises is given as
a sequence. On the left. the well-formed fOlmulas are given. On the right, we
indicate whether the proposition is a premise (hypothesis) or a conclusion. If
it is a conclusion. we indicate the premises and the rules of inference or logical
identities used for deriving the conclusion.
1. P =} Q
Premise (i)
2. P
=} R
Premise (ii)
3. (P
=} Q) /\ (P => R)
Lines 1. 2 and RI2
4.
---, (Q /\ R)
Premise (iii)
5.
---, Q \j
---, R
Line 4 and DeMorgan's law (h)
6.
---, P v
---, P
Lines 3. 5 and destructive dilemma (RI9)
7.
---, P
Idempotent law I]
8. S v P
Premise (iv)
9. S
Lines 7, 8 and disjunctive syllogism Rh
Thus, we can conclude 5 from the given premises.
EXAMPLE 1.18
Derive 5 from the following premises using a valid argument:
(i) P => Q
(ii) Q => ---, R
(iii) P v 5
(iv) R
Solution
1. P =} Q
Premise (i)
2. Q => ---, R
Premise (ii)
3. P => ---, R
Lines 1, 2 and hypothetical syllogism RI7
4.
R
Premise (iv)
5.
---, (---, R)
Line 4 and double negation h
6.
---, P
Lines 3. 5 and modus tollens RIs
7.
P \j 5
Premise (iii)
8.
5
Lines 6, 7 and disjunctive syllogism RI6
Thus, we have derived S from the given premises.
\
http://engineeringbooks.net
18
~
Theory ofComputer Science
EXAMPLE 1.19
Check the validity of the following argument:
If Ram has completed B.E. (Computer Science) or MBA, then he is
assured of a good job. If Ram is assured of a good job, he is happy. Ram is
not happy. So Ram has not completed MBA.
Solution
We can name the propositions in the following way:
P denotes 'Ram has completed B.E. (Computer Science)'.
Q denotes 'Ram has completed MBA'.
R denotes 'Ram is assured of a good job'.
S denotes 'Ram is happy'.
The given premises are:
(i) (P v Q) ~ R
(ii) R ~ S
(iii) --, S
The conclusion is --, Q.
1. (P v Q) ~ R
Premise (i)
2. R ~ S
Premise (ii)
3. (P v Q) ~ S
Lines 1, 2 and hypothetical syllogism RJ7
4. --, S
Premise (iii)
5. --, (P v Q)
Lines 3, 4 and modus tollens RJs
6. --, P /\ --, Q
DeMorgan's law h
7. --, Q
Line 6 and simplification RJ3
Thus the argument is valid.
EXAMPLE 1.20
Test the validity of the following argument:
If milk is black then every cow is white. If every cow is white then it has
four legs. If every cow has four legs then every buffalo is white and brisk.
The milk is black.
Therefore, the buffalo is white.
Solution
We name the propositions in the following way:
P denotes 'The milk is black'.
Q denotes 'Every cow is white'.
R denotes 'Every cow has four legs'.
S denotes 'Every buffalo is white'.
T denotes 'Every buffalo is brisk'.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
19
Premise (iv)
Premise (i)
Modus ponens RIJ,
Premise (ii)
Modus ponens RIJ,
Premise (iii)
Modus ponens RIJ,
Simplification
Rl~
valid.
The given premises are:
(i) p ~ Q
(ii) Q ~ R
(iii) R ~ S 1\ T
(iv) P
The conclusion is S.
1. P
2. P ~ Q
3. Q
4. Q ~ R
5. R
6. R ~ S
1\ T
7. 5 1\ T
8. S
Thus the argument is
1.4
PREDICATE CALCULUS
Consider two propositions 'Ram is a student', and 'Sam is a student'. As
propositions, there is no relation between them, but we know they have
something in common. Both Ram and Sam share the property of being a
student. We can replace the t\VO propositions by a single statement 'x is a
student'. By replacing x by Ram or Sam (or any other name), we get many
propositions. The common feature expressed by 'is a student' is called a
predicate. In predicate calculus we deal with sentences involving predicates.
Statements involving predicates occur in mathematics and programming
languages. For example. '2x + 3y = 4,:', 'IF (D. GE. 0.0) GO TO 20' are
statements in mathematics and FORTRAN. respectively, involving predicates.
Some logical deductions are possible only by 'separating' the predicates.
1.4.1
PREDICATES
A part of a declarative sentence describing the properties of an object or
relation among objects is called a predicate. For example, 'is a student' is a
predicate.
Sentences involving
predicateSCfe~-cribing the property of objects are
denoted by P(x), where P denotes the predicate and x is a variable denoting
any object. For example. P(x) can denote 'x is a student'. In this sentence, x
is a variable and P denotes the predicate 'is a student'.
The sentence 'x is the father of y' also involves a predicate 'is the father
of. Here the predicate describes the relation between two persons. We can
write
this sentence as F(x, y), Similarly, 2x + 3y = 4z can be described by
Sex, y, ,:).
http://engineeringbooks.net
20
~
Theory ofComputer Science
Note: Although P(x) involving a predicate looks like a proposition, it is not
a proposition. As P(x) involves a variable x, we cannot assign a truth value
to P(x). However, if we replace x by an individual object, we get a
proposition. For example, if we replace x by Ram in P(x), we get the
proposition 'Ram is a student'. (We can denote this proposition by P(Ram).)
If we replace x by 'A cat', then also we get a proposition (whose truth value is
F). Similarly, S(2, 0, 1) is the proposition 2 . 2 + 3 . 0 =4 . 1 (whose truth
value is T). Also, S(l, 1, 1) is the proposition 2 . 1 + 3 . 1 = 4 . 1 (whose
truth value is F).
The following definition is regarding the possible 'values' which can be
assigned to variables.
Definition 1.13
For a declarative sentence involving a predicate, the
universe of discourse, or simply the universe, is the set of all possible values
which can be assigned to variables.
For example, the universe of discourse for P(x): 'x is a student', can be
taken as the set of all human names; the universe of discourse for £(n): 'n is
an even integer', can be taken as the set of all integers (or the set of all real
numbers).
Note: In most examples. the universe of discourse is not specified but can be
easily given.
Remark We have seen that by giving values to variables, we can get
propositions from declarative sentences involving predicates. Some sentences
involving variables can also be assigned truth values. For example, consider
'There exists x such that
.~ = 5', and 'For all x,
.~ = (_x)2,. Both these
sentences can be assigned truth values (T in both cases). 'There exists' and
'For all' quantify the variables.
Universal and Existential Quantifiers
The phrase 'for all' (denoted by V) is called the universal quantifier. Using
this symbol, we can write 'For all x, x2 = (_x)2, as Vx Q(x), where Q(x) is
'x2 = (_x)2 ..
The phrase 'there exists' (denoted by 3) is called the existential quantifier.
The sentence 'There exists x such that x2 =5' can be written as 3x R(x),
where R(x) is 'r = 5'.
P(x) in Vx P(x) or in 3x P(x) is called the scope of the quantifier V or 3.
Note: The symbol V can be read as 'for every', 'for any', 'for each',
'for arbitrary'. The symbol 3 can be read as 'for some', for 'at least one'.
When we use quantifiers, we should specify the universe of discourse. If
we change the universe of discourse, the truth value may change. For example,
consider 3x R(x), where R(x) is x2 =5. If the universe of discourse is the set
of all integers. then 3x R(x) is false. If the universe of discourse is the set of
all real numbers. then 3x R(x) is true (when x = ±.J5 ' x2 = 5).
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
,\;!
21
The logical connectives involving predicates can be used for declarative
sentences involving predicates. The following example illustrates the use of
connectives.
EXAMPLE 1.21
Express the following sentences involving predicates in symbolic form:
1. All students are clever.
2. Some students are not successful.
3. Every clever student is successful.
4. There are some successful students who are not clever.
5. Some students are clever and successful.
Solution
As quantifiers are involved. we have to specify the universe of discourse. We
can take the universe of discourse as the set of all students.
Let C(x) denote 'x is clever'.
Let Sex) denote 'x is successful'.
Then the sentence 1 can be written as 'IIx C(x). The sentences 2-5 can be
written as
::Jx (, Sex»~,
::Jx (S(x) /\ ,C(x»,
'IIx (cex)
:::::}
Sex»~,
::Jx (C(x) ,
Sex»~
1.4.2
WELL-FORMED FORMULAS OF PREDICATE CALCULUS
A well-formed formula (wff) of predicate calculus is a string of variables such
as Xl, x2, •.• , X/1' connectives. parentheses and quantifiers defined recursively
by the following rules:
(i) PCYl, ..., x,J is a wff. where P is a predicate involving n variables
Xl,
X20
••. ,
J.-11'
(ii) If a is a wff. then , a is a wff.
(iii) If a and [3 are wffs, then a v [3, a i\ [3, a :::::} [3, a¢:::}[3 are also
wffs.
(iv) If a is a wff and x is any v~e; then 'IIx (a), ::Jx (a) are wffs.
(v) A string is a wff if and only if it is obtained by a finite number of
applications of rules (i)-(iv).
Note: A proposition can be viewed as a sentence involving a predicate with 0
Variables. So the propositions are wffs of predicate calculus by rule (i).
We call wffs of predicate calculus as predicate formulas for convenience.
The well-formed formulas introduced in Section 1.1 can be called proposition
formulas (or statement formulas) to distinguish them from predicate formulas.
http://engineeringbooks.net
22
~
Theory ofComputer Science
DefInition 1.14
Let ex and [3 be two predicate formulas in variables Xb
XII' and let Ube a universe of discourse for ex and [3. Then ex and f3 are
equivalent to each other over U if for every possible assignment of values to
each variable in ex and [3 the resulting statements have the same truth values.
We can write ex = [3 over U.
We say that ex and f3 are equivalent to each other (ex == fJ> if ex == [3 over
U for every universe of discourse U.
Remark
In predicate formulas the predicate val;ables mayor may not be
quantified. We can classify the predicate variables in a predicate formula,
depending on whether they are quantified or not. This leads to the following
definitions.
Definition 1.15
If a formula of the form 3x P(x) or "Ix P(x) occurs as part
of a predicate formula ex, then such part is called an x-bound part of ex, and
the occurrence of x is called a bound occurrence of x. An occurrence of x is
free if it is not a bound occurrence. A predicate variable in a is free if its
occurrence is free in any part of a.
In a = (3x l P(XI' x:)) /\ (VxcQ(x:> X3))' for example, the occurrence of
Xl in 3.1:) P(Xl' xc) is a bound occurrence and that of Xc is free. In VX2 Q(x:> X3),
the occurrence of Xc is a bound occurrence. The occurrence of X3 in ex is
free.
Note: The quantified parts of a predicate formula such as "Ix P(x) or 3x P(x)
are
propositions. We can assign values from the universe of discourse only
to the free variables in a predicate f01lliula a.
DefInition 1.16
A predicate formula is valid if for all possible assignments
of values from any universe of discourse to free variables, the resulting
propositions have the truth value T.
Defmition 1.17
A predicate formula is satisfiable if for some assignment of
values to predicate variables the resulting proposition has the truth value T.
DefInition 1.18
A predicate formula is unsatisfiable if for all possible
assignments of values from any universe <5fdiscourse to predicate variables the
resulting propositions have the truth value F.
We note that valid predicate formulas correspond to tautologies among
proposition formulas and the unsatisfiable predicate formulas correspond to
contradictions.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
23
1.5
RULES OF INFERENCE
FOR PREDICATE
CALCULUS
Before discussing the rules of inference, we note that: (i) the proposition
formulas are also the predicate formulas; (ii) the predicate formulas (w'here all
the variables are quantified) are the proposition formulas. Therefore, all the
rules of inference for the proposition formulas are also applicable to predicate
calculus wherever necessary.
For predicate formulas not involving connectives such as A(x), P(x, y). we
can get equivalences and rules of inference similar to those given in
Tables 1.11 and 1.13. For Example, corresponding to 16 in Table 1.11 we get
--, (P(x) v Q(x)) == --, (P(x)) i\
--, (Q(x)). Corresponding to RI3 in Table 1.13
P i\ Q ::::} P, we get P(x) i\ Q(x) ::::} P(x). Thus we can replace propositional
variables by predicate variables in Tables 1.11 and 1.13.
Some necessary equivalences involving the two quantifiers and valid
implications are given in Table 1.14.
TABLE 1.14
Equivalences Involving Quantifiers
Distributivity of j
over
\I
3x CP(x)
\I O(x)) = 3x PCI)
\I 3x O(x)
3x (P
\I O(x)) = P
\I (3x O(x))
Distributivity of ';I over ,A:
';Ix (P(x) "
O(x)) =1x P(x)
1\ ';Ix O(x)
';Ix (P /\ O(x)) = P .\ (';Ix O(x))
--, (3x P(x)) = ';Ix --. (P(x))
--, ('dx P(x)) = 3x --, (P(x))
J',7
3x (P .\ O(x)) = P /\ (3x O(x))
-
•
_ _ ~_.
"_____
_
o.
J18
';Ix (P
\I O(x)) = P
\I (';Ix OCr))
--------------,--
RJlO
';Ix P(x)
==; 3x P(x)
RJll
';Ix P(x)
\I ';Ix O(x)
==; ';Ix (P(x)
\I O(x))
RJ12
3x (P(x)
1\ O(x))
==; 3x P(x)
1\ 3x O(x)
Sometimes when we wish to derive
s1Jme-co~lusion from a given set of
premises involving quantifiers. we may have to eliminate the quantifiers
before applying the rules of inference for proposition formulas. Also, when
the conclusion involves quantifiers, we may have to introduce quantifiers. The
necessary rules of inference for addition and deletion of quantifiers are given
in Table 1.15.
http://engineeringbooks.net
r
24
~
Theory ofComputer Science
TABLE 1.15
Rules of Inference for Addition and
Deletion of Quantifiers
RI13:
Universal instantiation
'ix P(x)
~
c is some element of the universe.
RI,4 :
Existential instantiation
?:.xP(x)
. P(c)
c is some element for which P(c) is true.
Rl1s :
Universal generalization
P(x)
'ix P(x)
x should not be free in any of the given premises.
---- -~...~---
RI., 6
Existential generalization
P(c)
·. .=x P(x)
c is some element of the universe.
.EXAMPLE 1.22
Discuss the validity of the following argument:
All graduates are educated.
Ram is a graduate.
Therefore. Ram is educated.
Solution
Let G(x) denote 'x is a graduate'.
Let E(x) denote 'x is educated'.
Let R denote 'Ram'.
So the premises are (i) 'If.r (G(x) =} E(x)) and (ii) G(R). The conclusion is E(R).
'lfx (G(x)
=} E(x))
Premise (i)
G(R)
=} E(R)
Universal instantiation RI13
G(R)
Premise (ii)
:. E(R)
Modus ponens RI4
Thus the conclusion is valid.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
);J,
25
-------------'----------'---
EXAMPLE 1.23
Discuss the validity of the following argument:
All graduates can read and write.
Ram can read and write.
Therefore, Ram is a graduate.
Solution
Let G(x) denote 'x is a graduate'.
Let L(x) denote 'x can read and write'.
Let R denote 'Ram'.
The premises are: 'IIx (G(x) =? L(x)) and L(R).
The conclusion is G(R).
((G(R) =? L(R)) /\ L(R)) =? G(R) is not a tautology.
So we cannot derive G(R). For example, a school boy can read and write
and he is not a graduate.
EXAMPLE 1.24
Discuss the validity of the following argument:
All educated persons are well behaved.
Ram is educated.
No well-behaved person is quarrelsome.
Therefore. Ram is not quarrelsome.
Solution
Let the universe of discourse be the set of all educated persons.
Let PCx) denote 'x is well-behaved'.
Let y denote ·Ram'.
Let Q(x) denote 'x is quarrelsome'.
So the premises are:
(i) 'II.Y PCx).
(ii) y is a particular element of the universe of discourse.
(iii) 'IIx (P(x)
=? -, Q(x)).
To obtain the conclusion. we have the following arguments:
1. 'IIx P(x)
Premise (i)
2. PC\')
Universal instantiation RI13
3. 'IIx (P(x)
=? -, Q(x))
Premise (iii)
4.
PCy) =? -, QC:y)
Universal instantiation RI13
5. pry)
Line 2
6. -, Q(y)
Modus ponens RIc;
-, Q(y) means that 'Ram is not quarrelsome'. Thus the argument is valid.
http://engineeringbooks.net
26
Q
Theory ofComputer Science
1.6
SUPPLEMENTARY
EXAMPLES
EXAMPLE 1.25
Write the following sentences in symbolic form:
(a) This book is interesting but the exercises are difficult.
(b) This book is interesting but the subject is difficult.
(c) This book is not interesting. the exercises are difficult but the subject
is not difficult.
(d) If this book is interesting and the exercises are not difficult then the
subject is not difficult.
(e) This book is interesting means that the subject is not difficult, and
conversely.
(f) The subject is not difficult but this book is interesting and the
exercises are difficult.
(g) The subject is not difficult but the exercises are difficult.
(h) Either the book is interesting or the subject is difficult.
Solution
Let P denote 'This book is interesting'.
Let Q denote 'The exercises are difficult'.
Let R denote 'The subject is difficult'.
Then:
(a) P /\ Q
(b) P /\ R
(c) -,P /\ Q /\ ,R
(d) (P /\ -, Q)
=:} -, R
(e) P <=:> -, R
(f) (-,R) /\ (P /\ Q)
(g) -,R /\ Q
(h) -, P v R
EXAMPLE 1.26
Construct the truth table for ex = (-, P <=:> -, Q)
<=:> Q <=:> R
Solution
The truth table is constructed as shown in Table 1.16.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
!ml
27
TABLE 1.16
Truth Table of Example 1.26
P
Q
R
Q<=;R
-,P
-,Q
-,P<=;-,Q
ex
T
T
T
T
F
F
T
T
T
T
F
F
F
F
T
F
T
F
T
F
F
T
F
T
T
F
F
T
F
T
F
F
F
T
T
T
T
F
F
F
F
T
F
F
T
F
F
T
F
F
T
F
T
T
T
F
F
F
F
T
T
T
T
T
EXAMPLE 1.27
Prove that: ex = «P :::::} (Q v R)) !\
(---, Q)) :::::} (P :::::} R) is a tautology.
Solution
Let f3 = (P :::::} (Q v R)) !\
(---, Q)
The truth table is constructed as shown m Table 1.17. From the truth
table, we conclude that ex is a tautology.
TABLE 1.17
Truth Table of Example 1.27
P
Q
R
-,Q
QvR
P => (Q v R)
f3
P=>R
ex
T
T
T
F
T
T
F
T
T
T
T
F
F
T
T
F
F
T
T
F
T
T
T
T
T
T
T
T
F
F
T
F
F
F
F
T
F
T
T
F
T
T
F
T
T
F
T
F
F
T
T
F
T
T
F
F
T
T
T
T
T
T
T
F
F
F
T
F
T
T
T
T
EXAMPLE 1.28
State the converse, opposite and contrapositive to the following statements:
(a) If a triangle is isoceles, then two of its sides are equal.
(b) If there is no unemployment in India, then the Indians won't go to
the USA for employment.
Solution
If P
:::::} Q is a statement, then its converse, opposite and contrapositive
st~tements are, Q :::::} P, ---, P :::::} ---, Q and ---, Q :::::} ---, P, respectively.
(a) Converse-If two of the sides of a triangle are equal, then the triangle
is isoceles.
http://engineeringbooks.net
28
~
Theory ofComputer Science
Opposite-If the triangle is not isoceles, then two of its sides are not
equal.
Contrapositive-If two of the sides of a triangle are not equal, then
.the triangle is not isoceles.
(b) Converse-If the Indians won't go to the USA for employment, then
there is no unemployment in India.
Opposite-If there is unemployment in India. then the Indians will go
to the USA for employment.
(c) Contrapositive-If the Indians go to the USA for employment, then
there is unemployment in India.
EXAMPLE 1.29
Show that:
(-, P /\ (-, Q /\ R)) v (Q /\ R) v (P /\ R) ¢:::> R
Solution
(-, P /\ (-, Q /\ R)
v (Q /\ R) v (P /\ R)
¢:::> «-, P /\ -, Q) /\ R) v (Q /\ R) v (P /\ R) by using the associative law
¢:::> (-, (P v Q) /\ R) v (Q /\ R) v (P /\ R)
by using the DeMorgan's law
¢:::> h
(P v Q) /\ R) v (Q v P) /\ R)
by using the distributive law
¢:::> (-, (P v Q) v (P v Q) /\ R
by
using
the
commutative
and distributive laws
by using Is
by using 19
EXAMPLE 1.30
Using identities, prove that:
Q v (P /\ -, Q)
V
(-, P /\ -, Q) is a tautology
Solution
Q v (P /\ -, Q)
V (-, P /\ -, Q)
¢:::> «Q v P) /\ (Q
V
---, Q)
v (-, P 1\ -, Q) by using the distributive law
¢:::> «Q v P) /\ T) v (-, P /\ -, Q)
by using Is
¢:::> (Q v P) v
---, (P v Q)
by using the DeMorgan'slaw
and 19
¢:::> (P
V Q)
V
-, (P v Q)
by
using
the
commutative
law
¢:::>T
Hence the given fonnula is a tautology.
by using Is
http://engineeringbooks.net
-~------~-~~-
Chapter 1: Propositions and Predicates
);!
29
EXAMPLE 1.31
Test the validity of the following argument:
If
I get the notes and study well, then I will get first class.
I didn't get first class.
So either I didn't get the notes or I didn't study well.
Solution
Let P denote '1 get the notes'.
Let Q denote 'I study well'.
Let R denote '1 will get first class.'
Let S denote 'I didn't get first class.'
The given premises are:
(i) P ;\ Q =:} R
(ii) -, R
The conclusion is -, P v -, Q.
l.P;\Q=:}R
2. -, R
3. -, (P ;\ Q)
4. -,P v-,Q
Thus the argument is valid.
EXAMPLE 1.32
Premise (i)
Premise (ii)
Lines I, 2 and modus tollens.
DeMorgan's law
Explain (a) the conditional proof rule and (b) the indirect proof.
Solution
(a) If we want to prove A =:} B, then we take A as a premise and construct
a proof of B. This is called the conditional proof rule. It is denoted
by CPo
(b) To prove a formula
0:, we construct a proof of -, 0:
=:} F. In
particular. to prove A
=:} B. we construct a proof of A ;\ -, B =:} F.
EXAMPLE 1.33
Test the validity of the following argument:
Babies are illogical.
Nobody is despised who can manage a crocodile.
Illogical persons are despised.
Therefore babies cannot manage crocodiles.
http://engineeringbooks.net
30
Q
Theory ofComputer Science
Solution
Let B(x) denote 'x is a baby'.
Let lex) denote 'x is illogical'.
Let D(x) denote 'x is despised'.
Let C(x) denote 'x can manage crocodiles'.
Then the premises are:
(i) Vx (B(x) ::::} I(x))
(ii) Vx (C(x) ::::} ,D(x))
(iii) Vx (l(x) ::::} D(x))
The conclusion is Vx (B(x) ::::} ,
C(x)).
1. Vx (B(x) ::::} I(x))
Premise (i)
2. Vx (C(x) ::::} ,D(x))
Premise (ii)
3. Vx (l(x) ::::} D(x))
Premise (iii)
4. B(x) ::::} I(x)
1, Universal instantiation
5. C(x) ::::} ,D(x)
2, Universal instantiation
6. I(x) ::::} D(x)
3, Universal instantiation
7. B(x)
Premise of conclusion
8. I(x)
4,7 Modus pollens
9. D(x)
6,8 Modus pollens
10. ,C(x)
5,9 Modus tollens
11. B(x) ::::} ,
C(x)
7,10 Conditional proof
12. Vx (B(x) ::::} ,C(x))
11, Universal generalization.
Hence the conclusion is valid.
EXAMPLE 1.34
Give an indirect proof of
(, Q, P ::::} Q, P v S) ::::} S
Solution
We have to prove S. So we include (iv) ,S as a premise.
1. P v S
Premise (iii)
2. ,S
Premise (iv)
3. P
1,2, Disjunctive syllogism
4. P ::::} Q
Premise (ii)
5. Q
3,4, Modus ponens
6. ,Q
Premise (i)
7. Q /\ ,Q
5.6, Conjuction
8. F
18
We get a contradiction. Hence (, Q, P ::::} Q, P v S) ::::} S.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
~
31
EXAMPLE 1.35
Test the validity of the following argument:
All integers are irrational numbers.
Some integers are powers of 2.
Therefore, some irrational number is a power of 2.
Solution
Let Z(x) denote 'x is an integer'.
Let I(x) denote 'x is an irrational number'.
Let P(x) denote 'x is a power of 2'.
The premises are:
(i) 'ix (Z(x) => I(x))
(ii) ::Ix (Z(x) /\ P(x))
The conclusion is ::Ix (I(x) /\ P(x)).
1. ::Ix (Z(x) /\ P(x))
2. Z(b) /\ PCb)
3. Z(b)
4. PCb)
5. 'ix (Z(x) => l(x))
6. Z(b) => l(b)
7. l(b)
8. l(b) /\ PCb)
9. ::Ix (I(x) /\ P(x))
Hence the argument is valid.
Premise (ii)
1, Existential instantiation
2, Simplification
2, Simplification
Premise (i)
5. Universal instantiation
3,6, Modus ponens
7,4 Conjunction
8, Existential instantiation.
SELF-TEST
Choose the correct answer to Questions 1-5:
1. The following sentence is not a proposition.
(a) George Bush is the President of India.
(b) H
is a real number.
(c) Mathematics is a difficult subject.
(d) I wish you all the best.
2. The following is a well-formed formula.
(a) (P /\ Q) => (P v Q)
(b) (P /\ Q) => (P v Q) /\ R)
(c) (P /\ (Q /\ R)) => (P /\ Q))
(d) -, (Q /\ -, (P
V -, Q)
http://engineeringbooks.net
32
~
Theory ofComputer Science
3
P 1\ Q.
l' d
• --
IS ca,Je :
:. P
(a) Addition
(b) Conjunction
(c) Simplification
(d) Modus tollens
4. Modus ponens is
(a) -, Q
P=::;,Q
:. -, P
(b) -,P
PvQ
:. Q
(C) P
P=::;,Q
:. Q
(d) none of the above
5. -, P
1\ -, Q 1\ R is a minterm of:
(a) P v Q
(b) -, P 1\ -, Q 1\ R
(c) P 1\ Q 1\ R 1\ S
(d) P
1\ R
6. Find the truth value of P ~ Q if the truth values of P and Q are F and
T respectively.
7. For \vhat truth values of P. Q and R, the truth value of (P ~ Q) ~ R
is F?
(P, Q, R have the truth values F, T, F or F, T, T)
8. If P, Q. R have the truth values F, T, F, respectively, find the truth
value of (P ~ Q) v (P ~ R).
9. State universal generalization.
10. State existential instantiation.
EXERCISES
1.1 \Vhich of the following sentences are propositions?
(a) A tdangle has three sides.
(b) 11111 is a prime numbel:.
(c) Every dog is an animal.
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
&;;l
33
(d) Ram ran home.
(e) An even number is a prime number.
(f) 10 is a root of the equation .~ - lO02x + 10000 = 0
(g) Go home and take rest.
1.2 Express the following sentence in symbolic form: For any two numbers
a and h, only one of the following holds: a < b, a = h, and a > h.
1.3 The truth table of a connective called Exclusive OR (denoted by v) is
shown in Table 1.18.
TABLE 1.18
Truth Table for Exclusive OR
p
Q
P v
Q
T
T
F
T
f=
T
F
T
T
F
F
F
Give an example of a sentence in English (i) in which Exclusive OR
is used, (ii) in which OR is used. Show that v is associative,
commutative and distributive over I\.
1.4 Find two connectives, using which any other connective can be
desClibed.
1.5 The connective Ni\ND denoted by i (also called the Sheffer stroke) is
defined as follO\l/s: P i
Q = ....., (P ;\ Q). Show that every connective
can be expressed in terms of NAND.
1.6 The connective NOR denoted by 1 (also called the Peirce arrow) is
defined as follows: P 1 Q = ....., (P v Q). Show that every connective
can be expressed in terms of NOR.
1.7 Construct the truth table for the following:
(a) (P v Q) => ((P v R) => (R v Q)
(b) (p v (Q => R)
<=::? ((P v .....,R) => Q)
1.8 Prove the follmving equivalences:
(a)
(....., P => (-, P => (-, P ;\ Q»)
== P v Q
(b) P == (P v Q) ;\ (P v ....., Q)
(c)
....., (P
<=::? Q) == (P
1\
....., Q)
V
(-, P /\ Q)
1.9 Prove the logical identities given in Table 1.11 using truth tables.
1.10 Show that P => (Q => (R => (....., P => (-, Q => ....., R») is a tautology.
1.11 Is (P => ....., Pl => ....., P (i) a tautology. (ii) a contradiction. (iii) neither
a tautology nor a contradiction?
http://engineeringbooks.net
34
l;l
Theory ofComputer Science
1.12 Is the implication (P i\ (P ~ ---, Q)) v (Q ~ ---, Q) ~ ---, Q a tautology?
1.13 Obtain the principal disjunctive normal form of the following:
(a) P ~ (P ~ Q i\
(---, (---, Q v
---, P)))
(b) (Q i\
---, R i\
---, S) v (R i\ S).
1.14 Simplify the formula whose principal disjunctive normal form is
110 v 100 v 010 v 000.
1.15 Test the validity of the following arguments:
(a) P ~ Q
R=> -,Q
:. P =>-,R
(b) R ~ ---, Q
P~Q
-,R => S
:. P => S
(c) P
Q
---,Q ~ R
Q=> ---,R
:. R
(d)
P ~ Q i\ R
Q v S ~ T
SvP
:. T
1.16 Test the validity of the following argument:
If Ram is clever then Prem is well-behaved.
If Joe is good then Sam is bad and Prem is not well-behaved.
If Lal is educated then Joe is good or Ram is clever.
Hence if Lal is educated and Prem is not well-behaved then Sam is bad.
1.17 A company called for applications from candidates, and stipulated the
following conditions:
(a) The applicant should be a graduate.
(b) If he knows Java he should know C++.
(c) If he knows Visual Basic he should know Java.
(d) The applicant should know Visual Basic.
Can you simplify the above conditions?
1.18 For what universe of discourse the proposition '\Ix (x
~ 5) is true?
http://engineeringbooks.net
Chapter 1: Propositions and Predicates
s;;;I,
35
1.19 By constructing a suitable universe of discourse, show that
3x (P(x)
=} Q(x))
¢::> (3x P(x)
=} 3x Q(x))
is not valid.
1.20 Show that the following argument is valid:
All men are mortal.
Socrates is a man.
So Socrates is mortal.
1.21 Is the following sentence true? If philosophers are not money-minded
and some money-minded persons are not clever, then there are some
persons who are neither philosphers nor clever.
1.22 Test the validity of the following argument:
No person except the uneducated are proud of their wealth.
Some persons who are proud of their wealth do not help others.
Therefore, some uneducated persons cannot help others.
http://engineeringbooks.net
2
Mathematical
Preliminaries
In this chapter we introduce the concepts of set theory and graph theory. Also,
we define strings and discuss the properties of stlings and operations on strings.
In the final section we deal with the principle of induction, which will be used
for proving many theorems throughout the book.
2.1
SETS, RELATIONS AND FUNCTIONS
2.1.1
SETS AND SUBSETS
A set is a well-defined collection of objects, for example, the set of all students
in a college. Similarly. the collection of all books in a college library is also a
set. The individual objects are called members or elements of the set.
We use the capital letters A, B, C, ... for denoting sets. The small letters
a, b, c, ... are used to denote the elements of any set. When a is an element
of the set A. we write a E A. "\Then a is not an element of A, we write a rl. A.
Various Ways of Describing a Set
(i) By listing its elements. We write all the elements of the set (without
repetition) and enclose them within braces. We can write the elements
in any order. For example, the set of all positive integers divisible by
15 and less than 100 can be wlitten as {IS. 30, 45. 60. 75. 90}.
(ii) By describing the properties ofthe elements ofthe set. For example. the
set {IS, 30. 45. 60. 75. 90} can be described as: {n In is a positive
integer divisible by 15 and less than 100}. (The descliption of the
property is called predicate. In this case the set is said to be implicitly
specified.)
36
http://engineeringbooks.net
Chapter 2: Mathematical Preliminaries
l;!
37
(iii) By recursion. We define the elements of the set by a computational
rule for calculating the elements. For example. the set of all natural
numbers leaving a remainder 1 when divided by 3 can be described as
{alii ao = 1, an+l = ai! + 3}
When the computational rule is clear from the context, we simply specify
the set by some initial elements. The previous set can be written as {1. 4, 7,
10, " .}.The four elements given suggest that the computational rule is:
all+l = all + 3.
Subsets and Operations on Sets
A set A is said to be a subset of B (written as A
~ B) if every element of
A is also an element of B.
Two sets A and B are equal (we write A = B) if their members are the same.
In practice. to prove that A = B. we prove A ~ Band B ~ A.
A set with no element is called an empty set, also called a null set or a void
set, and is denoted by 0.
We define some operations on sets.
A u
B = {x Ix E A or x
E B}, called the union of A and B.
A n
B = {x I x E A and x E B}, called the intersection of A and B.
A -
B = {x Ix
E
A and x !l B}. called the complement of B in A.
N denotes U - A, where U is the universal set, the set of all elements
under consideration.
The set of all subsets of a set A is called the pmver set of A. It is denoted
by 2A.
Let A and B be two sets. Then A x B is defined as {(a, b) Ia E A and
b E B}. ((a. b) is called an ordered pair and is different from (b, a).)
DefInition 2.1 Let 5 be a set. A collection (AI' A2, ..., All) of subsets of 5 is
II
called a partition if Ai n Ai = 0(i ;;,t. j) and 5 = U Ai (i.e. Al u A 2 U ...
i=l
U
All)'
For example, if 5 = {l. 2. 3, ..., 1O}, then {{l. 3, S, 7. 9}, {2, 4, 6,
8, 1O}} is a partition of S.
2.1.2
SETS WITH ONE BINARY OPERATION
A binary operation "' on a set 5 is a rule which assigns. to every ordered pair
(a, b) of elements from S. a unique element denoted by a " b.
Addition, for example, is a binary operation on the set Z of all integers.
n hroughout this book. Z denotes the set of all integers.)
Union is a binary operation on :r\ where A is any nonempty set. We give
belO\v five postulates on binary operations.
Postulate 1: Closure. If a and b are in S. then a" b is in S.
http://engineeringbooks.net
38
g
Theory ofcomputer Science
Postulate 2: Associativity. If a, b, c are in S, then (a * b) * c = a * (b * c).
Postulate 3: Identity element. There exists a unique element (called the
identity element) e in S such that for any element x in S,
x * e = e * x = x.
Postulate 4: Inverse. For every element x in S there exists a unique element x'
in S such that x
'" x' = x' * x = e. The element x' is called the
inverse of x W.r.t. ".
Postulate 5: Commutativity. If a, b E S. then a * b = b * a.
It may be noted that a binary operation may satisfy none of the above five
postulates. For example, let S ={1. 2, 3, 4, ...}, and let the binary operation
be subtraction (i.e. a * b =a - b). The closure postulate is not satisfied since
2 - 3 =-1 eo S. Also, (2 - 3) - 4 =F 2 - (3 - 4), and so associativity is not
satisfied. As we cannot find a positive integer such that x - e = e - x =x, tht1
postulates 3 and 4 are not satisfied. Obviously, a - b
=F b - a. Therefore,
commutativity is not satisfied.
Our interest lies in sets with a binary operation satisfying the postulates.
Defmitions (i) A set S with a binary operation * is called a semigroup if the
postulates 1 and 2 are satisfied.
(ii) A set S with a binary operation * is called a monoid if the postulates
1-3 are satisfied.
(iii) A set S with * is called a group if the postulates 1-4 are satisfied.
(iv) A semigroup (monoid or group) is called a commutative or an abelian
semigroup (monoid or group) if the postulate 5 is satisfied.
Figure 2.1 gives the relationship between semigroups, monoids, groups,
etc. where the numbers refer to the postulate number.
~ No operation
~tulates 1,2
r-----'----,
Semigroup
5
5
Fig. 2.1
Sets with one binary operation.
We interpret Fig. 2.1 as follows: A monoid satisfying postulate 4 is a group.
A group satisfying postulate 5 is an abelian group, etc.
Chapter 2: Mathematical Preliminaries
~
39
We give below a few examples of sets with one binary operation:
(i) Z with addition is an abelian group.
(ii) Z with multiplication is an abelian monoid. (It is not a group since it
does not satisfy the postulate 4.)
(iii) {I, 2. 3, ...} with addition is a commutative semigroup but not a
monoid. (The identity element can be only 0, but 0 is not in the set.)
(iv) The power set 24 of A(A -j; 0) with union is a commutative monoid.
(The identity element is 0.)
(v) The set of all 2 x 2 matrices under multiplication is a monoid but not
an abelian monoid.
2.1.3
SETS WITH Two BINARY OPERATIONS
Sometimes we come across sets with two binary operations defined on them
(for example, in the case of numbers we have addition and multiplication). Let
5 be a set with two binary operations * and o. We give below 11 postulates
in the following way:
(i) Postulates 1-5 refer to * postulates.
(ii) Postulates 6. 7. 8. 10 are simply the postulates L 2. 3, 5 for the binary
operation o.
(iii) Postulate 9: If 5 under
8 satisfies the postulates 1-5 then for every x
in S. with x -j; e, there exists a unique element x' in 5 such that x' 0 x =
x 0 x' = e', where e' is the identity element corresponding to o.
(iv) Postulate 11: Distributivil\'. For a. b. c. in 5
a 0 (b * c) = (a 0 b) * (a 0 c)
A set with one or more binary operations is called an algebraic system.
For example, groups, monoids, semigroups are algebraic systems with one
binary operation,
We now define some algebraic systems with two binary operations.
Definitions (i) A set \vith two binary operations * and 0 is called a ring if
(a) it is an abelian group W.f.t.
8, and (b) 0 satisfies the closure, associativity
and distributivity postulates (i.e. postulates 6. 7 and 11).
(ii) A ring is called a commutative ring if the commutativity postulate is
satisfied for o.
(iii) A commutative ring with unity is a commutative ring that satisfies the
identity postulate (i,e. postulate 8) for o.
(iv) A field is a set with two binary operations * and 0 if it satisfies the
postulates 1-11.
We now give below a few examples of sets with two binary operations:
(i) Z with addition and multiplication (in place of * and 0) is a
commutative ring with identity. (The identity element W.f.t. addition is
O. and the identity element \V.r.t. multiplication is 1.)
40
g
Theory ofComputer Science
Oi) The set of all rational numbers (i.e. fractions which are of the form
alb. where a is any integer and b is an integer different from zero)
is a field. (The identity element W.r.t. multiplication is 1. The inverse
of alb, alb
;f:; 0 is bla.)
(iii) The set of all 2 x 2 matrices with matrix addition and matrix
multiplication is a ring with identity, but not a field.
(iv) The power set 24 (A
;f:; 0) is also a set with two binary operations
u and n. The postulates satisfied by u and n are 1, 2, 3, 5, 6, 7, 8,
10 and 11. The power set 2...1 is not a group or a ring or a field. But
it is an abelian monoid W.r.t. both the operations u and n.
Figure 2.2 illustrates the relation between the various algebraic systems we
have introduced. The interpretation is as given in Fig. 2.1. The numbers refer
to postulates. For example, an abelian group satisfying the postulates 6, 7 and
11 is a ring.
10r--
Ring
10
1-5
8
Ring with identity
Field
1-11
Fig. 2.2
Sets with tvvo binary operations.
2.1.4
RELATIONS
The concept of a relation is a basic concept in computer science as well as in
real life. This concept arises \vhen we consider a pair of objects and compare
one \vith the other. For example, 'being the father of' gives a relation between
two persons. We can express the relation by ordered pairs (for instance, 'a is
the father of b' can be represented by the ordered pair (a, b)).
While executing a program, comparisons are made, and based on the result.
different tasks are performed. Thus in computer science the concept of relation
arises just as in the case of data structures.
Definition 2.2
A relation R in a set S is a collection of ordered pairs of
elements in S (i.e. a subset of S x S). When (x, y) is in R, we write xRy. When
(x, v) is not in R. we write xR
t
)".
Chapter 2: Mathematical Preliminaries
);l,
41
EX-AMPLE 2.1
A relation R in Z can be defined by xKy if x > y.
Properties of Relations
(i) A relation R in S is ref7exive if xRx for every x in S.
(ii) A relation R in S is .n'l1lmetric if for x, y in S. ,'R, whenever xRy.
(iii) A relation R in S is transitive if for x, y and::: in S. xRz whenever xRy
and yR:::.
We note that the relation given in Example 2.1 is neither reflexive nor
symmetric. but transitive.
EXAMPLE 2.2
A relation R in {1, 2. 3. 4. 5. 6} is given by
{(l. 2). (2. 3), (3.4), (4. 4). (4, 5)}
This relation is not reflexive as 1R'L It is not symmetric as 2R3 but 3R'2. It
is also not transitive as 1R2 and 2R3 but 1R'3.
EXAMPLE 2.3
Let us define a relation R in {1. 2, .... 10} by aRb if a divides b. R IS
reflexive and transitive but not symmetric (3R6 but 6R'3).
EXAMPLE 2.4
If i, j,
Il are integers we say that i is congruent to j modulo n (written as
i == j modulo Il or i == j mod 11) if i - j is divisible by 11. The 'congruence modulo
/1' is a relation which is reflexive and symmetric (if i - j is divisible by n, so
is j - i). If i == j mod 11 and j == k mod n, then \ve have i - j =an for some a
and j - k =bn for some b. So.
i - k = i - j + j - k = an + bn
which means that i == k mod n. Thus this relation is also transitive.
DefInition 2.3
A relation R in a set S is called an equivalence relation if it is
retlexive. symmetric and transitive.
Example 2.5 gives an equivalence relation in Z.
42
~
Theory ofComputer Science
EXAMPLE 2.5
We can define an equivalence relation R on any set S by defining aRb if
a =b. (Obviously, a =a for every a. So, R is reflexive. If a =b then b =a.
So R is symmetric. Also, if a = band b =c, then a =c. So R is transitive.)
EXAMPLE 2.6
Define a relation R on the set of all persons in New Delhi by aRb if the persons
a and b have the same date of birth. Then R is an equivalence relation.
Let us study this example more carefully. Corresponding to any day of the
year (say, 4th February), we can associate the set of all persons born on that
day. In this way the ~et of all persons in New Delhi can be partitioned into 366
subsets. In each of the 366 subsets, any two elements are related. This leads to
one more property of equivalence relations.
DefInition 2.4
Let R be an equivalence relation on a set S. Let a E S. Then
CG is defined as
{b
E S IaRb}
The Ca is called an equivalence class containing a. In general, th6Ca's are
called equivalence classes.
EXAMPLE 2.7
For the congruence modulo 3 relation on {I, 2, ..., 7},
C2 = {2, 5},
C j = {l, 4, 7},
C3 = {3, 6}
For the equivalence relation 'having the same birth day' (discussed in Example
2.6), the set of persons born on 4th February is an equivalence class, and the
number of equivalence classes is 366. Also, we may note that the union of all
the 366 equivalence classes is the set of all persons in Delhi. This is true for
any equivalence relation because of the following theorem.
Theorem 2.1 Any equivalence relation R on a set S partitions S into disjoint
equivalence classes.
Proof Let U Ca denote the union of distinct equivalence classes. We have to
prove that: aES
(i) S = U CO'
aES
(ii) C" Ii Ch = 0 if C" and Ch are different, i.e. C" ;f. C/r
Let 5 E S. Then 5 E C, (since sRs, R being reflexive). But C\ \;;; U C(/'
aES
So S \;;; U Ca' By definition of C", Ca \;;; S for every a in S. So U Ca \;;; S.
"ES
aES
Thus we have proved (i).
Before proving (ii), we may note the following:
if aRb
(2.1)
Chapter 2: Mathematical Preliminaries
&;I,
43
As aRb, we have bRa because R is symmetric. Let d E
CCI' By definition of
CIi' we have aRd. As bRa and aRd. by transitivity of R, we get bRd. This
means d E Cb. Thus we have proved Ca <;;;;; Cb. In a similar way we can show
that Cb
<;;;;; Cd' Therefore. (2.1) is proved.
Now we prove (ii) by the method of contradiction (refer to Section 2.5).
We want to prove that Ca II Cb = 0 if Cil 1= C/,. Suppose Cil II Cb ::;t: 0. Then
there exists some element d in 5 such that d E Ca and d E Cb. As dEC",
we have aRd. Similarly. we have bRd. By symmetry of R, dRb. As aRd
and dRb, by transitivity of R, we have aRb. Now we can use (2.1) to
conclude that C" = Cb. But this is a contradiction (as C({ 1= Cb). Therefore,
Ca II Cb = 0. Thus (ii) is proved.
I
If we apply Theorem 2.1 to the equivalence relation congruence modulo
3 on {1. 2. 3, 4. 5, 6. 7}, we get
C1 = C. = C7 = {1. 4, 7}
C2 = Cs = {2, 5}
C3 = C6 = {3. 6}
and therefore.
{1. 2...., 7} = C j
U C2 U C3
EXERCISE
Let 5 denote the set of all students in a particular college.
Define aRb if a and b study in the same class. What are the equivalence
classes? In what way does R partition 5?
2.1.5
CLOSURE OF RELATIONS
A given relation R may not be reflexive or transitive. By adding more ordered
pairs to R we can make it reflexive or transitive. For example, consider a
relation R ={(1. 2), (2, 3). 0.1). (2, 2)} in {1. 2. 3}. R is not reflexive as 3R'3.
But by adding (3, 3) to R, we get a reflexive relation. Also, R is not transitive
as 1R2 and 2R3 but 1R'3. By adding the pair (1. 3). we get a relation
T = {(1. 2), (2, 3), (1. 1), (2, 2), (1. 3)} which is transitive. There are many
transitive relations T containing R. But the smallest among them is interesting.
Definition 2.5
Let R be a relation in a set 5. Then the transitive closure of R
(denoted by R+) is the smallest transitive relation containing R.
Note:
We can define ref1exive closure and symmetric closure in a similar way.
Definition 2.6
Let R be a relation in 5. Then the reflexive-transitive
closure of R (denoted by R*) is the smallest reflexive and transitive relation
containing R.
For constructing R+ and R*, we define the composite of two relations.
Let R j and R2 be the two relations in 5. Then,
(i)
R 1oR2 = {(a, c) E 5 x 5 IaRjb and bR2c for some b E 5}
(ii)
Rr = R j
0 R j
(iii)
R{l =
R{,~j
0 R 1
for all IJ ~2
44
~
Theory ofComputer Science
Note:
For getting the elements of R 1 0 R2, we combine (a, b) in R 1 and
(b,
c) in R2 to get (a,
c) in Rr 0 R2.
Theorem 2.2
Let 5 be a finite set and R be a relation in 5. Then the
transitive closure R+ of R exists and R+ = R U
R2 U
R3 .. , •
EXAMPLE 2.8
Let R = {(L 2). (2. 3), (2, 4)} be a relation in {l. 2, 3, 4}. Find R+.
Solution
R = {(1, 2), (2, 3), (2, 4)}
R2 = {(1, 2), (2, 3), (2, 4)}
0 {(L 2), (2, 3), (2, 4)}
= {(1, 3), (1, 4)}
(We combine (a. b) and (b, c) in R to get (a, c) in R2.)
R3 = R2 0 R = {(l. 3). (1, 4)}
0 {(1, 2), (2, 3), (2, 4)} = 0
(Here no pair (a, b) in R2 can be combined with any pair in R+---
R4 = RS = ... = 0
~,
R+ = R U
R2 = {(1, 2), (2. 3). (2. 4). (1, 3), (1, 4)}
EXAMPLE 2.9
Let R = (Ca. b),
(b, c). (c. a)}. Find R+.
Solution
R = {(a. b), (b, c), (c, a)}
R 0 R = {(a. b), (b. c), (c, a)}
0
{(a, b), (b, c), (c. a)}
= {(a. c), (b, a), (c. b)}
(This is obtained by combining the pairs: (a, b) and (b, c), (b, c) and (c, a),
and (c, a) and (a, b).)
R3 = R2 0 R = {(a, c), (b, a), (c, b)}
0
{(a, b), (b, c), (c, a)}
=
{(a,
a), (b, b), (c. c)}
R4 = R3 0 R = {(a, a), (b. b), (c, c)}
0
{(a, b), (b. c), (c, a)}
= {(a,
b), (b, c), (c, a)} = R
So.
~=~oR=RoR=~
~=~oR=~oR=~
R7 =R6 0 R =R3 0 R =~ = R
Then any R
I1 is one of R. R2 or R3. Hence,
R+ =R U
R2 U R3
={(a, b), (b, c), (c. a), (a, c). (b, a), (c, b), (a, a), (b, b), (c, c)}
Note: R* = R+
U
{(a, a)! a E
5}.
Chapter 2: Mathematical Preliminaries
~
45
EXAMPLE 2.10
If R = {(a, b), (b, c), (c, a)} is a relation in {a. b. c}, find R*.
Solution
From Example 2.9,
R* = R+ u
{(a, a), (b, b), (c, c)}
= {(a, b), (b, c), (c. a). (a, c), (b, a), (c, b), (a. a), (b, b), (c, c)}
EXAMPLE 2.11
What is the symmetric closure of relation R in a set S?
Solution
Symmetric closure of R = R u
{(b, a) IaRb}.
2.1.6
FUNCTIONS
The concept of a function arises when we want to associate a unique value (or
result) with a given argument (or input).
Definition 2.7
A function or map f from a set X to a set Y is a rule which
associates to every element x in X a unique element in Y. which is denoted by
j(x). The element f(x) is called the image of .y underf The function is denoted
by f
X ~ Y.
Functions can be defined either (i) by giving the images of all elements
of X, or (ii) by a computational rule which computes f(x) once x is given.
EXAMPLES (a)f: {l. 2. 3. 4}
~ {a, b, c} can be defined byf(1) = a,
f(2) = c, f(3) = a, f(4) = b.
(b) f: R ~ R can be defined by f(x) = .J + 2x + 1 for every x in R.
(R denotes the set of all real numbers.)
Definition 2.8
f: X ~ Y is said to be one-to-one (or injective) if different
elements in X have different images. i.e. f(Xl)
=i= f(X2) when Xl
=i= X2'
Note:
To prove that f is one-to-one. we prove the following: Assume
f(.I:J = f(X2) and show that Xl = X2'
Definition 2.9
f: X ~ Y is onto (sUljective) if every element y in Y is the
image of some element x in X.
Dpfmition 2.10
f: X ~ Y is said to be a one-to-one correspondence or
biJection if f is both one-to-one and onto.
46
~
Theory ofcomputer Science
EXAMI?LE 2.12
f : Z ~ Z given by fen) = 211 is one-to-one but not onto.
Solution
Suppose f(nl) =fen;). Then 2111 = 211;. So nl = 11;. Hence f is one-to-one. It
is not onto since no odd integer can be the image of any element in Z (as any
image is even).
The following theorem distinguishes a finite set from an infinite set.
Theorem 2.3
Let S be a finite set. Then f: S ~ S is one-to-one iff it is onto.
Note: The above result is not tme for infinite sets as Example 2.12 gives a
one-to-one function f : Z ~ Z which is not onto.
EXAMPLE 2.13
Show that f : R ~ R -
{1} given by lex) = (x + l)/(x -
1) is onto.
Solution
Let y E R. Suppose y =lex) = (x + l)/(x -
1). Then y(x - 1) = x + 1, i.e.
yx - x = 1 + Y. SO. x = (l + y)/(y -
1). As (1 + y)/(y - 1) E R for all y
-::;!:. 1,
y is the image of (l + y)/(y - 1) in R - {I}. Thus, f is onto.
The Pigeonhole Principlet
Suppose a postman distributes 51 letters in 50 mailboxes (pigeonholes). Then
it is evident that some mailbox will contain at least two letters. This is
enunciated as a mathematical principle called the pigeonhole principle.
If 11 objects are distributed over m places and n > m, then some place
receives at least two objects.
EXAMPLE 2.14
If we select 11 natural numbers between 1 to 380, show that there exist at least
two among these 11 numbers whose difference is at most 38.
Solution
Arrange the numbers 1. 2, 3, ..., 380 in 10 boxes, the first box containing
1. 2. 3..... 38. the second containing 39, 40, ..., 76, etc. There are 11
numbers to be selected. Take these numbers from the boxes. By the pigeonhole
principle, at least one box will contain two of these eleven numbers. These two
numbers differ by 38 or less.
- The pigeonhole principle is also called the Dirichlet drawer principle, named
after the French mathematician G. Lejeune Dirichlet (1805-1859).
Chapter 2: Mathematical Preliminaries
~
47
2.2
GRAPHS AND TREES
The theory of graphs is widely applied in many areas of computer science-
formal languages, compiler writing, artificial intelligence (AI), to mention only
a few. Also. the problems in computer science can be phrased as problems in
graphs. Our interest lies mainly in trees (special types of graphs) and their
properties.
2.2.1
GRAPHS
Defmition 2.11
A graph (or undirected graph) consists of (i) a nonempty set
V c...lled the set of vertices. (ii) a set E called the set of edges, and (iii) a map
<I> which assigns to every edge a unique unordered pair of vertices.
Representation of a Graph
Usually a graph. namely the undirected graph. is represented by a diagram
where the vertices are represented by points or small circles, and the edges by
arcs joining the vertices of the associated pair (given by the map <I».
Figure 2.3. for example, gives an undirected graph. Thus. the unordered
pair {VI, v:} is associated with the edge el: the pair (v:' v:) is associated with
e6' (e6 is a self-loop. In generaL an edge is called a self-loop if the vertices in
its associated pair coincide.)
Fig. 2.3
An undirected graph.
Defmition 2.12
A directed graph (or digraph) consists of (i) a nonempty set
V called the set of vertices, (ii) a set E called the set of edges, and (iii) a map
<I> which assigns to every edge a unique ordered pair of vertices.
Representation of a Digraph
The representation is as in the case of undirected graphs except that the edges
;...:'e represented by directed arcs.
Figure 2.4. for example. gives a directed graph. The ordered pairs (v:, 1'3),
(1'3, 1'4), (VI. 1'3) are associated with the edges e3' e4, e:, respectively.
48
l;\
Theory ofComputer Science
Fig. 2.4
A directed graph.
DefInitions
(i) If (Vi, Vi) is associated with an edge e, then Vi and Vj are called
the end vertices of e; Vi is called a predecessor of Vj which is a successor of Vi'
In Fig. 2.3.
1'~ and
1'3 are the end vertices of e3' In Fig. 2.4,
v~ is a
predecessor of 1'3 which is a successor of V~. Also, 1'4 is a predecessor of v~ and
successor of 1'3'
(ii) If G is a digraph, the undirected graph corresponding to G is the
undirected graph obtained by considering the edges and vertices of G, but
ignoring the 'direction' of the edges. For example, the undirected graph
corresponding to the digraph given in Fig. 2.4 is shown in Fig. 2.5.
Fig. 2.5
A graph.
DefInition 2.13
The degree of a vertex in a graph (directed or undirected) is
the number of edges with V as an end vertex. (A self-loop is counted twice while
calculating the degree.) In Fig. 2.3, deg(1']) = 2, deg(1'3) =3, deg(1'2) = 5. In
Fig. 2.4,
deg(1'~) = 3, deg(1'4) = 2.
We now mention the following theorem without proof.
Theorem 2.4
The number of vertices of odd degree in any graph (directed or
undirected) is even.
DefInition 2.14
A path in a graph (undirected or directed) is an alternating
sequence of vertices and edges of the form
v]e]1'~e~ ... vn_len_]VI1' beginning
and
ending with vertices such that ei has Vi and Vi+] as its end vertices and
no edge or vertex is repeated in the sequence. The path is said to be a path
from
1'1 to
VIZ"
For example.
1'je~1'3e3v2 is a path in Fig. 2.3. It is a path from V] to 1'2' In
Fig. 2.4. Vje2V3e3v2 is a path from v] to 1'2' vlej1'2 is also a path from Vj to 1'2'
Chapter 2: Mathematical Preliminaries
g
49
And v3e4v4e5v~ is a path from 1'3 to v~. We call 1'3e41'4e5v~ a directed path since
the edges e4 and es have the forward direction. (But 1'je~V3e3v~ is not a directed
path as
e~ is in the forward direction and e3 is in the backward direction.)
Defmition 2.15
A graph (directed or undirected) is connected if there is a
path bet\veen every pair of vertices.
The graphs given by Figs. 2.3 and 2.4. for example, are connected.
Definition 2;16
A circuit in a graph is an alternating sequence
1'lelv2e~ ...
en-lVI of vertices and edges starting and ending in the same vertex such that
ei has Vi and Vi+l as the end vertices and no edge or vertex other than VI is
repeated.
In Fig. 2.3. for example. V3e3 v~e5v4e4v3'
~'1 e~1'3e4V4e5v~elVI are circuits. In
Fig. 2.4. l'je2v3e31'2ejvl and
v2e3v3e4V4eSl'~ are circuits.
2.2.2
TREES
Definition 2.17
A graph (directed or undirected) is called a tree if it is
connected and has no circuits.
The graphs given in Figs. 2.6 and 2.7, for example, are trees. The graphs
given in Figs. 2.3 and 2.4 are not trees,
Note:
A directed graph G is a tree iff the corresponding undirected graph
is a tree.
Fig. 2.6
A tree with four vertices.
Fig. 2.7
A tree with seven vertices.
We no",,- discuss some properties of trees (both directed and undirected)
used in developing transition systems and studying grammar rules.
A tree is a connected graph with no circuits or loops.
In a tree there is one and only one path between every pair of
50
£;!
Theory ofComputer Science
Property 1
Property 2
vertices.
Property 3
If in a graph there is a unique (i.e. one and only one) path
between every pair of vertices, then the graph is a tree.
Property 4
Property 5
a tree.
Property 6
it is a tree.
A tree with n vertices has
11 - 1 edges.
If a connected graph with n vertices has
11 -
1 edges, then it is
If a graph with no circuits has n vertices and 11 - 1 edges,tben
A leaf in a tree can be defined as a vertex of degree one. The vertices
other than leaves are called internal ve11ices.
In Fig. 2.6. for example, 1'1, "'3, "'4 are leaves and "'2 is an internal vertex.
In Fig. 2.7. "'2, 1'5. V6' Vi are leaves and 1'1, 1'3' 1'4 are internal vertices.
The following definition of ordered trees will be used for representing
derivations in context-free grammars.
Defmition 2.18
An ordered directed tree is a digraph satisfying the following
conditions:
T1:
There is one vertex called the root of the tree which is distinguished
from all the other vertices and the root has no predecessors.
T::
There is a directed path from the root to every other vertex.
T3:
Every ve11ex except the root has exactly one predecessor.
T4:
The successors of each vertex are ordered 'from the left'.
Note:
The condition T4 of the definition becomes evident once we have the
diagram of the graph.
Figure 2.7 is an ordered tree with VI as the root. Figure 2.8 also gives an
ordered directed tree with V1 as the root. In this figure the successors of 1'1 are
ordered as 1':1'3' The successors of 1'3 are ordered as 1'51'6'
!\
v, !\
v4
®
®
Fig. 2.8
An ordered directed tree.
Chapter 2: Mathematical Preliminaries
J;1
51
By adopting the following convention, we can simplify Fig. 2.8. The root
is at the top. The directed edges are represented by arrows pointing downwards.
As all the arrows point downwards, the directed edges can be simply
represented by lines sloping downwards, as illustrated in Fig. 2.9.
Fig. 2.9
Representation of an ordered directed tree.
Note:
An ordered directed tree is connected (which follows from T2). It has
no circuits (because of T3). Hence an ordered directed tree is a tree (see
Definition 2.17).
As we use only the ordered directed trees in applications to grammars, we
refer to ordered directed trees as simply trees.
Defmition 2.19
A binary tree is a tree in which the degree of the root is 2 and
the remaining vertices are of degree 1 or 3-
Note:
In a binary tree any vertex has at most two successors. For example, the
trees given by Figs. 2.11 and 2.12 are binary trees. The tree given by Fig. 2.9
is not a binary tree.
Theorem 2.5
The number of vertices in a binary tree is odd.
Proof
Let n be the number of vertices. The root is of degree 2 and the
remaining
n - 1 vertices
are of odd degree
(by Definition
2.19). By
Theorem 2.4, n - 1 is even and hence 11 is odd.
I
We now introduce some more terminology regarding trees:
(i) A son of a vertex v is a successor of 1'.
(ii) The father of v is the predecessor of 1'.
(iii) If there is a directed path from v] to 1'2> VI is called an ancestor of V.:,
and V2 is called a descendant of V1' (Convention: v] is an ancestor of
itself and also a descendant of itself.)
(iv) The number of edges in a path is called the length of the path.
(v) The height of a tree is the length of a longest path from the root. For
example, for the tree given by Fig. 2.9, the height is 2. (Actually there
are three longest paths, 1'1 -+ 1'2 -+ V.., 1'1 -+ 1'3 -+ VS, VI -+ V2 -+ V6'
Each is of length 2.)
(vi) A vertex V in a tree is at level k if there is a path of length k from the
root to the vertex V (the maximum possible level in a tree is the height
of the tree).
52
~
Theory of Computer Science
Figure 2.10. for example. gives a tree where the levels of vertices are
indicated.
Root
r--,
Level 0
\L""'~
Level :2-0cf- Level 2.:0
Level 3
Fig. 2.10
illustration of levels of vertices.
EXAMPLE 2.15
For a binary tree T with n vertices. shO\v that the minimum possible height
is rlog=(n + 1) - n where r k 1 is the smallest integer 2 k. and the maximum
possible height is (n -
1)12.
Solution
In a binacy tree the root is at level O. As every vertex can have at most t\vo
successors. vve have at most two vertices at level 1. at most 4 vertices at level
2. etc. So the maximum number of vertices in a binary tree of height k is
1 + 2 + 2= + ... + i'. As T has n vertices. 1 + 2 + 2= + ... + 2k 2 11, i.e.
(2k+1 -
1)/(2 -1) 2': 11. so k 2': log=(n + 1) -
1. As k is an integer, the smallest
possible value for k is
log=(n + 1) - n Thus the minimum possible height
is r log=(n + 1) - n
To get the maximum possible height. we proceed in a similar way. In
a binary tree we have the root at zero level and at least two vertices at level
1. 2, .... When T is of height k. we have at least 1 + 2 + ... + 2 (2 repeated
k times) vertices. So. 1 + 2k
~ n, i.e. k
~ (n -
1)/2. But, n is odd by
Theorem 2.4. So (n -
1)/2 is an integer. Hence the maximum possible value
for k is
(11 -
1)/2.
EXAMPLE 2.16
When
11 = 9. the trees \vith minimum and maximum height are shown
in Figs. 2.11 and 2.12 respectively. The height of the tree in Fig. 2.11 is
!log.:'(9 + 1) -
11 = 3. For the tree in Fig. 2.12. the height = (9 - 1)/2 = 4.
Chapter 2: Mathematical Preliminaries
);l,
53
Fig. 2.11
Binary tree of minimum height with 9 vertices.
Fig. 2.12
Binary tree of maximum height with 9 vertices.
EXAMPLE 2.1 7
Prove that the number of leaves in a binary tree Tis
(/1 + 1)/2, where /1
IS
the number of vertices.
Solution
Let in be the number of leaves in a tree with /1 vertices. The root is of degree
2 and the remaining /1 -
in -
1 vertices are of degree 3. As T has /1 vertices,
it has
/1
-
1 edges (by Property 4). As each edge is counted twice while
calculating the degrees of its end vertices. 2(/1 - 1) = the sum of degrees of all
vertices = 2 + m + 3(11 -
In -
1). Solving for in. we get in = (/1 + 1)12.
EXAMPLE 2.18
For the tree shown in Fig. 2.13, answer the following questions:
(a) Which vertices are leaves and \vhich internal vertices?
S4
g,
Theory ofComputer Science
(b) Which vertices are the sons of 57
(c) Which vertex is the father of 57
(d) \Xlhat is the length of the path from 1 to 97
(e) What is the left-right order of leaves?
(f) What is the height of the tree?
2
10
7
8
o
Fig. 2.13
The directed tree for Example 2.18.
Solutions
(a) 10, 4, 9, 8, 6 are leaves. 1, 2. 3, 5, 7 are internal vertices.
(bi 7 and 8 are the sons of 5.
(c) 3 is the father of 5.
(d) Four (the path is 1 ~ 3 ~ 5 ~ 7 ~ 9).
(e) 10 - 4 - 9 - 8 - 6.
(f) Four (1 ~ 3 ~ 5 ~ 7 ~ 9 is the longest path).
2.3
STRINGS AND THEIR PROPERTIES
A string over an alphabet set 2: is a finite sequence of symbols from 2:.
NOTATION:
2:* denotes the set of all strings (including A, the empty string)
over the alphabet set 2:. That is, 2:+ = 2:* - {A}.
2.3.1
OPERATIONS ON STRINGS
The basic operation for strings is the binary concatenation operation. We
define this operation as follows: Let x and y be two strings in 2:*. Let us form
a new string :: by placing y after x, i.e. z = xy. The string z is said to be
obtained by concatenation of x and y.
Chapter 2: Maihematical Preliminaries
~
55
EXAMPLE 2.1 9
Find J:::V and yx, where
(a) x = 010,
(b) x = a1\.,
y = 1
y = ALGOL
Solution
(a)
X)' = 0101, yx = 1010.
(b) xy = a /\ i\LGOL
yx = ALGOL aA.
We give below some basic properties of concatenation.
Property 1
Concatenation on a set 12* is associative since for each x, y, ::: in
12*, x(y:::) = (xy):::.
Property 2
Identity element.
The set 12* has an identity element A W.r.t.
the binary operation of concatenation as
x1\. = Ax = x
for every x in 12*
Property 3
12* has left and right cancellations. For x, y, Z in 12*,
z:x- = ;:;y implies x = y (left cancellation)
x:: = yz implies x = y (right cancellation)
Property 4
For x, y in 12*, we have
Ixyl = Ixl + Iyl
where Ixi, Iy , . 'xv i denote the lengths of the strings x, y, xy, respectively.
We introduce below some more operations on strings.
Transpose Operation
We extend the concatenation operation to define the transpose operation as
follows:
For any x in 12* and a in 12,
(xayT = a(x)T
For example. (aaabab;T is babaaa.
Palindrome. A palindrome is a string which is the same whether written
forward or backward, e.g. Malayalam. A palindrome of even length can be
obtained by concatenation of a string and its transpose.
Prefix and suffix of a string. A prefix of a string is a substring of leading
symbols of that string. For example, w is a prefix of y if there exists y' in 12*
such that y = H·y'. Tuen we write w < y. For example, the string 123 has four
prefixes, i.e. A. L 12, 123.
Similarly, a suffix of a string is a substring of trailing symbols of that
string, i.e. w is a suffix of y if there exists y' E 12* such that y = y'w. For
example, the string 123 has four suffixes, i.e. 1\., 3, 23, 123.
56
l;;\
Theory ofComputer Science
Theorem 2.6
(Levi's theorem) Let v, W, x and Y E 1:* and vw =Ay. Then:
(i) there exists a unique string z in 1:* such that v =xz and y = zw if
11'1> Ixl;
(ii) v = x, Y = w, i.e. z = A if Iv I = 1x I;
(iii) there exists a unique stling z in 1:* such that x = vz, and
= zy if
II'I < 14
Proof
We shall give a very simple proof by representing the strings by a
diagram (see Fig. 2.14).
t==x--...------y-----~
Case 1: Ivl > I.xl
v=xZ
y =zw
I
1-_:===:===:::.==;==::1
Case 2:
Ivl =!xi
v=x
W =Y
--
Case 3:
Ivl < Ixl
x = vz
w = zy
Fig. 2.14
Illustration of Levi's theorem.
2.3.2
TERMINAL AND NONTERMINAL SYMBOLS
The definitions in this section will be used in subsequent chapters.
A tenninal symbol is a unique indivisible object used in the generation of
strings.
A nonterminal symbol is a unique object but divisible, used in the
generation of strings. A nonterminal symbol will be constructed from the
terminal symbols: the number of terminal symbols in a nontenninal symbol
may vary; it is also called a variable. In a natural language, e.g. English, the
letters a, b, A, B, etc. are
terminals and the words boy, cat, dog, go are
nonterrninal symbols. In programming languages, A, B, C, ..., Z, :, =, begin,
and. if, then, etc. are terminal symbols.
The following will be a variable in Pascal:
< For statement > .~ for < control variable >
=
< for list > do < statement >
Chapter 2: Mathematical Preliminaries
);J,
57
2.4
PRINCIPLE OF INDUCTION
The process of reasoning from general observations to specific truths is called
induction.
The following propelties apply to the set N of natural numbers~the
principle ofinduction.,
Property 1
Zero is a natural number.
Property 2
The successor of any natural number is also a natural number.
Property 3
Zero is not the successor of any natural number.
Property 4
No two natural numbers have the same successor.
Property 5
Let a property pen) be defined for every natural number n. If
(i) P(O) is true. and (ii) P(successor of II) is true whenever pen) is true, then
P(n) is true for all II.
A proof by complete enumeration of all possible combinations is called
perfect induction. e.g. proof by truth table.
The method of proof by induction can be used to prove a property pen) for
all n.
2.4.1
METHOD OF PROOF BY INDUCTION
This method consists of three basic steps:
Step 1 Prove P(n) for II = 0/1. This is called the prooffor the basis.
Step 2 Assume the result/properties for pen). This is called the induction
hypothesis.
Step 3 Prove P(II + l) using the induction hypothesis.
EXAMPLE 2.20
Prove that 1 + 3 + 5 + ... + r = n~. for all n > O. where r is an odd integer
and n is the number of terms in the sum. (Note: r = 211 -
1.)
Solution
(a) Prooffor the basis.
For n =1. L.H.S. = 1 and R.H.S. = 12 = 1. Hence
the result is true for n = 1.
(b) By induction hypothesis. we have 1 + 3 + 5 + ... + r = Il~. As r =2n - 1.
L.H.S. = 1 + 3 + 5 + ... + (2n -
1) = 112
(e) We have to prove that 1 + 3 + 5 + .. , + r + r + 2 = (n + 1)2:
L.R.S. = (1 + 3 + 5 + '"
+ r + (r + 2))
= 112 + r + 1 = n: + 2n - 1 + 2 = (ll + 1)= = R.H.S.
58
!i2
Theory ofComputer Science
EXAMPLE 2.21
Prove the following theorem by induction:
1 + 2 + 3 + ... + n = n(n + 1)/2
Solution
(a) Proof for the basis.
For n = L L.H.S. = 1 and
R.H.5. = 1(1 + 1)/2 = 1
(b) Assume 1 + 2 + 3 + ... + n = n(1l + 1)/2.
(c) We have to prove:
1 + 2 + 3 +
+ (n + 1) = (n + 1)(n + 2)/2
1 + 2 + 3 +
+ n + (n + 1)
=n(n + 1)/2 + (n + 1)
(by induction hypothesis)
=(n + 1)(n + 2)/2
(on simplification)
The proof by induction can be modified as explained in the following
section.
2.4.2
MODIFIED METHOD OF INDUCTION
Three steps are involved in the modified proof by induction.
Step 1 Proof for the basis (n = 0/1).
Step 2 Assume the result/properties for all positive integers < n + 1.
Step 3 Prove the result/properties using the induction hypothesis (i.e. step 2),
forn+l.
Example 2.22 below illustrates the modified method of induction. The
method we shall apply will be clear once we mention the induction hypothesis.
EXAMPLE 2.22
Prove the following theorem by induction: A tree with n vertices has (n - 1)
edges.
Solution
For n = 1, 2, the following trees can be drawn (see Fig. 2.15). 50 the theorem
is true for n = 1, 2. Thus, there is basis for induction.
o
I
n=1
n=2
Fig. 2,15
Trees with one or two vertices.
Chapter 2: Mathematical Preliminaries
~
59
Consider a tree T with (n + 1) vertices as shown in Fig. 2.16. Let e be
an edge connecting the vertices Vi and 1} There is a unique path between Vi
and vi through the edge e. (Property of a tree: There is a unique path between
every pair of vertices in a tree.) Thus, the deletion of e from the graph will
divide the graph into two subtrees. Let nl and n: be the number of vertices
in the subtrees. As
111
::;
11 and
11:
::; n. by induction hypothesis, the total
number of edges in the subtrees is 11] - 1 + n: - 1. i.e. n - 2. So, the number
of edges in T is n - 2 + 1 = 11 -
1 (by including the deleted edge e). By
induction. the result is true for all trees.
e
Fig. 2.16
Tree T with (n + 1) vertices.
EXAMPLE 2.23
Two definitions of palindromes are given below. Prove by induction that the
two definitions are equivalent.
Definition 1
A palindrome is a string that reads the same forward and
backward.
Definition 2
(i) A is a palindrome.
(ii) If a is any symboL the string a is a palindrome.
(iii) If a is any symbol and x is a palindrome. then axa is a palindrome.
(iv) Nothing is a palindrome unless it follows from (i)-(iii).
Solution
Let x be a string which satisfies the Definition L i.e. x reads the same forward
and backward. By induction on the length of x we prove that x satisfies
the Definition 2.
If Ix I ::; 1. then x =a or A. Since x is a palindrome by Definition L i\
and a are also palindromes (hence (i) and (ii», i.e. there is basis for induction.
If !x I > 1. then x =mva, where w. by Definition 1. is a palindrome: hence the
rule (iii). Thus. if x satisfies the Definition L then it satisfies the Definition 2.
Let x be a string which is constructed using the Definition 2. We
show by
induction on i x I that it satisfies the Definition 1. There is basis
for induction by rule (ii). Assume the result for all strings with length < n.
Let x be a string of length
n. As x has
to be constructed using the
rule (iii). x = aya. where y is a palirrlr'Jme. As y is a palindrome by
Definition 2 and Iy I < 71, it satisfies the Definition 1. So, x = aya also satisfies
the Definition 1.
60
J;l
Theory of Computer Science
EXAMPLE 2.24
Prove the pigeonhole principle.
Proof
We prove the theorem by induction on m. If m = 1 and
11 > L then
all these
11 items must be placed in a single place. Hence the theorem is true
for III = 1.
Assume the theorem for m. Consider the case of III + 1 places. We prove
the theorem for 11 =In + 2. (If 11 > In + 2. already one of the In + 1 places will
receive at least two objects from m + 2 objects, by what we are going to prove.)
Consider a particular place. say, P.
Three cases arise:
(i) P contains at least two objects.
(ii) P contains one object
(iii) P contains no object.
In case (i). the theorem is proved for n = 111 + 2. Consider case (ii). As P
contains one object, the remaining m places should receive 111 + 1 objects. By
induction hypothesis, at least one place (not the same as P) contains at least two
objects. In case (iii), III + 2 objects are distributed among In places. Once again,
by induction hypothesis, one place (other than P) receives at least two objects.
Hence. in aD the cases, the theorem is true for (m + 1) places. By the principle
of induction. the theorem is true for all Ill.
2.4.3
SIMULTANEOUS INDUCTION
Sometimes we may have a pair of related identities. To prove these, we may
apply two induction proofs simultaneously. Example 2.25 illustrates this
method.
EXAMPLE 2.25
A sequence Fo, F], F2, ... called the sequence of Fibonacci numbers (named
after the Italian mathematician Leonardo Fibonacci) is defined recursively as
follows:
Prove that:
Fo = O.
P" :
(2.2)
(2.3)
Proof
We prove the two identities (2.2) and (2.3) simultaneously by
simultaneous induction. PI and Q] are F12 + F0
2 = F1 and F 2F 1 + F]Fo = Fc
Chapter 2: Mathematical Preliminaries
);!
61
respectively. As Fo = 0, F I = 1,
F~ =1, these are true. Hence there is basis
for induction. Assume Pn and Qw So
(2.2)
(2.3)
Now.
= (F,~-l + F,~) + F"-lF,, + F,; + F"-lF,,
=F,;-l + F,~ + (F,,-l + F" )F" + F"F,,-l
=(F,~-l + F,;) + F,'~lF" + F"F,'-l
(by (2.3»)
This proves Pn+l •
Also.
(By P'1+1 and (2.3))
This proves Qn+l'
So. by induction (2.2) and (2.3) are true for all
11.
We conclude this chapter with the method of proof by contradiction.
2.5
PROOF BY CONTRADICTION
Suppose we want to prove a property P under certain conditions. The method
of proof by contradiction is as follows:
Assume that property P is not true. By logical reasoning get a conclusion
which is either absurd or contradicts the given conditions.
The following example illustrates the use of proof by contradiction and
proof by induction.
EXAMPLE 2.26
Prove that there is no string x in {a. b}* such that (L-r: = xb. (For the definition
of strings. refer to Section 2.3.)
62
~
Theory ofComputer Science
Proof
We prove the result by induction on the length of x. When Ix , = 1,
x =a or x =b. In both cases ax ;f. xb. So there is basis for induction. Assume
the result for any string whose length is less than 11. Let x be any string of length
11. We prove that ax ;f. xb through proof by contradiction. Suppose ax =xb. As
a is the first symbol on the L.H.S., the first symbol of x is a. As b is the last
symbol on R.H.S., the last symbol of x is b. So, we can write x as ayb with
Iy I=n - 2. This means aayb =aybb which implies ay =yb. This contradicts
the induction hypothesis. Thus, ax * xb. By induction the result is true for all
strings.
2.6
SUPPLEMENTARY
EXAMPLES
EXAMPLE 2.27
In a survey of 600 people, it was found that:
250 read the Week
260 read the Reader's Digest
260 read the Frontline
90 read both Week and Frontline
110 read both Week and Reader's Digest
80 read both Reader's Digest and Frontline
30 read all the three magazines.
(a) Find the number of people who read at least one of the three
magazmes.
(b) Find the number of people who read none of these magazines.
(c) Find the number of people who read exactly one magazine.
Solution
Let W, R, F denote the set of people who read Week, Reader's Digest
and Frontline, respectively. We use the Venn diagram to represent these sets (see
Fig. 2.17).
Fig. 2.17
Venn diagram for Example 2.27.
= 260.
IF I = 260,
I W n F 1 = 90,
IR n F 1 = 80,
I W n R n F I = 30
Chapter 2: Mathematical Preliminaries
~
63
It is given that:
IWI = 250,
IRI
IW n Ri = 1l0,
··IWuRuFI
= IWI + IRI + IFj -I W n FI -
!W n RI -
IR n FI + IW n R n FI
= 250 + 260 + 260 - 90 - 110 - 80 + 30 = 520
So the solution for (a) is 520.
(b) The number of people who read none of the magazines
=600 - 520 =80
Using the data, we fill up the various regions of the Venn diagram.
(c) The number of people who read only one magazine
= 80 + 100 + 120 = 300
EXAMPLE 2.28
Prove that (A u B u C)' = (A u B)' n
(A u C)'
Solution
(A u B u C) = (A u A) u B u
C
=A u
(A u B) u
C = (A u
B) u
(A u
C)
Hence (A u B u C)' = (A u B)' n
(A u C)'
(by DeMorgan's law)
EXAMPLE 2.29
Define aRb if b = ak for some positive integer k; a, bE;:. Show that R is
a partial ordering.
(A relation is a pattial ordering if it is reflexive,
antisymmetric and transitive.)
Solution
As a =a 1, we have aRa. To prove that R is anti5ynunetric, we have to prove
that aRb and bRa ~ a = b. As aRb, we have b = ak . As bRa, we have a = hi.
Hence a =hi =(ak)i =(/I. This is possible only when one of the following holds
good:
(i)
a =
1
(ii)
a =-1
(iii)
kl =
1
In case (i), b = ak = 1. So a = b.
In case (ii), a =-1 and so kl is odd. This implies that both k and I are
odd. So
b = ak = (-1i = -1 = a
In case (iii), kl = 1. As k and I are positive integers, k = I = 1. So
b =ak =a.
64
~
Theory ofComputer Science
If aRb and bRc, then b = c/ and c = bl for some k, l. Therefore, c :::: aiel.
Hence aRc.
EXAMPLE 2.30
Suppose A = {1, 2, ..., 9} and - relation on A x A is defined by (m, n) -
(p, q) if m + q = n + p, then prove that - is an equivalence relation.
Solution
(m, n) -
(m, n) since m + 11 = n + m. So - is retlexive. If (m, n) - (P, q),
then m + q =
11 + p; thus p + 11 = q + m. Hence (P, q) -
(m, n). So - is
symmenic.
If (m, n) -
(P, q) and (P, q) -
(I', s), then
Adding these,
In+q=n+p
and
p+s=q+r
That is,
m+q+p+s=n+p+q+r
l11+s=n+r
which proves (m, n) - (1', s).
Hence - is an equivalence relation.
EXAMPLE 2.31
If f : A ---t Band g : B ---t C are one-to-one, prove that g of is one-to-one.
Solution
Let us assume that g of(aJ = g of(a2)' Then, g(f(al») :::: g(f(a2)' As g is
one-to-one, fear) = f(a2)' As f is one-to-one, al = a2' Hence g of is one-to-
one.
EXAMPLE -2.32
Show that a connected graph G with n vertices and
11 -
1 edges (n ~ 3) has
at least one leaf.
Solution
G has n vertices and n - 1 edges. Every edge is counted twice while computing
the degree of each of its end vertices. Hence
L deg(v) = 2(n -
1)
where sunill1ation is taken over all vertices of G.
So, L deg(v) is the sum of II positive integers. If deg(v)
?::: 2 for every
vertex v of G, then
211 :; L deg(v) = 2(n -
1)
which is not possible.
Hence deg(v) = 1 for at least one vertex l' of G and this vertex v is a leaf.
Chapter 2: Mathematicai Preiiminaries
~
65
EXAMPLE 2.33
Prove Property 5 stated in Section 2.2.2.
Solution
We prove the result by induction on n. Obviously, there is basis for induction.
Assume the result for connected graphs with n -
1 vertices. Let T be a
connected graph with II vertices and J1 -
1 edges. By Example 2.32, T has at
least one leaf v (say).
Drop the vertex '.' and the (single) edge incident \vith v. The resulting graph
Of is still connected and has
11 -
1 vertices and n
2 edges. By induction
hypothesis. Of is a tree. So 0' has no circuits and hence 0 also has no circuits.
(Addition of the edge incident with v does not create a circuit in G.) Hence G
is a tree. By the principle of induction, the property is true for all n.
EXAMPLE 2.34
A person climbs a staircase by climbing either (i) two steps in a single stride
or (ii) only one step in a single stride. Find a fonnula for Sen), where Sen)
denotes the number of ways of climbing n stairs.
Solution
When there is a single stair. there is only one way of climbing up. Hence
S(l) = 1. For climbing two stairs, there are t\'iO ways. viz. two steps in a single
stride or two single steps. So 5(2) :::: 2. In reaching n steps, the person can climb
either one step or two steps in his last stride. For these two choices, the number
of 'ways are sen -
1) and Sen - 2).
So,
Sen) :::: Sen -
1) + Sen -
2)
Thus. Sen) = F(n). the nth Fibonacci number (refer to Exercise 2.20.
at the end of this chapter).
EXAMPLE 2.35
How many subsets does the set {I, 2, .... n} have that contain no two
consecutive integers?
Solution
Let Sn denote the number of subsets of (1. 2, ..., n} having the desired
property. If n = 1. S] = I{ 0, {lr = 2. If 11 = 2, then
S~ = i{ 0, {l}. {2}! :::: 3.
Consider a set A with n elements. If a subset having the desired property
'ontains
n, it cannot contain n -
1. So there are Sn-~ suchmbsets. If it does
not contain n. there are Sn-l such subsets. So S" = Sn-J + Sn-~' As S\ = 2 = F3
and
S~ = 3 = Fl,.
the (n + 2)th Fibonacci numbeL
66
~
Theory ofComputer Science
EXAMPLE 2.36
If n
~ 1, show that
1·1 r + 2 -2! + ... + n· n! = (n + I)! - 1
Solution
We prove the result by induction on n. If n = L then 1· I! = 1 = (1 + I)! - I.
So there is basis for induction.
Assume the result for n, i.e.
1 . I! + 2·2! + . . . + n· n r = (n + I)! -
1
Then,
I-I! + 2-2! + ". + n·n! + (71 + 1)·(n + I)!
=(n+1)!
1+(n+1)·(n+1)!
= (n + 1)! (l + 11 + 1) -
1 = (n + 2)! -
1
Hence the result is true for 71 + 1 and by the plinciple of induction, the
result is true for all n ~ 1.
EXAMPLE 2.37
Using induction, prove that 21/ < n! for all n
~ 4.
Solution
For 11 = 4. 24 < 4!. So there is basis for induction. Assume 21/ < n!.
Then.
2"+1 = 2" - 2 < n! . 2 < (n + l)n! = (n + I)!
By induction, the result is true for a]] n
~ 4.
SELF-TEST
(c) B
Choose the correct answer to Questions 1-10:
1. (A u A) n
(B n
B) is
(a) A
(b) A n
B
(d) none of these
2. The reflexive-transitive closure of the relation {(1, 2), (2, 3)} is
(a) {(1, 2), (2, 3), (1, 3)}
(b)
{(1, 2), (2. 3), (1, 3), (3, I)}
(c) {(l, 1). (2, 2), (3, 3), O. 3), 0, 2), (2, 3)}
(d) {(1, 1). (2, 2), (3, 3). (1, 3)}
3. There exists a function
f: {I. 2, , ... 1O} ~ {2. 3,4.5,6.7,9, 10. 11, 12}
Chapter 2: Mathematical Preliminaries
);l,
67
-------------'---------
which is
(a) one-to-one and onto
(b) one-to-one but not onto
(c) onto but not one-to-one
(d) none of these
4. A tree with 10 vertices has
(a) 10 edges
(b) 9 edges
(el 8 edges
(d) 7 edges.
5. The number of binary trees \vith 7 vertices is
(a) 7
(b) 6
(c) 2
(d) 1
6. Let N = {I, 2, 3, ... }. Then f: IV -7 N defined by j(n) = n + 1 is
(a) onto but not one-to-one
(b) one-ta-one but not onto
(c) both one-to-one and onto
(d) neither one-ta-one nor onto
7. QST is a substring of
(s) PQRST
(b) QRSTU
(c) QSPQSTUT
(d) QQSSTT
8. If x = 01, y = 101 and:: = OIL then xy::y is
(a) 01011011
(b) 01101101011
(c) 01011101101
(d) 01101011101
9. A binary tree with seven vertices has
(a) one leaf
(b) two leaves
(c) three leaves
(d) four leaves
10. A binary operation
0 on N = {L 2, 3....} is defined by a
0 b =
a + 2b. Then:
(a)
0 is commutative
(b)
0 is associative
(c) N has an identity element with respect to 0
(d) none of these
EXERCISES
2.1 If A = {a, b} and B = {b, c}, find:
(a) (A u
B)*
(b) (A n
B)*
(c) A * u
B*
68
);1
Theory ofComputer Science
(d) A * (\ B*
(e) (A - B)*
(f) (B - A)*
2.2 Let S = {a, b} *. For x, \' E
S, define x 0 y = xy, i.e. x 0 .y is obtained
by concatenating x and y.
(a) Is 5 closed under a?
(b) Is 0 associative?
(c) Does 5 have the identity element with respect to 0'1
(d) Is 0 conunutative?
2.3 LeI 5 = i\ where X is any nonempty set. For A,
B
<;:; X, let
A
0 B = A u B.
(a) Is 0 commutative and associative?
(b) Does 5 have the identity element with respect to 0'1
(c) If A 0 B = A 0 C. does it imply that B = C?
2.4 Test whether the following statements are tme or false. Justify your
answer.
(a) The set of all odd integers is a monoid under multiplication.
(b) The set of all complex numbers is a group under multiplication.
(c) The set of all integers under the operation
0 given by a
0 b =
a + b - ab is a monoid.
(d) 2s under symmenic difference V defined by A VB = (A - B) u
(B - A) is an abelian group.
2.5 Show that the following relations are equivalence relations:
(a) On a set 5. aRb if a = b.
(b) On the set of all lines in the plane, 1]RI: if IJ is parallel to I:.
(c) On N = {O, 1, 2, ...}. mRn if m differs from n by a multiple
of 3.
2.6 Show that the follO\ving are not equivalence relations:
(a) On a set 5, aRb if a i= b.
(b) On the set of lines in the plane, I]RI: if 11 is perpendicular to 12,
(e) On N = {O, L 2
}. mRn if m divides n.
(d) On 5 = {L 2
, 1O}. aRb if a + b = 10.
2.7 For x, y in {a, b} *, define a relation R by xRy if Ix I=Iy I. Show that
R is an equivalence relation. What are the equivalence classes?
2.8 For x, \' in {a, b}*, define a relation R by xRy if x is a substring of
y (x is a substring of y if y = ZjXZ: for some string Zj, z:). Is R an
equivalence relation?
2.9 Let R = {(L 2). (2. 3). n, 4), (4. 2), (3. 4)}
Find R+, R*.
2.10 Find R* for the following relations:
Ca) R =
{(l~ 1).
(l~
2)~
(2~
1)~ (2, 3), (3. 2)}
(b) R = {(l. 1). (2. 3), 0, 4), (3. 2)}
Chapter 2: Mathematical Preliminaries
~
69
(c) R = {(L 1), (2, 2), (3, 3), (4, 4)}
(d) R = {(1, 2), (2. 3), (3, 1), (4, 4)}
2.11 If R is an equivalence relation on S, what can you say about R+. R*?
2.12 Letf:{a, b} * ----:> {a, b} * be given by f(x) = ax for every x E {a, b}*.
Show that f is one-to-one but not onto.
2.13 Let g: {a, b}*
----:> {a, b}* be given by g(x) = xT. Show that g is
one-to-one and onto.
2.14 Give an example of (a) a tree \vith six vertices and (b) a binary tree
with seven vertices.
2.15 For the tree T given in Fig. 2.18, ansv,er the following questions:
(a) Is T a binary tree?
(b) Which vertices are the leaves of T?
(c) How many internal vertices are in T?
(d) \Vnat is the height of T?
(e) Wnat is the left-to-right ordering of leaves?
(f) Which vertex is the father of 5?
(g) Which vertices are the sons of 3?
Fig. 2.18
The tree for Exercise 2.15.
2.16 In a get-together. show that the number of persons who know an odd
number of persons is even,
[Hi/){: Use a graph.]
2.17 If X is a finite set show that !2XI = ?!Xj
2.18 Prove the following by the principle of induction:
(a) i
k2 = n(n + li2n + 1)
k=l
(b)
11
1
I
-k-(k-+-l)
k=l
n
(17 + 1)
(c) 10211 -
1 is divisible by 11 for all II > 1.
70
~
Theory of Computer Science
2.19 Prove the following by the principle of induction:
(a)
I + 4 + 7 + ... + (3n _ 2) =
n(3n - 1)
2
(b) 2" > n for all n > 1
(c) If f(2) = 2 and f(2 k ) = 2f(2k- 1) + 3. then f(2 k ) = (5/2) . 2k -
3.
2.20 The Fibonacci numbers are defined in the following way:
F(O) = 1.
F(l) = 1.
F(n + 1) = F(n) + F(n -
1)
Prove by induction that:
n
(a) F(2n + 1) = L F(2k)
k=O
II
(b) F(2n + 2) = L F(2k + 1) + 1
k=l
2.21 Show that the maximum number of edges in a simple graph (i.e. a
graph having no self-loops or parallel edges) is
n(n2- 1) .
2.22 If W E {a, b} * satisfies the relation abw =wab, show that IwI is even.
2.23 Suppose there are an infinite number of envelopes arranged one after
another and each envelope contains the instruction 'open the next
envelope'. If a person opens an envelope, he has to then follow the
instruction contained therein. Show that if a person opens the first
envelope. he has to open all the envelopes.
The Theory of
Automata
In this chapter we begin with the study of automaton. We deal with transition
systems which are more general than finite automata. We define the
acceptability of strings by finite automata and prove that nondeterministic finite
automata have the same capability as the deterministic automata as far as
acceptability is concerned. Besides. we discuss the equivalence of Mealy and
Moore models. Finally, in the last section. we give an algorithm to construct a
minimum state automaton equivalent to a given finite automaton.
3.1
DEFINITION OF AN AUTOMATON
We shall give the most general definition of an automaton and later modify
it to computer applications. An automaton is defined as a system where
energy, materials and information are transformed. transmitted and used for
performing some functions without direct participation of man. Examples are
automatic machine tools, automatic packing machines, and automatic photo
printing machines.
In computer science the term 'automaton' means 'discrete automaton' and
is defined in a more abstract way as shown in Fig. 3.1.
/1
'j
Automaton
°1
/2
°2
-----~
/p
01,02' ... , On
Oq
Fig. 3.1
Model of a discrete automaton.
71
72
li\
Theory ofComputer Science._---------------
The characteristics of automaton are now desClibed.
(i) Input.
At each of the discrete instants of time tb t2, ...• tm the input
. values Ii' 12..... fp • each of which can take a finite number of fixed
values from the input alphabet
~, are applied to the input side of the
model shown in Fig. 3.l.
iii) Output.
a"
0:;., ..., Oq are the outputs of the model, each of which.
can take a finite number of fixed values from an output O.
(iii) States.
At any instant of time the automaton can be in onenf the
states
ql'
q:;., ..., qJl'
(iv) State relation.
The next state of an automaton at any instant of time
is determined by the present state and the present input.
(v) Output relation.
The output is related to either state only or to both
the input and the state. It should be noted that at any instant of time
the automaton is in some state. On 'reading' an input symbol, the
automaton moves to a next state which is given by the state relation.
Note:
An automaton in which the output depends only on the input is called
an automaton without a memory. An automaton in which the output depends
on the states as well. is called automaton with a finite memory. An automaton
in which the output depends only on the states of the machine is called a
Moore machine. An automaton in. which the output depends on the state as
well as on the input at any instant of time is called a Mealy machine.
EXAMPLE 3.1
Consider the simple shift register shown in Fig. 3.2 as a finite-state machine
and study its operation.
. ID
0 11--,t1>II~
QI.
liD
0,H,D
--,Ol--s-eria-I
7:~~~
rU
~_
ootpol
I
I
I
.
i
Fig. 3.2
A 4-bit serial shift register using D flip-flops.
Solution
The shift register (Fig. 3.2) can have 2+ = 16 states (0000. 0001, .... 1111).
and one serial input and one serial output. The input alphabet is ~ = {O, I}.
and the output alphabet is 0 = {O. I}. This 4-bit selial shift register can be
further represented as in Fig. 3.3.
Chapter 3: The Theory of Automata
&;!
73
I
i
1
1
q1' q2' '
~
I
Q15' Q16'
Fig. 3.3
.A shift register as a finite-state machine,
From the operation. it is clear that the output will depend upon both the input
and the state and so it is a Mealy machine.
In general. any sequential machine behaviour can be represented by an
automaton.
3.2
DESCRIPTION OF A FINITE AUTOMATON
Definition 3.1
Analytically. a finite automaton can be represented by a
5-tuple (Q, E. 0. qo. F). where
(i) Q is a finite nonempty set of states.
(ii) I
is a finite nonempty set of inputs called the input alphabet.
(iii)
(5 is a function which maps Qx I into Q and is usually called the direct
transition function. This is the function which describes the change of
states during the transition. This mapping is usually represented by a
transition table or a transition diagram.
(iv)
qo E Q is the initial state.
(v) F
<;:;;; Q is the set of final states. It is assumed here that there may be
more than one final state.
Note:
The transition function \vhich maps Q x I* into Q (i.e. maps a state
and a string of input symbols including the empty stling into a state) is called
the indirect transition function. We shall use the same symbol (5 to represent
both types of transition functions and the difference can be easily identified
by the nature of mapping (symbol or a string), i.e. by the argument. (5 is also
called the next state function. The above model can be represented graphically by
Fig. 3.4.
D
c
r-------------,t String being processerJ
I
I
I
[
I I I
S
Ilt~~~t
n
--
1- ""dieg h,,,
i Finite
!control
Fig. 3.4
Block diagram of a finite automaton.
74
~
Theory ofComputer Science
Figure 3.4 is the block diagram for a finite automaton. The various
components are explained as follows:
(i) Input tape.
The input tape is divided into squares, each square
containing a single symbol from the input alphabet L. The end squares
of the tape contain the
endmarker ¢ at the left end and the end-
marker $ at the right end. The absence of endmarkers i~clLcates11iat
the tape is of infinite length. The left-to-right sequence of symbols
between the two endmarkers is the input string to be processed.
(ii) Reading head.
The head examines only one square at a time and can
move one square either to the left or to the right. For further analysis,
we restrict the movement of the R-head only to the right side.
(iii) Finite control.
The input to the finite control will usually be the
symbol under the R-head, say a, and the present state of the machine,
say q, to give the following outputs: (a) A motion of R-head along
the tape to the next square (in some a null move, i.e. the R-head
remaining to the same square is permitted); (b) the next state of the
finite state machine given by b(q, a).
3.3
TRANSITION SYSTEMS
A transition graph or a transition system is a finite directed labelled graph in
which each vertex (or node) represents a state and the directed edges indicate
the transition of a state and the edges are labelled with inputJoutput.
A typical transition system is shown in Fig. 3.5. In the figure, the initial
state is represented by a circle with an arrow pointing towards it, the final state
by two concentric circles, and the other states are represented by just a circle.
The edges are labelled by input/output (e.g. by 1/0 or 1/1). For example, if the
system is in the state qo and the input 1 is applied, the system moves to state
qJ as there is a directed edge from qo to qJ with label 1/0. It outputs O.
0/0
1/0
111
010
Fig. 3.5
A transition system,
We now give the (analytical) definition of a transition system.
Defmition 3.2
A transition system is a 5-tuple (Q, L, 0, Qo, F), where
(i) Q. Land F are the finite nonempty set of states, the input alphabet,
and the set of final states, respectively, as in the case of finite automata;
(ii) Qo ~ Q. and Qo is nonempty: and
(iii) 0 is a finite subset of Q x L* x Q.
Chapter 3: The Theory ofAutomata
);1
75
In other words, if (qj, w. q'C) is in 8. it means that the graph starts at the
vertex
(jj,
goes along a set of edges, and reaches the vertex
(j2' The
concatenation of the label of all the edges thus encountered is w.
DefInition 3.3 .A transition system accepts a string w in I* if
(i) there exists
a path which originates from some initial state, goes
along the arrows, and terminates at some final state; and
/
(ii) the path value obtained by concatenation of alJ edge-labels of the path
is equal to H'.
Example 3.2
Consider the transition system given in Fig. 3.6.
1/0
Fig. 3.6
Transition system for Example 3.2.
Determine the initial states. the final states. and the acceptability of 101011,
111010.
Solution
The initial states are qo and q!. There is only one final state, namely ({3'
The path-value of (joQOq:q3 is 101011. As (j3 is the final state, 101011 is
accepted by the transition system. But. 111010 is not accepted by the transition
system as there is no path with path value 111010.
Note: Every finite automaton (Q, I, 8, qo, F) can be vie\ved as a transition
system (Q, I, 8/ Qo, F) if we take Qo = {Qo} and 8/ = {(q, w, 8(q, w»lq E
Q,
W
E I*}. But a transition system need not be a finite automaton. For
example, a transition system may contain more than one initial state.
3.4
PROPERTIES OF TRANSITION FUNCTIONS
Property 1
8{Q, A) =(j is a finite automaton. This means that the state of the
system can be changed only by an input symbol.
76
~
Theory ofComputer Science
Property 2
For all stlings wand input symbols a,
O(q, aw)= O(O(q, a),
IV)
O(q, .va) = O(O(q, 'v), a)
This property gives the state after the automaton consumes or reads the
first symbol of a string aw and the state after the automaton consumes a
of the string .va.
EXAMPLE 3.3
Prove that for any transition function 0 and for any two input strings x and y,
O(q, xy) = o(O(q, x), y)
Proof
By the method of induction on Iy I, i.e. length of y.
Basis: When I)' I = 1, y = a E 1:
L.R.S. of (3.1) = O(q, xa)
= O(O(q, x), a)
by Property 2
= R.H.S. of (3.1)
(3.1)
Assume the result. l.e. (3,1) for all stlings x and strings)' with IY I = n. Let
y be a stling of length n + 1. Write v = )'la where i)'1 I =
11.
L.H.S. of (3.1) = O(q, ,'--:i'Ia) = 8(q, xIa),
Xl = XYI
= O(O(q,
Xl), a)
by Property 2
= O(O(q, XYIl, a)
= O(O(O(q, x), Yj), a)
by induction hypothesis
R.H.S. of (3.1) = O(O(q, x), Yla)
= O(O(&q, x),
)'1), a)
by Property 2
Hence, L.H.S. = R.H.S. This proves (3.1) for any string y of length 11 + 1.
By the principle of induction. (3.1) is true for all strings.
I
EXAMPLE 3.4
Prove that if O(q, x) = O(q, y), then O(q, xz) = O(q, yz) for all strings
~: in 1:+.
Solution
O(q, xz) = O(O(q, x), z)
= o( o(q, y),
;:)
by Example 3.3
(3.2)
By Example 3.3.
O(q, yz) = O(O(q, y), z)
= O(q, xz)
(3.3)
Chapter 3: The Theory of Automata
iii,
77
3.5
ACCEPTABILITY OF A STRING BY A FINITE
AUTOMATON
Definition 3.4
A stling x is accepted by a finite automaton
M = (Q, .L 8, C/o' F)
if D(qo, x) =q for some q E F.
This is basically the acceptability of a string by the final state.
Note: A final state is also called an accepting state.
EXAMPLE 3.5
Consider the finite state machine whose u'unsition function 0 is given by Table 3.1
in the form of a transition table. Here, Q = {CI(), qi.
q2> q3}, L = {O, I},
F = {qo}. Give the entire sequence of states for the input string 110001.
TABLE 3.1
State
Transition Function Table for Example 3.5
Input
o
Solution
(--:\
-7
~/
q,
q2
q3
Hence.
l
l
8{qo, 110101) = D(C/I.IOIOI)
I..v
= 0(% 0101)
l
= D(q~, 101)
l
= 8(q3,Ol)
l
= D(q], 1)
= 8(qo, ..1\)
J
1
a
1
a
j
qo ~ qj ~ qo ~ q~ ~ q3 ~ qj ~ qo
The symbol l indicates that the current input symbol is being processed by the
machine.
78
~
Theory ofComputer Science
3.6
NONDETERMINISTIC FINITE STATE MACHINES
We explain the concept of nondeterministic finite automaton using a transition
diagram (Fig. 3.7).
o
1
~)~
O----7
/
"'-..,
/1
"tr
Fig. 3.7
Transition system representing nondeterministic automaton.
If the automaton is in a state {qa} and the input symbol is 0, what wii! be
the next state? From the figure it is clear that the next state will be either {qo}
or {qj}. Thus some moves of the machine cannot be determined uniquely by
the
input
symbol
and
the
present
state.
Such
machines
are
called
nondeterministic automata. the formal definition of which is now given.
Definition 3.5
A nondeterministic finite automaton (NDFA) is a 5-tuple
(Q, L. 0, qo, F), where
(i) Q is a finite nonempty set of states;
(ii) L is a finite nonempty set of inputs;
(iii) 0 is the transition function mapping from Q x L into 2Q which is the
power set of Q, the set of all subsets of Q;
(iv) qo E Q is the initial state; and
(v) F ~ Q is the set of final states.
We note that the difference between the deterministic and nondeterministic
automata is only in 0. For deterministic automaton (DFA), the outcome is a
state, i.e. an element of Q; for nondeterministic automaton the outcome is a
subset of Q.
Consider, for example, the nondeterministic automaton whose transition
diagram is described by Fig. 3.8.
The sequence of states for the input string 0100 is given in Fig. 3.9. Hence,
8(qo, 0100) = {qo, q3' q4}
Since q4 is an accepting state. the input string 0100 will be accepted by
the nondeterministic automaton.
o
Chapter 3: The Theory ofAutomata
,g
79
oo
o
o
Fig. 3.8
Transition system for a nondeterministic automaton.
0
• qo
0
~ qo
0
• qo
qo
~ qo
~
\
~
\0
\
\
\
q3
q1
q3
q3
\
q4
Fig. 3.9
States reached while processing 0100.
DefInition 3.6
A string W E L* is accepted by NDFA At if o(q(j, w) contains
some final state.
Note: As At is nondetenninistic, O(q(j, 'v) may have more than one state. So
w is accepted by At if a final state is one among the possible states that At can
reach on application of w.
We can visualize the working of an NDFA M as follows: Suppose At
reaches a state q and reads an input symbol a. If 8(q, a) has n elements,
the automaton splits into n identical copies of itself: each copy pursuing
one choice determined by an element of 8(q,
a). This type of parallel
computation continues. When a copy encounters (q, a) for which 8(q, a) = 0,
this copy of the machine 'dies'; however the computation is pursued by the
other copies. If anyone of the copies of At reaches a final state after
processing the entire input string
',1', then we say that At accepts w. Another
way of looking at the computation by an NDFA At is to assign a tree structure
for computing 8(q. w). The root of the tree has the label q. For every input
symbol in ',1', the tree branches itself. When a leaf of the tree has a final state
as lts labeL then At accepts w.
Definition 3.7
The set accepted by an automaton At (deterministic or
nondetenninistic) is the set of all input strings accepted by At. It is denoted by
T(At).
80
9
Theory of Computer Science
-------------------
3.7
THE EQUIVALENCE OF DFA AND NDFA
We naturally try to find the relation between DFA and NDFA. Intuitively we
now feel that:
(i) A DFA can simulate the behaviour of NDFA by increasing the number
of states. (In other words. a DFA (Q, L, 8, qQ, F) can be viewed as an
NDFA (Q, L, 8', qQ, F) by defining 8'(q, a) = {8(q, a)}.)
Oi) Any NDFA is a more general machine without being more powelfuL
We now give a theorem on equivalence of DFA and NDFA.
Theorem 3.1
For every NDFA, there exists a DFA which simulates the
behaviour of NDFA. Alternatively, if L is the set accepted by NDFA, then there
exists a DFA which also accepts L.
Proof
Let M = (Q, L, 8, qQ, F) be an NDFA accepting L. We construct a DFA
M' as:
M' = (Q', L, 8, (/o, F')
where
(i) Q' = 2Q (any state in Q' is denoted by [qio q2, .... q;], where qio q2,
.... qj E
Q):
(ii) q'0 = [clo]; and
(iii) r
is the set of all subsets of Q containing an element of F.
Before defining 8'. let us look at the construction of Q', q'o and r. M is
initially at qo. But on application of an input symbol, say a, M can reach any
of the states 8(qo, a). To describe M, just after the application of the input
symbol a. we require all the possible states that M can reach after the
application of a. So, lvI' has to remember all these possible states at any instant
of time. Hence the states of M' are defined as subsets of Q. As M starts with
the initial state qo, q'o is defined as [qo]. A string w belongs to T(M) if a final
state is one of the possible states that M reaches on processing w. So, a final
state in M' (i.e. an element of F') is any subset of Q containing some final
state of M.
Now we can define 8':
(iv) 8'([qb q2. .... qi], a) = 8(q[, a) u
8(q2, a) u '"
U
8(qi' a).
Equivalently.
if and only if
8({ql' ..., q;}, a) = {PI, P2• ..., Pi}'
Before proving L = T(M'), we prove an auxiliary result
8'(q'o, x) = [qj, .. '. q;],
if and only if 8(qo, x) = {qj, ""
q;} for all x in P.
We prove by induction on Ix I. the 'if part. i.e.
8'(q'o. x) = [qJ' q2. , ... q;]
if 8(qo. x) = {qJ..... q;}.
(3.4)
(3.5)
Chapter 3: The Theory ofAutomata
);;I,
81
When Ixl = O.
o(qo, A) = {qoL and by definition of
o~
[/(q6. A) =
qa =[qol So. (3.5) is true for x with Ixi ::: O. Thus there is basis for induction.
Assume that (3.5) is true for all strings y with Iy I S; m. Let x be a
string of length In + 1. We can write x as va, where 'y' = 111 and a E L. Let
o(qo, 1') = {p, ..., Pi} and o(qo. ya) = {rl' r2' .... rd· As Iyi S;
In, by
Induction hypothesis we have
O'(qo· y) = [PI, .... p;]
(3.6)
Also.
{rl'
r~ .... r;} = O(qo, ya) = O(o(qo,
Y). a) ::: O({PI, .... Pj}, a)
By definition of
o~
(3.7)
Hence.
O'(q(;. yay = o/(o/(q'o. y), a) = O'([PI . ..., Pi], a)
by (3.6)
::: [rl' .... rd
by (3.7)
Thus we have proved (3.5) for x = .va.
By induction. (3.5) is true for all strings x. The other part (i.e. the 'only if'
part), can be proved similarly, and so (3.4) is established.
Now. x
E
T(M) if and only if O(q. x) contains a state of F. By (3.4).
8(qo, x) contains a state of F if and only if 8'(qo, x) is in F'. Hence. x E T(M)
if and only if x E T(Af'). This proves that DFA M' accepts L.
I
Vote: In the construction of a deterministic finite automaton M 1 equivalent to
a given nondetelministic automaton M, the only difficult part is the construction
of 8/ for M j • By definition.
k
8'([q1 ... qd, a) = U 8(q;, a)
l=l
So we have to apply 8 to (q;. a) for each i = 1. 2..... k and take their
union to get o'([q] .. , qd. a).
When 8 for /1'1 is given in telms of a state table. the construction is simpler.
8(q;, a) is given by the row corresponding to q; and the column corresponding
to a. To construct 8'([ql ... qk]. a), consider the states appearing in the rows
corresponding to qj
, ql;o and the column corresponding to a. These states
constitute 8'([q]
qd. a).
Note: We write 8' as 8 itself when there is no ambiguity. We also mark the
initial state with ~ and the final state with a circle in the state table.
EXAMPLE 3.6
Construct a deterministic automaton equivalent to
M
::: ({qo.
qd· {O. l}. 8, qo,
{qoD
where 8 is defined by its state table (see Table 3.2).
82
g
Theory ofComputer Science
TABLE 3.2
State Table for Example 3,6
State/L
o
~@
qo
q,
__--'qC-'-'
q--'i
qo, ql
Solution
For the deterministic automaton MI.
(i) the states are subsets of {qo. Ql}, i.e. 0. [qo], [c/o' q;], [q;];
(ii) [qoJ is the initial state;
(iii)
[qo] and [qo, q;] are the final states as these are the only states
containing qo; and
(iv) 8 is defined by the state table given by Table 3.3.
TABLE 3.3
State Table of M, for Example 3,6
State/'L
o
[qo]
[q,]
[qo, q,]
o
o
[qo]
[q,]
[qo, q,]
o
[q,]
[qo, q,]
[qo, q,]
The states qo and ql appear in the rows corresponding to qo and qj and the
column corresponding to O. So. 8([qo. qt], 0) = [qo. qd·
When M has
11 states. the corresponding finite automaton has 2" states.
However. we need not construct 8 for all these 2" states, but only for those
states that are reachable from [Qo]. This is because our interest is only in
constructing M 1 accepting T(M). So, we start the construction of 8 for [qo]. We
continue by considering only the states appearing earlier under the input
columns and constructing 8 for such states. We halt when no more new states
appear under the input columns.
EXAMPLE 3.7
Find a detefIIljnistic acceptor equivalent to
M = ({qo. qIo q:J. {a. hI. 8. qo. {q2})
where 8 is as given by Table 3.4.
TABLE 3.4
State Table for Example 3.7
State/'L
a
b
~qo
qo. q,
q2
q,
qo
q,
@
qo. q,
Chapter 3: The Theory of Automata
~
83
Solution
The detenninisticautomaton M 1 equivalent to M is defined as follows:
M) = (2 Q,
{a, b}, 8. [q01 F')
where
F = {[cd· [qo, Q2J, [Ql' q2], [qo· %
q2]}
We start the construction by considering [Qo] first. We get [q2] and [qo. q:]. Then
we construct 8 for [q2] and [qo. q1]. [q), q2J is a new state appearing under the
input columns. After constructing 8 for [ql' q2], we do not get any new states
and so we terminate the construction of 8. The state table is given by Table 3.5.
TABLE 3.5
State Table of M, for Example 3.7
EXAMPLE 3.8
State/I
[001
[02]
[00, 0']
[a,. 02]
a
[00. 01]
o
[00. a,]
[00]
b
[02]
[00, a,]
[a"
02]
[00, 01]
Construct a deterministic finite automaton equivalent to
M = ({qCh qj, 12, q:}, {a, I}), 8. qo, {q3})
where /5 is given by Table 3.6.
TABLE 3.6
State Table for Example 3.8
State/I
a
b
---'>00
00. a'
00
a,
02
a,
02
03
03
.'C0
02
'.:.::.)
Solution
Let Q == {qo, q1' q:, q:,}. Then the deterministic automaton M1 equivalent to
M is given by
M I = (2 Q• {a. b}. 8, [qo], F)
where F consists of:
[q3]. [qo· q3].
[q1- q3j· [q2' q3]. [qo. qj. Q3], [qo. q2' q3].
[Q]o Q2' Q3]
and
[Qo· Q!. q2. q3]
and where 15 is defined by the state table given by Table 3.7.
84
!;'
Theory ofcomputer Science
TABLE 3.7
State Table of Mi for Example 3.8
State/I
a
-_..._--_._------
b
[qo, Cli]
[qc.
q1
q2]
[qJ
Clil
[qol
[qo
q,. q2l
[qo. q,]
[qQ. q-,. Q2. q3]
[qo. q,. q3]
[qo. qi. q3]
[qo. q1
Cl2]
[qo. Q,. q2]
[qc•. qi. q2. q3]
[qc. q,
Cl2
q3]
[qo. qi. q2.{f31
---------------- --------------'---
3.8
MEALY AND
MOORE MODELS
3.8.1
FINITE AUTOMATA WITH OUTPUTS
The finite automata \vhich we considered in the earlier sections have binary
output i.e. either they accept the string or they do not accept the string. This
acceptability \vas decided on the basis of reachahility of the final state by the
initial state. Now. we remove this restriction and consider the mode1 where the
outputs can be chosen from some other alphabet. The value of the output
function Z(t) in the most general case is a function of the present state q(t) and
the present input xU), i.e.
Z(n :::;
where Ie is called the output function. This generalized model is usua!Jy called
the !'v!ca!l' machine. If the output function Z(t) depends only on the present state
and is independent of the cunent input. the output function may be written as
Z(t) :::;
This restricted model is called the j'vJoore machine. It is more convenient to use
Moore machine in automata theory. We now give the most general definitIons
of these machines.
Definition 3.8
A Moore machine is a six-tuple (Q. L, ~. 8, I,. (]ol. where
(il Q is a finite set of states:
I is the input alphabet:
(iii)
~ is the output alphabet:
8 is the transition function I
x Q into Q:
I, is the output function mapping Q into ~; and
(]o is the initial state.
Definition 3.9
/'I. Ivlealy machine is a six-tuple (Q, I,
~, 8. )e, qo). "vhere
all the symbols except Ie have the same meaning as in the Moore machine. A.
is the output function mapping I
x Q into ,1,
For example. Table 3.8 desclibes a Moore machme. The initial state iJo is
marked with an arrow. The table defines 8 ane! A...
Chapter 3: The Theory of Automata
,\;l
85
TABLE 3.8
A Moore Machine
Present state
Next state
()
a=O
a=1
Output
~qc
q,
q2
q3
o
1
oo
For the input string 0111. the transition of states is given by qo -7 Cf" -7
qo -7 qt -7 q2' TIle output string is 00010. For the input string A, the output
is A(qo) = O.
Transition Table .3.9 describes a Mealy machine.
TABLE 3.9
A Mealy Machine
Present state
Next state
a
~ 0
state
output
q3
0
q"
1
q:;
1
q4
state
a =
output
o
o
1
o
Note:
For the input stling 0011. the transition of states is given by qi -7 q"
.~ q: -7 qj, -7 q~. and the output string is 0100. In the case of a Mealy machine.
'Nt get an output only on the application of an input symbol. So for the input
string A the output is only A It may be observed that in the case of a Moore
machine. we get ),(qo) for the input string A.
Remark
A finite automaton can be converted into a Moore machine by
introducing 2, = {O. I} and defining
)~(q) = 1 if q E
F and
)~(q) = 0 if
q if. F.
For a Moore machine if the input string is of length 11, the output string
is of length
11 + 1. The first output is Ic(qo) for all output strings. In the case
of a 1'1ealy machine if the input string is of length n. the output string is also
of the same length II.
3.8.2
PROCEDURE
FOR TRANSFORMING
A MEALY MACHINE
INTO A MOORE
MACHINE
VJ~ develop procedures for transforming a Mealy machine into a Moore
machine and vice versa so that for a given input string the output strings are the
same (except for the first symbol) in both the machines.
86
&;l
Theory ofComputer Science
EXAMPLE 3.9
Consider the Mealv machine described
Table 3.10. Construct a Moore machine
machine.
by the transItIOn table given by
which is equivalent to the Mealy
----- ------
- -
Present
state
TABLE 3.10
Mealy Machine of Example 3.9
Next
state
Input a = 0
state
output
Input a =
state
output
Solution
o
1
1
1
o
o
1
o
At the first stage we develop the procedure so that both machines accept
exactly the same set of input sequences. Vve look into the next state column
for any state, say q/. and determine the number of different outputs associated
with q; in that column.
We split qi into several different states, the number of such states being
equal to the number of different outputs associated with q,. For example, in this
problem. ql is associated with one output 1 and q: is associated with two
different outputs 0 and 1. Similarly, q3 and '14 are associated with the outputs
o and O. 1. respectively. So. we split q: into q-:w and <721' Similarly, q4 is split
into q.11) and '141' Now Table 3.10 can be reconstructed for the new states as
given by Table 3.11.
TABLE 3.11
State Table for Example 3.9
Present state
Next state
Input a = 0
Input a = 1
state
output
state
output
-"q,
qa
0
q20
0
Q20
q,
q40
0
q21
q,
Q40
0
qa
q2
q,
1
q4G
q4,
qa
0
q41
q4,
qa
0
The pair of states and outputs in the next state column can be rearranged as
given by Table 3.12.
Chapter 3: The Theory of Automata
g
87
TABLE 3.12
Revised State Table for Example 3.9
Present state
a = 0
Next
state
a = 1
Output
--+q.,
q3
G2C
1
q20
q"
q40
0
q2',
q,
Q4C
1
q3
q21
q,
0
Q4CJ
q41
q3
~
q41
q4'
q3
1
Table 3.12 gives the Moore machine. Here we observe that the initial state
ql is associated with output 1. This means that with input A we get an output
of 1, if the machine starts at state ql' Thus this Moore machine accepts a zero-
length sequence (null sequence) which is not accepted by the Mealy machine.
To overcome this situation, either we must neglect the response of a Moore
machine to input A. or we must add a new starting state qQ, whose state
transitions are identical with those of q\ but whose output is O. So Table 3.12
is transformed to Table 3.13.
TABLE 3.13
fv100re Machine of Example 3.9
Present state
a = 0
Next
state
a = 1
Output
-'tqo
q3
q2C
q,
q3
q2C
q2C
q,
Q40
q21
q,
q4C
q3
q21
01
q4J
q41
q3
q41
Q4'
q2,
o
1
o
o
o
1
From the foregoing procedure it is clear that if we have an m-output, 11-
state Mealy machine. the corresponding m-output Moore machine has no more
than
11111 + 1 states.
3.8.3
PROCEDURE FOR TRANSFORMING A MOORE
MACHINE
INTO A MEALY MACHINE
We modify the acceptability of input string by a Moore machine by neglecting
the response of the Moore machine to input A. We thus define that Mealy
Machine M and Moore Machine 11;1' are equivalent if for all input strings lV,
b7 ,t\,w) =Zw(w}. where b is the output of the Moore machine for its initial
state. We give the following result: Let AIL = (Q, L, ll, 8, A.. qo) be a Moore
machine. Then the following procedure may be adopted to construct an
equivalent Mealy machine Mo.
88
Theory of Computer Science
Construction
(i) We have to define the output function;: for the Mealy machine as
a function of the present state and the input symbol. We define;: by
X('1, a) = }eCoCq, a»
for all states '1 and input symbols a.
(ii) The transition function
IS the :..;ame as that of the gIven Moore
machine.
EXAMPLE 3.l0
Construct a Mealy Machine which is equivalent to the Moore machine given
by Table 3.14.
TABLE 3.14
Moore Machine of Example 3.10
Present state
-;qo
q,
q2
q3
Solution
a = 0
Next
state
a = 1
Output
o
1
oo
We must follow the reverse procedure of converting a Mealy machine into a
Moore machine. In the case of the Moore machine, for every input symbol we
form the pair consisting of the next state and the corresponding output and
reconstruct the table for the Mealy Machine. For example, the states CJ3 and '11
in the next state column should be associated with outputs 0 and I, respectively.
The transition table for the Mealy machine is given by Table 3.15.
TABLE 3.15
Mealy Machine of Example 3.10
Present
state
Next
state
a = 0
a =
state
output
state
output
-;qc
q3
0
q,
1
q
q,
1
q2
0
q2
q2
0
q3
0
q3
q3
0
qo
0
Note:
We can reduce the number of states in any model by considering states
with identical transitions. If two states have identical transitions (i.e. the rows
cOlTesponding to these two states are identical), then we can delete one of them.
EXAMPLE 3.11
Consider the Moore machine desclibed by the transition table given by
Table 3.16. Construct the corresponding Mealy rnachine.
Chapter 3: The Theory of Automata
~
89
-~--
TABLE 3,16
Moore Machine of Examp!e 3.11
Present state
Solution
a = 0
Next
state
a = 1
Output
oo
1
We construct the transition table as in Table 3,17 by associating the output
\vith the transitions.
In Table 3.17. the rows cOlTesponding to '1: and '1." are identical. So. we can
delete one of the two states. i.e. if: or {h. We delete if.". Table 3,18 gives the
reconstructed table.
TABLE 3,17
Transition Table for Example 3.11
Present state
Next
state
a = 0
a = 1
state
output
state
output
-:;.Gi
q:
0
q2
0
q2
q"
0
q3
1
q3
q"
0
q3
1
TABLE 3,18
Mealy rJlachine of Example 3.11
Present
state
Next
state
a = 0
a =
--- ,,------------_...-
In Table 3.18. we have deleted the '1.,,·1'ow and replaced '10, by '12 in the other
rows.
-
~-- - .
~
-~
-
EXAMPLE 3.12
Consider a Mealy machine represented by Fig. 3.10, Construct a Moore
machine equivalent to this Mealy machine.
90
~
Theory ofComputer Science
1/Z2
Fig. 3.10
Mealy machine of Example 3.12.
Solution
Let us convert the transition diagram into the transition Table 3.19. For the
given problem: qJ is not associated with any output; q2 is associated with two
different outputs Zj and 2 2; (]3 is associated with two different outputs 2 1 and
2 2, Thus we must split q2 into q21 and (]22 with outputs 2 1 and Z2, respectively
and q3 into (]31 and q32 with outputs 2 1 and 2 2, respectively. Table 3.19 may be
reconstructed as Table 3.20.
TABLE 3.19
Transition Table for Example 3.12
Present
state
Next
state
a = 0
a =
state
output
state
output
q2
Z,
q3
Z,
q2
Z2
q3
Z-I
q2
Z,
q3
Z2
TABLE 3.20
Transition Table of Moore Machine for Example 3.12
Present state
--,>q,
q21
q22
q31
q32
a =0
Next
state
a =1
Output
Figure 3.11 gives the transition diagram of the required Moore machine.
Chapter 3: The Theory of Automata
~
91
o
Fig. 3.11
Moore machine of Example 3.12.
3.9
MINIMIZATION OF FINITE AUTOMATA
In this section we construct an automaton with the minimum number of states
equivalent to a given automaton M.
As our interest lies only in strings accepted by i\!I, what really matters is
whether a state is a final state or not. We define some relations in Q.
DefInition 3.10
Two states ql and q: are equivalent (denoted by qj == q:) if
both o(qj. x) and O(q:. x) are final states. or both of them are nonfinal states
for all x E 2:*.
As it is difficult to construct O(qj, x) and O(q:, x) for all x E 2:* (there
are an infinite number of stlings in 2:*). we give one more definition.
Definition 3.11
Two states qj and q: are k-equivalem (k
;::: 0) if both
O(qj, x) and O(q:. ;r) are final states or both nonfinal states for all strings x
of length k or less. In particular, any two final states are O-equivalent and any
t\VO nonfinal states are also O-equivalent.
We mention some of the properties of these relations.
Property 1
The relations we have defined. i.e. equivalence and k-equivalence,
are equivalence relations. i.e. they are reflexive, symmetric and transitive.
Property 2
By Theorem 2.1. these induce partitions of Q. These partitions
can be denoted by Jr and Jrl' respectively. The elements of Jrl are k-equivalence
classes.
Property 3
If qj and q: are k-equivalent for all k ;::: O. then they are equivalent.
Property 4
If ql and q: are (k + I)-equivalent. then they are k-equivalent.
Property 5
ir" = Jr,,+] for some 11. (Jr" denotes the set of equivalence classes
under l1-equivalence.)
The following result is the key to the construction of minimum state
automaton.
RESULT
Two states qj and q: are
(k + I)-equivalent if (i) they are
k-equivalent; (ii) O(q!. a) and O(q:, a) are also k-equivalent for every a E 2:.
92
~
Theory ofComputer Science
Proof
We prove the result by contradiction. Suppose qj and q2 are not
(k + I)-equivalent. Then there exists a string W =aWl of length k + 1 such that
8(ql. aWj) is a final state and 8(q'b aWl) is not a final state (or vice versa; the
proof is similar). So 8(8(qb a}. wI) is a final state and 8(8(q:, a), WI) is not
a final state. As
WI is a string of length k,
8(q), a) and 8(q:, a) are not
k-equivalent. This is a contradiction. and hence the result is proved.
I
Using the previous result we can construct the (k + I)-equivalence classes
once the k-equivalence classes are known.
3.9.1
CONSTRUCTION OF MINIMUM AUTOMATON
Step 1
(Construction of no)· By definition of O-equivalence, no ={Q?, Qf }
where Q? is the set of all final states and Qf =Q - Q?
Step 2
(Construction of lrk+i from lr;:). Let Q/' be any subset in lrk' If q) and
q2 are in Q/'. they are (k + I)-equivalent provided 8(% a) and 8(q']) a) are
k-equivalent. Find out whether 8(qj, a) and 8(q2' a) are in the same equivalence
class in
~k for every a E L. If so. q) and q: are (k + I)-equivalent. In this way,
Q/' is further divided into (k + I)-equivalence classes. Repeat this for every Q/'
in lrk to get all the elements of lrk+)'
Step 3
Construct lr" for
11 = 1. 2, .... until lr" = lrn+!·
Step 4
(Construction of minimum automaton). For the required minimum
state automaton. the states are the equivalence classes obtained in step 3. i.e. the
elements of !rl!' The state table is obtained by replacing a state q by the
conesponding equivalence class [q].
Remark In the above construction, the crucial part is the construction of
equivalence classes; for. after getting the equivalence classes, the table for
minimum automaton is obtained by replacing states by the conesponding
equivalence classes. The number of equivalence classes is less than or equal to
IQ I· Consider an equivalence class [qd = {qj, q:, ..., qd· If qj is reached
while processing WIW: E
T(M) with 8(qo, Wj) = qb then 8(Ql. w:) E F. So,
8(q;, w:) E F for i =2...., k. Thus we see that qj. i =2, .. " k is reached on
processing some IV E T(M) iff qj is reached on processing w, i.e. ql of [qd can
play the role of q:. ..., qk' The above argument explains why we replace a state
by the conesponding equivalence class.
Note:
The construction of no, lrj, lr:, etc. is easy when the transition table
is given.
Jl1J = {QF. Q:o}. where Q? = F and Q:o = Q - F. The subsets in
lrl are obtained by further partitioning the subsets of no· If qj, q:
E Q?,
consider the states in each a-column. where a E l: conesponding to qj and q2'
If they are in the same subset of 71';), q) and q: are I-equivalent. If the states
under some a-column are in different subsets of 7i'Q. then ql and q: are not
I-equivalent. In generaL (k + 1)-equivalent states are obtained by applying the
above method for ql and q: in rj/'.
o
o
0
Chapter 3: The Theory ofAutomata
,!;l
93
EXAMPLE 3.13
Construct a minimum state automaton equivalent to the finite automaton
described by Fig. 3.12.
Fig. 3.12
Finite automaton of Example 3.13.
Solution
It will be easier if we construct the transition table as shown in Table 3.21.
TABLE 3.21
Transition Table for Example 3.13
StatelI.
-'tqo
q1
®
q3
q4
qs
q6
q7
By applying step 1, we get
Q? = F = {Q21,
So.
o
94
);2
Theory ofComputer Science
The {cd in 7ru cannot be further partitioned. SO, Q\ = {q::}. Consider qo and
qj
E Qt The entries under the O-column corresponding to qo and qj are q]
and qr,: they lie in Q!/ The entries under the I-column are qs and q::. q:: E
Qp and qs E Q::o. Therefore. qo and q\ are not i-equivalent. Similarly, qo is
not i-equivalent to q3' qs and q7'
Now, consider qo and q4' The entries under the O-column are qj and q7'
Both are in Q::o. The entries under the I-column are qs, qs· So q4 and qo are
I-equivalent. Similarly, qo is I-equivalent to q6' {qo. q4, q6} is a subset in n\.
SO, Q':: = {qo, q4' q(,}.
Repeat the construction by considering qj and anyone of the states
q3, qs· Q7' Now, qj is not I-equivalent to qj or qs but I-equivalent to q7' Hence,
Q'3 = {qj, q7}' The elements left over in Q::o are q3 and qs. By considering the
entries under the O-column and the I-column, we see that q3 and q) are
l-equivalent. So Q/4 = {qj, qs}' Therefore,
ITj = {{q:;}. {qo· q4' q6}. {qj. q7}, {q3. qs}}
The {q::} is also in n:: as it cannot be further partitioned. Now, the entries
under the O-column corresponding to qo and q4 are qj and q7' and these lie
in the same equivalence class in nj. The entries under the I-column are qs,
qs. So qo and q4 are 2-eqnivalent. But qo and q6 are not 2-equivalent. Hence.
{qo.
Cf4, qd is partitioned into {qo, qd and {qd· qj and q7 are 2-equivalent.
q3 and qs are also 2-equivalent. Thus. n:: = {{q::L {qo, q4}, {Q6}, {Qj, Q7L
{qj, qs}}. qo and q4 are 3-equivalent. The qj and Q7 are 3-equivalent. Also.
q3 and qs are 3-equivalent. Therefore.
nj = {{qJ,
{qo, q4},
{q6}, {qj, q7L
{Q3, qs}}
As n: = nj. n2 gives us the equivalence classes, the minimum state automaton
is
M' = (Q" {O. I}. 8" qo, Fj
where
Q' = {[q2J. [qo, q4J, [q6]. [qt- Q7], [Q3' Qs]}
qo = [qo, Q4]'
F = [cd
and 8/ is defined by Table 3.22.
TABLE 3.22
Transition Table of Minimum State
Automaton for Example 3.13
State/I.
0
[qo, q4]
[0', OJ]
[03. q5]
[Q1, OJ]
[q6]
[q2]
[02]
[00' 04]
[q2]
(Q3· q5]
[q2]
[06]
[06]
[q6]
[qo, q4]
Chapter 3: The Theorv of Automata
~
95
-----------------"----
Note: The transition diagram for the minimum state automaton is described
by Fig. 3.13. The states qo and q.', are identified and treated as one state.
(SO also are ql, q7 and q3' qs) But the transitions in both the diagrams (i.e.
Figs. 3.12 and 3.13) are the same. If there is an arrow from qi to (jj with label
a. then there is an arrow from
[q;J to lq;] with the same label in the
diagram for minimum state automaton. Symbolically, if O(qi' a) = 'ii' then
8'([q;], a) = [q;J.
1
I---~o-__A
7
[q2]
o
o
!'\
Fig. 3.13
Minimum state automaton of Example 3,13,
EXAMPLE 3.14
Construct the minimum state automaton equivalent to the transition diagram
given by Fig. 3.14.
b
a
-G-
b 0- ' ~
!'~
I~
b
t\
I
\
I
\
I
I
I
'
\
aI \
'I :'
I'
b
I r
\;1
,
I
~
\ ,
~
~
\~
0-
--\:~)
~--(i))b
b
Fig. 3.14
Finite automaton of Example 3,14,
Solution
We construct the transition table as given by Table ,3.23.
--- --------------
96
};l,
Theory ofComputer Science
TABLE 3.23
Transition Table for Example 3.14
State/I.
a
b
-7 qa
q1
qa
q1
qa
q2
q2
q3
q1
®
q3
qa
q4
q3
C15
q5
qe
q4
qe
q5
qe
q7
qe
q3
Since there is only one final state q3' QIO= {q3}, Ql = Q - Qt Hence,
1ro = {{q3}, {qo, ql' q2' q4> %
q6, q7}}' As {q3l cannot be partitioned further,
Q'I = {q3}' Now qo is I-equivalent to ql' q5, qIY but not to q2, q4, q7' and so
Q'). ={qo, ql> q5' q6}' q). is I-equivalent to Q4' Hence, Q'3 ={Q2> Q4}' The only
element remaining in Q).o is Q7' Therefore, Q4 = {Q7}' Thus,
Jrl = {{Q3},
{qo,
qh qs, Q6},
{QJ., q4}, {Q7}}
Q12 = {Q3}
qo is 2-equivalent to q6 but not to ql or Qs. So,
Qi = {Qo, Q6}
As ql is 2-equivalent to qs,
As q). is 2-equivalent to q4,
Ql = {q)., q4},
Thus,
Jr2 = {{q3}' {qo, qd, {ql> qs}, {q)., q4}, {q7}}
Q? = {q3}
As qo is 3-equivalent to q6,
As qj is 3-equivalent to qs,
As q2 is 3-equivalent to Q4,
Ql = {q)., Q4},
Q{ = {Q7}
Therefore,
As Jr3 = Jr)., Jr2 gives us the equivalence classes. the minimum state automaton
is
M' = (Q',{a, b}, 8', q'o, F')
Chapter 3: The Theorv of Automata
i;l
97
where
Q' = {[cI3], [qo, Q6], [ql' qs], [q:> q4], [q7]}
q'o = [qo,
qd,
r = [q3]
and 8' is defined by Table 3.24.
TABLE 3.24
Transition Table of Minimum State
Automaton for Example 3,14
State/I.
a
b
[qQ, q6]
[q1' q5]
[qQ, q6]
[q1' q5]
[qQ, q6]
[q2, q4]
[q2, q4]
[q31
[q1, q5]
[q3]
[q3]
[qQ' q6]
[q7]
[qQ, q6]
[q3]
Note:
The transition diagram for lVI' is given by Fig. 3.15.
b
b
Fig. 3.15
Minimum state automaton of Example 3,14,
3.10
SUPPLEMENTARY
EXAMPLES
EXAMPLE' 3.15
Construct a DFA equivalent to the l\i'DFA M whose transition diagram is given
by Fig. 3.16.
a, b
a, 0
q1
I
b
I
~ j'
q2
a
Fig. 3.16
NDFA of Example 3,15
98
Q
Theory ofComputer Science
Solution
The transition table of M is given by Table 3.25.
TABLE 3.25
Transition Table for Example 3.15
State
a
b
For the equivalent DFA:
(i) The states are subsets of Q = {qo, CJl, CJ2, Q3, Q4}'
(ii)
[qo] is the initial state.
(iii) The subsets of Q containing Q3 or Q4 are the final states.
(iv) 8 is defined by Table 3.26. We start from [qoJ and construct 8, only
for those states reachable from [qoJ (as in Example 3.8).
TABLE 3.26
Transition Table of DFA for
Example 3.15
State
[qQ]
[qQ. qzl
[qQ. q4]
a
[qQ]
[qo. q4]
[qo]
b
[qQ, q2]
[qQ- qzJ
[qQ. q2]
EXAMPLE 3.16
Construct a DFA equivalent to an NDFA whose transition table is defined by
Table 3.27.
TABLE 3.27
Transition Table of NDFA for
Example 3.16
State
a
b
Solution
Let i'v! be the DFA defined by
M = C{'!"·()I·'i2· Q3}, {a, b}, 0, [gal F)
~- -~-~-- ----------~~-
Chapter 3: The Theory ofAutomata
~
99
where F is the set of all subsets of {qo, qj' Q2' (/3} containing CJJ. <5 is defined
by Table 3.28.
TABLE 3.28
Transition Table of DFA for
Example 3.16
State
[qo]
[q,. q3]
[qd
[q3]
[Q2. q3]
[q2]
o
a
[q1. q3]
[q,]
[QJ
o
[q3]
[q3]
o
b
[Q2. q3]
[q3]
[Q3]
o
EXAMPLE 3.17
Construct a DFA accepting all strings Hover {O, I} such that the numher of
l' s in H' is 3 mod 4.
Solution
Let ;\J be the required NDFA. As the condition on strings of T(M) does not
at all involve 0. we can assume that ;\J does not change state on input O. If 1
appears in H' (4k + 3) times. M can come back to the initial state. after reading
4 l's and to a final state after reading 31's.
The required DFA is given by Fig. 3.17.
Fig. 3.17
DFA of Example 3.17.
EXAMPLE 3.18
Construct a DFA accepting all strings over {a. h} ending in abo
Solution
We require t\\/O transitions for accepting the string abo If the symbol b is
processed after an or ba. then also we end in abo So we can have states for
100
~
Theory of Computer Science
remembering aa, ab, ba, bb. The state corresponding to ab can be the final
state in our DFA. Keeping these in mind we construct the required DFA. Its
transition diagram is described by Fig. 3.18.
b
a
Fig. 3.18
DFA of Example 3.18.
a
EXAMPLE 3.19
Find TCM) for the DFA M described by Fig. 3.19.
b
Fig. 3.19
DFA of Example 3.19.
Solution
r(M) = {H' E
{a, b}* 1vI' ends in the substring ab}
Note: If we apply the minimization algorithm to the DFA in Example 3.18,
we get the DFA as in Example 3.19. (The student is advised to check.)
EXAMPLE 3.20
Construct a mimmum state automaton equivalent to an automaton whose
transition table is defined by Table 3.29.
Chapter 3: The Theory ofAutomata
);J
101
TABLE 3.29
DFA of Example 3.20
State
-'>qo
q,
q2
q3
q4
®
a
b
Solution
Qp = {q)}. QJ = {qo, %
q2' Q3, q-d. So, Jro = {{q)}. {qo, qi' q2' q3' q4}}
Q? cannot be partitioned further. So {q)} E
JrI' Consider Qt qo is equivalent
to Ql' q2 and q4' But qo is not equivalent to q3 since 6(qo, b) = q2 and
6(q3' b) = {qsJ·
Hence.
Ql _
{
}
1 -
Cf).
Therefore,
JrI = {{q)}, {qo. qi. q2. q4}' {q3}}
qo is 2-equivalent to q4 but not 2-equivalent to qJ or Q2'
Hence.
{qo, q4}
E
Jro
ql and q2 are not 2-equivalent.
Therefore.
Jr2 = {{q)}. {q3}, {qo. q4}, {qd. {qJ}
As qo is not 3-equivalent to q4, {qo. q4} is further partitioned into {qr} and
{Cf4}'
So.
Tr3 = {{qo}, {qd.
{Cf2}, {q3},
{Cf4}, {qs}}
Hence the minimum state automaton M' is the same as the given M.
EXAMPLE 3.21
Construct a minimum state automaton equivalent to a DFA whose transition
table is defined by Table 3.30.
TABLE 3.30
DFA of Example 3.21
State
-'>qo
q,
q2
®
®
q5
qs
q7
a
b
102
~
Theory orComputer Science
Solution
Q? = {q3'
(r~},
Q~ = {qo. ql' q2' q), q6' q7}
1fo = {{Q3, q4}, {qo. qj, Q2, qs, q6' q7}}
q3
IS I-equivalent to q4' So, {q3' q4}
E
1f1'
qo is not I-equivalent to ql, q2, Cis but q() is I-equivalent to Q6'
Hence {q(). qd
E
1f1. ql is I-equivalent to q2 but not I-equivalent to
qs, q6 or q7' So, {Ql'
C!2}
E
1f1'
qs is not I-equivalent to q6 but to q7' So, {Cis, q7}
E
1fJ
Hence,
Jrl = {{q3' CJ4}' {q(),
qe,}, {ql' q2}, {q5' (j7}}
q3 is 2-equivalent to q4' So, {q3, q4}
E
Jr2'
qo is not 2-equivalent to Q6' So. {qo}·
{CJ6}
E
Jr2'
qJ is 2-equivalent to q2' So. {qj, Q:J
E
Jr~.
qs is 2-equivalent to q7. So, {qs, q7}
E
Jr2'
Hence.
q3 is 3-equivalent to q4; qj is 3-equivalent to q2 and Cis is 3-equivalent to qi'
Hence.
As
Jr3 = Jr2' the minimum state automaton is
where 8/ is defined by Table 3.31.
TABLE 3.31
Transition Table of OFA for
Example 3.21
Slate
a
b
[qo]
[q1. q2]
[q1' q2]
[q1
q2]
[q3, q4]
[Q3, q4]
[q3, q4]
[qs, q7]
[qs]
[qs. q7]
[q3. q4]
[qs]
[qs]
[qs]
[qs]
Chapter 3: The Theory ofAutomata
g
103
SELF-TEST
Study the automaton given in Fig. 3.20 and choose the correct answers
to Questions 1-5:
-iGJpo
1,/-\
/
\
oc£5
1
.8Jo
Fig. 3.20
Automaton for Questions 1-5
1. M is a
(a) nondeterministic automaton
(b) deterministic automaton accepting {O. I} *'
(c) deterministic automaton accepting all strings over {O, I} having
3m O's and 3n ]'s, m.
11 2 1
(d) detelministic automaton
2. M accepts
(a) 01110
(b) 10001
(e) 01010
(d) 11111
3. T(M) is equal to
(a)
{03111 1311 1m. 11 2 O}
(b)
{O'm 1311 Im. n 2:: I}
(e)
{1\ IH' has III as a substring}
(d)
{H IH' has 31i l·s.
11 2 I}
4. If q2 is also made a final state. then At accepts
(a) 01110 and 01100
(b) 10001 and 10000
(c) 0110 but not 0111101
(d) 0311• 11 2:: 1 but not 1
311• 11 2:: 1
5. If q: is also made a final state, then T(M) is equal to
(a)
{03111 13
/1 I In,
11 2 O} u
{02111 1" I m,
11 2 O}
(b) {03m 13n I m.
Ii 2 I} u
{O:'" I" I m.
11 2:: I}
(c)
{H' IH' has III as a substring or 11 as a substring}
(d) {w I the number of l's in
]V is divisible by 2 or 3}
Study the automaton given in Fig. 3.21 and state whether the Statements
6-15 are true or false:
0.1
0,1
@1---O-'~0
Fig. 3.21
Automaton for Statements 6-15
104
~
Theory ofComputer Science
6. M is a nondeterministic automaton.
7. 8(ql' 1) is defined.
8. 0100111 is accepted by M.
9. 010101010 is not accepted by M.
10. 8(qo. 01001) = {qd·
11. 8(qo, 011000) = {qo, qj, q2}'
12. 8(q2' 11') = q2 for any string W E
{O, 1}*.
13. 8(q!, 11001) :t 0.
14. T(M) = {w I w = xOOy, where x, y E
{O, 1}*}.
15. A string having an even number of O's is accepted by M.
EXERCISES
3.1 For the finite state machine M des.cribed by Table 3.1, find the strings
among the following strings which are accepted by M: (a) 101101,
(b) 11111, (c) 000000.
3.2 For the transition system M described by Fig. 3.8, obtain the sequence
of states for the input sequence 000101. Also. find an input sequence not
accepted by M.
3.3 Test whether 110011 and 110110 are accepted by the transition system
described by Fig. 3.6.
3.4 Let M = (Q, L. 8, qo, F) be a finite automaton. Let R be a relation in
Q defined by q j Rq2 if 8(qj, a) = 8(% a) for all a
E
L. Is R an
equivalence relation'?
3.5 Construct a nondeterministic finite automaton accepting {ab, ba}, and
use it to find a deterministic automaton accepting the same set.
3.6 Construct a nondeterministic finite automaton accepting the set of all
strings over {G, b} ending in aba. Use it to construct a DFA accepting the
same set of strings.
3.7 The transition table of a nondeterministic finite automaton M is defined
by Table 3.32. Construct a deterministic finite automaton equivalent to M.
TABLE 3.32
Transition Table for Exercise 3.7
State
0
2
---7qo
q,q4
q4
q2q3
ql
q4
q2
q2q3
®
q4
q4
Chapter 3: The Theory ofAutomata
~
105
3.8 Construct a DFA equivalent to the NDFA described by Fig. 3.8.
3.9 M = ({qb q2' q3},
{O, I}, 8, ql'
{q3}) is a nondeterministic finite
automaton. where 0 is given by
o(Qj. 0) = {q2' q3},
o(qj, 1) = {qd
O(q2> 0) = {qlo Q2},
O(Q2' 1) = 0
O(q3' 0) = {Q2},
O(Q3'
1) = {Q1'
CJ2}
Construct an equivalent DFA.
3.10 Construct a transition system which can accept strings over the alphabet
a, b. .., containing either cat or rat.
3.11 Construct a Mealy machine which is equivalent to the Moore machine
defined by Table 3.33.
TABLE 3.33
Moore Machine of Exercise 3.11
Present state
a = 0
Next
state
a = 1
Output
1
o
3.12 Construct a Moore machine equivalent to the Mealy machine M defined
by Table 3.34.
TABLE 3.34
Mealy Machine of Exercise 3.12
Present state
Next
state
a = 0
a =
state
output
state
output
-'7q1
q,
q2
0
q2
q4
q4
1
q3
q2
q3
1
q4
q3
0
q,
1
3.13 Construct a Mealy machine which can output EVEN, ODD according
as the total number of l's encountered is even or odd. The input
symbols are 0 and 1.
3J 4 Construct a minimum state automaton equivalent to a given automaton
M \vhose transition table is defined by Table 3.35.
106
~
Theory ofComputer Science
TABLE 3.35
Finite Automaton of Exercise 3.14
State
Input
a
b
-}qo
qo
q3
q,
q2
q5
q2
q3
q4
q3
qo
q5
q4
qo
q6
q5
q1
q4
@
q1
q3
3.15 Construct a minimum state automaton equivalent to the DFA described
by Fig. 3.18. Compare it with the DFA described by Fig. 3.19.
3.16 Construct a minimum state automaton equivalent to the DFA described
by Fig. 3.22.
o
o
~
I-----.~---_....
tJ
0.1
Fig. 3.22
DFA of Exercise 3.16.
4
Formal Languages
In this chapter \ve introduce the concepts of grammars and formal languages
and discuss the Chomsky classification of languages. We also study the
inclusion relation between the four classes of languages. Finally. we discuss the
closure properties of these classes under the variuus operations.
4.1
BASIC DEFINITIONS AND EXAMPLES
The theory of formal languages is an area with a number of applications in
computer science. Linguists were trying in the early 1950s to define precisely
valid sentences and give structural descriptions of sentences. They wanted to
define a fomlal grammar (i.e. to describe the rules of grammar in a rigorous
mathematical way) to describe English. They tbought that such a desCliption of
natural languages (the languages that we use in everyday life such as English,
Hindi. French, etc.) would make language translation using computers easy. It
was Noam Chomsky who gave a mathematical model of a grammar in 1956.
Although it was not useful for describing natural languages such as English, it
turned alit to be useful for computer languages. In fact. the Backus-Naur form
used to describe ALGOL followed the definition of grammar (a context-free
grammar) given by Chomsky.
Before giving the definition of grammar, we shall study, for the sake of
simplicity. two types of sentences in English with a view to formalising the
construction of these sentences. The sentences we consider are those \vith a
nOL"; and a verb, or those with a noun-verb and adverb (such as 'Ram ate
quickly' or 'Sam ran'). The sentence 'Ram ate quickly' has the words 'Ram',
'ate', 'quickly' written in that order, If we replace 'Ram' by 'Sam', 'Tom',
'Gita', etc. i.e. by any noun, 'ate' by 'ran', 'walked', etc. i.e, by any verb in the
107
<adverb>
\
,
108
~
Theory of Compurer Science
past tense, and 'quickly' by 'slowly', l.e. by any adverb. we get other
grammatically correct sentences. So the structure of 'Ram ate quickly' can be
given as (noun) (verb) (adverb). For (noun) we can substitute 'Ram'. 'Sam',
'Tom'. 'Gita', etc. . Similarly. we can substirure "ate'. 'walked', 'ran', etc. for
(verb). and 'quickly". 'slowly' for (adverb). Similarly, the structure of 'Sam ran'
can be given in the form (noun)
We have to note that (noun) (vdb)
is not a sentence but only the
description of a particular type of sentence. If we replace (noun), (verb) and
(adverb) by suitable \vords, we get actual grammatically correct sentences. Let
us call (noun),
(adverb) as variables. Words like 'Ram', 'Sam', 'ate',
'ran'. 'quickly", 'slowly' which form sentences can be caHed terminals. So our
sentences tum out to be strings of terminals. Let S be a variable denoting a
sentence. Now. we can form the following rules to generate two types of
sentences:
S -+ (noun) (verb)
5 --.+ (noun) (verb)
(noun) -+ Sam
(noun) -+ Ram
(noun) -+ Gita
-+ ran
(verb) -+ ate
(verb) -+ walked
(adverb) -+ slowly
(adverb) -+ quickly
(Each arrow represents a rule meaning that the word on the right side of the
alTOW can replace the word on the left side of the arrow.) Let us denote the
collection of the mles given above by P.
If our vocabulary is thus restricted to 'Ram', 'Sam', 'Gila', 'ate', 'ran"
'walked', 'quickly' and 'slowly', and our sentences are of the fonn (noun)
(verb) (adverb) and (noun) (verb). we can describe the grammar by a 4-tuple
(V\" I, P, S), where
li\ = {(noun). (verb). (adverb)!
I
= {Ram, Sam, Gita. ale. ran. walked, quickly, slowly}
P is the collection of rules described above (the rules may be called
productions),
S ]s the special symbol denoting a sentence.
The sentences are obtained by (i) starting with S. (ii) replacing words
using the productions. and (iii) terminating when a string of terminals is
obtained.
\Vith this background \ve can give the definition of a grmmnar. As
mentioned earlier. this defil1ltion is due to Noam Chomsky.
Chapter 4: Formal Languages
J;;;;\
109
4.1.1
DEFINITION OF A GRAMMAR
Definition 4.1
A phrase-structure grammar (or simply a grammar)
IS
WI, L, P, 5), where
(i) Vv is a finite nonempty set \V'hose elements are called variables,
(ii) L is a finite nonempty set 'whose elements are called terrninals,
VI (', L = 0.
(iv) 5 is a special variable (i.e, an element of Ii,J called the start symboL
and
P is a finite set \vhose elements are a -7 {3. \vhere a and {3 are strings
on \\ u 2:. a has at least one symbol from V
The elements of Pare
called productions or production rules or re\vriting rules.
Note: The set of productions is the kemel of grammars and language
specification. We obsene the following regarding the production rules.
0) Reverse substitution is not permitted. For example, if S -7 AB is a
production, then we can replace S by AB. but we cannot replace AB
by S.
(ii) No inversion operation is permitted. For example. if S -7 AB IS a
production. it is not necessary that AB -7 S is a production.
- -
- - -
-
EXAMPLE 4.1
G = (VI' L
P, S) is a grammar
where
Vy = {(sentence). (noun). (verb). (adverb)}
L = [Ram. Sam. ate, sang. well]
5 = (sentence)
P consists of the follmving productions:
(sentence) -7 (noun) (verb)
(semence) -7 (noun) (verb) (adverb)
(noun) --7 Ram
(noun)
----7 Sam
(verb) -7 ate
(\ erb) -7 sang
(adverb)
----7 well
NOTATION:
(i) If A is any set. then A'" denotes the set of all strings over A.
A+ denotes A. ':' -
{;\}. where ;\ is the empty string.
(ii) A, B. C, A 1, A2• ... denote the variables.
(i ll) a, b, c. ' .. denote the terminals.
(iv) x. y. ;.
H •... denote the strings of terminals.
lY, {3,
Y. ... denote the elements of (t', u D*.
(vi)
":'{J = <\ for any symbol X in V\ u "
11 0
~
Theory ofComputer Science----------------
4.1.2
DERIVATIONS AND THE LANGUAGE GENERATED
BY A GRAMMAR
Productions are used to derive one stling over \/N U L from another string.
We give a formal definition of derivation as follows:
Definition 4.2
If a ~ f3 is a production in a grammar G and y, 8 are any
two strings on
U 2:, then we say that ya8 directly derives yf38 in G (we
"\!fite this as ya8 ~ yf38). This process is called one-step derivation. In
G
particular. if a ---1 f3
is a production. then a ~ f3.
G
Note:
If a is a part of a stling and a ~ f3 is a production. we can replace
a by f3 in that string (without altenng the remaining parts). In this case we
say that the string we started with directly derives the new string.
For example,
G = US}.
{O. I}, {S -t 051. S -t Ol}, 5)
has the production 5 ---1 OS1. So, 5 in 04S14 can be replaced by 051. The
resulting string is 040511". Thus. we have 0451"+ ~ 0"OS114.
G
Note:
~
induces a relation R on IVy U 2:)*. i.e. aRf3 if a ~
[3.
G
G
Defmition 4.3
If a and f3 are strings on \/v U :E, then we say that a derives
~,
'"
f3 if a ~ f3. Here ~ denotes the reflexive-transitive closure of the relation ~
G
G
G
in (Fy
U :E)* (refer to Section 2.1.5).
Note:
We can note in particular that a 7 a. Also, if a 7
f3.
[X '1'= f3, then
there exist strings [Xl- a2, .. "
(tll' where II ;::: 2 such that
(X = (X]
~ a2 ~ a3 . ..
~ all = f3
G
G
G
When a ~ f3 is in n steps. we write a b
f3.
G
G
Consider. for example. G = ({5}, {O. I}. {S -t OSl, 5 -t OIl, 5).
*
As S ~ 051 ~ 02S12
::::? 03S13, we have S ~ 03S1 3. We also have
G
G
G
G
0351 3 ~ 0351 3 (as
(X ~ a).
G
G
Definition 4.4
The language generated by a grammar G (denoted by L(G)) is
defined as {w E :E* IS 7 H}. The elements of L(G) are called sentences.
Stated in another way, L(G) is the set of all terminal strings derived from
the start symbol S.
Definition 4.5
If 5 ,;, ex, then a is called a sentential form. We can note
G
that the elements of L(G) are sentential forms but not vice versa.
Chapter 4: Formal Languages
~
111
Definition 4.6
Gi and G: are equivalent if L(GJ = L(G:).
Remarks on Derivation
1. Any derivation involves the application of productions. When the
number of times we apply productions is one, we write a =? {3; when
.,.
e
it is more than one, \ve \vrite
CY. ~ f3 (Note: a ~ a).
e
G
")
The string generated by the most recent application of production is
called the working string.
3. The derivation of a string is complete when the working string cannot
be modified. If the final string does not contain any variable. it is a
sentence in the language. If the final string contains a variable. it is a
sentential form and in this case the production generator gets 'stuck'.
NOTATION: (i) We \vrite CY. ~ {3 simply as CY. :b {3 if G is clear from the context.
e
(ji) If A
~
CY. is a production where A
E
Vv, then it is called an
A-production.
(iii) If A
~ ai'
A.
~
a:.
.. nA.
~
CY.!/! are A-productions. these
productions are written as A ~ ai i CY.: j ... I am'
We give several examples of grammars and languages generated by them.
EXAMPLE 4.2
If G = ({5}. {a. I}. {5 ~ 051, s ~ A}. S). find L(G).
Solution
As 5 ~ A is a production. S =?
A. So A is in L(G). Also. for all n :::: 1.
G
=? 0"51"
=? 0"1"
G
G
Therefore.
0"1" E L(G) for n :::: a
(Note that in the above derivation, S ~ 051 is applied at every step except
in the last step. In the last step, we apply 5 ~ A). Hence, {O"I" In:::: O} ~ UG).
To show that L(G)
~ {O''1'' i
17
:::: A}. we start with
].V in L(G). The
derivation of It' starts with 5. If S ~ A is applied first. we get A. In this case
].V =A. Othenvise the first production to be applied is 5 ~ 051. At any stage
if we apply 5 ~ A, we get a terminal string. Also. the terminal string is
obtained only by applying 5 ~ A. Thus the derivation of IV is of the foml
l.e.
5 =? 0
1151"
=? 0"1"
G
G
for some n :::: 1
112
~
Theory ofComputer Science
Therefore.
LeG) = {Qlll
l1 /n 2: Q}
EXAMPLE 4.3
If G = ({ 5}, {a}, {5 ----;; 55}, 5), find the language generated by G.
Solution
L(G) = 0. since the only production 5 -> 55 in G has no terminal on the
right-hand side.
EXAMPLE 4.4
Let G =({ S. C}, {a, b}, P, 5), where P consists of 5 ----;; aCa. C ----;; aCa I b.
Find L(G).
Solution
S:=;. aea :=;. aba. So aba E
L(G)
5 :=;. aCa
(by application of 5 ----;; aCa)
b
d'Cd'
(by application of C ----;; aCa (n -
1) times)
:=;. d'bafl
(by application of C ----;; b)
Hence. a"ba"
E LeG), where n :2: 1. Therefore.
{d'ba"ln 2: I} s:: L(G)
As the only S-production is 5 ----;; aCa, this is the first production we have
to apply in the derivation of any terminal string. If we apply C ----;; b. we get aba.
Otherwise we have to apply only C ----;; aCa. either once or several times. So
we get d'Ca" with a single variable C. To get a terminal string we have to
replace C by b. by applying C ----;; b. So any delivation is of the fonn
S b
a"bun with n 2: 1
Therefore.
L(G) s:: {a"bail In 2: ]}
Thus.
L(G) = {(/ba ll i Jl 2: I}
EXERCISE
Construct a grammar G so that UG) = {a"bc/
1I
1 n. m 2: l}.
Remark By applying the com'ention regarding the notation of variables.
terminals and the start symbol. it \vill be clear from the context whether a
symbol denotes a variable or terminal. We can specify a grammar by its
productions alone.
Chapter 4: Formal Languages
~
113
EXAMPLE 4.5
If Gis S ~ as i ItS [ a[ h, find L(G).
Solution
We sho\v that U C) = {a.
b} 7. As
V·le have only two terminals a,
h,
UG) :;;;;; {a. b} *. All productions are S-productions. and so A can be in L(G)
on1\ when S ~ A is a production in the grammar G. Thus.
UG)
:;;;;; {a. h} ':' -
{A} = {a, b} +
To show {Cl, br :;;;;; ICG). consider any string al a: ... ali' where each ai
is either a or h. The first production in the delivation of ClI{l2 ... all is S ~
as or 5 ~ bS according as a] = a or (lj = b. The subsequent productions are
obtained in a similar way. The last production is S ~ a or S ~ b according
as
= a or a" = b. So aja2 ... ali E UG). Thus. we have L(G) = {a, h]+.
EX~RCISE
If G is S ~ as [a, then show that L(G) = {a} +
Some of the following examples illustrate the method of constructing a
grammar G generating a gi ven subset of stlings over E. The difficult P<hrt is the
construction of productions. \Ve try to define the given set by recursion and then
de\clop productions generating the strings in the given subset of E*.
EXAMPLE 4.6
Let L be the set of all pahndromes over {a. h}. Construct a grammar G
generating L.
Solution
S=>b
S => (I,
For constructing a grammar G generating the set of all palindromes. \ve use
the recursive definition (given in Section 2.4) to observe the following:
ii) A is a palindrome.
Iii) a. b are palindromes.
(Jii) If x is a palindrome axo. then bxb are palindromes.
So \\e define P as the set consisting of:
S ~.\
S ~
(f and S ~ b
Oii) S ~ aSa and S ~ hSb
Let G = ({5}
{a. b}, P. S). Then
5 => A,
The. fore.
A. a. h E L(G)
If x is a palindrome of even length, then x =a1a2 .. " ([III {[ill .•• a!, where
"'3"ron
'
's
el"tJ'e~
'1
(), lJ
Tlf1e11 S
=>':'
"
-
(,1"1
a
'I
b\' app'''l'TIa
.......
~
L Ui
L
_d
1
L.
~
•
i.
d
\ U2
. ..
.'1! (Ii;
!1i-l
...
l-1
"..:
<:: _
If
b
S --" aSa or S ~ bSb. Thus. x E L(G).
114
9
Theory ofComputer SCience
If x is a palindrome of odd length, then x = ala:
anca" ... al' where
ai's and c are either a or b. So S :::S al ... anSan
al
=::} x by applying
S --'7 aSa, S --'7 bSb and finally, S --'7 a or S --'7 b. Thus. x E L(G). This proves
L = L(G}.
EXAMPLE 4.7
Construct a grammar generating L = {wnvT I w
EO
{a, b} *}.
Solution
Let G = ({S}, {a, b. c}, P, S), where P is defined as S --'7 aSa I bSb I c. It
is easy to see the idea behind the construction. Any string in L is generated
by recursion as follo\\'s: (i) eEL: (ii) if x E L. then wxwT
EO L. So, as in
the earlier example. we have the productions S --'7 aSa I bSlJ i c.
EXAMPLE 4.8
Find a grammar generating L = {d'lJ"ci I II 2: L i 2: O}.
Solution
L = L 1 u L:
L 1 = {al/b"l II 2: I}
L, = {a"b"ci i II 2: L i 2: I}
We construct L 1 by recursion and L: by concatenating the elements of L 1
and ci• i 2: 1. We define P as the set of the following productions:
S--'7A,
A --'7 ab,
A --'7 aAb,
S --'7 Sc
Let G = ({S. A}, {a. b, c}, P. S). For n 2: L i --'7 0, we have
Thus.
{a"b"c' In 2: L i 2: O}
;;;::; L(G)
To prove the reverse inclusion. we note that the only S-productions
are S --'7 Sc and S --'7 A. If we start with S --'7 A, we have to apply
A
=::} d ,- IAb,,-l :::S al1 b". and so d 1b"co
EO LCG)
If we start \vith S --'7 Sc, we have to apply S --'7 Sc repeatedly to get SCi. But
to get a tenninal string. \ye have to apply S --'7 A. As A :::S a"lJ", the resulting
terminal string is a"b"ci. Thus, we have shown that
L(G)
;;;::; {a"b"ci III 2: L i 2: O}
Therefore.
L(G) = {allb"ci In 2: L i 2: O}
Chapter 4: Formal Languages
~
115
EXAMPLE 4.9
Find a grammar generating {a'b"e"! 11
~ L j
~ O}.
Solution
Let G = ({5, A}, {a. b, e}, P, 5). where P consists of 5 ~ as, 5 ~ A.
A ~ bAe ! be. As in the previous example, we can prove that G is the required
grammar.
EXAMPLE 4.10
Let G =({5. Ad. {O. L 2}. p. 5), where P consists of 5 ~ 05A[2. 5 ~ 012,
2A1 ~ A]2. lA] ~ 11. Show that
L(G) = {0"1"2" I 11
~ I}
Solution
As 5 ~ 012 is a production, we have 5 ::::} 012, i.e. 012
E L(G).
Also.
5 :b 0"-1 5(A12t-1
::::}
0"12(A 12)"-]
::::} 0"IA{'-12"
~ 0"1"2"
Therefore.
by applying 5 ~ 05A]2
(11 -
1) times
by applying 5 ~ 012
by applying 2A] ~ Al2
several times
by applying lA I ~ 11
(11 - 1) times
0"1"2" E UG)
for all 11 ~ 1
To prove that L(G)
<:;::;; {0"1"2"1
11 ~ I}, \ve proceed as follows: If the first
production that we apply is 5 ~ 012, we get 012. Otherwise we have to apply
5 ~ 05A lL once or several times to get 0"-]S(A12t-1. To eliminate 5, we have
to apply 5 ~ 012. Thus we arrive at a sentential form 0" 12(A] 2),,-1. To
eliminate the variable A 1• we have to apply 2A 1 ~ Al2 or lA] ~ 11. Now.
LA 1 ~ A12 interchanges 2 and A l' Only lA 1 ~ 11 eliminates A 1. The sentential
form we have obtained is O"12A}2A 12 ... A 12. If we use lA} ~ 11 before
taking all 2's to the right. \ve wilJ get 12 in the middle of the string. The A. i •s
appearing subsequently cannot be eliminated. So we have to bring all 2's to the
right by applying 24]
~ A]2 several times. Then we can apply 1/\1 ~ 11
repeatedly and get 0" 1" 2" (as derived in the first part of the proof). Thus.
L(G)
<:;::;; {0"1"2"111
~ l}
This shows that
L(G) = {O" 1" 2" In 2: I}
In the next example we constmct a grammar generating
{a"ll'e" i'l1
~ I}
116
~
Theory ofComputer Science
EXAMPLE 4.11
Construct a grammar G generating {a"b"ell i n ~ l}.
Solution
Let L = {a"b"e"l n ~ I}. We try to construct L by recursion. We already know
how to construct a"Y' recursively.
As it is difficult to construct allb"e" recursively, we do it in two stages:
(i) we construct alld' and (ii) we convert d' into bne". For stage (i), we can have
the following productions S ~ aSa Iaa. A natural choice for a (to execute
stage (ii)) is be. But converting (be)" into bile" is not possible as (be)" has no
variables. So we can take a = BC, where Band C are variables. To bring
B's together we introduce CB ~ BC. We introduce some more productions
to convert 8's into b's and Cs into e·s. So we define G as
G = ({ S, B, C}, {a, b, e}, P. S)
where P consists of
S ~ aSBC I aBC, CB ~ BC, aB ~ ab, bB ~ bb, bC ~ be, eC ~ ee
S =? aBC =? abC =? abc
Thus,
abc E UG)
Also,
S ~ o"-1S(BO,,-1
=? a"-laBC(BO,,-1
=?
all-IabBII-IC"
=> a"b"C"
=? allb"-lbeC"-l
=? a"b"e"
Therefore.
by applying S ~ aSBe
by applying S ~ aBC
by applying CB ~ Be
(since CB ~ BC
interchanges Band 0
by applying aB ~ ab
by applying bB ~ bb
by applying bC ~ be
by applying eC ~ cc
(n - 1) times
several times
once
several times
once
several times
L(G)
<;;;; {a"b"c" In ~ I}
To show that {d'b"c"ln ~ I}
<;;;; L(G), it is enough to prove that the only
way to arrive at a terminal string is to proceed as above in deriving a"b"e"
(n
~ 1).
To start with, \ve have to apply only S-production. If we apply S ~ aBC.
first we get abc. Otherwise we have to apply S ~ aSBC once or several times
and get the sentential form a"-1S(BCjJ1-1. At this stage the only production we
can apply is S ~ aBC, and the resulting string is o"(BOII.
Chapter 4: Forma! Languages
J;1
117
-------------------'------~~
In the derivation of a"b"e", we converted all B's into b's and only then
converted Cs into e·s. We show that this is the only way of arriving at a
tenninal string.
a"(BC)il is a string of terminals followed by a string of variables. The
productions we can apply to a"(BC)" are either CB .~ BC or one of aB -'> abo
bB -'> bb. bC -'> be, eC -'> ee. By the application of anyone of these
productions. the we get a sentential form which is a string of terminals followed
by a string of variables. Suppose a C is converted before converting all B's.
Then we have a"(BC)" ~ (/'bicex. where i < 11 and rx is a string of B's and Cs
containing at least one B. In ailbierx, the variables appear only in ex. As c appears
just before rx, the only production we can apply is eC -'> ee. If rx starts with
B, we cannot proceed. Otherwise we apply eC -'> ee repeatedly until we obtain
the string of the form d'biejBrx'. But the only productions involving Bare
aB -'> ab and bB -'> bb. As B is preceded by c in a"biciBrx'. we cannot convert
B, and so we cannot get a terminal string. So L(G)
<;;;; {d'lJ"e/1 11 ~ I}. Thus,
we have proved that
L(G) = {a"b"eli [ 11
~ I}
EXAMPLE 4.12
Construct a grammar G generating {xx I x
E
{a. b} *}.
Solution
We construct G as follows:
G = U5, 51.
5~. 53' A. B}, {a. b}, P, 5)
where P consists of
PI
5 -'>
515~53
P~, P3: 5.5, -'> a5 1A.
1
_
p~, Ps : A53 -'>
S~aS3'
Pfj.. p,
Ps· P9 :
Aa -'> aA,
i'
PlO, P I1
as, -'>
S~a,
PI~' Pl3 :
SIS~ -'> A.
SIS~ -'> bSIB
BS3 -'>
S~lJS3
Ab -'> hA,
Ba -'> aB,
Bb -'> bB
Remarks
The following remarks give us an idea about the construction of
productions P j-P13'
1. PI is the only S-production.
2. Using SjS~ -'> aSIA, we can add tenninal a to the left of 5] and variable
A to the right. A is used to make us remember that we have added the
terminal a to the left of S1' Using AS3 -'> 5~aS3' we add a to the right
of
S~.
118
~
Theory ofComputer Science
3. Using 5152 ~ b51B, we add b to the left of 51 and variable B to the
right. Using B53 ~ 52b53, we add b to the right of 52'
4.
52 acts as a centre-marker.
5. We can add terminals only by using P2-PS'
6. P6-P9 simply interchange symbols. They push A or B to the right. This
enables us to place A or B to the left of 53' (Only then we can apply
P4 or Ps.)
7. 5152 ~ A, S3 ~ A are used to completely eliminate 5j, 5'J 53'
8. PIO, PlJ are used to push 52 to the left. This enables us to get 52 to
the right of 51 (so that we can apply Pd·
Let L = {x.x Ix E {a, b}*}. We first prove that L ~ L(G). Now, we have
5 => 51S2S3 => aSlAS3 => aSlS2aS3
(4.1)
or
S => SI5253 => bSjBS3 => bSIS2b53
(4.2)
Let us start with xx with x
E
{ab}*. We can apply (4.1) or (4.2),
depending on the first symbol of x. If the first two symbols in x are ab (the
other cases are similar), we have
S :b a 51S2aS3 => abSj Ba S3 => abSjaBS3
=> abS1aS2 b53 => ab5152abS3
Repeating the construction for every symbol in x, we get XSlS2xS3' On
application of P12 and P13, we get
5 :b XSlS2XS3 :b xAxA = xx
Thus, L ~ L(G).
To prove that L(G) ~ L, we note that the first three steps in any derivation
of L(G) are given by (4.1) or (4.2). Thus in any derivation (except S ~ A), we
get aSjS2a53 or bS152bS3 as a sentential form.
We can discuss the possible ways of reducing aS IS::a53 (the other case is
similar) to a terminal string. The first production that we can apply to aS152aS3
is one of SIS:: ~ A. 53 ~ A S1S2 ~ aSIA, S1S2 ~ bS1B.
Case 1
We apply 5IS:: ~ A to aSlS::aS3' In this case we get aAa53. As the
productions involving S3 on the left are P4, Ps or PD , we have to apply only
S3 ~ l\. to aa53 and get aa E L.
Case 2
We apply 53 ~ A to a5152aS3' In this case we get a51S2aA. If we
apply S1S2 ~ A, we get aAaA = aa E L; or we can apply S1S2 ~ aSIA to
aSIS2a to get aaSIAa. In the latter case, we can apply only Aa ~ aA to aaSjAa.
The resulting string is aaSlaA which cannot be reduced further.
From Cases 1 and 2 we see that either we have to apply both PI:: and P 13
or neither of them.
Case 3
In this case we apply 51S2 ~ aSjA or 5 j 52 ~ bS1B. If we apply
5 j S2 ~ a5jA to as\S2aS3' we get aaSjAaS3' By the nature of productions we
have to follow only aaSjAa53 => aaSjaAS3 => a2S1aS2aS3 => a2SjS2a2S3' If
Chapter 4: Formal Languages
~
119
we apply 5)52 ---,) b5\B, we get ab5152ab53. Thus the effect of applying
5)52 ---,) a5)A is to add a to the left of 5152 and 53'
If we apply 5)S2 ---,) A, S3 ---,) A (By Cases 1 and 2 we have to apply both)
we get abab.E L. Otherwise, by application of P2 or P3, we add the same
terminal symbol to the left of SIS2 and S3' The resulting string is of the form
x51S2XS3' Ultimately, we have to apply P)2 and P i3 and get xAxA =xx E L. So
L(G) ~ L. Hence, L(G) =L.
EXAMPLE 4.1 3
Let G = ({S, AI- AJ, {a, b}, P, S), where P consists of
5 ---,) aA 1A2a, A) ---,) baAIA2b, A2 ---,) Alab, aA I
---,) baa, bA2b ---,) abab
Test whether w =baabbabaaabbaba
is in L(G).
Solution
We have to start with an 5-production. At every stage we apply a suitable
production which is likely to derive w. In this example, we underline the
substring to be replaced by the use of a production.
S::::} aAI A2a
::::} baa A2 a
::::} baabbaa A2 baba
::::} baabba aA) abbaba
::::} baabbabaaabbaba = w
Therefore.
11' E L(G)
EXAMPLE 4.14
If the grammar G is given by the productions S ---,) aSa I bSb i aa i bb I A,
show that (i) L(G) has no strings of odd length, (ii) any string in L(G) is of
length 2n, n 2: 0, and (iii) the number of strings of length 2n is 2".
Solution
0- application of any production (except S ---,) A), a variable is replaced by
two terminals and at the most one variable. So, every step in any derivation
increases the number of terminals by 2 except that involving 5 ---,) A. Thus,
we have proved (i) and (ii).
120
!;i
Theory ofComputer Science
To prove (iii), consider any string 11' of length 2n. Then it is of the form
([1([2 ... al/a/1 ... ([1 involving n 'parameters' aj, a2, ...,
([11' Each ai can be
either a or b. So the number of such strings is 2/1. This proves (iii).
4.2
CHOMSKY CLASSIFICATION OF LANGUAGES
In the definition of a grammar (V.v, 2:, P, S), VV and 2: are the sets of symbols
and SEVy. So if we want to classify grammars. we have to do it only by
considering the form of productions. Chomsky classified the grammars into
four types in terms of productions (types 0-3).
A type 0 grammar is any phrase structure grammar without any restrictions.
(All the grammars we have considered are type 0 grammars.)
To define the other types of grammars. we need a definition.
In a production of the fom1 cp Alfl ~ rpCXljf, where A is a variable, rp is called
the left context, ljf the right context and rpcxlfl the replacement string.
EXAMPLE 4.1 5
(a) In abdbed ~ abABbed, ab is the left context, bed is the right context.
ex = AS-
(b) In A.C ~ A, A is the left context A is the right context ex = A. The
production simply erases C when the left context is A and the right
context is A
(c) For C ~ A. the left and right contexts are A. And
CX = A. The
production simply erases C in any context.
A production without any restrictions is called a type 0 production.
A production of the form rpA ljf ~ rpcxlfl is called a type I production if
ex ::j:: A
In type I productions, erasing of A is not permitted.
EXAMPLE 4.16
(a) adbeD ---7 ({beDbeD is a type 1 production where where a, beD are the
left context and right context. respectively. A is replaced by beD ::j:: A.
(b) All ~ AbBe is a type 1 production. The left context is A, the right
context is A
(C) A ~ abA is a type 1 production. Here both the left and right contexts
are A.
Definition 4.7
A grammar is called type I or context-sensItIve or context-
dependent if all its produclions are type 1 productions. Tne production S ~ A
is also allowed in a type 1 grammar. but in this case S does not appear on the
light-hand side of any production.
Definition 4.8
The language generated by a type 1 grammar is called a
type 1 or context-sensitive language.
Chapter 4: Formal Languages
);I,
121
Note:
In a context-sensitive grammar G, we allow S ~ A for including A
in L(G). Apart from S -1 A, all the other productions do not decrease the
length of the working string.
A type 1 production epA Iff ~ dJalff does not increase the length of the
working string. In other words, i epA IffI ::; ! ep alffI as a
=;t: A. But if a ~ f3
is a production such that Ia I ::; I13 I, then it need not be a type 1 production.
For example. BC ~ CB is not of type 1. We prove that such productions can
be replaced by a set of type 1 productions (Theorem 4.2).
Theorem 4.1
Let G be a type 0 grammar. Then we can find an equivalent
grammar G j in which each production is either of the form a ~ 13, where a
and 13 are strings of variables only. or of the form A ~ a, where A is a variable
and a is a terminal. G j is of type 1, type 2 or type 3 according as G is of type
L type 2 or type 3.
Proof
We construct G j as follows: For constructing productions of G 1,
consider a production a -1 13 in G, where a or 13 has some terminals. In both
a and f3 we replace every terminal by a new variable Cu and get a' and f3'.
Thus. conesponding to every a ~ 13, where a or 13 contains some terminaL we
construct a' ~ f3' and productions of the form Ca ~ a for every terminal
a appearing in a or 13. The construction is performed for every such a ~ 13. The
productions for G] are the new productions we have obtained through the above
construction. For G] the variables are the variables of G together with the new
variables (of the form C,,). The terminals and the start symbol of G] are those
of G. G] satisfies the required conditions and is equivalent to G. So L(G) =
L(G]).
I
Defmition 4.9
A grammar G = (Vy, L, P, S) is monotonic (or length-
increasing) if every production in P is of the form a ~ 13 with I a I ::; I131
or 5 ~ A. In the second case,S does not appear on the right-hand side of any
production in P.
Theorem 4.2
Every monotonic grammar G is equivalent to a type 1 grammar.
Proof
We apply Theorem 4.1 to get an equivalent grammar G]. We construct
G' equivalent to grammar G] as follows: Consider a production A jA2 ... Am -1
B]B]. ... Bn with
11
::::: m in G!. If m = 1, then the above production is of
type 1 (with left and right contexts being A). Suppose In
::::: 2. Conesponding
to A]A2 ... Am ~ B jB2 ... Bm we construct the following type 1 productions
introducing the new variables Cj • C]., ..., Cm.
Aj A2 ... Am ~
C] A2 ... Am
122
~
Theory ofComputer Science
The above construction can be explained as follows. The production
A jA 2 ... A", ~ B]B2 •.. Bn
is not of type 1 as we replace more than one symbol on L.R.S. In the chain of
productions we have constructed, we replace A j by Cl, A2 by C2 •• ., Am by
C",Bm+ i ••• Bn. Afterwards. we start replacing Cl by B 1, C2 by B 2, etc. As we
replace only one variable at a time. these productions are of type 1.
We repeat the construction for every production in G] which is not of
type 1. For the new grammar G'. the variables are the variables of Gl together
with the new variables. The productions of G' are the new type 1 productions
obtained through the above construction. The tenninals and the start symbol
of G' are those of Gi .
G' is context-sensitive and from the construction it is easy to see that
L(G') = L(GJ = L(G).
Defmition 4.10
A type 2 production is a production of the fonn A ~ a,
where A
E Vv and
CI. E lVv U L)*. In other words. the L.R.S. has no left
context or light context. For example. S ~ A.a, A ~ a. B ~ abc, A ~ A are
type 2 productions.
Definition 4.11
A grammar is called a type 2 grammar if it contains only
type 2 productions. It is also called a context-free granl<"'I1ar (as A can be
replaced by a in any context). A language generated by a context-free grammar
is called a type 2 language or a context-free language.
Definition 4.12
A production of the fonn A ~ a or A ~ aBo where
A.. B E \lv and a E I. is called a type 3 production.
Definition 4.13
A grammar is called a type 3 or regular grammar if all its
productions are type 3 productions. A production S ~ A is allowed in type 3
grammar. but in this case S does not appear on the right-hand side of any
production.
EXAMPLE 4.1 7
Find the highest type number which can be applied to the following
productions:
(a)
(b)
(el
S ~ Aa.
A
S ~ ASB Id,
S ~ as Iab
~ elBa.
A
~ aA
B ~ abc
Chapter 4: Formal Languages
Q
123
Solution
(a) 5 ~ Aa. A ~ Ba, B ~ abc are type 2 and A ~ c is type 3. So the
highest type number is 2.
(b) 5 ~ ASB is type 2. S ~ d. A ~ aA are type 3. Therefore. the highest
type number is 2.
(c) S ~ as is type 3 and 5 ~ ab is type 2. Hence the highest type
number is 2.
4.3
LANGUAGES AND THEIR RELATION
In this section we discuss the relation between the classes of languages that we
have defined under the Chomsky classification.
Let
xes), J cfl and
denote the family of type 0 languages, context-
sensitive languages. context-free languages and regular languages, respectively.
Property 1
From the definition. it follows that i'l S
J cfl'
S
S
Property 2
S 1 csl' The inclusion relation is not immediate as we allow
A ~ A in context-free grammars even \-vhen A i= S, but not in context-sensitive
grammars (we allmv only S ~ A in context-sensitive grammars). In Chapter 6
we prove that a context-free grammar G with productions of the form A ~ A
is equivalent to a context-free grammar G] which has no productions of the
fonn A ~ A (except S ~ A). Also. when G] has S ~ A, S does not appear
on the right-hand side of any production. So G[ is context-sensitive. This
proves
S
Property 3
S
oLefl S
S
This follows from properties 1 and 2.
Property 4
L rl c""
oL eil C" 1 c51 c""
In Chapter 5. we shall prove that
c""
. In Chapter 6. we shall
prove that
c=
. In Section 9.7. we shall establish that
c"" .to.
Remarks
1. The grammars given in Examples 4.1-4.4 and 4.6-4.9 are
context- free but not regular. The gramm.ar given in Example 4.5 is regular. The
grammars given in Examples 4.10 and 4.11 are not context-sensitive as we have
productions of the form 2A) ~ All, CB ~ BC which are not type 1 rules. But
they are equivalent to a context-sensitive grammar by Theorem 4.2.
2. Two grammars of different types may generate the same language. For
example, consider the regular grammar G given in Example 4.5. It generates
{a. b}+. Let G/be given by S ~ SSla5IbSla!b. Then L(G') =L(G) as the
productions S ~ as IbS I {{ ! b are in G as welL and S ~ 5S does not generate
any more string.
3. The type of a given grammar is easily decided by the nature of
productions. But to decide on the type of a given subset of L*. it is more
difficult. By Remark 2, the same set of strings may be generated by a grammar
of higher type. To prove that a given language is not regular or context-free.
we need powerful theorems like Pumping Lemma.
124
Q
Theory ofComputer Science
4.4
RECURSIVE AND RECURSIVELY ENUMERABLE SETS
The results given in this section will be used to prove £cs1 e" £0 in Section 9.7.
For defining recursive sets, we need the definition of a procedure and an
algorithm.
A procedure for solving a problem is a finite sequence of instructions
which can be mechanically carried out given any input.
An algorithm is a procedure that terminates after a finite number of steps
for any input.
Definition 4.14
A set X is recursive if we have an algorithm to determine
whether a given element belongs to X or not.
Defmition 4.15
A recursively enumerable set is a set X for which we have
a procedure to determine whether a given element belongs to X or not.
It is clear that a recursive set is recursively enumerable.
Theorem 4.3
A context-sensitive language is recursive.
Proof
Let G = (ViV, I, P, S) and ,v E I*. We have to construct an algorithm
to test whether W E L(G) or not. If w = A, then W E L(G) iff S ~ A is in
P. As there are only a finite number of productions in P, we have to test
whether S ~ A is in P or not.
Let Iw I=n 2:: 1. The algorithm is based on the construction of a sequence
{Wd of subsets of (Vv u I)*. Wi is simply the set of all sentential forms of
length less than or equal to n. derivable in at most i steps. The construction
is done recursively as follows:
(i) Wo = is}.
(ii) Wi+1 = Wi U {f3 E (Vv u I)*I there exists a in Wi such that a=:;> f3
and I f3 I ::;; n}.
W;'s satisfy the following:
(iii) Wi ~ Wi+1 for all i 2:: O.
(iv) There exists k such that Wk = Wk+!.
(v) If k is the smallest integer such that Wk = Wk+1, then Wk =
{a E
CVv u I}*IS :b a and lal ::;; n}.
The point (iii) follows from the point (ii). To prove the point (iv), we
consider the number N of strings over Vv u I of length less than or equal to
11. If IVv u I I = 111, then N = 1 + m + 1112 + ... + mil since m i is the number
of strings of length i over Vv U L. i.e. N =(m"+ I - 1)/(m - 1), and N is fixed
as it depends only on
11 and m. As any string in Wi is of length at most 11,
IWi I ::;; N. Therefore, Wk = Wk+1 for some k ::;; N. This proves the point (iv).
From point (ii) it follows that Wk = Wk+ 1 implies Wk+1 = Wk+2'
{a E
CVv U L)* IS :b a. Ia I ::;; n} = WI
U W2 U
U Wk U lVk+1 ...
= W1 U
W2 U
U Wk
=Wk from point (iii)
This proves the point (v).
Chapter 4: Formal Languages
J;!
125
From the point (v) it follows that W
E L(G) (i.e. S ~ w) if and only if
W E
W~. Also. Wi, W2, •.• , Wk can be constructed in a finite number of steps.
We give the required algorithm as follows:
Algorithm to test whether w E L(G).
1. Construct Wi, W2• ... using the
points (i) and (ii). We terminate the construction when Wk+] = Wk for the first
time.
2. If yi' E Wk, then W E L(G). Otherwise, W E L(G). (As IWk Is N, testing
whether W is in Wk requires at most N steps.)
I
EXAMPlE 4.18
Consider the grammar G given by S ~ OSA]2. S ~ 012, 2A)
~ A]2,
lA] ~ 11. Test whether (a) 00112
E L(G) and (b) 001122 E L(G).
Solution
(a) To test whether w =00112 E L(G), we construct the sets Wo, WI, W2
etc. Iwl = 5.
Wo= {S}
W] = {012, S, OSA]2}
Wo = {012, S, OSA I2}
As W2 = WI, we terminate. (Although OSA]2 => 0012A]2, we cannot
include 0012A]2 in W] as its length is> 5.) Then 00112 E WI' Hence,
00112 E LCG).
(b) To test whether w =001122 E L(G). Here, Iw 1= 6. We construct Wo,
WI. W2, etc.
Wo= {S}
W] = {012. S, OSA]2}
Wo = {012, S, OSA]2, 0012A]2}
W3 = {OIl, S. OSA]2. 0012A]2, 001A]22}
W4 = {012. S, OSA]2, 0012A]2. 001A]22, 001122}
W5 = {012, S. OSA]2. 0012A]2, 001A]22. 001122}
As Ws = W4, we terminate. Then 001122 E W4. Thus. 001122 E L(G).
The following theorem is of theoretical interest, and shows that there
exists a recursive set over {O, I} which is not a context-sensitive language. The
proof is by the diagonalization method which is used quite often in set theory.
1 neorem 4.4
There exists a recursive set which is not a context-sensitive
language over {O, I}.
Proof
Let 2: = {O, I}. We write the elements of 2:* as a sequence (i.e. the
elements of 2:* are enumerated as the first element, second element, etc.) For
126
Q
Theory ofComputer Science
example, one such way of writing is A, 0, I, 00, 01, 10, 11, 000, .... In this
case. 010 will be the 10th element.
As every grammar is defined in terms of a finite alphabet set and a finite
set of productions, we can also write all context-sensitive grammars over 2.: as
a sequence, say Gj; G2, .•.•
We define X = {>Vi E 2.:* IWi
r;z: L(G;)}. We can show that X is recursive.
If W E 2.:*, then we can find i such that W = >Vi' This can be done in a finite
number of steps (depending on IW 1). For example, if >v = 0100, then W = W20'
As G20 is context-sensitive, we have an algorithm to test whether vI' =W20 E
L(G20) by Theorem 4.3. So X is recursive.
We prove by contradiction that X is not a context-sensitive language. If it
is so, then X = L(G,,) for some n. Consider w" (the 11th element in 2.:*). By
definition of X,
W n E X implies
W n
r;z: L(G,,). This contradicts X = L(Gn)·
W n
r;z: X implies W n E L(Gn) and once again, this contradicts X = L(Gn). Thus,
X::t L(Gn) for any n. i.e. X is not a context-sensitive language.
I
4.5
OPERATIONS ON LANGUAGES
We consider the effect of applying set operations on £0, £c51' ;[efl, elr!. Let
A and B be any two sets of strings. The concatenation AB of A and B is
defined by AB = {uv I U E A, v E B}. (Here, uv is the concatenation of the
strings u and v.)
We define Al as A and A,,+I as AliA for all n
:::::: 1.
The transpose set AT of Ais defined by
AT = {Ill U E A}
Theorem 4.5
Each of the classes ;[0,
, £ef); elr] is closed under union.
Proof
Let L I and L2 be two languages of the same type i. We can apply
Theorem 4.1 to get grammars
and
of type i generating L] and L2• respectively. So any production in G] or G2
is either ex ~ {3, where ex, {3 contain only variables or A ~ a, where A E Vv,
a E
2.:.
We can further assume that V;v n V';; = 0. (This is achieved by renaming
the variables of
V'~ if they occur in V'N')
Define a new grammar Gli as follows:
Gil = (V'N U
V'~, U {5}, 2.:] U
2.:2, PII' S)
where 5 is a new symboL i.e. 5
r;z: V'N U
V',~
P li = PI
U
P2 U
{5 ~ 51' 5 ~ 52}
Chapter 4: Formal Languages
~
127
*
We prove L(G,,) = L l
U L2 as follows: If W
E L j
U L2, then 5 j =>
W or
52 => w.
Therefore,
G,
Go
*
or
5 =>
50 => w, l.e. W E L(G,,)
Gu
Gu
Thus, L j
U
L2 ~ L(G,).
To prove that L(G,J ~ Ll U L2• consider a derivation of w. The first step
should be 5 ~ 5 j or 5 ~ 52' If 5 ~ 51 is the first step, in the subsequent steps
5: is changed. As Vi.: (l V;": :f:- 0, these steps should involve only the variables
of V:v and the productions we apply are in Pj. So 5 ~ w. Similarly, if the
G,
first step is 5 ~ 52, then 5 => 52 ~ w. Thus, L(G,,) =Ll U L2. Also, L(G,J
G,
G,
is of type 0 or type 2 according as L 1 and L2 are of type 0 or type 2. If A
is not in L j
U L2, then L(G,,) is of type 3 or type 1 according as L j and L2
are of type 3 or type 1.
Suppose A
E L j • In this case, define
G" =
(V~\, U V:v U {5, 5'}, I j
U I2> P,!, 5')
where (i) S' is a new symbol, i.e. 5' e: V'N U
V',~ U
{5}, and (ii) P" =
Pi
U
P2 U
{5' -? 5, 5 -? 5\> 5 -? 52}' So, L(Gu ) is of type 1 or type 3
according as L] and L2 are of type 1 or type 3. When A E L2, the proof is
similar.
I
Theorem 4.6
Each of the classes
,10, L es \> L cf \> ;irl is closed under
concatenation.
Proof
Let L j and L2 be two languages of type i. Then, as in Theorem 4.5, we
get G j = CV:v, I\> P l, 51) and G2 = (V':v , I:, P2, 52) of the same type i. We
have to prove that L1L: is of type i.
Construct a new grammar Geon as follows:
Geon = (V'N U V\, U
{5}, L 1 U L2, Peon' 5)
where 5 e:
V~v U V(
Peon = Pl U P2 U {5 -? 5i52}
We prove L1L: = L(Geon)' If W = WjW:
E LjL:, then
*
51 =>
lVI'
G,
So,
*
5
=>
5152
=>
WjW2
Geoa
Gcon
Therefore.
128
];I,
Theory ofComputer Science
If >t'
E
L(Gcon)' then the first step in the derivation of w is S ~ SIS2'
As V;v n
V'~ = 0 and the productions in G I or G2 involve only the variables
*
(except those of the form A ~ a), we have W = WIW', where S =>
WI and
*
-
G]
S ~
W2' Thus L IL2 = L(Gem,). Also, Gcon is of type 0 or type 2 according
as G1 and G2 are of type 0 or type 2. The above construction is sufficient when
G] and G2 are also of type 3 or type 1 provided A
~ L] U L2.
Suppose GI and G2 are of type 1 or type 3 and A
E L] or A E Lo. Let
LIt =L 1 -
{A}, LI
2 =L2 -
{A}. Then
{
L;L; U L;
L]L2 =
L;L; u L;
L;L; u L; u L; u {A}
if A is in LI but not in L2
if A is in L2 but not in LI
if A is in L] and also in L:c
As we have already shown that L csi and 1'rl are closed under union, L IL2 is
of type 1 or type 3 according as L I and L2 are of type 1 or type 3.
Theorem 4.7
Each of the classes Lo, Lcslo Lcflo
£rl is closed under the
transpose operation.
Proof
Let L be a language of type i. Then L =L(G), where G is of type i.
We construct a new grammar CTas follows: GT=(VN> ~,pT, S), where the
productions of pT are constructed by reversing the symbols on L.R.S. and
R.R.S. of every production in P. Symbolically, cXt ~ f3T is in pT if a ~ f3 is
in P.
From the construction it is obvious that GT is of type 0, 1 or 2 according
as G is of type 0, 1 or 2 and L(GT) = LT. For regular grammar, the proof is given
in Chapter 5.
It is more difficult to establish the closure property under intersection at
present as we need the properties of families of languages under consideration.
We state the results without proof. We prove some of them in Chapter 8.
Theorem 4.8
(i) Each of the families Lo], Lcslo Lrl is closed under
intersection.
(ii) L et1 is not closed under intersection. But the intersection of a context-
free language and a regular language is context-free.
4.6
LANGUAGES AND AUTOMATA
In Chapters 7 and 9, we shall construct accepting devices for the four types
of languages. Figure 4.1 describes the relation between the four types of
languages and automata: TM, LBA, pda, and FA stand for Turing machine,
linear bounded automaton,
pushdown
automaton
and finite
automaton,
respectively.
Languages
Chapter 4: Formal Languages
~
129
Automata
Type 0
---
TM
Context-sensitive
or type 1
LBA
I
I Context-free
~'
I
or type 2 i
I
IRegUiarl
8
or type
I
I
I
3
I LJ
I~
I I
I
I
I
I
I
I
Fig. 4.1
Languages and the corresponding automata.
4.7
SUPPLEMENTARY
EXAMPLES
EXAMPLE 4.19
Construct a context-free grammar generating
(a) L l = {d'b 211 In;::: I}
(b) L, = {d
l1b
l1 1m> n,
Tn, n ;::: I}
(c) L3 = {amb" Im < n. m, n,
;::: I}
(d) L. = {d
l1b" 1m, n ;::: 0, m
:;t:. n}
Solution
(a) Let GI = ({5}, {a. b}. P, 5) where P consists of 5 -0 a5bb, 5 -0 abb.
(b) Let G2 = ({5, A}, {a, b}, P, 5) where P consists of 5 -0 as 1aA,
A -0 aAb, A -0 abo It is easy to see that L(Gj s;;;: L2. We prove the
difficult part. Let d"b" E L2• Then, m > 11
;::: 1. As m > n, we have
111 -
n ;::: 1. So the derivation of d
l1y' = a"H'a"b" is
(c) Let G3 = ({5, B}. {a, b}, P, S) where P consists of 5 -0 5b IBb.
B -0 aBb, B -0 abo This construction is similar to construction in (b).
5 -0 5b I Bb are used to generate b"-IIl. The remaining productions
will generate alllblll. Hence L(G3) = L3.
(d) Note that LJ, = L: u L3 u L'
U L" where L' = {b" In;::: I} and
LI! = {all
1 n
;::: I}. It is easy to see how to construct grammars
generating L:. L3 and L' and L". Define GJ, by combining these
constructions. Let
G.j = ({5. 51. 52, 53, 5.j, A}. {a, b}, PJ,. 5) where PJ, consists of
5 -0 51 152153 i 5J,.
51 -0 a5 I 1Aa I aAb lab,
5: -0 5:b lAb.
53 -0 bS3 1b,
and
54 -0 aS41 a.
It is easy to see that L(GJ,) = L4·
130
~
Theory at"Computer Science
EXAMPLE 4.20
Construct a grammar accepting
L = {w E
{a, b}*! the number of a's in w is divisible by 3}.
Solution
Consider a string over {a. b} having three a's. These three a's appear amidst
the strings of b. A typical string is blllab"ab'abt• For generating bill, we can have
the productions S ---:t bS. For getting the first a, we can have S ---:t aA. For
getting Y' afterwards, we have A --7 bA. For getting the second a, we can have
A ---:t aBo For getting b
l
, we have B ---:t bB. For getting the third a and repetition
of this pattern. we can have B ---:t a Ias.
Now we construct G as follows:
G = ({S, A, B}, {a. b}, P. S) where P consists of
S ---:t bS, S ---:t aA. A ---:t bA, A ---:t aB, B ---:t bB, B ---:t a, B ---:t as.
A string in L is of the fonn )'IY2 ... YII where each Yi is of the fonn
b'"ab"ab'a for some nonnegative integers In, nand s.
S =s, billS => blllaA =s, billal/'A => blllal/'aB =s, blllab"ab'S
Hence G is the required grammar.
EXAMPLE 4.21
Construct a grammar G such that
L(G) = {w E
{a. b} Iw has an equal number of a's and b's}.
Solution
Define G = ({S, A. B}, {a, b}. P, S) where P consists of
S ---:t aB IbA.
A ---:t as IbAA Ia,
B ---:t bS IaBB Ib
To prove that G accepts the given language. we prove the following by
induction on [wi.
(i) S =s, w if and only if ~w consists of an equal number of a's and b's.
(ii) A =s, w if and only if 11' consists of one more a than the number of b' S.
(iii) B =s,
j1' if and only if w consists of one more b than the number of a's.
The ponits (i). (ii). (iii) are true when Iw I = 1. For A => a and a is the
only string of length one which can be derived from B. Also, no string of
length one is derivable from S. Thus. there is basis for induction.
Assume points (i), (ii), (iii) to be true for all strings of length k - 1. Let
Iwl = k.
We prove the 'only if part of (i). Let S =s, ,v. Then the first production
has to be either S ---:t aB or S ---:t bA. If the first production is S ---:t aB, then
Chapter 4: Formal Languages
l;l
131
S ~ aB ::; w. Hence
W = aWl' B ::; 'VI and
I11JI! = k -
1. By induction
hypothesis, the point (iii) is true for WI' This means that \1'1 has one more b
than the number of a's. Hence 11' = aWl has an equal number of a's and b's.
(The proof is similar if the first production is S -+ bA.)
To prove the 'if part', assume that w has an equal number of a's and b's,
If 11' starts with a, then W = aWb where IWII = k - 1. (If 11' starts with b the
proof is similar.) Also 11'1 has one more b than the number of a's. By induction
hypothesis, the point (iii) is true for WI' Then B ~
11'1' As S -+ aB is a
production, we have S ~ aBo So S ~ aB ::; aWl = 11', i.e, S ::;
11', which
proves the 'if part' of point (i).
Similarly we can prove the points (ii) and (iii) for a string 11' of length k.
By the principle of induction the points (i), (ii), (iii) are true for all strings W.
In particular, from point (i), we can conclude that
L(G) = {w
E
{a, b} Iw has an equal number of a's and b's}
EXAMPLE 4.22
Construct a grammar G accepting the set L of all strings over {a, b} having
more a's than b's.
Solution
For generating strings with a's but \vith no b's, we can have the production
S -+ a, S -+ as, S -+ Sa. (If xa E L, then CLY E L. Hence we have S -+ as and
S -+ Sa.) If x, y E L, then xy E L (also yx E L). So .\y or yx has at least two
more a's than b's. So we can add a b. This can be done by having the
productions S -+ bSS, S -+ SbS, S -+ SSb. (i.e. b can be added at the beginning,
or at the end of SS, or between Sand S). With this motivation, we can construct
G as follows:
G =({S}, {a, b}, P, S) where P consists of
S -> a Ias I Sa IbSS ISbS ISSb
We can easily prove that G accepts all stlings over {a, b} having more a's
than b's.
EXAMPLE 4.23
Construct a grammar G accepting all strings over {a, b} containing an unequal
number of a's and b's.
Solution
As ;n Example 4.20, we can construct a grammar accepting all strings having
more b's than a's. The required language is the union of the language of
Example 4.20 and a similar one having more b's than a's. So we construct
G as follows:
132
~
Theory ofComputer Science
G = ({S, SI. S:J, {a, b}, P, S) where P consists of
S ---+ S1IS2
SI ---+ a IaSI ISla IbSlS I ISl bSI IS1S1b
S2 ---+ b! bS2 ! S2b IaS2S2 !S2aS2 ! SS2a.
G generates all strings over {a, b} having an unequal number of a's and b's.
EXAMPLE 4.24
If L 1 and L2 are the subsets of {a, b} *, prove or disprove:
(a) If L 1 k
L2 and L l is not regular. then L2 is not regular.
(b) If L I k
L2 and L 2 is not regular, then L l is not regular.
Solution
(a) Let L l = {aI/bIZ I n :2 I}. L l is not regular. Let L2 = {a, b}*. By
Example 4.5, L2 is regular. Hence (a) is not true.
(b) Let L2 = {a"b" !
11 :2 I}. It is not regular. But any finite subset is
regular. Taking L 1 to be a finite subset of L2, we disprove (b).
EXAMPLE 4.25
Show that the set of all non-palindromes over {a, b} is a context-free
language.
Solution
Let W E {a, b}* be a non-palindrome. Then w may have the same symbol in
the first and last places, same in the second place from the left and from the
right, etc.: this pattern will not be there after a particular stage. The
productions S ---+ aSa IbSb !1\ may be used for fulfilling the palindrome-
condition for the first and last few places. For violating the palindrome
condition, the productions of the form it ---+ aBb IbBa and B ---+ aB! bB I1\
will be usefuL So the required grammar is G = ({ S, it, B}, {a, b}, P, S)
where P consists of
S ---+ aSa IbSb IA
it ---+ aBb! bBa
B ---+ aB IbB !1\
SELF-TEST
Choose the correct answers to Questions 1-12:
1. For a grammar G with productions S ---+ SS, S ---+ aSb, S ---+ bSa, S ---+ 1\,
(al S ~ abba
(b) S :b abba
(c) abba
EO L(G)
(d) S :b aaa
Chapter 4: Formal Languages
);\
133
2. If a ~ f3 in a grammar G, then
(a) a ~ f3
(c) f3 ~ a
(b) f3 ~ a
(d) none of these
3. If a -0 f3 is a production in a grammar G, then
(a) aa ~ f3f3
(b) aaf3 ~ f3f3a
(c) aa ~ f3a
(d) aaa ~ f3f3f3
4. If a grammar G
has three productions 5 -0 aSa I bsb i e, then
(a) abeba and baeab E L(G)
(b) abcba and abeab E L(G)
(c) aeeea and beecb E L(G)
(d) aeeeh and bceea E L(G)
5. The minimum number of productions for a grammar G = ({S}, to, 1,
2, ..., 9}, P, 5) for generating {O, 1. 2, .... 9} is
(a) 9
(b)
10
(c) 1
(d) 2
6. If G j = (N, T. Pi, S) and G: = (N, T, P:, 5) and Pi
C P:, then
(a) L(G i ) C L(G:)
(b) L(G:) C L(G))
(c) L(G!) = L(GJ
(d) none of these.
7. The regular grammar generating {a" : n :::: I} is
(a) ({S}, {a}, {S -0 as}, S)
(b) ({S}, {a}, {S -0 55, 5 -0 aD
(c) ({S}, {a}, {S -0 as}, S)
(d) ({S}, {a}, {S -0 as,S -0 a,S)
8. L = {theory, of. computer, science} can be generated by
(a) a regular grammar
(b) a context-free grammar but not a regular grammar
(c) a context-sensitive grammar but not a context-free grammar
(d) only by a type °grammar.
9. {a" In:::: I} is
(a) regular
(b) context-free but not regular
(c) context-sensitive but not context-free
(d) none of these.
10. {d' b" III :::: I} is
(a) regular
(b) context-free but not regular
(c) context-sensitive but not context-free
(d) none of these.
11. {d'b"c" In :::: I} is
(a) regular
(b) context-free but not regular
(c) context-sensitive but not context-free
(d) none of these.
134
~
Theory ofComputer Science
12. {d'b"cflll n, m 2: 1} is
(a) regular
(b) context-free but not regular
(c) context-sensitive but not context-free
(d) none of these.
State whether the following Statements 13-20 are true or false:
13. In a grammar G = (\!,y, 2:, P, S),
\!,V and 2: are finite but P can be
infinite.
14. Two grammars of different types can generate the same language.
15. If G = (VV, 2:. P, S) and P
"j:. 0, then L(G)
"j:. 0.
16.
If a grammar G has three productions, i.e. S ~ AA, A -) aa, A
-~
bb, then L(G) is finite.
17. If L I = {d'blllim, n 2: 1} and L2 = {bflld'lm, p 2: 1}, then L I n L 2 =
{a"b"c/l11 2: I}.
18. If a grammar G has productions S -~ as IbS Ia, then L(G) =the set of
all strings over {a, b} ending in a.
19. The language {a"bc" In 2: 1} is regular.
20.
If the productions of G are S ~ as ISb IaI17, then abab E L(G).
EXERCISES
4.1 Find the language generated by the following grammars:
(a) S ~ OSll OA1, A ~ IA 11
(b) S ~ OSII OA I0 IIIB 11, A ~ OA I0, B ~ lB 11
(c) S ~ OSBA lOlA, AB ~ BA, IB ~ 11, lA ~ 10, OA ~ 00
(d) S ~ OSlIOAl, A ~ lAOllO
(e) S ~ OA lIS 1011, A ~ lA lIS 11
4.2 Construct the grammar, accepting each of the following sets:
(a) The set of all strings over {O, I} consisting of an equal number of
O's and l's.
(b) {O"IIIlOIllI"!m, n 2: I}
(c)
{Oil1211 In 2: l}
(d)
{Oil 111 In 2: I} u {III/Oil/1m 2: I}
(e)
{Oil 111/0" 1m,
11 2: I} u
{0IlIII/2"'lm,
11 2: I}.
4.3 Test whether 001100, 001010, 01010 are in the language generated by
the grammar given in Exercise 4.I(b).
4.4 Let G = ({A, B, S}, {O, I}, P, S), where P consists of S ~ OAB,
Ao ~ SOB, Al ~ SBI. B ~ SA, B ~ 01. Show that L(G) = 0.
4.5
4.6
4.7
4.8
4.9
4.10
4.11.
4.12.
4.13.
4.14.
4.15.
4.17.
Chapter 4: Formal Languages
~
135
Find the language generated by the grammar 5 ~ AB, A ~ Alia,
B ~ 2B I3. Can the above language be generated by a grammar of
higher type?
State whether the following statements are true or false. Justify your
answer with a proof or a counter-example.
(a) If G] and G2 are equivalent then they are of the same type.
(b) If L is a finite subset of L*, then L is a context-free language.
(c) If L is a finite subset of L*, then L is a regular language.
Show that {a"
2
1 n :::: I} is generated by the grammar 5 ~ a, 5 ~ A3A4'
A3 ~ AlA~2> A 3 ~ AlA}, AlA} ~ aA2A l> Ala ~ aA t, A 2a ~ aA2>
AlA, ~ A"a, A 2A 4 ~ A 5a, A 2A 5 ~ A 5a, A 5 ~ a.
Construct (i) a context-sensitive but not context-free grammar, (ii) a
context-free but not regular grammar, and (iii) a regular grammar to
generate {a" 111 :::: I}.
Construct a grammar which generates all even integers up to 998.
Construct context-free grammars to generate the following:
(a) {all/I" 1m :;t 11, m, n:::: I}.
(b) {all/"e" lone of I, m,
11 equals 1 and the remaining two are equal}.
(c) {all/I" I 1 ::; m ::; 11}.
(d) {alb
li1e"[1 + In = 11}.
(e) The set of all strings over {a. I} containing twice as many O·s as
I's,
Construct regular grammars to generate the following:
(a) {a
211 1n :::: I}.
(b) The set of all strings over {a. h} ending in a.
(c) The set of all strings over {a. b} beginning with a.
(d) {all/"e" II, m, n :::: I}.
(e)
{(ab)"f n
:::: I},
Is => an equivalence relation on (Vv u L)*?
G
.
Shmv that G] = ({5},
{a, b}, Pj, S), where p] = {S ~ a5blab} is
equivalent to G2 = ({S, A, B. C}. {a, b}. P2, 5). Here P2 consists of
5 ~ AC, C ~ 5B. 5 ~ AB, A
~ a, B ~ b.
If each production in a grammar G has some variable on its right-hand
side, what can you say about L(G)?
Show that {abc,
bca. eab} can be generated by a regular grammar
whose terminal set is {a, b, e}.
Construct a grammar to generate {(ab)"[II:::: I} u
{(ba)"!n:::: I}.
Show that a grammar consisting of productions of the form A ~ xB Iy.
where x, yare in L* and A, B E Vv. is equivalent to a regular grammar.
Regular Sets and
Regular Grammars
In this chapter. we first define regular expressions as a means of representing
certain subsets of strings over 2: and prove that regular sets are precisely those
accepted by finite automata or transition systems. We use pumping lemma for
regular sets to prove that certain sets are not regular. We then discuss closure
properties of regular sets. Finally. we give the relation between regular sets
and regular grammars.
5.1
REGULAR EXPRESSIONS
The regular expressions are useful for representing certain sets of strings in an
algebraic fashion. Actually these describe the languages accepted by finite state
automata.
We give a formal recursive definition of regular expressions over 2: as
follO\\/s:
1. Any terminal symbol (i.e. an element of 2:), A and 0 are regular
expressions. When we view a in 2: as a regular expression, we denote
it by a.
2. The union of two regular expressions R j and R2• written as R j + R2,
is also a regular expression.
3. The concatenation of two regular expressions R j and R2, written as
R j R2, is also a regular expression.
4. The iteration (or closure) of a regular expression R written as R*, is
also a regular expression.
S. If R is a regular expression, then (R) is also a regular expression.
6. The
regular
expressions
over
2:
are
precisely
those
obtained
recursively by the application of the rules 1-5 once or several times.
136
Chapter 5: Regular Sets and Regular Grammars
~
137
Notes: (1) We use x for a regular expression just to distinguish it from the
symbol (or string) x.
(2) The parentheses used in Rule 5 influence the order of evaluation of
a regular expression.
(3) In the absence of parentheses, we have the hierarchy of operations as
follows: iteration (closure). concatenation, and union. That is, in evaluating a
regular expression involving various operations. we perform iteration first, then
concatenation. and finally union. This hierarchy is similar to that followed for
arithmetic expressions (exponentiation. multiplication and addition).
DefInition 5.1
Any set represented by a regular expression is called a regular
set.
If for example, a, bEL. then (i) a denotes the set {a}, (ii) a + b denotes
{a, b}, (iii) ab denotes {ab}, (iv) a* denotes the set {A. a, aa. aaa, ...} and
(v) (a + b)* denotes {a, b}*.
The set represented by R is denoted by L(R),
Now we shall explain the evaluation procedure for the three basic
operations. Let R j and R: denote any two regular expressions. Then (i) a
string in L(R j + R:) is a string from R] or a string from R:; (ii) a string in
L(R j R2) is a string from R j followed by a string from R,. and (iii) a string
in L(R*) is a string obtained by concatenating
11 elements for some II 2: O.
Consequently. (i) the set represented by R j + R 2 is the union of the sets
represented by R] and R 2• (ii) the set represented by RjR: is the concatenation
of the sets represented by R j and R:. (Recall that the concatenation AB of sets
A and B of strings over I is given by AB = {H'(W:IWj
E A,
1~': E B}, and
(iii) the set represented by R* is {WI"': ... It',JWi is in the set represented by
Rand Il 2: O.} Hence.
L(R j + R 2) = L(R]) u L(R2),
L(R]R:) = L(RI)L(R:)
L(R*) = (L(R»)*
Also.
L(R*) = (L(R)* = U
L(R)"
11=0
L(0) = 0,
L(a) = {a}.
Note:
By the definition of regular expressions, the class of regular sets over
I is closed under union, concatenation and closure (iteration) by the conditions
2. 3. 4 of the definition.
EXAMPLE 5.1
Describe the following sets by regular expressions: (a) {101 L (b) {abba},
(c'{OL 1O}, (d) {A. ab}, (e) {abb. a, b, bba}, (f) {A, 0, 00, 000.... J, and
(g) {1, 1L 11 L ... }.
Solution
(a) Now. {l}. {OJ are represented by 1 and O. respectively. 101 is obtained
by concatenating L 0 and L So. {1OI} is represented by 101.
138
&2
Theory ofComputer Science
(b) abba represents {abba}.
(c) As {01, 1O} is the union of {01} and {10}, we have {Ot 1O}
represented by 01 + 10.
(d) The set {A, ab} is represented by A + abo
(e) The set {abb, a, b, bba} is represented by abb + a + b + bba.
(f) As {A, 0, 00, 000, ... } is simply {O}*. it is represented by 0*.
(g) Any element in {L 11, 11 L ... } can be obtained by concatenating
1 and any element of {l}*. Hence 1(1)* represents {I, 11, Ill, ...}.
EXAMPLE 5.2
Describe the following sets by regular expressions:
(a) L 1 =the set of all strings of O's and l's ending in 00.
(b) L 2 = the set of all strings of O's and l's beginning with 0 and ending
with 1.
(c) L3 = {A, 11, 1111, 111111, ... }.
Solution
(a) Any string in L] is obtained by concatenating any string over {O, I}
and the string 00. {O, I} is represented by 0 + 1. Hence L 1 is
represented by (0 + 1)* 00.
(b) As any element of L2 is obtained by concatenating 0, any stling over
{O, I} and 1, L 2 can be represented by 0(0 + 1)* 1.
(c) Any element of L3 is either A or a string of even number of l's, i.e.
a string of the form (11)",
11 ~ O. So L3 can be represented by (11)*.
5.1.1
IDENTITIES
FOR
REGULAR
EXPRESSIONS
Two regular expressions P and Q are equivalent (we write P = Q) if P and
Q
represent the same set of strings.
We now give the identities for regular expressions; these are useful for
simplifying regular expressions.
II
0 + R = R
I,
0R = R0 = 0
13
AR = RA = R
14
A* = A and 0* = A
Is
R+R=R
16
R*R* = R*
17
RR* = R*R
Is
(R*)* = R*
19
A + RR* = R* = A + R*R
110
(PQ)*P = P(QP)*
Chapter 5: Regular Sets and Regular Grammars
J;;,l,
139
III
(P + Q)* = (P*Q*)* = (p* + Q*)*
112
(P + Q)R = PR + QR
and
R(P + Q) = RP + RQ
Note: By the 'set P' we mean the set represented by the regular expression P,
The following theorem is very much useful in simplifying regular
expressions (i.e. replacing a given regular expression P by a simpler regular
expression equivalent to P).
Theorem 5.1
(Arden' s theorem) Let P and Q be two regular expressions
over I. If P does not contain A, then the following equation in R, namely
R = Q + RP
(5.1)
has a unique solution (i.e. one and only one solution) given by R = QP*.
Proof
Q + (QP*)P = Q(A + p*P) = QP* by 19
Hence (5.1) is satisfied when R =QP*. This means R =QP* is a solution
of (5.1).
To prove uniqueness. consider (5.1). Here, replacing R by Q + RP on the
R.H.S .. we get the equation
Q + RP = Q + (Q + RP)P
= Q + QP + RPP
= Q + QP + Rp2
= Q + QP + QP2 +
+ QP i + RP i+1
= Q(i\ + P + p 2 +
+ pi) + RP i+!
From (5.1),
R = Q(A + P + p 2 + ... + pi) + RP i+!
for i ~ 0
(5.2)
We nmv show that any solution of (5.1) is equivalent to QP*. Suppose R
satisfies (5.1), then it satisfies (5.2). Let w be a string of length i in the set
R Then
H" belongs to the set Q(A + P + p 2 + ... + pi) + RPi+1. As P does
not contain A, RPi+1 has no string of length less than i + 1 and so w is not
in the set RP+1. This means that w belongs to the set Q(A + P + p 2 + ...
+ P'J, and hence to QP*.
Consider a string w in the set QP*. Then tV is in the set QPk for some
k ~ 0, and hence in Q(A + P + p 2 + . . . + pk). So w is on the R.H.S. of
(5.2). Therefore, w is in R (L.H.S. of (5.2). Thus Rand QP* represent the
same set. This proves the uniqueness of the solution of (5.1).
I
Note:
Henceforth in this text, the regular expressions will be abbreviated Le.
~xampte 5.3
(a) Giye an Le. for representing the set L of strings in which every °is
immediately followed by at least two r s.
(b) Prove that the regular expression R =A + 1*(011)*(1* (011)*)* also
describes the same set of strings.
letting P::: = I, P3 = 011
140
J;!
Theory ofComputer Science
Solution
(a) If w is in L, then either (a) w does not contain any 0, or (b) it contains
a °preceded by 1 and followed by 11. So w can be written as
H' 1w::: ... wn , where each Wi is either 1 or OIl. So L is represented
by the Le. (1 + 011)*.
(b) R = A + PtPt, where P 1 = 1*(011)*
= p{
using 19
= (1*(011)*)*
= W:::*PJ')*
using III
= (1 + 011)*
EXAMPLE 5.4
Prove (1 + 00*1) + (1 + 00*1)(0 + 10*1)* (0 + 10*1) =0*1(0 + 10*1)*.
Solution
L.R.S. = (1 + 00*1) (A + (0 + 10*1)* (0 + 10*1)A
using 11:::
= (1 + 00*1) (0 + 10*1)*
using 19
= (A + 00*)1 (0 + 10*1)*
using 11::: for 1 + 00*1
= 0*1(0 + 10*1)*
using 19
= R.H.S.
5.2
FINITE AUTOMATA AND REGULAR EXPRESSIONS
In this section we study regular expressions and their representation.
5.2.1
TRANSITION SYSTEM CONTAINING A-MOVES
The transition systems can be generalized by permitting A-transitions or
A-moves which are associated with a null symbol A. These transitions can
occur when no input is applied. But it is possible to convert a transition system
with A-moves into an equivalent transition system without A-moves. We shall
give a simple method of doing it with the help of an example.
Suppose we want to replace a A-move from vertex V1 to vertex V2' Then
we proceed as follows:
Step 1
Find all the edges starting from v:::.
Step 2
Duplicate all these edges stalting from V1' without changing the edge
labels.
Chapter 5: Regular Sets and Regular Grammars
Q
141
Step 3
If 1') is an initial state, make
V~ also as initial state.
Step 4
If
1'~ is a final state. make Vj also as the final state.
EXAMPLE 5.5
Consider a finite automaton, with A-moves, given in Fig. 5.1. Obtain an
equivalent automaton without A-moves.
o
2
Fig. 5.1
Finite automaton of Example 5.5.
Solution
We first eliminate the A-move fram qo ta q) to get Fig. 5.2(a). qj is made
an initial state. Tnen we eliminate the A-move from qo to
q~ in Fig. 5.2(a)
to get Fig. 5.2(b). As
q~ is a final state. qo is also made a final state. Finally,
the A-move from q) to
q~ is eliminated in Fig. 5.2(c).
0
1
~
-8
A
i
A
(a)
2
A
2
(b)
o
2
2
2
(c)
Fig. 5.2
Transition system for Example 5.5, without A-moves.
142
~
Theory ofComputer Science
EXAMPLE 5.6
Consider a graph (i.e. transItIOn system), contammg a A-move, given in
Fig. 5.3. Obtain an equivalent graph (i.e. transition system) without A-moves.
Fig. 5.3
Finite automaton
of Example 5.6.
Solution
There is a A-move from qo to Q3' There are two edges, one from q3 to q2 with
label°and another from (]3 to q4 with label!. We duplicate these edges from
qo. As qo is an initial state. q3 is made an initial state. The resulting transition
graph is given in Fig. 5.4.
o
__~:GO
Fig. 5.4
Transition system for Example 5.6, without A-moves.
5.2.2
NDFAs WITH A-MOVES AND
REGULAR EXPRESSIONS
In this section, we prove that every regular expression is recognized by a
nondeterministic finite automaton (NDFA) with A-moves.
Theorem 5.2 (Kleene's theorem) If R is a regular expression over L
representing L ~ L*. then there exists an NDFA M with A-moves such that
L = TUvll.
Proof
The proof is by the pnnciple of induction on the total number of
characters in R. By 'character' we mean the elements of L, A, 0, * and +.
For example, if R =A + 10*11*0. the characters are A, +, 1, 0, *. 1, 1, *,
0, and the number of characters is 9.
Let L(R! denote the set represented by R.
Chapter 5: Regular Sets and Regular Grammars
~
143
Basis.
Let the number of characters in R be 1. Then R =A, or R = 0, or
R = ai, ai E I. The transition systems given in Fig. 5.5 will recognize these
regular expressions.
-O~~
R =A
R =$
R =8 i
Fig. 5.5
Transition systems for recognizing elementary regular sets.
Induction step. Assume that the theorem is true for regular expressions having
n characters. Let R be a regular expression having
11 + 1 characters. Then,
R=P+Q
or
R = PQ
or
R = p*
according as the last operator in R is +, product or closure. Also P and Q are
regular expressions having 11 characters or less. By induction hypothesis. L(P)
and L(Q) are recognized by M j and M: where M 1 and M: are NDFAs with
A-moves, such that L(P) = T(M j ) and L(Q) = T(M:). M j and M: are
represented in Fig. 5.6.
r
o
01
o
o
Fig. 5.6
Nondeterministic finite automata M1 and M2.
The initial state and the final states of M j and M: are represented in the usual
way.
Case 1
R = P + Q. In this case we construct an NDFA M with A-moves
that accepts L(P + Q) as fo11O\'/s: qo is the initial state of M, qo not in M j
or M:. qj is the final state of M: once again q( not in M j or M:. M contains
all the states of M, and M: and also their transitions. We add additional
A-transitions from qo to the initial states of M1 and M: and from the final
states of M j and M: to qt. The NDFA M is as in Fig. 5.7. It is easy to see
that TUI1) = T(MJ u
T(M2) = L(P + Q).
Case 2
R =PQ. In this case we introduce qo as the initial state of M and
qr as the final state of M. both qo, qr not in M] or M2. New A-transitions are
added between qo and the initial state of M j • between final states of M j and
the initial state of M2• and between final states of M: and the final state qr of
M. See Fig. 5.8.
144
l;.l
Theory ofComputer Science
A
M2
Fig. 5.7
NDFA accepting L(P + Q).
A
A
M1
~
Fig. 5.8
NDFA accepting L(PQ).
Case 3
R = (P)*. In this case, qo, q and qt are introduced. New A-transitions
are introduced from qo to q, q to qr, q to the initial state of M] and from the
final states of M] to q. See Fig. 5.9.
Thus in all the cases. there exists an NDFA M with A-moves, accepting
the regular expression R with n + 1 characters. By the principle of induction.
this theorem is true for all regular expressions.
I
A
A
qo )------J
l------JG
Fig. 5.9
NDFA accepting L(P*).
Chapter 5: Regular Sets and Regular Crammars
&2
145
Theorem 5.1 gives a method of constructing NDFAs accepting P + Q, PQ
and P* using the NDFAs corresponding to P and Q. In the later sections we
give a method of converting NDFA M with A-moves into an NDFA M]
without A-moves and then into a DFA M 2 such that T(lv!) = T(M]) =T(M2).
Thus, if a regular expression P is given, we can construct a DFA accepting
UP).
The following theorem is regarding the converse. Both the Theorems 5.2
and 5.3 prove the equivalence of regular expressions or regular sets and the
sets accepted by deterministic finite automata.
Theorem 5.3
Any set L accepted by a finite automaton M is represented by
a regular expression.
Proof
Let
M = ({ql ... q",}, L. 8, qj. F)
The construction that we give can be better understood in terms of the state
diagram of AI. If a string W E L* is accepted by M, then there is a path from
q] to some final state with path value w. So to each fmal state. say qi' there
corresponds a subset of L* consisting of path values of paths from qo to '7j'
As T(M) is the union of such subsets of L*. it is enough to represent them by
regular expressions. So the main part of the proof lies in the construction of
subsets of path values of paths from the state qi to the state qi'
Let Pi7 denote the set of path values of paths from qj to % whose
intermediate vertices lie in {qj, .. '. qd. We construct pb for k = 0, 1. ....
n recursively as follows:
p O= {a E
L
1 b(qj. a) = qi}
(5.3)
l.J
p.o = {a
E L! 8(qi' a) = q;}
U
{A}
(5.4)
I!
pl; = pk-l (pk-l) * pH
,pH
(5.5)
1J
Ik
kk
k)
U
I]
In terms of the state diagram, the construction can be understood better.
P;J simply denotes the set of path values (i.e. labels) of edges from qi to qi'
In p;7 we include A in addition to labels of self-loops from qi' This explains
(5.3) and (5.4).
Consider a path from
qj to
qi whose intermediate vertices lie in
{qj...., qd, If the path does not pass through qk' then its path value lies in
p;;-l. Othemise. the path passes through qk possibly more than once. The path
can be split into several paths with path values H.'l' H'2 ... 'VI as in Fig. 5.10.
W = 11'IW2 .. ,
WI'
Wi is the path value of the path from qi to qk (without
passing through qk. i.e. qk is not an intermediate vertex). W2' . , ., 11'1-1 are the
path values of paths from qk to itself without passing through qk' WI is the path
value of the path from qk to qj without passing through q/:. So Wi is in p;Z-l.
W2' ,."
WI_l are in (pA- i ). and WI is in
Pk~-l. This explains (5.5).
146
g
Theory of Computer Science
Fig. 5.10
A path from q, to qj'
We prove that the sets introduced by (5.3)-(5.5) are represented by
regular expressions by induction on k (for all i and j). P;J is a finite subset of
I.. say {aJ . ..., arlo Then. P;J is represented by PiJ = aJ + a2 + ... + at·
Similarly, we can construct p8 representing p;;.
Thus, there is basis for
induction.
Let us assume the result for k - 1, i.e. p;;-l is represented by a regular
.
p k- 1.,
all'
d' F
(5 ~)
h
pk
pk-l (pk-l) '" pk-l
pk-l
expresslOn
ii
lor
1 an j.
rom..), we
ave
ij =
ik
kk
"kj
U
ij
•
S
"
b'
h
pk.
d b
pk
pk-l(pk-J) '" pk-l
pk-I
o 1t IS
0
VIOUS t at
ij
IS represente
Y
ij =
ik
kk
.
kj
U
ij
.
Therefore, the result is true for all k. By the principle of induction, the sets
constructed by (5.3)-(5.5) are represented by regular expressions.
As Q= {q J••• " qlll}, prj denotes the set of path values of all paths from
11
ql to %. If F = {qii' ..., qr,,}, then T(M) = .~/y~. So T(N!) is represented
by the regular expression P{'f
+ ... + pn. Thus, L = T(M) is represented
by a regular expression.
Note: P;J and P;; are the subsets of I. U {A}, and so they are finite sets. So
every P;7 is obtained by applying union, concatenation and closure to the set
of all singletons in I. U {A}. Using this we prove Kleene's theorem (Theorem
5.4) at the end of this section. Kleene's theorem characterizes the regular sets
in terms of subsets of I. and operations (union, concatenation, closure) on
singletons in I. U
{A}.
5.2.3
CONVERSION OF
NONDETERMINISTIC SYSTEMS TO
DETERMINISTIC SYSTEMS
The construction we are going to give is similar to the construction of a DFA
equivalent to an ]\)1)FA and involves three steps.
Step 1
Convert the given transition system into state transition table where
each state corresponds to a row and each input symbol corresponds to a
column.
Step 2
Construct the successor table which lists the subsets of states
reachable from the set of initial states. Denote this collection of subsets by Qt.
Step 3
The transition graph given by the successor table is the required
deterministic system. The final states contain some final state of NDFA. If
possible. reduce the number of states.
Chapter 5: Regular Sets and Regular Grammars
Q
147
Note:
The construction is similar to that given in Section 3.7 for automata
except for the initial step. In the earlier method for automata, we started with
[qo]. Here we start with the set of all initial states. The other steps are similar.
EXAMPLE 5.7
Obtain the deterministic graph (system) equivalent to the transition system
given in Fig. 5.11.
\
I----a------{G
Fig. 5.11
Nondeterministic transition system of Example 5.7.
Solution
We construct the transition table corresponding to the given nondeterministic
system. It is given in Table 5.1.
TABLE 5.1
Transition Table for Example 5.7
StateII.
a
b
---<§J
q1. q2
q1
qQ
@
qQ, q1
We construct the successor table by starting with [qo, qj]. From Table 5.1
we see that [qo, qj,
q~] is reachable from [qo, qtJ by a b-path. There are no
a-paths from [qo, qtJ· Similarly, [qo, qd is reachable from [qQ, qj, q2] by an
a-path and
[qo, qj, q2] is reachable from itself. We proceed with the
construction for all the elements in
Q~
We terminate the construction when all the elements of Q' appear in the
successor table. Table 5.2 gives the successor table. From the successor table
it is easy to construct the deterministic transition system described by Fig. 5.12
TABLE 5.2
Deterministic Transition Table for Example 5.7
Q
a
b
[qQ. q,,]
0
[qQ. q1. q2]
[qQ, q1' q2]
[qQ, q1]
[qQ, q1, q2]
0
0
0
148
~
Theory ofComputer Science
a
a
b
Fig. 5.12
Deterministic transition system for Example 5.7.
as go and q2 are the final states of the nondeterministic system [qo, q]l and
kin, ql" q2] are the final states of the deterministic system.
5.2.4
ALGEBRAIC
METHOD USING ARDEN'S THEOREM
The following method is an extension of the Arden's theorem (Theorem 5.1).
This is used to find the Le. recognized by a transition system.
The following assumptions are made regarding the transition system:
(i) The transition graph does not have A-moves.
(ii) It has only one initial state, say VI'
(iii) Its vertices are
1'\ .••
V".
(i\) Vi the Le. represents the set of strings accepted by the system even
though v, is a final state.
(Y)
(Xii denotes the I.e. representing the set of labels of edges from l'i to
1> When there is no such edge. aU == 0. Consequently, we can get the
following set of equations in Vi .. , V,,:
VI == Viall + V 2a 21 + ... + Vila,,] + A
By repeatedly applying substitutions and Theorem 5.1 (Arden's theorem),
we can express Vi in terms of aii's.
For getting the set of strings recognized by the transition system, we have
to take the 'union" of all V;'s corresponding to final states.
EXAMPLE 5.R
Consider the transition system given in Fig. 5.13. Prove that the strings
recognized are (a + a(b + aa)*b)* arb + aa)* a.
Chapter 5: Regular Sets and Regular Grammars w
149
a
b
Fig. 5.13
Transition system of Example 5.8.
Solution
We can directly apply the above method since the graph does not contain any
A-move and there is only one initial state.
The three equations for q]. q2 and q3 can be written as
ql = q]a + q2b + A,
q2 = q]a + q2b + q3a.
q3 = q2a
It is necessary to reduce the number of unknowns by repeated substitution. By
substituting q3 in the q2-equation. we get by applying Theorem 5.1
q2 = q]a + q2b + q2aa
= qla + q2(b + aa)
= qla(b + aa)*
Substituting q2 in ql' we get
ql = qla + qla(b + aa)*b + A
= ql(a + a(b + aa)*b) + A
Hence,
ql = A(a + a(b + aa)*b)*
q2 = (a + a(b + aa)*b)* a(b + aa)*
q3 = (a + a(b + aa)*b)* a(b + aa)*a
Since q3 is a final state, the set of strings recognized by the graph is given by
(a + a(b + aa)*b)*a(b + aa)*a
EXAMPLE 5.9
Prove that the finite automaton whose transitIOn diagram is as shown in
Fig. 5.14 accepts the set of all strings over the alphabet {a, b} with an equal
number of a's and b's, such that each prefix has at most one more a than the
b's and at most one more b than the a's.
a
b
b
a
a,b
Fig. 5.14
Finite automaton of Example 5.9.
150
~
Theory ofComputer Science
Solution
We can apply the above method directly since the graph does not contain the
A-move and there is only one initial state. We get the following equations for
q), Q2. q3' q4:
qj = q2b + q3a + A
q2 = qla
q3 = qjb
q4 = q2a + q3b + q4a + q4b
As ql is the only final state and the qj-equation involves only q2 and q3' we
use only qr and qrequations (the q4-equation is redundant for our purposes).
Substituting for q2 and q3' we get
q 1 = q jab + q1ba + A = q1(ab + ba) + A
By applying Theorem 5.1, we get
ql = A(ab + ba)* = (ab + ba)*
As qj is the only final state, the strings accepted by the given finite automaton
are the strings given by (ab + ba)*. As any such string is a string of ab's,
and ba's, we get an equal number of a's and b's. If a prefix x of a sentence
accepted by the finite automaton has an even number of symbols, then it
should have an equal number of a's and b's since x is a substring formed by
ab's and ba's. If the prefix x has an odd number of symbols, then we can write
x as ya or yb. As y has an even number of symbols, y has an equal number
of a's and b's. Thus, x has one more a than b or vice versa.
EXAMPLE 5.10
DesClibe in English the set accepted by the finite automaton whose transition
diagram is as shown in Fig. 5.15.
Fig. 5.15
Finite automaton of Example 5.10.
Solution
We can apply the above method directly as the transition diagram does not
contain more than one initial state and there are no A-moves. We get the
following equations for qlo q2' q3'
qj = q10 + A
q2 = q11 + q21
q3 = q20 + q3(O + 1)
Chapter 5: Regular Sets and Regular Grammars
~
151
By applying Theorem 5.1 to the qj-equation, we get
qj = AO* = 0*
So.
Therefore,
q:; = (0*1)1*
As the final states are qj and q:;, we need not solve for q3:
qj + q:; = 0* + 0*(11*) = O*(A + 11*) = 0*(1*)
by 19
The strings represented by the transition graph are 0*1*. We can interpret the
strings in the English language in the following way: The strings accepted
by the finite automaton are precisely the strings of any number of O's (possibly
A) followed by a string of any number of l's (possibly A).
EXAMPLE 5.11
Construct a regular expression corresponding to the state diagram described by
Fig. 5.16.
o
o
o
Fig. 5.16
Finite automaton of Example 5.11.
Solution
There is only one initial state. Also, there
ar~ no A-moves. The equations are
ql = qjO + q30 +A
q:; = q11 + q:;1 + q31
q3 = q:;O
So.
q:; = qj1 + q:;1 + (q:;0)1 = qjl + q:;(l + 01)
By applying Theorem 5.L we get
q:; = q jl(1 + 01)*
152
g,
Theory of Computer Science
Also,
q] = q10 + q30 + A = qIO + q200 + A
= q10 + (q11(1 + 01)*)00 + A
= ql(O + 1(1 + 01)* 00) + A
Once again applying Theorem 5.1, we get
ql = NO + 1(1 + 01)* 00)* = (0 + 1(1 + 01)* 00)*
As ql is the only final state, the regular expression corresponding to the given
diagram is (0 + 1(1 + 01)* 00)*.
EXAMPLE 5.12
Find the regular expression corresponding to Fig. 5.17.
o
o
o
Gf------------i
o
Fig. 5.17
Finite automaton of Example 5.12.
Solution
There is only one initial state. and there are no A-moves. So, we form the
equations cOlTesponding to qj, q2, q3, q4:
ql = q10 + q30 + q40 + A
q2 = qll + q21 + q41
q3 = Q20
NO\v.
Thus. we are able to write q" q4 in terms of q2' Using the qTequation, we
get
Chapter 5: Regular Sets and Regular Grammars
Q
153
By applying Theorem 5.1. we obtain
q2 = (qi1)(1 + 011)* = q](l(l + 011)*)
From the ql-equation, we have
ql = qlO + q200 + C12010 + A
= q]O + q2(00 + 010) + A
= q]O + q]l(l + 011)* (00 + 010) + A
Again, by applying Theorem 5. L we obtain
ql = A(O + 1(1 + 011)* (00 + 010»*
q.. = Q201 = q1l(1 + 011)* 01
= (0 + 1(1 + 011)*(00 + 010»*(1(1 + 011)* 01)
5.2.5
CONSTRUCTION OF FINITE AUTOMATA EQUIVALENT
TO A
REGULAR
EXPRESSION
The method we are going to give for constructing a finite automaton
equivalent to a given regular expression is called the subset method which
involves two steps.
Step 1
Construct a transition graph (transition system) equivalent to the
given regular expression using A-moves. This is done by using Theorem 5.2.
Step 2
Construct the transition table for the transition graph obtained in
step L Using the method given in Section 5.2.3, construct the equivalent DFA.
We reduce the number of states if possible.
. EXAMPLE 5.13
Construct the finite automaton equivalent to the regular expression
(0 + 1)*(00 + 11)(0 + 1)*
Solution
Step 1
(Construction of transitIon graph) First of all we construct the
transition graph with A-moves using the constructions of Theorem 5.2. Then
we eliminate A-moves as discussed in Section 5.2. L
We start \vith Fig. 5.18(a).
V'le eliminate the concatenations in the given Le. by introducing new
vertices CJI and Ci2 and get Fig. 5.18(b).
We eliminate the'" operations in Fig. 5.18(b) by introducing two new
vertices Cis and CJ6 and the A-moves as shown in Fig. 5.18(c).
We eliminate concatenations and + in Fig. 5.18(c) and get Fig. 5.18(d).
We eliminate the A-moves in Fig.5.18(d) and get Fig. 5.18(e) which
gives the 1',TDFA equivalent to the gIven i.e.
154
g
Theory ofComputer Science
(0 +1 )*(00 + 11)(0 + 1)*
r:\
}------------1~
(a)
(b)
0+1
0+1
(c)
(d)
-
qo
°
0,1
(e)
Fig.5.18
Construction of finite automaton equivalent to (0 + 1)*(00 + 11)(0 + 1)*.
Step 2
(Construction of DFA) We construct the transition table for the
NDFA defined by Table 5.3.
TABLE 5.3
Transition Table for Example 5.13
State/I
0
---) qQ
qQ, q3
qQ, q4
q3
qf
q4
qf
(q';'\
qf
qf
\J
----------- ---------
~~~-
Chapter 5: Regular Sets and Regular Grammars
~
155
The successor table is constructed as given in Table 5.4.
TABLE 5.4
Transition Table for the DFA of Example 5.13.
o
---? [qQJ
[qQ, q3]
[qQ. q4]
[qQ. q3, qtl
[qQ. q4. qt]
OQ
[qQ, q31
[qQ, q3, qt]
[qQ, q3]
[qQ, q3· qtl
[qQ, q3· qt]
[qQ, q4]
[qQ, q4]
[qQ, q4, qfl
[qQ, q4, qfl
[qQ, q4, qtl
The state diagram for the successor table is the required DFA as described by
Fig. 5.19. As qf is the only final state of NDFA [qo, q3, qf] and [qo, %
qf]
are the final states of DFA
Fig. 5.19
Finite automaton of Example 5.13.
Finally. we try to reduce the number of states. (This is possible when two
rows are identical in the successor table.) As the rows conesponding to
[qo, q3' qf] and [qo, q4' qf] are identical. we identify them. The state diagram
for the equivalent automaton. where the number of states is reduced, is
described by Fig. 5.20.
°
0,1
Fig. 5.20
Reduced finite automaton of Example 5.13.
Note:
While constructing the transition graph equivalent to a given I.e., the
operation (concatenation. "'. +) that is eliminated first, depends on the regular
expresslon.
156
~
Theory ofComputer Science
EXAMPLE 5.14
Constmct a DFA with reduced states equivalent to the r.e. 10 + (0 + 11»)0*1.
Solutioh
Step 1
(Construction of NDFA) The l'<TIFA is constmcted by eliminating the
operation +. concatenation and *. and the A-moves in successive steps. The
step-by-step constmction is given in Figs. 5.21(a)-5.21(e).
-6)f-----:.1;;:..o_+.;.:(0,--(:_)1:-:1,L)0;;:..*.;..1__~.0
10
(0 + 11)0*1
(b) Eiimination of +,
o
(c\ Elimination of concatenation and *
'~qr
c
---+1~
o
(d) Elimination of +,
o
o
0
\
(\
1\
~
\
/'*
GJ'
(e) Elimination of .\-moves.
Fig. 5.21
Construction of finite automaton for Example 5.14,
Chapter 5: Regular Sets and Regular Grammars
~
157
Step 2
(Construction of DFA) For the NDFA given in Fig. 5.18(e). the
corresponding transition table is defined by Table 5.5.
TABLE 5.5
Transition Table for Example 5.14
StatelI
0
---0> qo
q3
q1, q2
q1
qr
q2
q3
q3
q3
qr
®
The successor table is constructed and given in Table 5.6.
In Table 5.6 the columns corresponding to [qtJ and 0 are identical. So we
can identify [qtJ and 0.
TABLE 5.6
Transition Table of DFA for Example 5.14
Q
Qo
Q1
---0> [Qo]
[q3]
[q1, q2]
[q3]
[q3]
[qr]
[q1' Q2]
[qrJ
[q3]
@
0
0
0
0
0
The DFA with the reduced number of states corresponding to Table 5.6
is defined by Fig. 5.22.
o
o
o
Fig. 5.22
Reduced DFA of Example 5.14.
5.2.6
EQUIVALENCE OF Two FINITE AUTOMATA
T\vo finite automata over L are equivalent if they accept the same set of strings
over L. When the two finite automata are not equivalent, there is some string
158
g
Theory ofComputer Science
vI' over 2: satisfying the following: One automaton reaches a final state on
application of w, whereas the other automaton reaches a nonfinal state.
\Ve give below a method. called the comparison method. to test the
equivalence of two finite automata over 2:.
Comparison Method
Let lVJ and 1);1' be two finite automata over 2:. \Ve construct a comparison table
consisting of n + 1 columns. where n is the number of input symbols. The first
column consists of pairs of vertices of the form (q, q'), where q E M and q'
EM'. If (q,
g') appears in some row of the first column, then the
corresponding entry in the a-column (a E 2:) is (q", q;'). where qa and q;' are
reachable from q and qt. respectively on application of a (i.e. by a-paths).
The comparison table is constructed by starting with the pair of initial
vertices qin. q(n of M and M'in the first column. The first elements in the
subsequent columns are (qa. q;,), where qa and q;' are reachable by a-paths
from qin and qili' We repeat the construction by considering the pairs in the
second and subsequent columns which are not in the first column.
The row-wise construction is repeated. There are t\VO cases:
Case 1
If we reach a pair (q. q') such that q is a final state of M, and q' is
a nonfinal state of M' or vice versa, we terminate the construction and
conclude that l'Y1 and /vI' are not equivalent.
Case 2
Here the construction is tenninated when no new element appears in
the second and subsequent columns which are not in the first column (i.e.
when all the elements in the second and subsequent columns appear in the first
column). In this case we conclude that M and M' are equivalent.
EXAMPLE 5.1 5
ConSIder the fo]]owing two DFAs AI and M' over {O, I} given in Fig. 5.23.
Determine whether M and i'v1' are equivalent.
c
(a)
(b)
Fig. 5.23
(a) Automaton M and (b) automaton M'.
___--=C=..:h=..:a~p=_t=_=_er 5: Regular Sets and Regular Grammars
~
159
Solution
The initial states in ",vI and [,yI' are
CJ1 and q.... respectively. Hence the first
element of the first column in the comparison table must be (q 1. q...). The first
element in the second column is (ql. CfJ,) since both CJi and CJ4 are c-reachable
from the respective initial states. The complete table is given in Table 5.7.
TABLE 5.7
Comparison Table for Example 5.15
(q. q',
(q•. q4)
(q2
q5)
(q3
q6)
(q2·
q~)
(qd. q')
0,
(q2
q5)
(q,.
q4)
lQ3· q6)
(q,. q4)
As we do not get a pair (q,
CJ' ), where Cf is a final state and q' is a nonfinal
state (or vice wrsa) at every rO\v, we proceed until all the elements in the
second and third COIUIIl.l1S are also in the first column. Therefore. M and J'vY
are equivalent.
EXAMPLE 5.16
Shm" that the automata IvI l and [\iI: defined by Fig. 5.24 are not equivalent.
c
(a)
(b)
Fig. 5.24
(a) Automaton M, and (b) automaton M2,
Solution
The initial states in !'vII and M: are ql and q.... respectively. Hence the first
column in the comparison table is (ql' Cf4)' Cfc and
Cf~ are d-reachable from Cfl
and q.... We see from the comparison table given in Table 5.8 that CJl and Cfr,
arc d-reachable from Cf: and Cfe;, respectively, As Cfl is a final state in Nit. and
Cf6 is a nonfinal state in Alc. we see that Ml and l\ilc are not equivalent: we can
also note that Cfl is dd-reachable from qj, and hence dd is accepted by MI' dd
is not accepted by M: as only Cfro is dd-reachable from Cj.'" but CJ6 is nonfinal.
160
~
Theory ofComputer Science
TABLE 5.8
Comparison Table for Example 5.16
(q. c()
(qi, q4)
(q2' qs)
(q'I' q4)
(Q3' q7)
5.2.7
EQUIVALENCE
OF Two
REGULAR
EXPRESSIONS
Suppose we
are interested in testing
the equivalence of two regular
expressions, say P and Q. The regular expressions P and Q are equivalent iff
they represent the same set. Also, P and Q are equivalent iff the corresponding
finite automata are equivalent.
To prove the equivalence of P and Q, (i) we prove that the sets P and Q
are the same. (For nonequivalence we find a string in one set but not in the
other.) Or (ii) we use the identities to prove the equivalence of P and Q. Or
(iii) we construct the corresponding FA M and M' and prove that M and M' are
equivalent. (For nonequivalence we prove that M and lvi' are not equivalent.)
The method to be chosen depends on the problem.
EXAMPLE 5.1 7
Prove (a + b)* = a*(ba*)*.
Solution
Let P and Q denote (a + b)* and a*(ba*)*, respectively. Using the construction
in Section 5.2.5, P is given by the transition system depicted in Fig. 5.25.
A
a, b
I-----A---..{O
a, b
Fig. 5.25 Transition system for (a + b)*.
The transition system for Q is depicted in Fig. 5.26.
It should be noted that Figs. 5.25 and 5.26 are obtained after eliminating
A-moves. As these two transition diagrams are the same, we conclude that
P = Q.
. ..
We now summarize all the results and constructions given in this section.
(i) Every r.e. is recognized by a transition system (Theorem 5.2).
(ii) A transition system M can be converted into a finite automaton
accepting the same set as M (Section 5.2.3).
(iii) Any set accepted by finite automaton is represented by an r.e.
(Theorem 5.3).
(iv) A set accepted by a transition system is represented by an r.e. (from
(ii) and (iii».
Chapter 5: Reguiar Sets and Reguiar Grammars
l;!
161
a
A
8f---
A _i\-:ur--_
c
\ -.10
/ya
b
~
a
t5
~
a
a
Fig. 5.26
Transition system for a*(ba*)*.
(v) To get the I.e. representing a set accepted by a transition system. we
can apply the algebraic method using the Arden's theorem (see
Section 5.2.4).
(vi) If P is an Le., then to construct a finite automaton accepting the set
P. we can apply the construction given in Section 5.2.5.
Ivii) A subset L of I* is a regular set (or represented by an Le.) iff it is
accepted by an FA (from (i), (ii) and (iii»).
(viii) A subset L of I" is a regular set iff it is recognized by a transition
system (from (i) and (iv)).
(ix) The capabilities of finite automaton and transition systems are the
same as far as acceptability of subsets of strings is concerned.
(x) To test the equinlence of two DFAs. we can apply the comparison
method given in Section 5.2.6.
We conclude this section with the Kleene' s theorem.
Theorem 5A
(Kleene's theorem) The class of regular sets over I
is the
smallest class R. containing {a} for every a
E I
and closed under union.
concatenation and closure.
Proof
The set {a} is represented by the regular expression a. So {a} is
regular for every a E I. As the class of regular sets is closed under union,
concatenation. and closure. R. is contained in the class of regular sets.
162
};l
Theory ofComputer Science
Let L be a regular set. Then L = T(M) for some DFA, M = ({ %
qll/}, L. 8,
qQ. F). By Theorem 5.3.
11
L = U P{'
j=l
f
where F = {q.r·· ... q,} and PIt is obtained by applying union, concatenation
1
.I I }
and closure to singletons in L. Thus, L is in n.
I
5.3
PUMPING
LEMMA FOR REGULAR SETS
In this section we give a necessary condition for an input string to belong to
a regular set. The result is called pumping lemma as it gives a method of
pumping (generating) many input strings from a given string. As pumping
lemma gives a necessary condition, it can be used to show that certain sets are
not regular.
Theorem 5.5
(Pumping Lemma) Let M = (Q, L, 8, qQ, F) be a finite
automaton with 11 states. Let L be the regular set accepted by M. Let It' E L
and I wi;:: m. If m ;:: 11, then there exists x, )', z such that w =xv.:::, y i= A and
.\}'i.::: E L for each i ;:: O.
Proof
Let
m;::n
8(qQ,
ala2 ... aJ = qi
for i = 1, 2. , .., 111;
That is, Ql is the sequence of states in the path with path value w = ala2 ... al/1'
As there are only Ii distinct states, at least two states in Q1 must coincide.
Among the various pairs of repeated states, we take the first pair. Let us take
them as % and q/.:(ql = q/.:). Then j and k satisfy the condition °:::; j < k :::; n.
The string w can be decomposed into three substrings ala2 ... aj. aj+! ...
ak and ak+l '"
al/1' Let x, y,
.::: denote these strings ala:'. ... ai' al+l ... ak>
a/.:+ 1 ... am. respectively, As k :::; n. Ix)'I :::;
J1 and w =xy.:::. The path with the
path value w in the transition diagram of M is shown in Fig. 5.27.
Tne automaton M starts from the initial state qQ. On applying the string
x, it reaches q/(=q/.:). On applying the string y, it comes back to q/= qJ So
after application of v' for each i ;:: 0, the automaton is in the same state qj'
On applying .:::. it reaches qll/' a final state. Hence, xy'.::: E L. As every state in
Ql is obtained by applying an input symbol, y i= A.
I
Fig. 5.27
String accepted by M.
Chapter 5: Regular Sets and Regular Crammars
;g,
163
lV'ote:
The decomposition is valid only for strings of length greater than or
equal to the number of states. For such a string w = XVZ, we can 'iterate' the
substring y in .ct)'Z as many times as we like and get strings of the form .\),iZ
which are longer than .HZ and are in L. By considering the path from qo to
qk and then the path from qk to q", (without going through the loop), we get
a path ending in a final state with path value xz. (This corresponds to the case
when i = 0.)
5.4
APPLICATION OF PUMPING
LEMMA
This theorem can be used to prove that certain sets are not regular. We now
give the steps needed for proving that a given set is not regular.
Step 1
Assume that L is regular. Let Il be the number of states in the
corresponding finite automaton.
Step 2
Choose a string H' such that I w I 2: n. Use pumping lemma to write
W = ,\}'Z. with I·y)' I 5
11 and Iy I > O.
Step 3
Find a suitable integer i such that x";z E
L. This contradicts our
assumption, Hence L is not regular.
Note:
The crucial part of the procedure is to find i such that .\'/z eo L. In
some cases we prove .\'}Jz It: L by considering i xyiz j. In some cases we may
have to use the 'structure' of strings in L
EXAMPLE 5.18
Show that the set L = {ail I i 2: I} is not regular.
Solution
Step 1
Suppose L is regular. Let n be the number of states in the finite
automaton accepting L.
Step 2
Let w =a"
l
. Then Iw 1= 112 > 11. By pumping lemma, we can write
w = x)'z vvith
I.ct'}, I 5 n and Iy I > 0,
Step 3
Consider .\}.2Z, 1.\}·2zl = Ixl + 211'1 + Izi > Ixl + Iyl + Izi as
Iy
> 0. This means n2 = I·\'}'z 1= Ix i + Iy 1 + i z I < i.ct'}·2;: I· As Ixy i 5
Il,
we have Iy I 5
Il. Therefore,
1·\)'2;: i = Ix I + 21 y i + !;: I 5 n2 + n
i.e.
o
'I'
,
n- < I.\\'-z I 5 n- +
11 < n- +
11 + n + 1
Hence, i x),2Z r strictly lies between n2 and (n + 1)2, but is not equal to any
one of them. Thus
1.\},2;: I is not a pelfect square and so .\\,2;: It: L. But by
pumping lemma, xy2;: E L. Trlis is a contradiction.
164
9
Theory ofComputer Science
EXAMPLE 5.19
Show that L = {ai'
1 p is a prime} is not regular.
Solution
Step 1
\Ve suppose L is regular. Let II be the number of states in the finite
automaton accepting L.
Step 2
Let p be a prime number greater than n. Let }t' = aF. By pumping
lemma. H' can be written as w =xyz. with
1 xy 1 ~ nand !y 1 > O. x, y. z are
simply strings of a's. So. y = a
lll for some 111 e: 1 (and ~ n).
Step 3
Let i =p + 1. Then !.r";z I = I·\\'z I + 1)'1-11 = p + (i - l)m =p +
pm. By pumping lemma. .\).iz E L. But
1 xylZ
I = p + pm =p(l + 111). and p(l
+ m) is not a prime. SO A}JZ EO L. This is a contradiction. Thus L is not regular.
EXAMPLE 5.20
Show that L = {Oil 1lie: I} is not regular.
Solution
Step 1
Suppose L is regular. Let n be the number of states in the finite
automaton accepting L.
Step 2
Let w = O"l/l. Then l}t,! = 271 > n. By pumping lemma, we write
W = :I.I'Z with IX}'I
~ n and IyI ::t O.
Step 3
We want to find i so that x.'/z
EO L for getting a contradiction. The
string \' can be in any of the following fonns:
Case 1
y has a's. i.e. y = Ok for some k e: l.
Case 2
,'has only l's. i.e. y = 11 for some I 2: 1.
Case 3
y has both O·sand l' s, i.e. y = Oklj for some k, j e: L
In Case 1. we can take i = O. As .'}.:: = 0"1". xz = on-kI". As k 2: 1. n -
k ::t n. So, xz
EO L.
In Case 2. take i = O. As before, xz is 0"1',-1 and n ::t n - I. So. xz liE L.
In Case 3. take i =2. As xvz =OH-kOkljl"-:i. AI/Z: =O"-k OkljOklj l"-:f. As xv2z
is not of the fonn Oil',
,\}'2Z EL.'
.
Thus in all the cases we get a contradiction. Therefore. L is not regular.
EXAMPLE 5.21
Show that L =
{WH' I W
E
{(I, b} *} is not regular.
Solution
Step 1
Suppose L is regular. Let n be the number of states in the automaton
1\1 accepting L.
Chapter 5: Regular Sets and Regular Grammars
~
165
Step 2
Let us consider ww = a"ba"b
in L. I H'H) I = 2(n + 1) > 11, Vv'e can
apply pumping lemma to write
WH' = ,i}';: with l.v I "* O. i xv i ::; n,
Step 3
We want to find i so that ~\'/;: ~ L for getting a contradiction. The
string y can be in only one of the following forms:
Case 1
:v has no b's. i.e. y = tl for some k ~ 1.
Case 2
y has only one b.
We may note that y cannot have two b's. If so. Iy I ~
11 + 2. But Iy I ::;
IX\ I::; n. In Case 1, we can take i = O. Then xvo;: = x;: is of the form el"ba"b,
\vhere III =
11 - k < 11 (or d'balllb). We cannot write x;: in the form lUI with
1I E
{a, b}*. and so x;: ~ L. In Case 2 too, we can take i =0, Then X).,o;: =x;:
has only one b (as one b is removed from xy;:, b being in y). So x;:
~ L as
any element in L should have an even number of a's and an even number of
b's.
Thus in both the cases we get a contradiction. Therefore. L is not regular.
Note:
If a set L of strings over I
is given and if we have to test whether
L is regular or not, we try to write a regular expression representing L using
the definition of L. If this is not possible. we use pumping lemma to prove
that L is not regular.
EXAMPLE 5.22
Is L = {a~" 111 ;:: I} regular?
Solution
We can write a~n as a(a~Ya, where i;:: O. NO\v
{(a~)i Ii;:: O} is simply
{a~}*.
So L is represented by the regular expression aW)*a, where P represents {a~}.
The corresponding finite automaton (using the construction given in Section
5.2.5) is shown in Fig. 5.28.
--G-
8----t:n
f---,_8--0
G
Fig. 5.28
Finite automaton of Example 5.22.
5.5
CLOSURE PROPERTIES OF
REGULAR SETS
In this section we discuss the closure properties of regular sets under (i) set
union, (ii) concatenation. (iii) closure (iteration), (iv) transpose. (v) set
intersection. and (vi) complementation.
Fig. 5.29
166
~
Theory ofComputer Science
--'------------
In Section 5.1. \ve have seen that the class of regular sets is closed under
union. concatenation and closure.
Theorem 5.6
If L is regular then [} is also regular.
Proof
As L is regular by (vii), given at the end of Section 5.2.7. we can
construct a finite automaton M = (Q, L, 8, qo, F) such that T(M) = L.
We construct a transition system M' by starting with the state diagram of
M, and reversing the direction of the directed edges. The set of initial states
of AI' is defined as the set F, and qo is defined as the (only) final state of LH~
i.e. M' = (Q, L
8'. F. {qo}).
If 11' E T(lvi), we have a path from qo, to some final state in F with path
value w. By 'reversing the edges', \ve get a path in M' from some final state
in F to qo' Its path value is w T. So wI" E T(Jvl'). In a similar way. we can
see that if 11'1 E T(M!), then 111 E T(Iv!). Thus from the state diagram it is
easy to see that T(M') = T(M/. We can prove rigorously that \V E
T(M) iff
w T E
T(M') by induction on
. So T(l\,,fyT = T(M'). By (viii) of Section
5.2.7. T(M') is regular. i.e. T(M)T is regular.
I
. EXAMPLE 5.23
ConSIder the FA AI given by Fig. 5.29. What is TCAi)? Show that T(Ml is
regular.
--01---------·e~1
U
/
o
/
I/0
~
0,1eJ:i)
Finite automaton of Example 5.23.
Solution
As the elements of TUv1) are given by path values of paths from qo to itself
or from
CJo to
CJl (note that we have two final states qo and qt), we can
construct T(A!) by inspection.
As arrows do not come into qo, the paths from qo to itself are self-loops
repeated any number of times. The corresponding path values are ai, i 2: l.
As no arrow comes from q: to qo or (Ji, the paths from qo to qI are of the
fom1 qo ... ---+ qo· .. ql ... ---+ ql' The corresponding path values are O'li,
where i 2: 0 and j 2: 1. As the initial state qo is also a final state, A E T(lvI).
Thus.
Hence.
Chapter 5: Regular Sets and Regular Grammars
~
167
The transition system M' is constructed as follows:
(i) The initial states of AI' are qo and qi'
(ii) The (only) final state of lvI' is qo.
(iii) The direction of the directed edges is reversed. M'is given in Fig. 5.30.
From (i)-(iii) it follows that
TUv!') = T(lUl
Hence. HM/ is regular.
o
0; (
(a
'
"0.2
)
"-.../
Fig. 5.30
Finite automaton of T(M) r
Note: In Example 5.23. we can see by inspection that T(M') = {Ii 01I i.
j :2: O}. The strings of TiM') are obtained as path values of paths from qo to
itself or from qj to C/o.
Theorem 5.7
If L is a regular set over I. then I* - L is also regular over I.
Proof
As L is regular by (viil. given at the end of Section 5.2.7, we can
construct a DFA fvl = (Q, I. 8. qo- F) accepting L. i.e. L = T(M).
We construct another DFA M' = (Q, I. 8. q!) r) by defining F' = Q - F,
i.e. ,'v! and M' differ only in their final states. A final state of [1,1' is a nonfinal
state of 1'1'1 and vice versa. The state diagrams of M and M' are the same except
for the final states.
t\" E HM') if and only if D(C/o. H) E r = Q - F. i.e. iff t\" eo L. This
pre-yes TUv1') = I'" - X.
I
Theorem 5.8
If X and Yare regular sets over I, then X n Y is also regular
over I.
Proof
By DeMorgan' s Im\ for sets. X n Y =I'" - ((I'" - Xl u (I* - Y). By
Theorem 5.7.
~> - X and I* - Yare regular. So. (I* - Xl u (2:* - Y) is
also regular. By applying Theorem 5.7. once agam 2:* -
((I* -
X, u
(I* -
Y)) is regular. i.e. )( n Y is regular.
I
5.6
REGULAR SETS AND
REGULAR GRAMMARS
We have seen that regular sets are precisely those accepted by DFA. In this
section we show that the class of regular sets over 2: is precisely the regular
languages mer the terminal set I.
168
J;!
Theory ofComputer Science
5.6.1
CONSTRUCTION OF A REGULAR GRAMMAR
GENERATING T(M) FOR A GIVEN DFA M
Let lv! = ({ q(j, .... qll}, L. 8. qo, F). If w is in T(M), then it is obtained by
concatenating the labels conesponding to several transitions, the first from qo
and the last tem1inating at some final state. So for the grammar G to be
constructed. productions should conespond to transitions. Also, there should
be provision for tenninating the derivation once a transition tenninating at
some final state is encountered. With these ideas in mind, we construct G as
G = ({Ao, Aj, .... All}, .L p. ,10)
where P is defined by the following rules:
(i) Ai -'> aA j is included in P if 8(qi' a) = qj eo F.
(ii) Ai -'> aA) and Ai -'> a are included in P if 8(qi' a) = qj E
F.
We can show that L(G) = T(M) by using the construction of P. Such a
construction gives
Ai:=} aAj
iff 8(qi, a) = qi
Ai:=} a
iff 8(qi' a) E
F
So.
,10
:=} alA 1
:=} ala:A:
:=} ...
:=} a1 ... ak-lAk
:=} ala2 ... ak
iff
8(qo. aj) = qjo
8(qj. a2) = q:• ...,
8(qk' ak) E
F
This proves that
H' = aj ... ak
E
L(G) iff 8(qo, aj ...
ak)
E
F, l.e. iff
W E
T(M).
EXAMPLE 5.24
Construct a regular grammar G generating the regular set represented by
P = a*b(a + b)*.
Solution
We construct the DFA conesponding to P using the construction given III
Section 5.2.5. The construction is shown in Fig. 5.31.
---+I'OI--_a*__'O}-__b
.~
A
a, b
Fig. 5.31
a
--o----:--Of----.-,\-I
b
j\
DFA of Example 5.24, with A-moves.
After eliminating the A-moves. we get the DFA straightaway, as shown in
Fig. 5.32.
Chapter 5: Regular Sets and Regular Grammars
~
169
a
a, b
r----b-l&
Fig. 5.32
DFA of Example 5.24, without A-moves.
Let G = ({AO, Ad. {a. b}. P. Ao). where P is given by
Al
---+ a.
,"
\
~ll!
G is the required regular grammar.
5.6.2
CONSTRUCTION OF A TRANSITION SYSTEM M
ACCEPTING
L(G) FOR A GIVEN REGULAR
GRAMMAR G
Let G = ({Ao. AI' .... A,,}. L. P, Ao). We construct a transition system ,11,1
whose (i) states conespond to variables. (ii) initial state conesponds to Ao.
and (iii) transitions in :H conespond to productions in P. As the last
production applied in any derivation is of the form Ai ---+ a, the conesponding
transition terminates at a ne,\' state. and this is the unique final state.
We define M as ({ qo- .... q/p qr}, L. O. qo, {qrD where <5 is defined as
follows:
(i) Each production Ai ---+aAi induces a transition from qi to CJi with
label a,
Each production Ak ---+ a induces a transition from
qk to
{fr with
label
a.
From the construction it is easy to see that Ao =? alA] =? aja.:?A2 =? ...
=? aj ... a,,_jAn_] =? aj ... an is a derivation of ala.:? ... all iff there is a
path in M starting from qo and temlinating in CJr with path value aja.:? ... all"
Therefore. L(Gl = TUv1).
EXAMPLE 5.25
Let G =({Ao. Ad, {a. b}. P, Ao). where P consists of Ao ---+ aA j • Al ---+ bA I ,
A I ---+ a, A!
---+ bAa. Construct a transition system M accepting L(G).
Solution
Let Iv! = ({ CJCi. {fl' {ft}· {a. b}. 8. {fCi,
{{ft})· where qo and qj correspond to Ao
and AI. respectively and Cit is the ne,v (final) state introduced. Ao ---+ aA j
induces a transition from
{fo to q] with label a. Similarly. A]
---+ bA l and
AI ---+ bAI) induce transitions from CJj to {fj with label b and from ql to qo with
170
g
Theorv ofComputer Science
label b, respectively. Al ~ a induces a transition from CJI to qf with label a.
M is given in Fig. 5.33.
b
~f--------a_00
b
Fig. 5.33
Transition system for Example 5.25.
EXAMPLE 5.26
If a regular grammar G is given by S ~ as Ia, find M accepting L(G).
Solution
Let qo correspond to 5 and qr be the new (final) state. M is given m
Fig. 5.34. Symbolically.
M = ({Cjo, qtl {a}. 8,
C/o' {qr})
a
@~_a-1
00
Fig. 5.34
Transition system for Example 5.26.
Note: If 5 ~ A is in P, the conesponding transition is from qo to qr with
label A.
By using the construction given in Section 5.2.3. we can construct a DFA
M accepting L(G) for a given regular grammar G.
5.7
SUPPLEMENTARY
EXAMPLES
EXAMPLE 5.27
Find a regular expression conesponding to each of the following subsets of
{a. b}.
(a) The set of all strings containing exactly 2(/ s.
(b) The set of all strings containing at least 2a·s.
Ic) The set of all strings containing at most 2a's.
Cd) The set of a]] strings containing the substring aa.
Chapter 5: Regular Sets and Regular Grammars
~
171
Solution
(a) b*ab*ab*
(b) (a + b)*a(a + b)*a(a + b)*
(c) b*ab*ab* + b*ab*
(d) (a + b)*aa(a + b)*
EXAMPLE 5.28
Find a regular expression consisting of all strings over {a, b} starting with any
number of a's, followed by one or more b's, followed by one or more a's,
followed by a single b, followed by any number of a's, followed by band
ending in any string of a's and b's.
Solution
The Le. is a*b b*a a*b(a + b)*.
EXAMPLE 5.29
Find the regular expression representing the set of all strings of the form
(a) d"b iid'
where In, n, p :2 1
(b) d"bc-lc3p
where
111, n, p :2 1
(c) a"bac-"'bc-
where
111 :2 O. n :2 1
Solution
(a) aa*bb*cc*
(b) aa*(bb)(bb)*ccc(ccc)*
(c) aa*b(aa)*bb
EXAMPLE 5.30
Find the sets represented by the following regular expressions.
(a) (a + b)*(aa + bb + ab + ba)*
(b) (aa)* + (aaa)*
(c) (1 + 01 + 001)*(1\ + 0 + 00)
(d) a + b(a + b)*
Solution
(a) The set of all strings having an odd number of symbols from {a, b}*
(b)
{x
E
{a}* I Ix I is divisible by 2 or 3}
(c) The set of all strings over {a, I} having no substring of more than
two adjacent O·s.
(d) {a. b, ba, bb, baa, bab, bba. bbb, ... }
172
~
Theory ofComputer Science
Shov, that {11
E
{a, b}* I W contains an equal number of a's and b's} is not
regular.
Solution
We prove this by contradiction. Assume that L =T(M) for some DFA M with
II states. Let W =a"b" ELand
1w,1 =2". Using the pumping lemma, we write
W =.\)'<' with Ixyl ~ nand Iyl > O. As xyz = a"b", A}' =ai where i ~ nand
hence y =d for some j, 1 ~j ~ n. Consider xy2Z. Now A)''::: has an equal number
of a's and b's. But xv2.::: has (n + j) a's and n b's. As n +.f 7:- II, x)'2.::: !C L.
This contradiction proves that L is not regular.
EXAMPLE 5.32
Show that L = {c/lJck I k > i + j} is not regular.
Solution
We prove this by contradiction. Assume L = T(A1) for some DFA with n
states. Choose IV = d'b"c3" in L. Using the pumping lemma, we write w =.\)'':::
with 1.\)'I ~ n and Iy! > O. As W = a"IJ"c3", xy = ai for some i ~ n. Tnis means
that Y = (Ii for some j, 1 ~ j
~ 11. Then A/+ 1
.::: = a"+ikb"c3". Choosing k large
enough so that n + jk > 2n, we can make 11 + jk + n > 3n. So, )::l+l.::: !C L.
Hence L is not regular.
EXAMPLE -S.33
Prove the identities Is, 16, 17, 18, III.
/ 12 given in Section 5.1.1.
Proof
L(R + R) = UR) u UR) = L(R). Hence Is.
L(R*R*) = L(R*)L(R*) = {Wll1'2Iwlo H': E L(R*)}. But WI = XIX2 '"
Xii
E UR)* for Xi E L(R). i = 1. 2, ..., II. Similarly. W2 = )'IY2 ... y", E
L(R*) for lj E
L(R). j = 1, 2.
m. SO WIW::: E L(R)*. proving h.
An
element
of
L(RR*l
is
of
the
form
XYI'"
YII
for
some
x, 'I' .... Y"
E
L(R).
As XYI ... Y" = (:rvl .. 'Y"_I)Y,, E UR+R), 17 follows.
It is easy to see that UR*)
<;: L«R*)*). Take
IV E L«R*)*). Then
W = Xl ...
X/I1 where
Xi
E
R*. i = L 2..... m. Each x, in turn can be
written in the form YI)'2
"
. Y" for some 'i
E
L(R),
i = 1. 2, .... n. So,
W = XI ...
Xiii = ':::1 ...
':::k E L(R*).
(Note:
XI = ':::1
'"
z" X: =
':::I+J
•••
Z,+( etc.)
Hence Is.
To prove Ill, take
W
E L(P + Q)*. Then ,v =
WI ...
yj.'" where Wi
E
L(P) u L(Q). By writing Wi = AWi = wiA we note that Wi E P*Q*. Hence
W
E UP*Q*)*. To prove L(P*Q*)*
<;: UP + Q)*, take
W
E L(P*Q*)*.
Chapter 5: Regular Sets and Regular Grammars
g
173
Then, >t' = 'VI' .. WI! where Wi E P*Q*. For simplicity take n = 2. (The result
can be extended by induction for any n). Then
WJ =
XIX2 .,.
Xk' where
Xi E UP) and 11'2 = )'J)'2 ... )'1 where Yi E UQ), SO vI' =X1X2 ' , . XkY1Y2 ..• YI'
Each Xi or Yi is in L(P + Q). Hence 11' E L(P + Q)*, proving the first identity
in I] j. The second identity can be proved in a similar way.
Finally, we prove 112,
L((P + Q)R) = L(P + Q)L(R)
= (L(P)
U
L(Q»L(R)
UPR + QR) = L(PR) u
L(QR)
= (L(P)L(R» u
(L(Q)L(R»
But (A u
B)C = AC u
BC for A, B,
C
<;;;; L*. For. a string
11' in
(A u B)C is the concatenation of a string H'] in A or B and a string W2 in e.
If WI
E A, then WjW2 E AC: if H']
E B, then WjW2 E Be. Hence W
E AC u
Be. The other inclusion can be proved similarly. /12 follows from (A u B)e
= AC u Be.
EXAMPLE 5.34
Prove that P + PQ*Q = a*bQ* where P = b + aa*b and Q is any regular
expreSSiOn.
Proof
L.H.S. = PA + PQ*Q
= peA +
Q*Q)
= PQ*
= (b + aa*b)Q*
= (Ab + aa*b)Q*
= (A + aa*)bQ*
= a*bQ*
= R.H.S.
by h
by 112
by 19
by definition of P
by I,
by 1]2
by /')
EXAMPLE 5.35
Construct a regular grammar accepting L = {w E
{a, b}* I W is a string over
{a. b} such that the number of b's is 3 mod 4}.
Solution
We construct a DFA M accepting L directly. The symbol a can occur in any
place in 11' and b has to occur in 4k + 3 places, where k 2': O. So we can have
stattOS q/, i = 0, 1. 2, 3. for remembering that the string processed so far has
4k. 4k + L 4k + 2 and 4k + 3 b's (k 2': 0). q3 is the only final state. Also M
does not change state on reading a's. The state diagram representing M is
gIven in Fig. 5.35.
174
I;!
Theory ofComputer Science
a
b
b
a
b
a
b
a
Fig. 5.35
DFA for Example 5.35.
By applying the construction given in Section 5.6.1. we can construct a
regular grammar G accepting L = T(M).
G = ({Ao, Al , A2, A3}. {a, b}, P. Ao) where P consists of Ao ~ aAo,
Ao ~ hAl, Al ~ hAl> Al ~ bA2, A2 ~ aA2, A2 ~ bA3, A2 ~ b, A3 ~ aA3,
A3 ~ GAo·
EXAMPLE 5.36
Let G = ({Ao, Al , A2• A3I. {a, b}, P. Ao), where P consists of Ao ~
aAo IhAl- Al ~ aA2
1 aA3, A3 ~ a IhAl IhA3, A3 ~ b IbAo· Construct an
NDFA accepting L(G).
Solution
The NDFA accepting L =L(G) is M where M =({qo, ql' q2, q3, q4}, {a, b},
8. qo, {q4})' 8 is described by the state diagram shown in Fig. 5.36.
b
a
Fig. 5.36
NDFA for Example 5.36.
Chapter 5: Regular Sets and Regular Grammars
~
175
SELF-TEST
(b) (01)*
(d) none of these.
Choose the correct answer to Questions 1-10.
1. The set of aJl strings over {a, b} of even length is represented by the
regular expression
(a) (ab + aa + bb + ba)*
(b) (a + b)*(a* + b)*
(c) (aa + bb)*
(d) (ab + ba)*
2. The set of all strings over {a, b} of length 4. starting with an a is
represented by the regular expression
(a) a(a + b)*
(b) a(ab)*
(c) (ab + ba)(aa + bb)
(d) a(a + b)(a + b)(a + b)
3. (0*1*)* is the same as
(a)
(0 + 1)*
(c) (10)*
4. If L is the set of aJl strings over {a, b} containing at least one a, then
it is not represented by the regular expression
(a) b*a(a + b)*
(b) (a + b)*a(b + a)*
(c) (a + b)*ab*
(d) (a + b)*a
5. {a211 I n ~ I} is represented by the regular expression
(a) (aa)*
(b) a*
(c) aa*a
(d) a*a*
(b) a*ba*ba*b
(d) a*ba*ba*ba*
(b) (a + b)*abab(a + b)*
(d) (a + b)*abab
6. The set of strings over {a, b} having exactly 3b's is represented by the
regular expression
(a) a*bbb
(e) ba*ba*b
7. The set of all stlings over {a, b} having abab as a substring is
represented by
(a) a*ababb*
(c) a*b*ababa*b*
(b) a*
(d) none of these.
8. (a + a*)* is equivalent to
(a) a(a*)*
(c) aa*
9. a*(a + b)* is equivalent to
(a) a* + b*
(b) (ab)*
(c) a*b*
(d) none of these,
ab* + b* represents all strings waver {a, b}
(a) starting 'vvirh an a and having no other a's or having no a's but
only b's
(b) starting with an a followed by b's
(e) having no a's but only b's
(d) none of these,
10.
176
Q
Theory ofComputer Science
State whether the following Statements 11-17 are true or false.
11. If L is finite then L* is finite.
12..Every finite subset of L* is a regular language.
13. Every regular language over L is finite.
14. a4h3 is in the regular set given by a*(a + b)b*.
15. aa* + bb* is the same as (a + b)*.
16. The set of all strings starting with an a and ending in ah is defined by
the regular expression a(a + b)*b.
17. The regular expression (a + b)*c* is the same as a*(b + c)*.
EXERCISES
5.1 Represent the following sets by regular expressions:
(a)
{O. 1. 2}.
(b) {1 21'+] In> O}.
(e)
{'vI' E
{a, b}* Iw has only one a}.
(d) The set of all strings over {O. 1} which has at most two zeros.
(e) {a 2, as. as, ... }.
(f) {d' In is divisible by 2 or 3 or n = 5}.
(g) The set of all strings over {a, b} beginning and ending with a.
5.2 Find all strings of length 5 or less in the regular set represented by the
following regular expressions:
(a) (ab + a)*(aa + b)
(b) (a*b + b*a)*a
(c) a* + Cab + a)*
5.3 Describe, in the English language, the sets represented by the following
regular expressions:
(a) a(a + b)*ab
(b) a*b + b*a
(c) (aa + b)*(bb + a)*
5.4 Prove the following identity:
(a*ab + ba)*a* = (a + ab + ba)*
5.5 Constmct the transition systems equivalent to the regular expressions
given in Exercise 5.2.
5.6 Constmct the transition systems equivalent to the regular expressions
given in Exercise 5.3.
5.7 Find the set of strings over L = {a, b} recognized by the transition
systems shown in Fig. 5.37(a-d).
5.8 Find the regular expression corresponding to the automaton given in
Fig. 5.38.
Chapter 5: Regular Sets and Regular Grammars
~
177
a,b
A
~
(a)
a, b
Q
-0
(b)
b
a
II
a I
\
b
"'-
a. b
~
a
b
a
b
a
~-;;
~
-0-
b
~~
~
~
(d)
Fig. 5.37
Transition systems of Exercise 5.7.
Q1
-
qo
o
o
Fig. 5.38
Transition system of Exercise 5.8.
178
~
Theorv ofComputer Science
5.9 Construct a transition system corresponding to the regular expressions
(i) (ab + c*)*b and (ii) a + bb + bab*a.
5.10 Find the regular expressions representing the following sets:
(a) The set of all strings over {O, I} having at most one pair of O's
or at most one pair of 1's.
(b) The set of all strings over
{a.
b} in which the number of
occurrences of a is divisible by 3.
(c) The set of all strings over {a, b} in which there are at least two
occurrences of b between any two occurrences of a.
(d) The set of all strings over {a, b} with three consecutive b's.
(e) The set of all strings over {O, I} beginning with 00.
(f) The set of all strings over {O. I} ending with 00 and beginning
with 1.
5.11 Construct a deterministic finite automaton corresponding to the regular
expression given in Exercise 5.2.
5.12 Construct a finite automaton accepting all strings over {O, I} ending in
010 or 0010.
5.13 Construct a finite automaton M which can recognize DFA in a given
string over the alphabet
{A,
B, ..., Z}. For example, M has to
recognize DFA in the string ATXDFAMN.
5.14 Construct a finite automaton for the regular expression (a + b)*abb.
5.15 Show that there exists no finite automaton accepting all palindromes
over {a, b}.
5.16 Show that {d 1b" In> O} is not a regular set without using the pumping
lemma.
5.17 Using the pumping lemma, show that the following sets are not regular:
(a) {a"b21 In> O};
(b) {a"bIllIO < n < m}.
5.18 Show that {on I" Ig.c.d. (m, n) = l} is not regular.
5.19 Show that a deterministic finite automaton with n states accepting a
nonempty set accepts a string of length m, m < n.
5.20 Construct a finite automaton recognizing L(G), where G is the grammar
S ~ as IbA Ib and A ~ aA IbS Ia.
5.21 Find a regular grammar accepting the set recognized by the finite
automaton given in Fig. 5.37(c).
5.22 Construct a regular grammar which can generate the set of all strings
starting with a letter (A to Z) followed by a string of letters or digits
(0 to 9).
Chapter 5: Regular Sets and Regular Grammars
~
179
5.23 Are the following true or false'? Support your answer by giving proofs
or counter-examples.
(a) If L 1 U L 2 is regular and L 1 is regular, then L2 is regular.
(b) If L rL2 is regular and L 1 is regular, then L2 is regular.
(c) If L" is regular, then L is regular.
5.24 Construct a deterministic finite automaton equivalent to the grammar
S ~ as I bS I aA, A ~ bB. B
-~ aC, C ~ A.
Context-Free Languages
In this chapter we study context-free grammars and languages. We define
de11vation trees and give methods of simplifying context-free grammars. The
two normal forms-Chomsky normal form and Greibach normal form-are
dealt \\,ith. We conclude this chapter after proving pumping lemma and giving
some decision algorithms.
6.1
CONTEXT-FREE LANGUAGES AND DERIVATION
TREES
Context-free languages are applied in parser design. They are also useful for
describing block structures in programming languages. It is easy to visualize
derivations in context-free languages as we can represent derivations using tree
structures.
Let us reca11 the definition of a context-free, grammar (CFG). G is
context-free if every production is of the form A ~ a, where A
E VN and
a E CVv U
L)*.
EXAMPLE 6.1
Construct a context-free grammar G generating all integers (with sign).
Solution
Let
G = (Vv. L, P, S)
where
Vv = {S, (sign), (digit). (Integer)}
L = {a, L 2. 3, ..., 9, +, -}
180
-~- .---------
----~-
Chapter 6: Context-Free Languages
);1
181
P consists of 5 ----+ (sign) (integer), (sign) ----+ + 1-,
(integer)
----+ (digit) (integer) I (digit)
(digit)
----+ 011121· .. 19
L(G) = the set of all integers. For example, the derivation of -17 can be
obtained as follows:
5 =? (sign) (integer) =? -
(integer)
=? -
(digit) (integer) =? -
1 (integer; =? - 1 (digit)
=? -
17
6.1.1
DERIVATION TREES
The derivations in
a CFG can be represented using trees. Such trees
representing derivations are called derivation trees. We give below a rigorous
definition of a derivation tree.
Definition 6.1
A derivation tree (also called a parse tree) for a CFG
G = (V\'. L. P, 5) is a tree satisfying the following conditions:
(i) Every vertex has a label which is a variable or terminal or A.
(ii) The root has label S.
(iii) The label of an internal vertex is a variable.
(iv) If the vertices 111'
11~•... , 11k written 'vvith labels Xl-
X~, ..., Xk are
the sons of vertex
11 with label A, then A
----+
X1X~ .. , Xk is a
production in P.
(v) A vertex
11 is a leaf if its label is a E L or A;
11 is the only son of
its father if its label is A.
For example. let G = ({S, A}. {a. b}. P, S). where P consists of 5
----+
aAS I a 155. A
----+ SbA 1ba. Figure 6.1 is an example of a derivation tree.
s
a
10
2
a
4
7
b
8
a
Fig. 6.1
An example of a derivation tree.
182
&;\
Theory ofComputer Science
Note:
Vertices 4-6 are the sons of 3 written from the left, and 5 ~ aA5 is
in P. Vertices 7 and 8 are the sons of 5 written from the left, and A ~ ba
is a production in P. The vertex 5 is an internal vertex and its label is A, which
is a variable.
Ordering of Leaves from the Left
We can order all the vertices of a tree in the following way: The successors
of the root (i.e. sons of the root) are ordered from the left by the definition
(refer to Section 1.2). So the vertices at level 1 are ordered from the left. If
1'1 and 1'2 are any two vertices at levelland 1'1 is to the left of 1'2, then we
say that 1'1 is to the left of any son of 1'2' Also, any son of 1'1 is to the left
of 1'2 and to the left of any son of 1'2. Thus we get a left-to-right ordering of
vertices at level 2. Repeating the process up to level k, where k is the height
of the tree, we have an ordering of all vertices from the left.
Our main interest is in the ordering of leaves.
In Fig. 6.1, for example, the sons of the root are 2 and 3 ordered from the
left. So, the son of 2, namely 10, is to the left of any son of 3. The sons of
3 ordered from the left are 4-5-6. The vertices at level 2 in the left-to-right
ordering are 10-4-5-6. The vertex 4 is to the left of 6. The sons of 5 ordered
from the left are 7-8. So 4 is to the left of 7. Similarly, 8 is to the left of
9. Thus the order of the leaves from the left is 10-4-7-8-9.
Note:
If we draw the sons of any vertex keeping in mind the left-to-right
ordering. we get the left-to-right ordering of leaves by 'reading' the leaves in
the anticlocbvise direction.
Definition 6.2
The yield of a derivation tree is the concatenation of the
labels of the leaves without repetition in the left-to-right ordering.
The yield of the derivation tree of Fig. 6.1. for example, is aabaa.
Note:
Consider the derivation tree in Fig. 6.1. As the sons of I are 2-3 in
the left-to-right ordering, by condition (iv) of Definition 6.1, we have the
production 5 ~ 55. By applying the condition (iv) to other veltices, we get
the productions 5 ~ a. 5 ~ aA5, A
~ ba and 5 ~ a. Using these
productions. we get the following derivation:
5 ~ 55 ~ as ~ aaA5 ~ aaba5 ~ aabaa
Thus the yield of the de11vation tree is a sentential form in G.
Definition 6.3
A subtree of a derivation tree T is a tree (i) whose root is
some vertex v of T. (ii) whose vertices are the descendants of v together with
their labels, and (iii) whose edges are those connecting the descendants of v.
Figures 6.2 and 6.3. for example, give two subtrees of the derivation tree
shown in Fig. 6. L
Chapter 6: Context-Free Languages
l;!
183
a
Fig. 6.2
A subtree of Fig. 6.1.
Fig. 6.3
Another subtree of Fig. 6.1.
Note:
A subtree looks like a derivation tree except that the label of the root
may not be S. It is called an A-tree if the label of its root is A.
Remark When there is no need for numbering the vertices, we represent the
vertices by points. The following theorem asserts that sentential forms in CFG
G are precisely the yields of derivation trees for G.
Theorem 6.1
Let G =(Vv, L, P, S) be a CFG. Then S ::S ex if and only if
there is a derivation tree for G with yield ex.
Proof
We prove that A :b ex if and only if there is an A-tree with yield ex.
Once this is proved. the theorem follows by assuming that A = S.
Let ex be the yield of an A-tree T. We prove that A :b ex by induction on
the number of internal vertices in T.
When the tree has only one internal vertex, the remaining vertices are
leaves and are the sons of the root. This is illustrated in Fig. 6.4.
A
Fig. 6.4
A tree with only one internal vertex.
By condition (iv) of Definition 6.1. A -7 A IA~ ... Am = ex is a production
III G, i.e. A => ex. Thus there is basis for induction. Now assume the result
for all trees with at most k - 1 internal vertices (k > 1).
Let T be an A-tree with k internal vertices (k 2: 2). Let
1'1,
v~, ... , Vm
be ;;le sons of the root in the left-to-right ordering. Let their labels be
Xl'
X~. .... X1I1 • By condition (iv) of Definition 6.1, A -7
XJX~ ... Xm is in
P. and so
(6.1)
184
J,;i
Theory ofComputer Science
As k ?: 2. at least one of the sons is an internal vertex. By the left-to-right
ordeling of leaves. a can be written as ala: ... am' where ai is obtained by
the concatenation of the labels of the leaves which are descendants of vertex
Vi' If Vi is an internal vertex, consider the subtree of T with Vi as its root. The
number of internal vertices of the subtree is less than k (as there are k internal
vertices in T and at least one of them, viz. its root, is not in the subtree). So
by induction hypothesis applied to the subtree, Xi ~ ai' If 1\ is not an internal
vertex. i.e. a leaf, then Xi = ai'
Using (6.1), we get
A ::::} XIX: ... XIII ~ a IX:X3 ... XIII ... ~ aj a2 ••• cx,n = a,
i.e. A ~ ex. By the principle of induction. A ~ a whenever a is the yield
of an A-tree.
To prove the 'only if' part. let us assume that A
~ a. We have to
constmct an A-tree whose yield is a. We do this by induction on the number
of steps in A ~ a.
When A ::::} a, A -7 a is a production in P. If a = XIX: ... XIII' the
A-tree with yield a is constmcted and given as in Fig. 6.5. So there is basis
for induction. Assume the result for derivations in at most k steps. Let
A b
(X: we can split this as A ::::} Xj ... XI/i
k~
a. Now, A ::::} Xj
XIII
implies A -7 XjX2 .•. XIII is a production in P. In the derivation XjX:
XIII
k~ (X, either (i) Xi is not changed throughout the derivation, or (ii) Xi is
changed in some subsequent step. Let (Xi be the substring of a delived from Xi'
Then Xi ~ ai in (ii) and Xi = (Xi in (i). As G is context-free, in every step of
the delivation XIX: ... XIII ~ a, we replace a single variable by a string. As
aj, a:. ..., aI/I' account for all the symbols in a, we have a = ala2 ... a III'
A
Fig. 6.5
Derivation tree for one-step derivation.
We constmct the derivation tree with yield a as follows: As A -7 Xl' .. XIII
is in P. we constmct a tree with Tn leaves whose labels are Xj, ..., XIlI in the
left-to-light ordeling. This tree is given in Fig. 6.6. In (i) above, we leave the
vertex Vi as it is. In (ii). Xi ~ ai is less than k steps (as XI ... XI/I I/~ a). By
induction hypothesis there exists an Xi-tree Ti with yield ai' We attach the tree
Ti at the vertex Vi (i.e.
Vi is the root of T;). The resulting tree is given in
Fig. 6.7. In this figure. let i and j be the first and the last indexes such that
Xi and Xj satisfy (ii). So. al ... ai-l are the labels of leaves at level 1 in T.
ai is the yield of the Xi-tree h
etc.
Chapter 6: Context-Free Languages
g
185
Fig. 6.6
Derivation tree with yield x 1 x2 ... Xm.
0'1 ... 0'i-1
Fig. 6.7
Derivation tree with yield
[i1C(2 ...
[in'
Thus we get a derivation tree with yield a By the principle of induction
we can get the result for any derivation. This completes the proof of 'only if'
part.
I
Note:
The derivation tree does not specify the order in which we apply the
productions for getting a
So. the same derivation tree can induce several
derivations.
The following remark is used quite often in proofs and constructions
involving CFOs.
Remark
If A derives a terminal string wand if the first step in the derivation
is A
::::::> A lA2 ...
Al1' then we can write
w as
WIW2 ...
W I1 so that
Ai ==>
H'i' (Actually, in the derivation tree for w, the ith son of the root has
the label Ai' and 'Vi is the yield of the subtree \vhose root is the ith son.)
EXAMPLE 6.2
Consider G whose productions are 5 ---'7 aASIa, A ---'7 SbA ISSIba. Show that
S ==> aabbaa and construct a derivation tree whose yield is aabbaa.
186
~
Theory ofComputer Science
Solution
S ~ aAS ~ asbAS ~ aabAS ~ a2bbaS ~ a2b2a2
(6.2)
Hence. S ~ a2b2a2. The derivation tree is given in Fig. 6.8.
s
a
a
Fig. 6.8
The derivation tree with yield aabbaa for Example 6.2.
Note:
Consider G as given in Example 6.2. We have seen that S ~ a2b2a2,
and (6.2) gives a derivation of a2b2a2.
Another derivation of a2b2a2 is
S ~ aAS ~ aAa ~ aSbAa ~ aSbbaa ~ aabbaa
(6.3)
Yet another derivation of a2b2a2 is
S ~ aAS ~ aSbAS ~ aSbAa ~ aabAa ~ aabbaa
(6.4)
In derivation (6.2), whenever we replace a variable X using a production,
there are no variables to the left of X. In derivation (6.3). there are no variables
to the light of X. But in (6.4), no such conditions are satisfied. These lead to
the following definitions.
DefInition 6.4
A derivation A ~ w is called a leftmost derivation if we
apply a production only to the leftmost variable at every step.
DefInition 6.5
A derivation A ~ w is a rightmost derivation if we apply
production to the rightmost variable at every step.
Relation (6.2). for example, is a leftmost derivation. Relation (6.3) is a
rightmost derivation. But (6.4) is neither leftmost nor rightmost. In the second
step of (6.4), the rightmost variable S is not replaced. So (6.4) is not a
rightmost derivation. In the fOUl1h step, the leftmost variable S is not replaced.
So (6.4) is not a leftmost derivation.
Theorem 6.2
If A
~
11,' in G. then there is a leftmost derivation of w.
Chapter 6: Context-Free Languages
~
187
Proof
We prove the result for every A in v'v by induction on the number
of steps in A :b
'i'. A ::::} W is a leftmost derivation as the L.H.S. has only
one variable. So there is basis for induction. Let us assume the result for
derivations in atmost k steps. Let A
n~
w. The derivation can be split as
A ::::} X jX2 ... XII! db w.
The string
W can be split as
\1'IW2 ... w'" such that Xi ::::} Wi (see the
Remark appended before Example 6.2). As Xi :b Wi involves atmost k steps
by induction hypothesis, we can find a leftmost derivation of Wi' Using these
leftmost derivations. we get a leftmost derivation of W given by
A ::::} X jX2 ... XII! :b w j X 2 ... x",:b WjW2X3 ... X", ... :b w!,'i'2
Will
Hence by induction the result is true for all derivations A :b w.
Corollary
Every derivation tree of W induces a leftmost derivation of w.
Once we get some derivation of \1'. it is easy to get a leftmost derivation of
W in the following way: From the derivation tree for w, at every level consider
the productions for the variables at that level, taken in the left-to-right ordering.
The leftmost derivation is obtained by applying the productions in this order.
EXAMPLE 6.3
Let G be the grammar 5 ~ OB 11A. A ~ 0 I05 11AA, B ~ 1115IOBB. For
the string 00110101, find (a) the leftmost derivation, (b) the rightmost
derivation, and (c) the derivation tree.
Solution
(a) 5::::} OB ::::} OOBB ::::} 001B ::::} 00115
::::} 02120B ::::} 0212015 ::::} 02120lOB ::::} 02}20101
(b) 5::::} OB ::::} OOBB ::::} 00B15 ::::} OOBlOB
::::} 02B1015 ::::} 02BlOlOB ::::} 02BlO101 ::::} 02110101.
(c) The derivation tree is given in Fig. 6.9.
s
B
s
o
Fig. 6.9
The derivation tree with yield 00110101 for Example 6.3.
o
188
~
Theory ofComputer Science
6.2
AMBIGUITY IN CONTEXT-FREE GRAMMARS
Sometimes we come across ambiguous sentences in the language we are using.
Consider the following sentence in English: "In books selected information is
given." The word 'selected' may refer to books or information. So the sentence
may be parsed in two different ways. The same situation may arise in context-
free languages. The same terminal string may be the yield of two derivation
trees. So there may be two different leftmost derivations of w by Theorem 6.2.
This leads to the definition of ambiguous sentences in a context-free language.
Deflnition 6.6
A terminal string W E L(G) is ambiguous if there exist two
or more derivation trees for w (or there exist two or more leftmost derivations
of w).
Consider, for example, G = ({S}, {a, b, +, *}, P. S), where P consists
of 5 -7 5 + 5 IS * 5 Ia Ib. We have two derivation trees for a + a " b given
in Fig. 6.10.
s
a
s
s
b
s
a
s
b
Fig. 6.10
Two derivation trees for a + a * b.
The leftmost derivations of a + a * b induced by the two derivation trees
are
5 => S + S => a + S => a + S * 5 => a + a " S => a + a " b
S => S * S => S + S * 5 => a + 5 " S => a + a * 5 => a + a * b
Therefore, a + a * b is ambiguous.
Deflnition 6.7
A context-free grammar G is ambiguous if there exists some
vI! E L(G), which is ambiguous.
EXAMPLE 6.4
If G is the grammar S -7 SbSIa, show that G is ambiguous.
Solution
To prove that G is ambiguous, we have to find aWE
L(G), which is
ambiguous. Consider w =abababa E L(G). Then we get two derivation trees
for
1\' (see Fig. 6.11). Thus. G is ambiguous.
Chapter 6: Context-Free Languages
l;;!
189
s
a
s
s
a
s
s
a
s
a
Fig. 6,11
Two derivation trees of abababa for Example 6.4.
6.3
SIMPLIFICATION OF CONTEXT-FREE GRAMMARS
In a CFG G, it may not be necessary to use all the symbols in VIy' u L, or
all the productions in P for deriving sentences. So when we study a context-
free language L(G), we try to eliminate those symbols and productions in G
which are not useful for the derivation of sentences.
Consider, for example,
G = ({S. A, B, C, E}, {n, b, c}, P, S)
where
P = {S ~ AB, A ~ n, B ~ b, B ~ C, E ~ ciA}
It is easy to see that L(G) = {nb}. Let G' = ({S, A. B}, {n, b}, p', S), where
P' consists of S ~ AB, A ~ n, B ~ b. UG) = L(G'). We have eliminated
the symbols C, E and c and the productions B ~ C, E ~ c IA. We note the
190
~
Theory ofComputer Science
following points regarding the symbols and productions which are eliminated:
(i) C does not derive any terminal string.
(ii) E and c do not appear in any sentential fonn.
(iii) E ---+ A is a null production.
(iv) B ---+ C simply replaces B by C.
In this section, we give the construction to eliminate (i) variables not
deriving terminal strings, (ii) symbols not appearing in any sentential form,
(iii) null productions. and (iv) productions of the fonn A ---+ B.
6.3.1
CONSTRUCTION OF REDUCED GRAMMARS
Theorem 6.3
If G is a CFO such that L(G) ;t:. 0. we can find an equivalent
grammar G' such that each variable in G' derives some tenninal string.
Proof
Let G = (Vv, 1:, P, S). We define G' = (V:v, 1:. G', S) as follows:
(a) Construction of V~v:
We define Wi
<:;;;; Vv by recursion:
WI = {A
E
Vv 1there exists a production A ---+
111 where W
E 1:*}. (If
Wj = 0. some variable will remain after the application of any production, and
so L(G) = 0.)
Wi+! = Wi U
{A
E Vv Ithere exists some production A ---+ a
with a E
(1: U W;)*}
By the definition of Wi, Wi
<:;;;; Wi+! for all i. As v'v has only a finite number
of variables, Wk = Wk+1 for some k :; IVV I. Therefore, Wk = Wk+j for j 2:: l.
We define V'v = Wk'
(b) Construction of pI:
P' = {A ---+ alA, a
E
(V~ U 1:n
We can define G' = (V;v, 1:, pt. S). S is in Vv. (We are going to prove that
every variable in Vv deJives some tenninal string. So if S e:
VV, L(G) = 0.
But L(G)
;t:. 0.)
Before proving that G
I is the required grammar, we apply the construction
to an example.
EXAMPLE 6.5
Let G = (yv, 1:. P, S) be given by the productions S ---+ AB, A ---+ a, B ---+ b,
B ---+ C. E ---+ c. Find G' such that every vaJiable in G
1 deJives some tenninal
string.
Solution
(a) Construction of V~v:
H\ = {A. B, E} since A ---+ a. B ---+ b. E
---+ c are productions with a
tenninal stJing on the R.H.S.
Chapter 6: Context-Free Languages
);,J
191
W~ = Wj U
{AI
E
V:vlAj
----t a for some a
E (2: U
{A, B, E})*}
= tV] u
{S} =
{A~ B, E, S}
W3 =W~ U
{AI
E V:vlAI ----t a for some a E (2: U
{S, A, B, E})*}
=W,
U
0 = Wo
Therefore,
VN= {S. A. B, F}
(b) Construction of p':
P' = {AI
----t aIA[. a
E
(V:v U 2:)*}
= {S ----t AB, A
----t a, B ----t b, E ----t c}
Therefore.
G' = ({S, A, B, E}, {a, b. c}, P'. S)
Now we prove:
*
(i) If each A E V',. then A ::S w for some W E 2:*; conversely. if A ~ w,
,
,
G'
G
then A E Vv,
(ii) L(G') = L(G),
To prove (i) we note that Wk = WI
U
W~ ...
U
Wk' We prove by
induction on i that for i = 1, 2..... k. A
E
Wi implies A ::S w for some
G'
W
E
2:*. If A
E
Wj • then A
~ w. So the production A
----t w is in P'.
G
Therefore. A ::S lV. Thus there is basis for induction. Let us assume the result
G'
for i. Let A
E
Wi+l • Then either A
E Wi' in which case. A ::S w for some
G'
w
E 2:* by induction hypothesis. Or. there exists a production A ----t a with
a
E
(2:
U wJ*. By definition of P'. A
----t a is in P'. We can write
a = XIX~ ... Xm, where Xj E 2: U Wi' If Xj E Wi by induction hypothesis.
Xl' ::S Wi for some Wi E 2:*. So, A ::S ~1)[W~ ... H'm E 2:* (when XI' is a terminaL
G'
'
'G'
,
Wi = Xi)' By induction the result is true for i = 1. 2, ' ... k.
The converse part can be proved in a similar way by induction on the
number of steps in the derivation A ~ w. We see immediately that L(G') k
G
L(G) as
V~. k
V:v and P' k P, To prove L(G) k L(G'), we need an auxiliary
result
~:
A ::; w
if A
~ w for some lV
E 2:*
(6.5)
G'
G
We prove (6.5) by induction on the number of steps in the derivation A ~
W.
G
If 11 ~ w, then A ----t w is in P and A E W] k
V'v. As A E V;v and vV E 2:*.
G
A ----t w is in P'. So A ~ w, and there is basis for induction. Assume (6.5)
G'
k+1
for derivations in at most k steps, Let A
~ w. By Remark appeating after
G
192
~
Theory ofComputer Science
*
Theorem 6.1, we can split this as A
~ X]X2 ... X lIl ~
W(l-V2 ... 'Vm such
that Xj ~ H} If Xi E L, then Wj = Xj'
. G'
"
If Xj
E
VN then by (i), Xj
E
V~v. As Xj
~ Wj in at most k steps,
Xj ~
Wj' Also, Xl, X 2, X lIl E (L U
V~v)* implies that A ~ XlX2 ... X I1l is
in P'. Thus, A ~
XIX~ ... XlIl ::b WlW2 ... Will' Hence by induction, (6.5)
G'
-
G'.
•
is true for all derivations. In particular, S ~
W implies S => w. This proves
G
G'
that L(G) ~ L(G'), and (ii) is completely proved.
I
Theorem 6.4
For every CFG G = (VN, L, P, S), we can construct an
equivalent grammar G' = (VN, L', p', S) such that every symbol in \1"1 U L'
appears in some sentential form (i.e. for every X in V:v U L' there exists a
such that S ::b a and X is a symbol in the string a).
G'
Proof
We construct G' = (V'N' L', P', S) as follows:
(a) Construction of Wi for i ?: 1:
(i)
WI = IS}.
(ii) Wi+l =Wi U {X E \1v U L Ithere exists a production A ~ a with
A E Wi and a containing the symbol X}.
We may note that Wi ~ v'v uLand Wi ~ Wi+l • As we have only a finite
number of elements in VN U L, Wk = Wk+1 for some k. This means that
Wk = Wk+j for all j ?: O.
(b) Construction of VN, L' and p':
We define
VN= Vv (l Wk,
L' = L U Wk
P'= {A ~ alA E
Wk }.
Before proving that G' is the required grammar, we apply the construction to
an example.
EXAMPLE 6.6
Consider G = ({S, A, B, E}, {a, b, c}, P, S), where P consists of S ~ AB,
A ~ a, B ~ b, E ~ c.
Solution
WI = IS}
W2 = IS} U {X E \!y U L I there exists a production A ~ a with
A E
WI and a containing X}
= IS}
U
{A, B}
Chapter 6: Context-Free Languages
);I,
193
W3 = is, A, B} u
{a, b}
W4 = W3
V;v= is, A, B}
2:' = {a, b}
P'= {S ~ AB, A
~ a. B~ b}
Thus the required grammar is G' = (ViY, 2:', pI, S).
To complete the proof, we have to show that (i) every symbol in VNU
2:' appears in some sentential form of G', and (ii) conversely. L(G') =L(G).
To prove (i), consider X E V'v U 2:' = Wb By construction Wk = WI U
W2 ... U Wk' We prove that X E
WI, i ::; k, appears in some sentential fOlm
by induction on i. When i = 1, X = Sand S ~ S. Thus, there is basis for
induction. Assume the result for all variables in Wi' Let X E Wi+1• Then either
X E
Wi, in which case. X appears in some sentential form by induction
hypothesis. Otherwise, there exists a production A ~
0:, where A E Wi and
0: contains the symbol Xi' The A appears in some sentential form, say f3A y.
Therefore.
5 =>
f3A Y =>
f3o:Y
c'
c'
This means that f3o:y is some sentential form and X is a symbol in f30:y. Thus
by induction the result is true for X E Wi, i ::; k.
Conversely, if X appears in some sentential form, say f3Xy. then 2: -:b f3Xy.
This implies X E WI' If I ::; k, then WI ~ Wk- If I > k, then WI = Wk-cHence
X appears in F;y U 2:'. This proves (i).
To prove (ii), we note L(G') ~ L(G) as
V~. ~ ,"",v, 2:' ~ 2: and pI ~ P.
Let ,v be in L(G) and 5 = 0:1 => 0:2 => 0:3 = ... ~
0:,,_1 => w. We prove
c
c
c
that every
symbol in
0:1+1 is in Wi+ 1 and
O:i => O:i+l by induction on i.
0:1 = S =? 0:2 implies 5 ~
0:2 is a production inc'P' By construction, every
C
symbol in 0:, is in W, and S ~ 0:, is in p', i.e. S =? 0:,. Thus, there is basis
-
-
-
c'
-
for induction. Let us assume the result for i. Consider O:i+l =? 0:1+2' This one-
step derivation can be written in the form
c
f31+1 A Yi+l
~
f31+JO:Yi+l
where A ~
0: is the production we are applying. By induction hypothesis,
A E Wi+1. By construction of WI+2, every symbol in 0: is in WI+2. As all the
symbols in f3i+J and Yi+l are also in WI+1 by induction hypothesis, every symbol
in
f31+ 1O:Yi+l = 0:1+2 is ill Wi+2. By the construction of pI, A ~
0: is in
p~
This means that
0:1+1
=?
0:1+2' Thus the induction procedure is complete.
C'
SO 5 = 0:1 =? 0:,
=?
0:3
~ ...
0:,,_1 =? w. Therefore,
W
E
L(G'). This
c'
-
c'
G'
c'
proves (ii).
194
g
Theory of Computer Science
Definition 6.8
Let G = (VN, L, P, S) be a CFG. G is said to be reduced or
non-redundant if every symbol in VN
U
L appears in the course of the
delivation of some terminal string, i.e. for every X in v'v U L, there exists
a delivation S ::S aX[3 ::S
W
E
L(G). (We can say X is useful in the
derivation of terminal strings.)
Theorem 6.5
For every CFG G there exists a reduced grammar G' which is
equivalent to G.
Proof
We construct the reduced grammar in two steps.
Step 1
We construct a grammar G j equivalent to the given grammar G so
that every variable in Gj derives some terminal string (Theorem 6.3).
Step 2
We construct a grammar G' = (V~v, L', p', S) equivalent to G j so that
every symbol in G' appears in some sentential form of G' which is equivalent
to G j and hence to G. G' is the required reduced grammar.
By step 2 every symbol X in G' appears in some sentential form, say
exX[3. By step 1 every symbol in exX[3 derives some terminal string. Therefore,
S ::S exX[3 ::S >t' for some w in L*, i.e. G' is reduced.
Note:
To get a reduced grammar, we must first apply Theorem 6.3 and then
Theorem 6.4. For, if we apply Theorem 6.4 first and then Theorem 6.3, we may
not get a reduced grallli'1lar (refer to Exercise 6.8 at the end of the chapter).
EXAMPLE 6.7
Find a reduced grammar equivalent to the grammar G whose productions are
C ~ aBlb
A ~ a.
B ~ BClAB,
S ~ ABICA,
Solution
Step 1
WI = {A, C} as A ~ a and C ~ b are productions with a terminal
string on R.H.S.
W~ = {A. C}
U {AIIA I ~ ex with ex E (L U {A, C})*}
= {k C} U
{S} as we have S ~ CA
W3 = {A, C, S}
U
{AI IAl ~ ex with ex E (L U
{S, A, C})*}
= {A~
C~ S} u 0
v~v=
W~ = {S, A, C}
p' = {A j
~ ex IAj, ex E
(V~", U L)*}
= {S ~ CA. A
~. a, C ~ b}
Thus.
G] = ({S, A. C}. {a, b}, {S ~ CA, A ~ a, C ~ b}, S)
Chapter 6: Context-Free Languages
g
195
Step 2
We have to apply Theorem 6.4 to G j • Thus,
WI = IS}
As we have production S -+ CA and S E
Wj, W2 = IS} u
{A, C}
As A -+ a and C -+ b are productions with A, C E W2, W3 ={S, A, C, a, b}
As W3 = V:v U L, p" = {S -+ a I Al
E
W3} = p'
Therefore,
G' = ({S, A, C}, {a, b}, {S -+ CA. A -+ a, C -+ b}, S)
is the reduced grammar.
EXAMPLE 6.8
Construct a reduced grammar equivalent to the grammar
S -+ aAa,
A -+ Sb IbCC IDaA.
C -+ abb IDD,
E -+ ac'
D -+ aDA
Solution
Step 1
Wj = {C} as C -+ abb is the only production with a terminal string
on the R.H.S.
W2 = {C} u
{E. A}
as E -+ aC and A -+ bCC are productions with R.H.S. in (L U
{C})*
W3 = {C, E. A}
U
IS}
as S -+ aAa and aAa is in (L U
W2) *
w.+ = W3 U 0
Hence,
Vv= W, = IS, A, C, E}
p' = {AI -+ al a E
(VN U L)*}
= {S -+ aAa, A -+ Sb I bCC, C -+ abb, E -+ aC}
G j = (V/
V' {a, b}, p', S)
Step 2
We have to apply Theorem 6.4 to G i . We start with
Wi = IS}
As we have S -+ aAa,
W2 = IS}
U
{A, a}
As A -+ Sb IbCC,
W, = IS, A, a}
U
IS, b, C} = IS, A, C, a, b}
As we have C -+ abb,
196
~
Theory ofComputer Science
Hence,
p" = {A)
~ cx
1 A j
E
W3}
= {S ~ aAa, A ~ Sb 1 bCC, C ~ abb}
Therefore.
G' = ({S. A, C}, {a. b}, p", S)
is the reduced grammar.
6.3.2
ELIMINATION OF NULL PRODUCTIONS
A context-free grammar may have productions of the form A ~ A. The
production A ~ A is just used to erase A. So a production of the form A ~
A, where A is a variable, is called a null production. In this section we give a
construction to eliminate null productions.
As an example, consider G whose productions are S ~ as
I aA
1 A,
A ~ A. We have two null productions S ~ A and A ~ A. We can delete
A ~ A provided \ve erase A whenever it occurs in the course of a derivation
of a terminal string. So we can replace S ~ aA by S ~ a. If G) denotes
the grammar whose productions are S ~ as 1 a 1 A, then L(G j ) = L(G) =
{a"
1 n ~ O}. Thus it is possible to eliminate the null production A ~ A. If
we eliminate S ~ A, we cannot generate A in L(G). But we can generate
L(G) - {A} even if we eliminate S ~ A.
Before giving the construction we give a definition.
DefInition 6.9
A variable A in a context-free grammar is nullable if A ~ A.
Theorem 6.6
If G = (~v, L, P. S) is a context-free grammar, then we can
find a context-free grammar G j having no null prodctions such that L(G) =
L(G)
- {A}.
Proof
We construct G j = (Vv, L, p', S) as follows:
Step 1
Construction of the set of nltllable variables:
We find the nullable variables recursively:
(i) W) = {A
E Vv 1 A ~ A is in P}
(ii) Wi+ j = Wi U {A E Vv Ithere exists a production A ~ CX with cx E Wi*}.
By definition of Wi' Wi k Wi+) for all i. As ~v is finite. Wk+1= Wk for some
k ::; 1~v I· SO, Wk+j = Wk for all j. Let VV = Wk' W is the set of all nullable
variables.
Step 2
(i) Construction of p':
Any production whose R.H.S. does not have any nullable variable is included
in p'.
(ii) If A ~ X jX2 ... Xk is in P, the productions of the form A ~
CXjCX2
CXk are included in p', where
CXj = Xi if Xi
€O
W.
CXi = X, or A if X, E
Wand
CX1CX2 ...
CXk 7:- A. Actually, (ii) gives several productions in
P~ The
productions are obtained either by not erasing any nullable vmiable on the
Chapter 6: Context-Free Languages
~
197
R.H.S. of A ~ Xj X2 ••• Xk or by erasing some or all nullable variables
provided some symbol appears on the R.H.S. after erasing.
Let G j = CV:v, 2.:, pI, S). G] has no null productions.
Before proving that G] is the required grammar, we apply the construction
to an example.
EXAMPLE 6.9
Consider the grammar G whose productions are S ~ as IAS, A ~ A,
S ~ A, D ~ b. Construct a grammar G] without null productions generating
L(G) -
{A}.
Solution
Step 1
Construction of the set W of all nullable variables:
Wj = {AI
E Vv IA j
~ A is a production in G}
= {A, B}
W2 = {A, B} u {S} as S ~ AB is a production with AS E wt
= {S, A, B}
W3 = W, U 0= w,
Thus.
W =W2 = {S, A, B}
Step 2
Construction of pI:
(i) D ~ b is included in pl.
(ii) S ~ as gives rise to S ~ as and S ~ a.
(iii) S ~ AB gives rise to S ~ AB, S ~ A and S ~ B.
(Note: We cannot erase both the nullable variables A and Bin S ~ AB as we
will get S ~ A in that case.)
Hence the required grammar without null productions is
G j = ({S, A, B. D}.{a, b}. P, S)
where pI consists of
D ~ b, S ~ as. S ~ AS, S ~ a, S ~ A, S ~ B
Step 3
L(G j ) = L(G) - {A}. To prove that L(G) = L(G) -
{A}, we prove
an auxiliary result given by the following relation:
For all A E Vv and
>1,' E 2.:*,
A ~ w if and only if A ~
>1,' and w ~ A
G,
G
(6.6)
We prove the 'if part first. Let A ~ wand Ii' ;f. A. We prove that A ~ w
G
G
by induction on the number of steps in the derivation A ~ w. If A => HI ~nd
G
G
198
l;l,
Theory ofComputer Science
W "* A, A ~ W is a production in pI, and so A
::::;> w. Thus there is basis for
G,
induction. Assume the result for derivations in at most i steps. Let A i,g wand
.
G
I
W "* A. We can split the derivation as A
~
XIX~ ... Xk 7" WjW~ ... Wk,
.'.
G
where W = 1V1W2 ... Wk and Ai ~ wi' As W "* A, not all ¥I'j'S are A. If wi "* A,
G
then by induction hypothesis, Xi b
¥lj. If ,Vi =A, then Xi E W So using the
.
G,
.
production A ~
AjA~ ... Ak in P, we construct A ~ al a~ ... ak in P',
where (Xi = Xi if wi "* A and ai = A if wi = A (i.e. Xi
E
W). Therefore,
A ~
ala~ ... ak ,;, Wja2 ... ak::::;>
... => wlw2 ... Wk = W
~
~
~
By the principle of induction, the 'if' part of (6.6) is proved.
We prove the 'only if' part by induction on the number of steps in the
derivation of A ,;, w. If A ~ w, then A ~ w is in Pj. By construction of P',
G,
G,
A ~ w is obtained from some production A ~ X jX2 .•• XII in P by erasing
some (or none of the) nullable variables. Hence A
::::;> X1X2 ••• XII ~ W. SO
G
G
there is basis for induction. Assume the result for derivation in at most j steps.
J-rl..
i
Let A
::::;> W. ThiS can be split as A ~
XjX~ ... Xk d WjW2 ... Wh where
G
G,
I
0::
X => Wi' The first production A ~ X j X2 ... Xk in P' is obtained from some
I
G
production A ~ a in P by erasing some (or none of the) nullable variables
,.
0
in a. So A
::::;> u. ~
XjX~ ... Xk. If Xi E L then Xi ~ Xi =Wi' If Xi E VN
G
G
G
then by induction hypothesis, Xi ~ Wi' So, we get A 7" XjX2 ... Xk ~
G
G
WjW~ ... Wk' Hence by the principle of induction whenever A ~ w, we have
G1
A ~ wand W "* A. Thus (6.6) is completely proved.
G
By applying (6.6) to S. we have W
E L(G j ) if and only if W
E L(G) and
W "* A. This implies L(G j ) = L(G) - {A}.
I
Corollary 1
There exists an algorithm to decide whether A E L(G) for a
given context-free grammar G.
Proof
A E L(G) if and only if SEW. i.e. S is nullable. The construction
given in Theorem 6.6 is recursive and terminates in a finite number of steps
(actually in at most IVy I steps). So the required algorithm is as follows:
(i) construct W; (ii) test whether SEW.
Chapter 6: Context-Free Languages
!;!
199
Corollary 2
If G = (Vv, L:. P. 5) is a context-free grammar we can find an
equivalent context-free grammar Gl = (V~v, L:, P, 51) without null productions
except 51 -j A when A is in L(G). If 51 -j A is in Pl' 51 does not appear
on the R.H.S. of any production in Pl'
Proof
By Corollary 1, we can decide whether A is in L(G).
Case 1
If A is not in L(G), G[ obtained by using Theorem 6.6 is the required
equivalent grammar.
Case 2
If A is in L(G), construct G' = (VV, L:. r. 5) using Theorem 6.6.
L(G') =L(G) -
{A}. Define Gl = (VN U
{51}, L:, PI' 51), where PI = p' U
{51 -j 5. 5 j -j A}. 51 does not appear on the R.H.S. of any production in
Pl. and so Gl is the required grammar with L(G l ) = L(G).
I
6.3.3
ELIMINATION OF UNIT PRODUCTIONS
A context-free grammar may have productions of the form A -j B, A, B
E Vy .
Consider. for example, G as the grammar 5 -j A, A -j B, B -j C,
C -j a. It is easy to see that L(G) = {a}. The productions 5 -j A, A -j B,
B -j C are
useful just to replace 5 by C. To get a terminal string, we need
C -j a. If Gj is 5 -j a, then L(G j ) = L(G).
The next construction eliminates productions of the form A -j B.
Definition 6.10
A unit production (or a chain rule) in a context-free
grammar G is a production of the form A -j B. where A and B are variables
in G.
Theorem 6.7
If G is a context-free grammar,we can find a context-free
grammar G j which has no null productions or unit productions such that
L(G[) = L(G).
Proof
We can apply Corollary 2 of Theorem 6.6 to grammar G to get a
grammar G' = (Vy, L:, P. 5) without null productions such that L(G') = L(G).
Let A be any variable in V".
Step 1
COl1stmction of the set of variables derivable from A:
Define Wr(A) recursively as follows:
Wo(A) = {A}
Wr+i (A) = Wr(A) u
{B
E
Vv I C -j B is in P with C E
Wr(A)}
By definition of R'r(A), W/A) \:: Wr+l(A). As Vv is finite. Wk+l (A) = Wk(A)
for some k s: IVvJ. So, Wk+iA) = Wr(A) for all j ;:: O. Let W(A) = Wk(A). Then
W(A
is the set of all variables derivable from A
Step 2
Construction of A-productions in G j :
The A-productions in G l are either (i) the nonunit production in G' or
(ii) A
-j a whenever B -j a is in G with B
E
W(A) and a .,; VN-
200
l;l
Theory ofComputer Science
(Actually, (ii) covers (i) as A E
W(A)). Now, we define G I = (VN,
~, PI'S),
where PI is constructed using step 2 for every A E
~v.
Before proving that G] is the required grammar, we apply the construction
to an example.
EXAMPLE 6.10
Let G be S ~ AB, A ~ a. B ~ CIb, C ~ D, D ~ E and E ~ a. Eliminate
unit productions and get an equivalent grammar.
Solution
Step 1
V;'o(S) = {S},
Wj(S) = WoeS} u
0
Hence W(S} = {S}. Similarly,
W(A} = {A},
Wee} = {E}
Wo(B} = {B},
WI(B) = {B} u
{C} = {B, C}
W2(B) = {B. C} u
{D}.
W3(B) = {B, C, D} u
{E},
W4(B} = W3(B}
Therefore,
W(B} = {B, C, D, E}
-Similarly,
Wo(C) = {C},
Therefore.
Hence,
W(C) = {C, D, E},
Wo(D} = {D}
Thus.
WeD} = {D, E}
in G',
Step 2
The productions in G j are
S ~ AB.
A ~ a.
B ~ b Ia,
C ~ a,
By construction. G] has no unit productions.
To complete the proof we have to show that L(G'} = L(G I }.
Step 3
L(G') = L(G}. If A ~ a is in PI - P, then it is induced by B ~ a
in P with B E
W(A}, a eo Vv. B E
W(A} implies A ~ B. Hence, A ~ B
.
~
~
=> a. So, if A => a, then A ~ a. This proves L(G j }
\;;; L(G'}.
G'
G,
G'
To prove the reverse inclusion, we start with a leftmost derivation
S => al => ao , .. => an = w
G
G
-
G
Chapter 6: Context-Free Languages
,!;l,
201
,i;
Let i be the smallest index such that
Cti ~ Cti+1 is obtained by a unit
production and j be the smallest index greater than i such that Ctj ~ Cti+J is
x
G
obtained by a nonunit production. So, S ~ ai, and
aj ~ aJ'+1 can be
G,
'
G'
.
written as
ai =
H'iA;{3i =} wiAi+!f3i =} .. ,
=} WiAjf3i =} wir f3i =
CXj+!
Aj
E W(A i) and Aj
~ r is a nonunit production. Therefore, Aj
~ r is a
production in PI' Hence,
Ctj ~ aj+I' Thus, we have S ~ aj+I'
.
G,
'
G,
Repeating the argument whenever some unit production occurs in the
remaining part of the derivation, we can prove that S ~ a" = w, This proves
G,
L(G')
s;;;; L(G).
I
Corollary
If G is a context-free grammar, we can construct an equivalent
grammar G' which is reduced and has no null productions or unit productions.
Proof
We construct G] in the following way:
Step 1
Eliminate null productions to get G] (Theorem 6.6 or Corollary 2 of
this theorem).
Step 2
Eliminate unit productions in G] to get G2 (Theorem 6.7).
Step 3
Construct a reduced grammar G' equivalent to G] (Theorem 6.5). G'
is the required grammar equivalent to G.
Note: We have to apply the constructions only in the order given in the
corollary of Theorem 6.7 to simplify grammars. If we change the order we
may not get the grammar in the most simplified form (refer to Exercise 6.11).
6.4
NORMAL FORMS FOR CONTEXT-FREE GRAMMARS
In a context-free grammar. the R.H.S. of a production can be any string of
variables and terminals. When the productions in G satisfy certain restrictions,
then G is said to be in a 'normal form'. Among several 'normal forms' we study
two of them in this section-the Chomsky normal form (CNF) and the
Greibach nOlmal form.
6.4.1
CHOMSKY
NORMAL FORM
In the Chomsky normal form (CNF). we have restrictions on the length of
R.H.S. and the nature of symbols in the R.H.S. of productions.
DefInition 6.11
A context-free grammar G is in Chomsky normal form if
every production is of the form A ~ G, or A ~ Be, and S ~ A is in G if
202
Q
Theory ofComputer Science
A E L(G). When A is in L(G), we assume that S does not appear on the
RH.S. of any production.
For example, consider G whose productions are S ~ AB IA, A ~ a,
B ~ b. Then G is in Chomsky normal form.
Remark
For a grammar in CNF, the derivation tree has the following
property: Every node has atmost two descendants-either two internal vertices
or a single leaf.
When a grammar is in CNF, some of the proofs and constructions are
simpler.
Reduction to Chomsky Normal Form
Now we develop a method of constructing a grammar in CNF equivalent to a
given context-free grammar. Let us first consider an example. Let G be S ~
ABC IaC, A ~ a. B ~ b, C ~ c. Except S ~ aC IABC, all the other
productions are in the form required for CNF. The terminal a in S ~ aC can
be replaced by a new variable D. By adding a new production D ~ a, the effect
of applying S ~ aC can be achieved by S ~ DC and D ~ a. S ~ ABC is not
in the required form. and hence this production can be replaced by S ~ AE and
E ~ Be. Thus, an equivalent grammar is S ~ AE IDC, E ~ BC, A ~ a,
B ~ b. C ~ c, D ~ a.
The techniques applied in this example are used in the following theorem.
Theorem 6.8
(Reduction to Chomsky normal form). For every context-free
grammar, there is an equivalent grammar G2 in Chomsky normal form.
Proof
(Construction of a grammar in CNF)
Step 1
Elimination of null productions and unit productions:
We apply Theorem 6.6 to eliminate null productions. We then apply
Theorem 6.7 to the resulting grammar to eliminate chain productions. Let the
grammar thus obtained be G = (V"" L, P, S).
Step 2
Elimination of terminals on R.H.S.:
We define G1 =(V"" L, PI, S'), where p] and V;,vare constructed as follows:
(i) All the productions in P of the form A ~ a or A ~ BC are included
in P lo All the variables in Vv are included in V;",.
(ii) Consider A ~ XjX2 ..• Xn with some terminal on RH.S. If Xi isa
terminaL say ai, add a new variable Ca to V;", and Ca ~ ai to Pl'
I
I
In production A ~ XjX2 ... Xii' every terminal on R.H.S. is replaced
by the corresponding new variable and the variables on the RH.S. are
retained. The resulting production is added to p]. Thus, we get
G1 = (V:"', L, Ph S).
Step 3
Restricting the number of variables on R.H.S.:
For any production in Plo the R.H.S. consists of either a single terminal (or
Chapter 6: Context-Free Languages
~
203
A in S ~ A) or two or more variables. We define G2 = (V~~, L, P2. S) as
follows:
(i) All productions in p[ are added to P2 if they are in the required form.
All the variables in V:v are added to
~~(:.
(ii) Consider A
~ A jA 2 .•• Am. where m
~ 3. We introduce new
productions A
~ A[C[.
C j
~ A 2C2• ..., Cm-2 ~ Am-lAm and
new variables C j , C2•... , Cm- 2. These are added to P" and VIVo
respectively.
Thus. we get G2 in Chomsky normal form.
Before proving that G2 is the required equivalent grammar. we apply the
construction to the context-free grammar given m Example 6.11.
EXAMPLE 6.11
Reduce the following grammar G to CNF. G is 5 ~ aAD, A ~ aB I bAB,
B ~ b. D ~ d.
Solution
As there are no null productions or unit productions, we can proceed to step 2.
Step 2
Let G[ = (V\. {a. b. el}, Pj. 5). where P l and VNare constructed
as follows:
(i) B ~ b, D ~ d are included in Pl'
(ii) 5 ~ aAD gives rise to 5 ~ CuAD and Cu ~ a.
A ~ aB gives rise to A ~ CuB.
A ~ bAB gives rise to A ~ ChAB and Ch ~ b.
V:v = {5. A. B. D. Cu' Ch }.
Step 3
P l consists of 5 ~ CuAD, A ~ C"B IChAB, B ~ b. D ~ d, Cu ~ a.
C/J ~ b.
A ~ CuB. B ~ b, D ~ d, Cu ~ a, Ch ~ b are added to P2
5 ~ CuAD is replaced by 5 ~ CuC j and Cl ~ AD.
A. ~ C/JAB is replaced by A ~ C/JC2 and C2 ~ AB.
Let
G2 = ({5. A. B, D. Cu' Ch• Cl • CJ. {a. b, d}, P2• 5)
where P2 consists of 5 ~ C"Clo A ~ CuB I C/JC2• Cj
~ AD. C2 ~ AB,
B ~ b, D ~ el. Cu ~ a. Ch ~ b. G2 is in CNF and equivalent to G.
Step 4
L(G) = L(G2). To complete the proof we have to show that L(G) =
L(G j ) = L(G2)·
fo show that L(G) ~ L(G[). we start with H
E L(G). If A ~ X jX2 ... XII
is used in the derivation of 1\'. the same effect can be achieved by using the
corresponding production in P j and the productions involving the new
variables. Hence, A ~ X jX2 ... XII' Thus. L(G) ~ L(Gl)·
G
204
J;l
Theory ofComputer Science
Let
W
E L(G j ). To show that
W
E
L(G), it
1S enough to prove the
following:
(6.7)
*
We prove (6.7) by induction on the number of steps in A => w.
G,
If A => w, then A ---+ H' is a production in Pl' By construction of Pj, .v is
G,
a single terminal. So A ---+
W is in P, i.e. A => w. Thus there is basis for
induction.
G
Let us assume (6.7) for derivations in at most k steps. Let A ~ w. We can
G,
split this derivation as A => A]A 2 . .. Alii !b
W] . Wm = W such that Ai ~
Wi'
~
~
~
*
Each Ai is either in Vy or a new variable, say C", When Ai E VN, Ai => Wi
•
1
~
~,'
is a derivation in at most k steps. and so by induction hypothesis, Ai => Wi'
.
.
.
G
When Ai = C"i' the production
CUi ---+ ai is applied to get Ai ::S
Wi' The
production A ---+ A jA2 ... Alii is induced by a production A ---+ X jX2 .•• XIII
in P where Xi = Ai if Ai E VN and Xi = Wi if Ai = CUI' So A => X j X2 ... XIII
G
b
H'jH'2 ...
Wm' i.e. A b
w.
Thus,
(6.7) is true for all derivations.
G
G
Therefore. L(G) = L(G]).
The effect of applying A ---+ A jA 2 •.. Am in a derivation for W E L(G j )
can be achieved by applying the productions A ---+ A] Cl , Cl
---+ A2C2,
Cm- 2 ---+ Am-lAm in P2' Hence it is easy to see that L(GI )
<;;;; L(G2).
To prove L(Gc)
<;;;; L(G j ), we can prove an auxiliary result
A=>w
G,
if A
E
V;v, A =>
tV
G.
(6.8)
Condition (6.8) can be proved by induction on the number of steps in A => w.
G,
Applying (6.7) to S. we get L(G2)
<;;;; L(G). Thus.
L(G) = L(G j ) = L(G2)
I
EXAMPLE 6.12
Find a grammar in Chomsky normal form equivalent to S ---+ aAbB, A ---+
aAla. B ---+ bBlb.
Chapter 6: Context-Free Languages
J;!
205
Solution
As there are no unit productions or null productions, we need not carry out
step 1. We proceed to step 2.
Step 2
Let G j = (V'y {a, b}, PI' S), where P j and V:v are constructed as
follows:
(i) A ~ a, B ~ b are added to Pl'
(ii) S ~ aAbB, A ~ aA, B ~ bB yield S ~ CaACbB. A ~ CaA.
B ~ C;,B. C{/ ~ a. Cb ~ b.
V'v = {S, A, B, C", Cb }·
Step 3
PI consists of S ~ CaACbB, A ~ CaA, B ~ C;,B, Ca ~ a.
Cb ~ b, A ~ a, B ~ b.
S ~ C{/AC/JB is replaced by S ~ CaC] , C 1 ~
AC~. Ce ~ CbB
The remaining productions in P j are added to Pc. Let
G: = ({S. A. B. CO' C/J' CI' Ce}, {a, b}, Pc. S),
where Pc consists of S ~ CaCjo Ci ~ ACe, Ce ~ C/JB, A ~ CaA. B ~ C/JB.
Ca ~ a, Cb ~ b. A ~ a. and B ~ b.
G: is in C:I'<'F and equivalent to the given grammar.
,
EXAMPLE 6.13
Find a grammar in C:I'<'F equivalent to the grammar
Solution
S ~ -S I[s :::J) S] Ipi q
(S being the only variable)
As the given grammar has no unit or null productions, we omit step 1 and
proceed to step 2.
Step 2
Let G j = (V\, L. PI' S). where P j and V~\ are constructed as fonows:
(i) S ~ pi q are added to Pj.
(ii) S ~ - S induces S ~ AS and A ~ -.
(iii) S ~ [S :::J S] induces S ~ BSCSD, B ~ [,C ~ :::J. D ~]
\1'\ = {So A. B. C. D}
Step 3
P j consists of S ~ plq. S ~ AS. A ~ -, B ~ LC
~:::J, D
~].
S ~ BSCSD.
S ~ BSCSD is replaced by S ~ BCI , Cj ~ SCe' Ce ~ CC3, C3 ~ SD.
Let
Ge = ({S, A. B. C. D, CI , C:, C3 }. L, Pc, S)
where P: consists of S ~ plq[AS[BC j , A ~ -, B ~ [. C ~ :::J, D
~],
CI ~ SCe' C: ~ CC3, C3 ~ SD. G: is in CNF and equivalent to the given
grammar.
206
g
Theory ofComputer Science
6.4.2
GREIBACH
NORMAL FORM
Greibach normal form (GNF) is another normal form quite useful in some
proofs and constructions. A context-free grammar generating the set accepted
by a pushdown automaton is in Greibach normal form as will be seen in
Theorem 7.4.
Deflnition 6.12
A context-free grammar is in Greibach normal form if every
production is of the form A ~ aa. where a E vt and a E L(a may be A),
and S ~ A is in G if A E
L(G). When A E
L(G), we assume that S
does not appear on the R.H.S. of any production. For example, G given by
S ~ aAB IA, A ~ bC, B ~ b, C ~ c is in GNF.
Note:
A grammar in GNF is a natural generalisation of a regular grammar.
In a regular grammar the productions are of the form A ~ aa, where a E L
and a E Vv U {A}, i.e. A ~ aa, with avt and 1al s 1. So for a grammar
in GNF or a regular grammar, we get a (single) terminal and a string of
variables (possibly A) on application of a production (with the exception of
S ~ A).
The construction we give in this section depends mainly on the following
t\VO technical lemmas:
Lemma 6.1
Let G = (Vy , L. P. S) be a CFG. Let A ~ Bybe an A-production
in P. Let the B-productions be B ~ f3j 11321 ... I13,. Define
P j = (P -
{A ~ By})
U
{A ~ f3iyl1 sis s}.
Then. G j = (Vy , L, Pj, S) is a context-free grammar equivalent to G.
Proof
If we apply A ~ By in some derivation for
]V E L(G), we have to
*
apply B ~ f3i for some i at a later step. So A 'Z' Biy. The effect of applying
A ~ By and eliminating B in grammar G is the same as applying A ~ f3iY
for some i in grammar G!. Hence H' E L(G!), i.e. L(G) s;;; L(G!). Similarly,
instead of applying A ~ f3iY. we can apply A ~ By and B ~ f3i to get
A
~ f3iY. This proves L(G]) s;;; L(G).
I
G
Note:
Lemma 6.1 is useful for deleting a variable B appearing as the first
symbol on the R.H.S. of some A-production, provided no B-production has B
as the first symbol on R.H.S.
The construction given in Lemma 6.1 is simple. To eliminate B in A ~ By,
we simply replace B by the right-hand side of every B-production.
For example. using Lemma 6.1. we can replace A ~ Bab by A ~ aAab,
A ~ bBab, A ~ aaab. A ~ ABab when the B-productions are B ~ aA 1 bB I
aaiAB.
The lemma is useful to eliminate A from the R.H.S. of A ~ Aa.
Lemma 6.2
Let G = (Vy , L, P, S) be a context-free grammar. Let the set
of A-productions be A ~ Aa! I... Aa, 11311 ... 113, (f3i'S do not start with A).
Chapter 6: Context-Free Languages
~
207
Let Z be a new variable. Let G1 = (Vv u {ZL l:, PI' S), where PI is defined
as follows:
(i)
The set of A-productions in PI are A ~ f3! If321 ... !f3s
A ~
f31 Z If32Z I ... lf3sZ
(ii) The set of Z-productions in PI are Z ~ a1 la2! ... !ar
Z ~ a1Z I a2Z ... !a,Z
(iii) The productions for the other variables are as in P. Then G! is a CFO
and equivalent to G.
Proof
To prove L(G) ~ L(G), consider a leftmost derivation of H' in G. The
only productions in P -
PI are A ~ AaI! Aa2 ... IA a,. If A ~ Aail'
A ~ Aai2, ..., A ~ Aah are used, then A ~ f3j should be used at a later
stage (to eliminate A). So we have A %f3;Aail ... ail while deriving w in
G. However.
.
l.e.
.
A => f3jAail ... ai;
G]
Thus. A can be eliminated by using productions in Gl. Therefore, w E L(G!).
To prove L(G)) ~ UG). consider a leftmost derivation of lV in G!. The
only productions in PI -
P are A ~ f31ZI f32ZI ... lf3sZ. Z ~ all· .. la"
Z ~ ajZ Ia2Z I ... Ia,z. If the new variable Z appears in the course of the
derivation of 11', it is because of the application of A ~ f3jZ in some earlier
step. Also. Z can be eliminated only by a production of the form Z ~ ~ or
*
Z ~ aJZ for some i and j in a later step. So we get A => f3J·ai ai ... ai
.
12k
~]
in the course of the derivation of w. But, we know that A => f3Pi l ai2 ... aik'
e
Therefore.
W
E L(G).
EXAMPLE 6.14
Apply Lemma 6.2 to the following A-productions in a context-free grammar
G.
A
~ aBD IbDB Ie
A ~ ABIAD
Solution
In tL..; example. al = B. a2 = D, f31 = aBD. f32 = bDB, f33 = c. So the new
productions are:
(i) A ~ aBDlbDBlc.
(ii) Z ~ B, Z ~ D.
A ~ aBDZI bDBZI cZ
Z ~ BZIDZ
208
~
Theory ofComputer Science
Theorem 6.9
(Reduction to Greibach normal fmID). Every context-free
language L can be generated by a context-free grammar G in Greibach normal
form.
Proof
We prove the theorem when A e: L and then extend the construction
to L having A.
Case 1
Construction of G (when A
E L):
Step 1
We eliminate null productions and then construct a grammar G in
Chomsky
normal
form
generating
L.
We
rename
the
variables
as
A b A2, .. .,AIl with S = AI' We write G as ({A b A 2, ..., All}, L, P, A)).
Step 2
To get the productions in the form Ai ~ ay or Ai ~ Air, where
j > i, convelt the Ai-Productions (i = 1, 2...., n -
1) to the form Ai ~ Aiy
such that j > i. Prove that such modification is possible by induction on i.
Consider A)-productions. If we have some A)-productions of the form
A) ~ A)r, then we can apply Lemma 6.2 to get rid of such productions. We
get a new variable. say ZI' and A)-productions of the form A) ~ a or AI ~ AjY~
where j > 1. Thus there is basis for induction.
Assume that \ve have modified AI-productions, Arproductions
'"
Ai-productions.
Consider
Ai+)-productions.
Productions
of
the
form
Ai+1 ~ ay required no modification. Consider the first symbol (this will be
a variable) on the R.H.S. of the remaining Ai+I-Productions. Let t be the
smallest index among the indices of such symbols (vmiables). If t > i + 1,
there is nothing to prove. Otherwise. apply the induction hypothesis to
At-productions for t
:::; i. So any At-production is of the form At ~ Air,
where j > t or At ~ ay'. Now we can apply Lemma 6.1 to A;+I-production
whose R.H.S. starts with At. The resulting Ai+)-productions are of the form
Ai+)
~ Aiy- \vhere j > t (or A;+I ~ ay').
We repeat the above construction by finding
t for the new set of
Ai+)-productions. Ultimately, the A;+)-productions are converted to the form
A;+I
~ Aiy; where j
2: i + 1 or Ai+)
~ ay'. Productions of the form
Ai+1 ~ A'+I Ycan be modified by using Lemma 6.2. Thus we have converted
Ai+l-productions to the required form. By the principle of induction, the
construction can be carried out for i = L 2, " ., 11. Thus for i = 1, 2, ...,
n - L any Ai-production is of form Ai ~ Air, where j > i or Ai ~ ay'. Any
AI)-production is of the form AI) ~ AnY or All ~ ay'.
Step 3
Convert An-productions to the form A" ~ ay. Here, the productions
of the form All
~ Allyare eliminated using Lemma 6.2. The resulting
All-productions are of the form AI) ~ oy.
Step 4
Modify the Ai-productions to the form Ai ~ oy for i = L 2, ....
n -
L At the end of step 3, the All-productions are of the form AI) ~ ay.
The All_I-productions are of the form AII_) ~ ay' or All-I ~ A"y- By applying
Lemma 6.L \ve eliminate productions of the form All-I ~ AnY- The resulting
Chapter 6: Context-Free Languages
J;;!
209
An_I-productions are in the required form. We repeat the construction by
considering An- 2• An- 3, .... A I'
Step 5
Modify Z;-productions. Every time we apply Lemma 6.2, we get a
new vfu'iable. (We take it as Z; when we apply the Lemma for ArProductions.)
The Zrproductlons are of the form Zi ~ aZi or Z; ~ a (where a is obtained
from Ai ~ Aia), and hence of the form Zi ~ ayor Zi ~ Aky for some k.
At the end of step 4. the R.H.S. of any Acproduction stmts with a terminal.
So we can apply Lemma 6.1 to eliminate Zi ~ Aky- Thus at the end of
step 5, we get an equivalent grammar G] in GNF.
It is easy to see that G] is in Gl\i'F. We start with G in CNG. In G any
A-production is of the form A ~ a or A ~ AB or A ~ CD. When we apply
Lemma 6.1 or Lemma 6.2 in step 2, we get new productions of the form
A ~ aa or A ~ [3, where a E
V~ and [3 E
V~y and a E L. In steps 3-5.
the productions are modified to the form A ~ aa or Z ~ a'a'. where a. a'
ELand a. a/ E V'(,.
Case 2
Construction of G when .A E L:
By the previous construction we get G' =
(V~" L. Pl' 5) in GNF such that
L(G') = L -
{A}. Define a new grammar GI as
GI = (Vy u {5'}, L. PI
U
{5' ~ 5. 5' ~ A}. 5')
5' ~ 5 can be eliminated by using Theorem 6.7. As 5-productions are in the
required form, 5'-productions are also in the required form. So L(G) =L(G])
and G] is in GNF.
I
Remark
Although we convert the given grammar to CNF in the first step.
it is not necessary to convert all the productions to the form required for CNF.
In steps 2-5. we do not disturb the productions of the form A ~ aa. a E L
and a E V\. So such productions can be allowed in G (in step 1). If we apply
Lemma 6.1 or 6.2 as in steps 2-5 to productions of the form A ~ a. where
a E vt and Ia I;:: 2. the resulting productions at the end of step 5 are in the
required form (for GNF). Hence we can allow productions of the form A ~ a,
where a E
V~. and IaI ;:: 2.
Thus we can apply steps 2-5 to a grammar whose productions are either
A ~ aa where a
E V\. or A ~ a
E
V'(, where Ia I ;:: 2. To reduce the
productions to the form A ~ a E V\. where Ia I;::
2, we can apply step 2
of Theorem 6.8.
EXAMPLE 6.1 5
Construct a grammar in Greibach normal form equivalent to the grammar
5
-'t AA I a. A ~ 55 Ib.
Solution
The given grammar
IS in C1'IF. 5 and A
are renamed as A] and A 2•
respectively. So the productions are A 1 ~ A IA2 1 a and A 2 ~ A]A 11 b. As the
21 0
.~
Theory ofComputer Science
given grammar has no null productions and is in CNF we need not carry out
step 1. So we proceed to step 2.
Step 2
(i) AI-productions are in the required fonn. They are Al ~ A2A2 1 a.
(ii) A2 ~ b is in the required fonn. Apply Lemma 6.1 to A2 ~ AjAj.
The
resulting
productions
are
A2
~
A2A2A I, A2
~
aA I •
Thus
the
ATproductions are
A2 ~ A2A2Aj,
A2 ~ aA],
Ao ~ b
Step 3
We have to apply Lemma 6.2 to ATProductions as we have
A2 ~ A2A2A l' Let Z2 be the new variable. The resulting productions are
A2 ~ aAI,
A2 ~ b
Z2 ~ ih4 I ,
Z2 ~ A2A IZ2·
Step 4
(i) The A2-productions are A2 ~ aAjl bl aAIZ2 1 bZ2·
(ii)
Among
the
A j-productions
we
retain
A j
~
a
and eliminate
A j ~ A2A2 using Lemma 6.1. The resulting productions are AI ---t aA IA2IbA:>
AI
~ aA IZ2A2 IbZ2A:> The set of all (modified) AI-productions is
A j ~ alaAIA2IbA2IaAIZ2A2IbZ::A2
Step 5
The ZTproductions to be modified are Z2 ~ A2A I , Z2 ~ A2A jZ2·
We apply Lemma 6.1 and get
Z2 ~ aAIA] IbAil aA IZ2A 1IbZ2A I
Z2 ~ aA IA j Z2 1 bA jZ2 1 aA JZ2A jZ2 1 bZ2A IZ2
Hence the equivalent grammar is
G' = ({AI' A2, Z2}, {a, b}, P], AI)
where PI consists of
AI ~ a I aA]A2 1 bA2 1aA IZ2A I I bZ2A2
A~ ~ aA j iblaA IZ21bZ2
Z2 ~ aA]AII bA] IaA 1Z2AI IbZ2A j
Zo ~ aA JA]Z21 bA IZ2 1aA IZ2AIZ2 1 bZ2A]Z2
EXAMPLE 6.16
Convert the grammar S ~ AB, A ~ BS Ib, B ~ SA Ia into ONF.
Solution
As the given grammar is in CNF, we can omit step 1 and proceed to step 2
after renaming S, A. B as AI, A2. A3, respectively. The productions are AI ~
A2A3• A2 ~ A011 b, A3 ~ A j A2 1 a.
(6.9)
(6.11)
(6.10)
(6.12)
Chapter 6: Context-Free Languages
g
211
Step 2
(i) The Aj-production A j ~ A2A3 is in the required form.
(ii) The ATproductions A2 ~ A~ll b are in the required form.
(iii) A3 ~ a is in the required form.
Apply Lemma 6.1 to A3 ~ AjA? The resulting productions are A3 ~ A2A~2'
Applying the lemma once again to A3 ~ A2A~2' we get
A3 ~ A~lA~21 bA 3A:;.
Step 3
The A3-productions are A3 ~ a 1 bA~2 and A3 ~ A~jA~:;. As we
have A 3 ~ A~!A~2' we have to apply Lemma 6.2 to A3-productions. Let
Z3 be the new variable. The resulting productions are
A3 ~ aIbAy42,
A 3 ~ aZ31 bA~2Z3
Z3 ~ AjA~2'
Z3 ~ AlA~2Z3
Step 4
(i) The ArProductions are
A3 ~ a 1 bA~:;1 aZ31 bA~:;Z3
(ii) Among the A:;-productions, we retain A:; ~ b and eliminate A:; ~
A~ j using Lemma 6.1. The resulting productions are
A:; ~ aA] IbA~:;AJiaZ3A IbA~2Z~!
The modified A:;-productions are
A:; ~ b 1aA!1 bA~:;A!1 aZ~ll bA,A:;Z~]
(iii) We apply Lemma 6.1 to A l ~ A:;A3 to get
A l ~ bA3 1aA jA31bAy4:;A 1A31aZ~jA31 bA~2Z3AjA3
Step 5
The Zrproductions to be modified are
Z3 ~ A]A021 A jA02Z3
We apply Lemma 6.1 and get
Z3 ~ bAy4y4.:;1 bAy42Z3
Z3 ~
aA]A~y42IaA]A~y42Z3
Z3 ~
bA~:;A]Ay4y421
bA3A:;AjA~3A:;Z3
Z3 ~
aZ~lA~3A:;1 aZ~]A3A~2Z3
Z3 ~
bA~2Zy4!A3Ay4:;1
bA~:;Z~IA3A3A:;Z3
The required grammar in GNP is given by (6.9)-(6.12).
The following example uses the Remark appearing after Theorem 6.9. In
this example we retain productions of the form A ~ aa and replace the
terni:1als only when they appear as the second or subsequent symbol on
R.H.S. (Example 6.17 gives productions to generate arithmetic expressions
involving a and operations like +, * and parentheses.)
212
~
Theory ofComputer Science
EXAMPLE 6.1 7
Find a grammar in G:t\TF equivalent to the grammar
E ---+ E + TIT, T ---+ T*FIF. F
---+ (E)la
Solution
Step 1
We first eliminate unit productions. Hence
So.
So.
Wo(E) = {EL
WI(E) = {E} u
{T} = {E, T}
W:(E) = {E, T} u
{F} = {E, T, F}
WeE) = {E, T, F}
Thus.
WoCT) = {T},
WI(T) = {T} u
{F} = {T. F}
(6.14)
WCT) = {T. F}
Wo(F) = {F},
WI(F) = {F} = W(F)
The equivalent grammar without unit productions is, therefore, G] =(VN,
L, Pl' S), where PI consists of
(i) E
---+ E + TIT"FI(E)la
(ii) T ---+ T * FI (E) Ia. and
(iii) F ---+ (E)! a.
We apply step 2 of reduction to CNF. We introduce new variables A, B,
C con'esponding to +. '.). The modified productions are
(i) E ---+ EATI TBFI (Eq a
(ii)
T ---+ TBF 1(EC 1a
(iii) F ---+ (EC Ia
(iv) A ---+ +. B ---+ ", C ---+ )
The variables A. B. C F. T and E are renamed as AI' A:, A 3, A4. As, A6.
Then the productions become
A 1 ---+ +,
Ao ---+
*.
A 3 ---+).
A4 ---+ (A03 1 a
(6.13)
As ---+ AsA:A+ I(A~31 a
A 6 ---+ AeA lAs IAsA:A4 ! (A~31 a
Step 2
We have to modify only the As- and A6-productions. As ---+ AsA:Aj
can be modified by using Lemma 6.2. The resulting productions are
As ---+ (A 6A3 Ia.
As
---+
(A~3ZslaZs
Zs ---+ A:A4
1 A:A4ZS
A 6
---+ AsA:A4 can be modified by using Lemma 6.1. The resulting
productions are
A 6 ---+
(A~:A:A41 aA:A4 1 (Ae;1.3ZsA:A41 aZsA:A4
A6 ---+ (A 6"4.3 1a are in the proper fonn.
Chapter 6: Context-Free Languages
l;!
213
Step 3
A6 ----,>
A(l~ lAs can be modified by using Lemma 6.2. The resulting
productions give all the A6-productions:
A6 ----,>
(A0:;A~A-l1 aA~A-l1 (A03ZSAY~-l
A6 ----,>
aZy4~A41 (Ac03 I a
(6.15)
A6
----,>
(A03A~A-lZ61
aA~A4Z61 (A03ZsA~A4Z6
A6
----,>
aZsA~A-lZ61 (A03Z61 aZ6
(6.16)
A6
----,> AIAsiAIAsZ6
Step 4
The step is not necessary as A,productions for i = 5, 4, 3, 2, 1 are
in the required form.
Step 5
The Zs-productions are Zs
----,> A~A41 A~A4ZS' These can be modified
as
Zs
----,>
'A-ll" A-lZs
(6.17)
The Z6-productions are Z6
----,> A lAs IA jASZ6. These can be modified as
Z6
----,> + k" 1+ ASZ6
(6.18)
The required grammar in Gl\;Tf is given by (6.13)-(6.18).
6.5
PUMPING
LEMMA FOR CONTEXT-FREE
LANGUAGES
The pumping lemma for context-free languages gives a method of generating
an infinite number of strings from a given sufficiently long string in a context-
free language L. It is used to prove that certain languages are not context-free.
The construction \ve make use of in proving pumping lemma yields some
decesion algorithms regarding context-free languages.
Lemma 6.3
Let G be a context-free grammar in CNF and T be a delivation
tree in G. If the length of the longest path in T is less than or equal to k, then
the yield of T is of length less than or equal to 21:-1.
Proof
We prove the result by induction on k, the length of the longest path
for all A-trees (Recall an A-tree is a derivation tree whose root has label A).
\~'hen the longest path in an A-tree is of length 1. the root has only one son
whose label is a terminal (when the root has two sons, the labels are variables).
So the yield is of length 1. Thus. there is basis for induction.
Assume the result for k - 1 (k > 1). Let T be an A-tree with a longest path
of length less than or equal to k. As k > 1. the root of T has exactly two sons
with labels A j and A~. The two subtrees with the t\VO sons as roots have the
longe::t paths of length less than or equal to k - 1 (see Fig. 6.12).
If WI and
,v~ are their yields. then by induction hypothesis, IWI I :::;
21:-~,
I ,v~ I :::;
21:-~. So the yield of T = WIWe' I WI H': I :::; 21:-: +
21:-~ = 21:-1. By the
principle of induction, the result is true for all A-trees. and hence for all
derivation trees.
214
~
Theory ofComputer Science
A
Fig. 6.12
Tree T with subtrees T1 and Tz.
Theorem 6.10
(Pumping lemma for context-free languages). Let L be a
context-free language. Then we can find a natural number n such that:
(i) Every Z E L with Iz I ~ n can be written as U1'WX)' for some strings
Lt, v,
W, x, y.
(ii)
l1'xl
~ 1.
(iii) I1'WXI :s; n.
(iv)
ll1'k~n):y E L for all k ~ O.
Proof
By Corollary 1 of Theorem 6.6, we can decide whether or not A E L.
When A E L, we consider L - {A} and construct a grammar G = (11,,,,, L, P, S)
in CNF generating L - {A} (when A ~ L, we construct Gin CNF generating
L).
Let IVvI = m and n = 21/l. To prove that n is the required number, we
start
with;, E L, IzI ~ 2/ll, and construct a derivation tree T (parse tree) of z. If
the length of a longest path in T is at most m, by Lemma 6.3, IzI :s; 2
111-] (since
z is the yield of T). But Iz I ~ 21/l > 2"-]. So T has a path, say r. of length
greater than or equal to m + 1. r has at least m + 2 vertices and only the last
vertex is a leaf. Thus in r all the labels except the last one are variables. As
Iv'v I = m. some label is repeated.
We choose a repeated label as follows: We start with the leaf of rand
travel along r upwards. We stop when some label, say B. is repeated. (Among
several repeated labels, B is the first.) Let v] and 1'2 be the vertices with label
B, VI being nearer the root. In r. the portion of the path from v] to the leaf has
only one labeL namely B, which is repeated, and so its length is at most m + 1.
Let T) and T2 be the subtrees with vj,
1'2 as roots and z),
W as yields,
respectively. As r is a longest path in T, the portion of r from v) to the leaf
is a longest path in T] and of length at most m + 1. By Lemma 6.3, Iz] I :s; 2m
(since z] is the yield of T]).
For better understanding, we illustrate the construction for the grammar
whose productions are S ~ AB, A ~ aB Ia, B ~ bA Ib, as in Fig. 6.13. In
the figure,
r= S ~ A ~ B ~ A ~ B ~ b
z = ababb.
;,) = bab,
w = b
V = ba.
x = A,
II = a,
y = b
Chapter 6: Context-Free Languages
~
215
As ;: and ;:1 are the yields of T and a proper subtree T1 of T, we can write
Z = UZ IY' As ;:1 and lV are the yields of T 1 and a proper subtree T:. of T 1, we
can write z1 =vwx. Also, 1vwx 1> 1w I· SO, I vx I ;::: 1. Thus, we have;: =uvwxy
with
1 vwx 1
::; 11 and
I vx I
;::: 1. This proves the points (i)-(iii) of the theorem.
As T is an S-tree and T1, T:. are B-trees, we get S :::b uBy, B :::b vBx and
B :::b w. As S :::b uBy::::;. uwy, uvOwxOy E L. For k ;::: 1, S :::b uBy :::b uvkB./y
:::b
U1,kwx.ky E L. This proves the point (iv) of the theorem.
I
S
A
B
a
v1
B
b
b
A
a
B
v2
b
B
B
b
Fig. 6.13
Tree T and its subtrees T1 and h
Co.rnllary
Let L be a context-free language and n be the natural number
obtained by using the pumping lemma. Then (i) L =t 0 if and only if there
exists w
E L with Iw I < n, and (ii) L is infinite if and only if there exists
z E L such that n ::;
1 z I < 2n.
216
~
Theory ofComputer Science
Proof
(i) We have to prove the 'only if part. If Z E L with 1::1 ;::: n, we
apply the pumping lemma to write z = llVWX.\', where 1 :s; Ivx I :s;
11. Also,
lfW)' ELand 1Inv)' I < Iz I. Applying the pumping lemma repeatedly, we can
get z' E L such that Iz'l < 11. Thus (i) is proved.
(ii) If z E L such that
11 :s;
1z I < 211. by pumping lemma we can write
z = llVW.:r)'. Also. llVkW.l)' E L for all k ;::: O. Thus we get an infinite number
of elements in L. Conversely, if L is infinite, we can find z E L with IzI ;::: 11.
If
1 z i < 2/1. there is nothing to prove. Otherwise, we can apply the pumping
lemma to write z = llVWXJ and get UWJ E L. Every time we apply the pumping
lemma we get a smaller string and the decrease in length is at most n (being
equal to i l'X I). SO. we ultimately get a string z' in L such that n :s; Iz' 1< 2n.
This proves (ii).
I
Note:
As the proof of the corollary depends only on the length of VX, we can
apply the corollary to regular sets as well (refer to pumping lemma for regular
sets).
The corollary given above provides us algorithms to test whether a given
context-free language is empty or infinite. But these algorithms are not efficient.
We shall give some other algOlithms in Section 6.6.
We use the pumping lemma to show that a language L is not a context-
free language. We assume that L is context-free. By applying the pumping
lemma we get a contradiction.
The procedure can be carried out by using the following steps:
Step 1
Assume L is context-free. Let
11 be the natural number obtained by
using the pumping lemma.
Step 2
Choose z E L so that IzI ;::: n. Write z = lIVWXJ using the pumping
lemma.
Step 3
Find a suitable k so that 111,kw.l.v E L. This is a contradiction, and so
L is not context-free.
EXAMPLE 6.18
Show that L = {a"b"c" l11 2 I} is not context-free but context-sensitive.
Solution
We have already constructed a context-sensitive grammar G generating L (see
Example 4.11). We note that in every string of L. any symbol appears the
same number of times as any other symbol. Also a cannot appear after b, and
c cannot appear before b. and so 'In.
Step 1
Assume L is context-free. Let 11 be the natura! number obtained by
using the pumping lemma.
Step 2
Let z =a"b"e". Then IzI= 311 > n. Write z = lIVW.\}', where Ivx I 2 1,
i.e. at least one of v or x is not :l\.
Chapter 6: Context-Free Languages
l;!
217
Step 3
UVH'.\j' = a"hllcll, As 1 :s; Ivx I :s; n, v or x cannot contain all the three
symbols a. h, c. So. (i) v or x is of the form aihi (or bici) for some i, j such that
i + j :s; n. Or (ii) v or x is a string formed by the repetition of only one symbol
among a. b, c.
When v or x is of the form ailJ,
v~ = (/lJaihi (or x~ = dbi(/lJ). As v~ is a
substring of
llV~WX~Y. we cannot have
uv~·wx\ of the form a"'b"'c"', So,
lIv~wx~y E L
When both v and x are formed by the repetition of a single symbol (e,g,
U =ai and v =bi for some i andj. i:S; n, j:S; n), the string IfWy will contain the
remaining symbol, say al' Also, a;' will be a substring of uwy as al does not
occur in v or x. The number of occurrences of one of the other two symbols
in lIWv is less than n (recallllvwxv =a"b"c"), and n is the number of occurrences
of a j.' So llVOWXOy = lIHy E L
.
Thus for any choice of i' or x, we get a contradiction. Therefore, L is not
context-free,
EXAMPLE 6.19
Show that L = {d' Ipis a prime} is not a context-free language,
Solution
We use the following property of L: If H' E L
then I W I is a pnme,
Step 1
Suppose L = UG) is context-free, Let n be the natural number
obtained by using the pumping lemma,
Step 2
Let p be a plime number greater than 11, Then:::: = al' E L. We wlite
:::: = lI1'WXV,
Step 3
By pumping lemma,
llVO,LCC\, =
lrwy
E
L
So' luwy I is a prime
number, say q, Let I1'X I = 1', Then,
IllV'iWX'iy I = q + qr, As q + qr is not a
prime. lll''fiVX'f" E L. This is a contradiction. Therefore, L is not context-f.ree,
6.6
DECISION ALGORITHMS FOR CONTEXT-FREE
LANGUAGES
In this section we give some decision algorithms for context-ti'ee languages and
regular sets.
(i) Algorithm for deciding whether a context}ree language L is empty.
We can apply the construction given in Theorem 6.3 for getting
V;, = Wk' L is
nonempty if and only if S E
Wk'
tii) Algorithm for deciding whether a context-free language L is finite.
Construct a non-redundant context-free grammar G in CNF generating
L - {A}. We draw a directed graph whose vertices are variables in
G. If A --+ Be is a production. there are directed edges from A to B
and A to C L is finite if and only if the directed graph has no cycles.
218
I;1
Theory ofComputer Science
(iii) Algorithm for deciding whether a regular language L is empty.
Construct a deterministic finite automaton M accepting L. We construct
the set of all states reachable from the initial state qo. We find the
states which are reachable from qo by applying a single input symbol.
These states are arranged as a row under columns corresponding to
every input symbol. The construction is repeated for every state
appearing in an earlier row. The construction terminates in a finite
number of steps. If a final state appears in this tabular column, then
L is nonempty. (Actually, we can terminate the construction as soon
as some final state is obtained in the tabular column.) Otherwise, L
is empty.
(iv) Algorithm for deciding yvhether a regular language L is infinite.
Construct a deterministic finite automaton M accepting L. L is infinite
if and only if M has a cycle.
6.7
SUPPLEMENTARY
EXAMPLES
EXAMPLE 6.20
Consider a context-free grammar G with the following productions,
5 ~ A5A I B
B ~ aCb IbCa
C~ ACA IA
A ~ alb
and answer the following questions:
(a) What are the variables and terminals of G?
(b) Give three strings of length 7 in L(G).
(c) Are the following strings in L(G)?
(i) aaa
(ii) bbb
(iii) aba
(iv) abb
(d) True or false: C => bab
(e) True or false: C ::; bab
(f) True or false: C ::; abab
(g) True or false: C ::; AAA
(h) Is A in L(G)?
Solution
(a)
V.\ = {5. A. B, C} and 2: = {a, b}
(b) 5 ::; A~5A~ => A~BA~ => A~aCbA~ => A~aAbA2 ::; ababbab
Chapter 6: Context-Free Languages
,i;l,
219
So ababbab E L(G).
S ~ A2aAbA2 (as in the derivation of the first string)
~ aaaabaa
S ~ A 2aAbA2 ~
bbabbbb
So ababbab, aaaabaa, bbabbbb are in L(G).
(c) S ~ ASA. If B ~
11', then 11' starts with a and ends in b or vice versa
and I,v I 2': 3. If aaa is in L(G), then the first two steps in the
derivation of aaa should be S ~ ASA ~ ABA or S ~ aBa. The
length of the terminal string thus derived is of length 5 or more.
Hence aaa
9!: L(G). A similar argument shows that bbb
9!: L(G).
S ~ B ~ acb ~ aAb ~ abb. So abb E L(G)
(d) False. since the single-step derivations starting with C can only be
C ~ ACA or C ~ A.
(e) C ~ ACA ~ AAA ~ bab. True
(f) Let 11' = abab. If C ~
11', then C ~ ACA ~
11' or C ~ A ~
11'.
In the first case 111'1 = 3, 5. 7..... As 111'1 = 4. and A ~ 11' if and
only if 11' = a or b, the second case does not arise. Hence (f) is false.
(g) C ~ ACA ~ AAA. Hence C ~ AAA is true.
(h) A
9!:
L(G).
EXAMPLE 6.21
If G consists of the productions S ~ aSa IbSb I aSb I bSa I A, show that L(G)
is a regular set.
Solution
First of all, we show that L(G) consists of the set L of all strings over {a, b}.
of even length. It is easy to see that L(G) I: L. Consider a string 11' of even
length. Then,
11' = aja2 ... a2n-Ia2n where each ai is either a or b. Hence
S ~ a 1Sa2n ~ aja2Sa211_ja2n ~ aja2 ... al1San+j ... a2n ~ a1a2 ... a21]'
Hence L I: L(G).
Next we prove that L = L(G1) for some regular grammar G j • Define
G1 = ({S, Sl' S2' S3' S4}. {a, b}. P, S) where P consists of S ~ aSj, Sl ~ as,
S ~ aS2, S2 ~ bS, S ~ bS3• S3 ~ bS, S ~ bS4• S4 ~ as, S ~ A.
Then S :b aja2S where a1 = a or band a2 = a or b. It is easy to see that
L(G1) = L. As G j is regular, L(G) = L(G j ) is a regular set.
EXAMPLE 6.22
Reduce the following grammar to CNF:
S ~ ASA IbA.
A ~ BIS,
B~c
220
,g,
Theory ofComputer Science
Solution
Step 1
Elimination of unit productions:
The unit productions are A ---7 B, A
---7 S.
WoeS) = {S}, Wl(S) = {S}
U
0 = {S}
Wo(A) = {A}, Wj(A) = {A} u
{S, B} = {S, A, B}
W:(A) = {S, A, B} u
0 = {S, A, B}
Wo(B) = {B}, Wj(B) = {B} u
0 = {B}
The productions for the equivalent grammar without unit productions are
S ---7 ASA IhA, B ---7
C
A
---7 ASA IhA, A
---7
C
So, G1 = ({S. A, B}, {b, c}, P, S) where P consists of S ---7 ASA IhA,
B
---7
C, A
---7 ASA I hA I c.
Step 2
Elimination of tenllinals in R.H.S.:
5 ---7 ASA, B ---7 C. A ---7 ASA Ic are in proper form. We have to modify
5 ---7 bA and A
---7 bA.
Replace S ---7 hA by S
---7 CIA, C"
---7 b and A
---7 hA by A
---7 CiA,
C" ---7 b.
So, G: = ({S. A, B, C,,}, {b, c}, P:, S) where P: consists of
S ---7 ASA I CIA
A ---7 ASA Ic ICiA
B ---7
C, C" ---7 h
Step 3
Restricting the number of variables on R.H.S.:
S ---7 ASA is replaced by S ---7 AD, D ---7 SA
A
---7 ASA is replaced by A
---7 AE, E ---7 SA
So the equivalent grammar in CNF is
G3 = ({S. A. B. C", D. E}, {b, c}, P3, S)
where P3 consists of
S ---7 CIA IAD
A ---7 c I CtA IAE
B ---7
C, Cli ---7 b, D
---7 SA, E
---7 SA
EXAMPLE 6.23
Let G =(V,v, :E, P, S) be a context-free grammar without null productions or
unit productions and k be the maximum number of symbols on the R.H.S. of
Chapter 6: Context-Free Languages
~
221
any production of G. Show that there exists an equivalent grammar GI m
CNF, which has at most (k - l)IPI + ILl productions.
Solution
In step 2 (Theorem 6.8). a production of the form A ~ XIX::: ... Xil is
replaced by A ~ Yj Y:::, ...• Yll where Yi = Xi if Xi
E
VN and Yi is a new
variable if Xi E I.. We also add productions of the form Yi ~ Xi whenever
Xi E I. As there are II I terminals, we have a maximum of II. I productions of
the form Yi ~ Xi to be added to the new grammar. In step 3 (Theorem 6.8),
A ~ AjA::: ... An is replaced by n - 1 productions, A ~ AIDj. D] ~ A 2D:::
... D Il_::: ~ An_jAil' Note that n S k. So the total number of new productions
obtained in step 3, is at most (k - 1) 1P I. Thus the total number of productions
in CNF is at most (k - l)IPI + II. I·
Example 6.24
Reduce the following CPG to GNF:
S ~ ABbla,
A ~ aaA.
B ~ hAb
Solution
The valid productions for a grammar in GNF are A ~ a (X, where a
E L,
0; E
V~v.
So, S ~ ABb can be replaced by S ~ ABC, C ~ b.
A ~ aaA can be replaced by A ~ aDA. D ~ a.
B ~ hAb can be replaced by B ~ hAC. C ~ b.
So the revised productions are:
S ~ ABC I a,
A ~ aDA,
B ~ hAC.
C ~ b.
D ~ a.
Name S, A, B, C, D as A b A:::, A 3, A4• As.
Now we proceed to step 2.
Step 2
G1 = ({AI, A:::, A3, A4, As}, {a, b}, PI, AI) where PI consists of
Al ~ A:::Afi41 a,
A::: ~ aAsA:::,
A 3 ~ bA:::A4,
A4 ~ b.
As ~ a
The only production to be modified using step 4 (refer to Theorem 6.9)
is AI ~ A:::Afi4'
Replace Al ~ A:::Afi4 by Al ~ aAsA:::Afi4'
The required grammar in Gr-..Tf is
G2 = ({A b A 2. ,13,
A~, As}, {a, b}, Pb AI) where P2 consists of
AI ~ aAsAfi41 a
A 2 ~ aAsA2
A 3 ~ bA2A4,
A4 ~ b,
As ~ a
222
g
Theory ofComputer Science
EXAMPLE 6.25
If a context-free grammar is defined by the productions
5 ~ a I5a Ib55 I55b I5b5
show that every string in L(G) has more a's than b's.
Proof
We prove the result by induction on I H' I, where W E
L(G).
\\tilen Iw I = 1, then w = a. So there is basis for induction.
Assume that 5 ::b w, Iw I < n implies that w has more a's than b's. Let
Iwl = n > 1. Then the first step in the derivation 5 ::b wis 5 =} b55 or
5 =} 55b or 5 =} 5b5. In the first case, 5 =} b55 ~ bWIW2 = w for some
W), W2 E L:* and 5 ~
Wb 5 ~ W2' By induction hypothesis each of W) and
H'2 has more a's than b's. So WjW2 has at least two more a's than b's. Hence
b,VIW2 has more a's than b's. The other two cases are similar. By the principle
of induction, the result is true for all
W
E L(G).
EXAMPLE 6.26
Show that a CFG G with productions 5 ~ 55 I(5)
'I A is ambiguous.
Solution
5 =} 55 =} 5(5) =} A(5) =} A(A) = (A)
Also.
5 =} 55 =} (5)5 =} (A)5 =} (A)A = (A)
Hence G is ambiguous.
EXAMPLE 6.27
Is it possible for a regular grammar to be ambiguous?
Solution
Let G = (Vv, L, P, S) be regular. Then every production is of the form
A ~ aB or A ~ b. Let W E L(G). Let 5 ~
W be a leftmost derivation. We
prove that any leftmost derivation A ~ w. for every A
E Vv is unique by
induction on Iw I. If !W I= 1, then w = a E T. The only production is A ~ a.
Hence there is basis for induction. Assume that any leftmost derivation of the
form A ~ w is unique when IW I = 11 -
1. Let IW I = 11 and A ~
11J be a
leftmost derivation.
Take W =aWj, a E T. Then the first step of A ~ w has to be A =} aB
for some B
E
Vv. Hence the leftmost derivation A ~ W can be split into
't =} aB ~ alt'j. So. we get a leftmost derivation B ~ Wj' By induction
hypothesis, B ~
'VI is unique. So. we get a unique leftmost derivation of w.
Hence a regular grammar cannot be ambiguous.
Chapter 6: Context-Free Languages
g
223
SELF-TEST
1. Consider the grammer G which has the productions
A ~ a IAa I bAA IAAb IAbA
and answer the following questions:
(a) Vvl1at is the start symbol of G?
(b) Is aaabb in L(G)?
(c) Is aaaabb in L(G)?
(d) Show that abb is not in L(G).
(e) Write the labels of the nodes of the following derivation tree T
which are not labelled. It is given that T is the derivation tree
whose yield is in {a, b}*.
A
2
b
A
8
10
13
,.,
"
7
6
b
9
12
Vb
14
Fig. 6.14
Derivation tree for Question 1(e).
2. Consider the grammar G which has the following productions
S ~ aBlbA. A ~ aSlbA..A.la. B ~ bSlaBBlb.
and state whether the following statements are true or false.
(a) L(G) is finite.
(b) abbbaa
E L(G)
(c) aab E L(G)
(d) L(G) has some strings of odd length.
(e) L(G) has some strings of even length.
224
~
Theory ofComputer Science
3. State whether the following statements are true or false.
(a) A regular language is context-free.
(b) There exist context-free languages that are not regular.
(c) The class of context-free languages is closed under union.
(d) The class of context-free languages is closed under intersection.
(e) The
class
of
context-free
languages
is
closed
under
complementation.
(f) Every finite subset of {a, b}* is a context-free language.
(g)
{a
l b"c
17 ln
~ I} is a context-free language.
(h) Any derivation tree for a regular grammar is a binary tree.
EXERCISES
6.1 Find a derivation tree of a , b + a * b given that a * b + a * b is in
L(G), where G is given by 5 ~ 5 + SiS " S, S ~ alb.
6.2 A context-free grammar G has the following productions:
S ~ OSOllSlIA,
A ~ 2B3.
B ~ 2B313
Describe the language generated by the parameters.
6.3 A derivation tree of a sentential form of a grammar G is gIven m
Fig. 6.15.
X1
X3
X3
Fig. 6.15
Derivation tree for Exercise 6.3.
(a) What symbols are necessarily in V:v?
(b) What symbols are likely to be in 2:?
(c) Determine if the following strings are sentential forms: (i) X4X20
(ii) X2X2X3X2X3X3, and (iii) X2X4X4X2.
6.4 Find (i) a leftmost derivation, (ii) a rightmost derivation, and (iii) a
derivation
which is neither leftmost nor rightmost of abababa,
given that abababa is in L(G), where G is the grammar given in
Example 6.4.
Chapter 6: Context-Free Languages
l;l
225
6.5 Consider the following productions:
S ---j aB IbA
A
---j as IbAA Ia
B
---j bS IaBB Ib
For the string aaabbabbba, find
(a) the leftmost derivation,
(b) the rightmost derivation, and
(c) the parse tree.
6.6 Show that the grammar S ---j a IabSb IaAb, A ---j bS IaAAb is ambiguous.
6.7 Show that the grammar S
---j aB Iab, A
---j aAB Ia. B
---j ABb Ib is
ambiguous.
6.8 Show that if we apply Theorem 6.4 first and then Theorem 6.3 to a
grammar G, we may not get a reduced grammar.
6.9 Find a reduced grammar equivalent to the grammar S
---j aAa, A
---j
bBB, B
---j ab, C
---j aBo
6.10 Given the grammar S ---j AB, A
---j a, B
---j C I b, C ---j D, D
---j E,
E ---j a, find an equivalent grammar which is reduced and has no unit
productions.
6.11 Show that for getting an equivalent grammar in the most simplified
form,
we have
to eliminate unit productions first and then the
redundant symbols.
6.12 Reduce the following grammars to Chomsky normal form:
(a) S ---j lA lOB,
A
---j lAA I05 I0,
B
---j OBB lIS 11
(b) G = ({S}, {a, b, c}, {5 ---j a Ib Ic5S}, S)
(c) 5
---j abSb I a I aAb,
A
---j bS IaAAb.
6.13 Reduce the grammars given in Exercises 6.1, 6.2, 6.6, 6.7, 6.9, 6.10
to Chomsky normal form.
6.14 Reduce the following grammars to Greibach normal form:
(a) 5
---j 55,
5
---j 051 I01
(b) S ---j AB,
A
---j BSB,
A
---j BE,
B ---j aAb,
B ---j a,
A
---j b
(c) S
---j AO,
A
---j OB.
B
---j AO,
B
---j 1
6.15 Reduce the grammars given in Exercises 6.1, 6.2, 6.6, 6.7, 6.9, 6.10
to Greibach normal form.
6.16 Construct the grammars in Chomsky normal form generating the
following:
(a) {wcwJ I,v E 0 {a, b}*},
(b) the set of all strings over {a, b} consisting of equal number of a's
and b's,
226
);!
Theory ofComputer Science
(c) {alllb" Im :;t 11, m, n ~ I}, and
(d) {a
I Y"c" I m,
11 ~ I}.
6.17 Construct grammars in Greibach normal form generating the sets given
in Exercise 6.16.
6.18 If W
E L(G) and Iw! = k, where G is in (i) Chomsky normal form,
(ii) Greibach normal form, what can you say about the number of steps
in the derivation of w7
6.19 Show that the language {d
,2 I11 ~ I} is not context-free.
6.20 Show that the following are not context-free languages:
(a) The set of all strings over {a, b, c} in which the number of
occurrences of a, b, c is the same.
(b) {alllblllc" 1m::; 11 ::; 2m}.
(c)
{alllb" In = ml}.
6.21 A context-free grammar G is called a right-linear grammar if each
production is of the form A -7 wB or A -7 w, where A, B are variables
and w E L:*. (G is said to be left-linear if the productions are of the
form A -7 Bw or A -7 w. G is linear if the productions are of the form
A -7 vB-w or A -7 .v.) Prove the following:
(a) A right-linear or left-linear grammar is equivalent to a regular
grammar.
(b) A linear grammar is not necessarily equivalent to a regular
grammar.
6.22 A context-free grammar G is said to be self-embedding if there exists
some useful variable A such that A :b uAv, where u, v
E L:*,
u,
v :;t A, Show that a context-free language is regular iff it is generated
by a nonselfembedding grammar.
6.23 Show that every context-free language without A is generated by a
context-free grammar in which all productions are of the form A -7 a,
A -7 aab.
Pushdown Automata
In this chapter we introduce pushdown automaton (pda). We discuss two types
of acceptance of sets by pushdown automata. Finally, we prove that the sets
accepted by pushdown automata are precisely the class of context-free
languages.
7.1
BASIC DEFINITIONS
We have seen that the regular languages are precisely those accepted by finite
automata. If M is a finite automaton accepting L, it is constructed in such a way
that states act as a form of primitive memory. The states 'remember' the
variables encountered in the course of derivation of a string. (In M, the states
correspond to variables.) Let us consider L = {a"b"ln ~ I}. This is a context-
free language but not regular. (S -----i aSh Iab generates L. Using the pumping
lemma we can show that L is not regular; cf. Example 5.20.)
A finite automaton cannot accept L, i.e. strings of the form a"b", as it has
to remember the number of a's in a string and so it will require an infinite
number of states. This difficulty can be avoided by adding an auxiliary
memory in the form of a 'stack' (In a stack we add the elements in a linear
way. While removing the elements we follow the last-in-first-out (LIFO)
basis. i.e. the most recently added element is removed first.) The a's in the
given string are added to the stack. When the symbol b is encountered in the
input string, an a is removed from the stack. Thus the matching of number
of c's and the number of b's is accomplished. This type of arrangement where
a finite automaton has a stack leads to the generation of a pushdown
automaton.
Before giving the rigorous definition, let us consider the components of a
pushdown automaton and the way it operates. It has a read-only input tape,
227
228
~
Theory ofComputer Science
an input alphabet a finite state control, a set of final states, and an initial state
as in the case of an FA. In addition to these, it has a stack called the pushdown
store (abbreviated PDS). It is a read-write pushdown store as we add elements
to PDS or remove elements from PDS. A finite automaton is in some state
and on reading, an input symbol moves to a new state. The pushdown
automaton is also in some state and on reading an input symbol and the
topmost symbol in ppS, it moves to a new state and writes (adds) a string of
symbols in PDS. Figure 7.1 illustrates the pushdown automaton.
We now give a formal definition of a pushdown automaton.
I a I
, .
\ Removing
1direction
1
Storing
direction
LJ
I
Z
I
Finite store
I
I
control
II
Pushdown store
Fig. 7.1
Model of a pushdown automaton.
DefInition 7.1
A pushdown automaton consists of
(i) a finite nonempty set of states denoted by Q,
(ii) a finite nonempty set of input symbols denoted by I:,
(iii) a finite nonempty set of pushdown symbols denoted by r,
(iv) a special state called the initial state denoted by qa,
(v) a special pushdown symbol called the initial symbol on the pushdown
store denoted by Zo.
(vi) a set of final states, a subset of Q denoted by F, and
(vii) a transition function /5 from Q x (I u {A}) x r to the set of finite
subsets of Q x 1*.
Symbolically. a pda is a 7-mple, namely (Q, I, r, 8, qo, Zo, F).
Note:
When /5(q. a. Z) = 0 for (q, a, Z) E Q x (I u {AD x 1, we do not
mention it
EXAMPLE 7.1
Let
where
I = {a,b}.
1 = {a, Zo}.
Chapter 7: Pushdown Automata
~
229
and 8 is given by
8(qo, a, Zo) = {(qo, aZD)}, 8(ql, b, a) = {(qj, A)}
8(qo, a, a) = {(qo, aa)}, D(q1' A, Zo) = {(q1, A)}
S(qo, b, a) = {(% A)}
Remarks 1.
Seq, a, Z) is a finite subset of Q x i*. The elements of
8(q, a. Z) are of the form (q', a), where q' E Q. a
E i*. 8(q, a, Z) may
be the empty set.
2.
At any time the pda is in some state q and the PDS has some symbols
from r. The pda reads an input symbol a and the topmost symbol Z in PDS.
Using the transition function 8, the pda makes a transition to a state q' and
writes a string a after removing Z. The elements in PDS which were below
Z initially are not disturbed. Here (q', a) is one of the elements of the finite
set 8(q, a, Z). When a = A. the topmost symbol, Z. is erased.
3.
The behaviour of a pda is nondeterministic as the transition is given
by any element of 8(q. a, Z).
4.
As 8 is defined on Q x (I U
{A}) x 1, the pda may make transition
without reading any input symbol (when 8(q, A, Z) is defined as a nonempty
set for q E Q and Z E l). Such transitions are called A-moves.
S.
The pda cannot take a transition when PDS is empty (We can apply
8 only when the pda reads an input symbol and the topmost pushdown symbol
in PDS). In this case the pda halts.
6.
When we write a =ZlZ: ... Zm in PDS, ZI is the topmost element,
Z2 is below ZI' etc. and Zm is belm\' Zm-l'
In the case of finite automaton, it is enough to specify the current state at
any time and the remaining input string to be processed. But as we have the
additional structure, namely the PDS in pda, we have to specify the current
state, the remaining input string to be processed, and the symbols in the PDS.
This leads us to the next definition.
Dermition 7.2
Let A = (Q, I, i, 8, qo. Zo, F) be a pda. An instantaneous
description (ill) is (q, x, a), where q E Q, x E I* and a E i"'.
For example, (q, ala: ... all' ZlZ2 ... Z/I,) is an ill. This describes the
pda when the current state is q, the input string to be processed is a1a2
an'
The pda will process ala: ... an in that order. The PDS has ZI- Z2'
, Zm
with Z1 at the top. Z: is the second element from the top. etc. and Z,n is the
lowest element in PDS.
Dermi~;,on 7.3
An initial ill is (qo, x. Zo). This means that initially the pda
is in the initial state qo. the input string to be processed is x. and the PDS has
only one symbol. namely Zoo
Note:
In an ill (q. x, a). x may be A. In this case the pda makes a A-move.
230
!ii
Theory of Computer Science
For a finite automaton. the working can be desclibed in terms of change
of states. In the case of pda, because of its additional structure, namely PDS,
the working can be described in terms of change of IDs. So we have the
following definition:
Definition 7.4
Let A be a pda. A move relation, denoted by r-' between IDs
is defined as
(q. a1 a2 ... (In' Z1Z2 ... Zm) r- (q', {l2a3 ... (1m ,8Z2 ... Zm)
if o(q,
(II' ZI) contains (q', {3).
Note:
The move relation
(q, a1a2 ... ({'" Z1Z2 ... Z",) r- (q', {l2{13 ... all' {3Z2
Zm)
can be described as follows: The pda in state q with Z1Z2
Zm in PDS
(ZI is at the top) reads the input symbol al' When (q', j3) E o(q, {lj, Zd. the
pda moves to a state q' and writes {3 on the top of Z2 . "
Zm. After this
transition. the input string to be processed is {l2a3 ... all"
If f3 = Y1 Y2
Y~. then Fig. 7.2 illustrates the move relation.
L--_la1Ia2j
lanl
b;j
~
I
.
I
I
2m
I
Fig. 7.2
An illustration of the move relation.
Remark
As r- defines a relation in the set of all IDs of a pda, we can define
the reflexive-transitive closure r- which represents a definite sequence of n
moves, where II is any non-negative integer.
If (q, x. a) r-
(q', y. j3) represents n moves. we write (q, x. ex) P-
(q'. }'. (3). In particular, (q, x. a) ~
(q, x, a). Also, (C], x. a) r-
(q', y, j3)
can be split as
(q, x, a) r- (Q1' Xl.
0'1) r- (Q2. x.:>
cee) r- ... r- (q', y. P)
Note:
When we deal \vith more than one pda, we also specify the pda while
desclibing the move relation. For example, a move relation in A is denoted
bv
I
•
-~
Chapter 7: Pushdown Automata
J;!
231
The next two results are properties of the relation ~ and are frequently
used in constructions and proofs.
Result 1
If
(7.1)
then for every y
E r*.
(qj, xy. a) ~
(q~, y, {3)
(7.2)
Conversely, if (Cjj. A}'. a) ~
(q~. Y. 13) for some :v E r*, then (qj. x. a) ~
(q~. A, /3).
Proof
The result can be easily understood once we refer to Fig. 7.2.
providing an illustration of the move relation.
If the pda is in state ql with a in PDS. and the moves given by (7.1) are
effected by processing the string x. the pda moves to state
q~ with 13 in PDS.
The same transition is effected by starting with the input string xy and
processing only x. In this case, y remains to be processed and hence we
get (7.2).
We can prove the converse part i.e. (7.2) implies (7.1) in a similar way.
Result 2
If
then for every 13 E f*.
(q. x. a)
~
(q'. A, y)
(7.3)
(7.4)
(q. x. a/3)
~
(q', A, yf3)
Proof
The sequence of moves given by (7.3) can be split as
(q, x. a) f- (qt- Xl' aj) f- (q2. X2· a2) f- ... f- (q'. A, y)
Consider (qi' Xi. ail f- (qi+l' xi+1' ai+l)' Let a i = ZIZ~ ... ZIII·As a result
of this move. Zj is erased and some string is placed above
Z~ . , . ZiJl' So.
Z2 .,.
ZIII is not affected. If we have 13 below Z2 ...
ZI'" then also
Z2 ... Zmf3 is not affected. So we obtain (qj, Xj, a i/3) f- (qi+l' Xj+1' aj+i/3)·
Therefore. we get a sequence of moves
.
(q. x. af3) f- (qj. XI. ajf3) f- ... f- (C]', A. yf3)
I.e.
(q, x. af3)
~
(q'. A. yf3)
Note: In general. (7.4) need not imply (7.3). Consider. for instance,
A = ({qo}, {a, b}, {Zo}, 8, qo· Zo, 0)
where
8(qo· a. Zo) = {(qo· A)}. 8(qo. b. Zo) = {(qo. ZoZo)}
(qQ. aab. ZoZoZoZo)
f- (qo, abo ZoZeJZo)
f- (qo. b. ZoZo)
f- (qo· A. ZoZoZo)
232
g
Theory ofComputer Science
l.e.
(qo· aab. ZoZoZo4J) p:-
(qo. A. ~4J)
However. (qo. aab. Zo) r- (qo. abo A); hence the pda cannot make any more
transitions as the PDS is empty. This shows that (7.4) does not imply (7.3)
if we assume a = 4JZo4J. f3 = Zo° Y = ZoZo·
EXAMPLE 7.2
A = ({qo. ql' qt}, {a. b. e}, {a. b. zo}, 8. qo. Zoo {qt})
is a pda. where 8 is defined as
8(q!. a. a) = 8(QI' b. b) = {(qIo A)}
8(QI' A. Zo) = {(Qt' Zo)}
8(CJo· a. Zo) = {(qo. aZo)}.
8(qo•. a. a) = {(qo. aa)},
8(qo. a. b) = {(qo· ab)},
8(qo. e. a) = {(qi. a)},
8(qo. b. Zo) = {(qo· bZrJ)}
8(qo. b. a) = {(qo. bay}
8(qo. b. b) = {(qo. bb)}
8(qo. e. b) = {(qIo b)}, 8(qo. e. Zo)
= {(ql. Zo)}
(7.5)
(7.6)
(7.7)
(7.8)
(7.9)
(7.10)
We can explain 8 as follows:
If A is in initial ill. then using Rule (7.5). A pushes the first symbol of
the input string on PDS if it is a or b. By Rules (7.6) and (7.7). the symbols
of the input string are pushed on PDS until it sees the centre-marker e. By
Rule (7.8). on seeing c. the pda moves to state ql without making any changes
in PDS. By Rule (7.9). the pda erases the topmost symbol if it coincides with
the current input symbol (i.e. if they do not match. the pda halts). By Rule
(7.10). the pda
reaches the final state qt only when the input string is
exhausted. and then the PDS has only 4J.
We can explain the concepts of ill. moves. etc. for this pda A. Suppose
the input string is aeab. We will see how the pda processes this string. An
initial configuration is (qo. baeab. 4J). We get the following moves:
(qo· baeab. Zo) r- (qo. aeab. bZo)
by Rule (7.5)
r- (qo. cab. ab4J)
by Rule (7.7)
r- (q!. abo abZo)
by Rule (7.8)
r- (qo b. bZ()
by Rule (7.9)
r- (q!. A. Zo)
by Rule (7.10)
r- (qr. A.
Z{)
by Rule (7.10)
l.e.
Chapter 7: Pushdown Automata
J;;;!
233
Proceeding in a similar way, we can show that
(qo, wewT. 4J) p:-
(qt. A, Zo)
for all
>t' E
{a, b} *
Suppose an initial configuration is (qo, abehb, Zo). Then, we have
(qo. abebb. Zo) r- (qo. bebb, aZo)
by Rule (7.5)
r- (qo, ebb, baZo)
by Rule (7.6)
r- (q]. bb. baZo)
by Rule (7.8)
r- (q], b. aZo)
by Rule (7,9)
Once the pda is in ill (q!> b, aZo), it has to haIt as /S(q], b. a) = 0. Hence,
we have
(qo, abebb. Zo) p:-
(q], b, aZo)
As /S(LJa. e. 4J) = 0, the pda cannot make any transition if it starts with an
ill of the form (qo. ew. 4J).
Note:
In Example 7.2. each /S(q, a. Zl is either empty or consists of a single
element. So for making transitions. the pda has only one choice and the
behaviour is deterministic.
In general. a deterministic pda can be defined as follows:
Definition 7.5
A pda A = (Q. L r.
/5. qo. Zoo F) is deterministic if
(i) /S(q. a, Z) is either empty or a singleton. and (ii) /S(q. A. Z) 1= 0 implies
/S(q, a, Z) = 0 for each a
E L.
Consider the pda given in Example 7.2.
/S(q. a. Z) given by Rules
(7.5)-(7.10) are singletons, Also. /S(q], a. Zo) = 0 and /S(q]. a, Zo) = 0 for
all a E L, So the pda given in Example 7.2 is deterministic.
7.2
ACCEPTANCE BY pda
A pda has final states like a nondeterministic finite automaton and has also the
additional structure. namely PDS, So we can define acceptance of input strings
by pda in terms of final states or in terms of PDS.
DefInition 7.6
Let A = CQ, L, r.
/S. qo, 20, F) be a pda. The set accepted
by pda by final state is defined by
T(A) = {>t' E L*I(qo. w. Zo) ~
(ql' A. cr.) for some qf E F and cr. E i*}
EXAMPLE 7.3
Construct a pda A accepting L = {wewT i>t' E
{a. b} *} by final state.
by Rules (7.5)-(7.7)
by Rule (7.8)
by Rule (7.9)
by Rule (7.10)
234
~
Theory ofComputer Science
Solution
Consider the pda given in Example 7.2. Let yVC1VT E L. Write w =ala2 ... a",
where each ai is either a or b. Then, we have
(qo· ata2 ... a"cwT
Zo)
~ (qo. cw T• allall_l
~ (qt· a,,czn-l ... aI- a,,czl/-I ... (ltZo)
~ (ql' A. Zo)
r- (qf' A. Zo)
Therefore, wcwT E
T(A), i.e. L ~ T(A).
To prove the reverse inclusion, it is enough to show that L' ~ T(Ar. Let
x E V'.
Case 1
x does not have the symbol c. In this case the pda never makes a
transition to ql' So the pda cannot make a transition to qf as we cannot apply
Rule (7.10). Thus. x
E
T(A)'.
Case 2
(qo,
WICl1/2' 20)
~
(qo,
CW2· wtZo)
r- (qt,
W2· wrZo)
As W2 1:- wr the pda cannot reach an ill of the form (qj, A, Zo). So we
cannot apply (7.10). Therefore. x E TCA)'.
Thus we have proved V' ~ T(Ar.
The next definition describes the second type of acceptance.
Definition 7.7
Let A = (Q, L, r. 8. qo. Zo, F) be a pda. The set N(A)
accepted by null store (or empty store) is defined by
N(A) = {w
E L*1(qo,
lV, 20) ~
(q, A, A) for some q E Q}
In other words,
y1/ is in N(A) if A is in initial ill (qo, w, 20) and empties
the PDS after processing all the symbols of
Y\'.
SO in defining N(A), we
consider the change brought about on PDS by application of w, and not the
transition of states.
EXAMPLE 7.4
Consider the pda A given by Example 7.2 with an additional rule:
8(qf' A. Zo) =
{(qt; A)}
Then.
N(A) = {wcwTlw E
{a. b}*}
(7.11)
Chapter 7: Pushdown Automata
l;l
235
Solution
From the construction of A, we see that the Rules (7.5)-(7.10) cannot erase
Zoo We make a provision for erasing Zo from PDS by Rule (7.11). By
Example 7.2, wcwT E T(A) if and only if the pda reaches the ill (qj' A, Zo)'
By (7.11), PDS can be emptied by A-moves if and only if the pda reaches the
ill (qr, A, Zo). Hence.
N(A) = {lvcwT I w
E
{a, b} *}
In the next theorem we prove that the set accepted by a pda A by null store
is accepted by some pda B by final state.
Theorem 7.1
If A = (Q, L, r, 0, qo, Zo, F) is a pda accepting L by empty
store. we can find a p
r1:t
B = (Q', L, r', os, q'o, Zo, F')
which accepts L by final state, i.e. L = N(A) = T(B).
Proof
B is constructed in such a way that (i) by the initial move of B, it
reaches an initial ill of A. (ii) by the final move of B, it reaches its final state,
and (iii) all intermediate moves of B are as in A.
Let us define B as follows:
where
q'o is a new state (not in Q).
F' = {qj}' with qf as a new state (not in Q),
Q' = Q u
{q'o, qj},
Z'o is a new start symbol for PDS of B.
r' = r u
{Zo}, and
0B is given by the rules R b R2, R3
with
R(
0s(q'o. A, Z'o) = {(qo, ZoZ'o)}.
R2: OB(q, a, Z) = O(q,
Cl, Z)
for all (q, a, Z) in Q x (L u {A}) x r
R3: 0B(q, A, Z'o) = {(qf' A)}
for all q E
Q.
By Rj, the pda B moves from an initial ill of B to an initial ill of A.
R j gives a A-move. As a result of Rj, B moves to the initial state of A with
the start symbol Zo on the top of PDS.
R2 is used to simulate A. Once B reaches an initial ill of A. R2 can be used
to simulate moves of A. We can repeatedly apply R2 until Z'o is pushed to the
top ('f PDS. As Z'o is a new pushdown symbol, we have to use R3.
R3 gives a A-move. Using R3• B moves to the new (final) state qf erasing
Z'o in PDS.
Thus the behaviour of B and A are similar except for the A-moves given
by R] and R3. Also, W E
T(B) if and only if Breaches qf, i.e. if and only
236
9
Theory ofComputer Science
if the PDS has no symbols from r (since B can reach qf only by the
application of RJ. This suggests that T(B) = N(A).
Now we prove rigorously that N(A) =T(B). Suppose 11' E N(A). Then by
the definition of N(A), (qo. w, Zo) Hf (q. A. A) for some q E Q. Using Rb
we see that
By Result 2,
(7.12)
By Result 1, we have
(7.13)
(q. A, Zo) hf (qt' A, A)
Combining (7.12)-(7.14). we have
(q'o, w, Zo) Hf (qt' A, A)
This proves that W
E
T(B), i.e. N(A)
~ T(B).
To prove T(B)
~ N(A), we start with W
~ T(B). Then
(q'o· w. Z'O) HJ (qt' A, a)
(7.14)
(7.15)
But B can reach qr only by the application of R3• To apply R3• Z/O should be
the topmost element on PDS. Z'O is placed initially, and so when it is on the
top there are no other elements in PDS. So a = A, and (7.15) actually reduces
to
(7.16)
In (7.16), the initial and final steps are effected only by A-moves. The
intermediate steps are induced by the conesponding moves of A. So (7.16) can
be
split as
(q'o,
Aw.
Z/O) hf (qo,
YV,
ZOZ/O) Hf (q.
A,
Z'O)
for
some
q E
Q. Thus, (q'o. Aw, Z(J) hf (qo, w. ZoZ(J) Hf (q, A, Z'o) hf (qt' A, A).
As we get (qo, w. ZoZ/O) Hf (q, A. Z'o) by applying R-; several times and R2
does not affect Z'o at the bottom, we have (qo, w, Zo) Hf (q, A, A). By the
construction of R2• we have (qo, w. Zo) ~ (q. A, A). which means vI' E N(A).
Thus. T(B)
~ N(A). and hence T(B) = N(A) = L.
I
Chapter 7: Pushdown Automata
J;L
237
Note:
From the construction of B. it is easy to see that B is deterministic if
and only if A is deterministic.
EXAMPLE 7.5
Consider the pda A given in Example 7.1 (Take F = 0). Determine N(A).
Also construct a pda B such that T(B) = N(A).
Solution
where 8 is given by
R j : 8(qo, a. Zo) = {(qo, aZo)}
R< 8(qo. a, a)
=
{(qo, aa)}
R3: 8(qo, b. a)
= {(qj, A)}
R+: 8(qj. b, a)
= {(qj. A)}
Rs: 8(qj, 1\. Zo) = {(qj. A)}
R j is used to store a in PDS if it is the first symbol of an input string.
R: can be used repeatedly to store a" in PDS. When b is encountered for the
first time in the input string. a is erased (in PDS) using R3. Also, the pda
makes a transition to state qj. After processing the entire input string, if Zo
remains in PDS, it can be erased using the null move given by Rs. So, if
],V = d'b", then we have
(qo. d'Y'. Zo) pc-
(qo· b", a"Zo)
by applying R j and R:
pc-
(q j. A. Zo)
by applying R3 and R4
r-
(qt, A, A)
by applying Rs
Therefore. a"b"
E
N(A).
If H' E N(A). then (qo. w, Zo) pc- (ql, 1\, A). (Note that the PDS can be
empty only when A is in state qj') Also,
],V should start with a. Otherwise, we
cannot make any move. We store the symbol a in PDS if the current input
symbol is a and the topmost symbol in PDS is a or Zoo On seeing the input
symbol b, the pda erases the symbol a in PDS. The pda enters the ill (qj, A, A)
only by the application of Rs. The pda can reach the ill (qj, A, Zo) only
by erasing the a's in pda. This is possible only when the number of b's is
equal to number of a's, and so
1\" = a"Yl. Thus. we have proved that
N(A) = {a"b"ln;:: l}.
~ow let
B = (Q'. {a, b}. r', 8B• q'o. Z'o. F')
where
r' = {a. b. Z'o}
for all a E 2:, q E Q, Z E r
for all Z E r u {Zo} and q E F
for all Z E f
u
{Z'o}
238
);!
Theory ofComputer Science
and DB is defined by
DB (qo, A, Z'o) = {(qo, ZoZo)}
DB(ql, a, Zo)= {(qo, aZo)}
DB (qo, a, a) = {(qo, aa)}
DB (qo, b, a) = {(qj, A)}
DB(qj, b, a) = {(qj, A)}
DB (qj, A, Zo) = {(qj, A)}
DB (qo, A, Z'o) = {(qt; A)}
DB(qj, A, Z'o) = {(ql' A)}
Thus.
T(B) = N(A) = {a"b"Jn 2': I}
The following theorem asserts that the set accepted by a pda A by final
state is accepted by some pda B by null store.
Theorem 7.2
If A =(Q, 2:, f, D, qo, Zo, F) accepts L by final state, we can
find a pda B accepting L by empty store: i.e. L =T(A) =N(B).
Proof
B is constructed from A in such a way that (i) by the initial move of
B an initial ill of A is reached, (ii) once B reaches an initial ill of A, it
behaves like A until a final state of A is reached, and (iii) when B reaches a
final state of A, it guesses whether the input string is exhausted. Then B
simulates A or it erases all the symbols in PDS.
The actual construction of B is as follows:
B = (Q u
{q'o, d}. 2:, f
u {Z'o}, DB, q'o, Z'o, 0)
where c/o is a new state (not in Q), d is a new (dead) state, and Zo is the new
start symbol for PDS of B.
DB is defined by rules R1, R2, R3 and R4 as
Rj: DB (q'o, A. Z'o) = {(qo, ZoZo)}
R2: DB(q, a, Z) =8(q, a, Z)
R3:
DB(q, A. Z) = 8(q, A, Z) u {(d, A)}
R4: DB(q. A, Z) ={(d, A)}
Using R j , B enters an initial ill of A and the start symbol Zo is placed on top
of PDS.
Using R2, B can simulate A until it reaches a final state of A. On reaching
a final state of A, B makes a guess whether the input string is exhausted or
----~--~=======-~
Chapter 7: Pushdown Automata
~
239
not. When the input string is not exhausted, B once again simulates A.
Otherwise, B enters the dead state d. Rule R:!, gives a A-move. Using these
A-moves, B erases all the symbols on PDS.
Now W E T(A) if and only if A reaches a final state. On reaching a final
state of A, the pda can reach the state d and erase all the symbols in the PDS
by A-moves. So, it is intuitively clear that 11' E T(A) if and only if W E N(B).
We now prove rigorously that T(A) =N(B).
Suppose W E
T(A). Then for some q E F, ex E r*,
Using R2, we get
(qo, W, 20) ~ (q, A, ex)
Applying Result 2, we obtain
(7.17)
As
using Result L we get
(q'o, ,t', 2'0) GJ (qo,
W, 202'0)
From (7.18) and (7.17), we can deduce
By applying R3 once and R:!, repeatedly. we get
(q, A, ex20) ~ (d, A, A)
(7.18)
(7.19)
(7.20)
Relations (7.19) and (7.20) imply that (qQ, w, 20) ~ (d, A, A). Thus we
have proved T(A)
~ N(B).
To prove that N(B) ~ T(A), we start with Hi E N(B). This means that for
some state q of B,
(7.21)
As the initial move of B can be made only by using R l' the first move of
(7.21) is (q'o, Aw, 2'0) GJ (qo, w, 202'0)'
2/0 in the PDS can be erased only when B enters d; B can enter d only
whefl it reaches a final state q of A in an earlier step. So (7.21) can be split as
240
~
Theory ofComputer Science
for some q E
F and ex E P'. But (qo. w,
ZoZ~)) ~ (q, A, exZ'o) can be
obtained only by the application of R2• So the moves involved are those
induced by the moves of A. As Z'o is not a pushdown symbol in A, Z'o lying
at the bottom is not affected by these moves. Hence
(c/o, Aw,
Z~)) rt (q. A. ex),
q
E
F
So w
E
T(A) and N(B)
~ T(A). Thus,
L = N(B) = T(A)
EXAMPLE 7.6
Construct a pda A accepting the set of all strings over {a, b} with equal
number of a's and b·s.
Solution
Let
A = ({q), [a. b]. [Zo, a, b], D, q, Zo, 0)
where Dis defined by the following rules:
D(q, a, Zo) = {(q, aZo)} D(q. b, Zo) = {(q, bZo)}
D(q, a. a) = {(q. aa)} D(q, b. b) = {(q, bb)}
D(q. a. b) = {(q. A)} D(q. b, a) = {(q. A)}
D(q. A, Zo) = {(q, A)}
The construction of Dis similar to that of the pda given in Example 7.2.
But here we want to match the number of occurrences of a and b: so, the
construction is simpler. We start by storing a symbol of the input string and
continue storing until the other symbol occurs. If the topmost symbol in PDS
is a and the current input symbol is b. a in PDS is erased. If lV has equal
number of a's and b's, then (q. w. Zo) ~ (q, A, Zo) 1- (q, A, A). So
WE N(A). We can show that N(A) is the given set of strings over {a. b} using
the construction of 0.
7.3
PUSHDOWN AUTOMATA AND CONTEXT-FREE
LANGUAGES
In this section we prove that the sets accepted by pda (by null store or final
state) are precisely the context-free languages.
Theorem 7.3
If L is a context-free language, then we can construct a pda
A accepting L by empty store, i.e. L = N(A).
Chapter 7: Pushdown Automata
j;!
241
Proof
We construct A by making use of productions in G.
Step 1
(Construction of A)
Let L = L(G), where G = (V';v, L, P, 5) is a
context-free grammar. We construct a pda A as
A = ceq), L, VV U L, 8, q, 5, 0)
where 8 is defined by the following rules:
R l : 8(q, A. A) = {(q. a) I A ~ a is in P}
R2: 8(q. G, G) = {(q, A)}
for every G in L
We can explain the construction in the following way: The pushdown
symbols in A are variables and telminals. If the pda reads a variable A on the
top of PDS, it makes a A-move by placing the R.H.S. of any A-production (after
erasing A). If the pda reads a terminal a on PDS and if it matches with the
CUlTent input symboL then the pda erases G. In other cases the pda halts.
If W E L(G) is obtained by a leftmost derivation
5
=:} lljAjaj
=:} Ujll2A2a2aj
=:} .,.
=:} w,
then A can empty the PDS on application of input string
}t'. The first move
of A is by a A-move corresponding to 5 ~ ulAjai' The pda erases 5 and
stores II lAj a j • Then using R2 • the pda erases the symbols in Ilj by processing
a prefix of w. Now. the topmost symbol in PDS is Al . Once again by applying
the A-move corresponding to A 1 ~
112A2a2' the pda erases A 2 and stores
1I2A2a2 above
al' Proceeding in this way, the pda empties the PDS by
processing the entire string w.
Before proving that L(G) = N(A) (step 2). we apply the construction to
an example.
EXAMPLE 7.7
Construct a pda A equivalent to the following context-free grammar: 5 ~
OBB. B ~ 05115 i O. Test whether 010'+ is in N(A).
Solution
Define pda A as follows:
A = ({q}, {a. I}. {5, B, O. I}. 8. q. 5, 0)
8 is defined by the following rules:
8(q, A. 5) = {(q. OBB)}
8(q. 1'\, B) = {(q. OS). (q. @J. (q. a)}
~
IS
b(q& O. 0) =
{(q~ i\.)}
8(q. 1. 1) = {(q. A)}
242
g
Theory ofComputer Science
(q, 0104, S)
r- (q, 0104, OBB)
r- (q, 104, BB)
r- (q, 104, ISB)
r- (q, 04, SB)
r- (q, 04, OBBB)
r- (q, 03, BBB)
f-2-
(q, 03, 000)
f-2-
(q, A, A)
Thus.
by Rule R[
by Rule R3
by Rule R2 since (q, IS) E a(q, A, B)
by Rule R4
by Rule R[
by Rule R3
by Rule R: since (q, 0) E
a(q, A, B)
by Rule R3
0104
(;;; N(A)
Note:
After entering (q. 104. BB), the pda may halt for a different sequence
of moves, for example, (q, 104, BB) r- (q, 104, OB) r- (q, 104, 00). As
6(q, 1, 0) is the empty set, the pda halts.
Let us continue with the proof of the theorem.
Step 2
(Proof of the construction, i.e. L(G) = N(A)). First we prove L(G)
(;;; N(A). Let W E L(G). Then it can be derived by a leftmost derivation. Any
sentential form in a leftmost delivation is of the form f.1Aa, where
U E L*,
A
E
VN and a E (VN u L)*. We prove the following auxiliary result: If
S ~ uAa by a leftmost derivation, then
(q, UV. S) f-2- (q. v, Aa)
for every v E L*
(7.22)
We prove (7.22) by induction on the number of steps in the derivation of
uAa. If S b
uA, then u =A, a =A, and S =A. As (q, v, S) f-2- (q, v, S), there
is basis for induction.
Suppose S ~ uAa by a leftmost derivation. This derivation can be split
11
as S =>
u[A[a[ ~ uAa. If the A[-production we apply in the last step is
A[ ---'} u:Aa:, then u = UjU::> a = a:aj'
n
As S => u [A ja j, by induction hypothesis,
(q, UjU:V, S) f-2- (q, U:V, Ajaj)
(7.23)
As Ai
---'} II:Aa: is a production in P, by Rule R j we get (q, A, A j ) r-
(q, A, U2Aa:). Applying Results 1 and 2 in Section 7.1, we get
(q, U2V, A[ at) r- (q, U:V. u2Aa: a [)
Hence,
(7.24)
Chapter 7: Pushdown Automata
~
243
But U111: = u and a2a1 = ex. So from (7.23) and (7.24), we have
(q, uv, S) P-
(q, v, Aa)
Thus (7.22) is true for S ~ uAa. By the principle of induction, (7.22) is true
for any derivation. Now we prove that L(G) k
N(A). Let W
E L(G). Then,
w can be obtained from a leftmost derivation,
S ~ uAv ~ uu'v = W
From (7.22),
(q, uu'v, S) P- (q, u'v, Av)
As A ~ u' is in P,
(q, u'v, Av) P- (q, z/v. u'v)
By Rule R:.
(q, u'v, u'v) p- (q, A, A)
Therefore,
Next we prove
tV = uu'v E N(A)
proving L(G) k
N(A)
N(A) k L(G)
Before proving the inclusion, let us prove the following auxiliary result:
S ~ ua
if (q. uv, S) P- (q, v, a)
(7.25)
We prove (7.25) by the number of moves in (q. uv, S) P- (q, v, a).
o
If (q, uv, S) p1- (q, v, a). then 1I =A, S = a; obviously, S => Aa. Thus
there is basis for induction.
Let us assume (7.25) when the number of moves is n. Assume
l"+l
(q,
UV, S) r- (q, v, a)
(7.26)
The last move in (7.26) is obtained either from (q, A, A) I- (C], A, a') or
from (q, a, a) I- (q, A, A). In the first case. (7.26) can be split as
(q, uv, S) P- (q, v, Aa:) I- (q, v, ala:) = (q, v, a)
By induction hypothesis, S ~ uAa2, and the last move is induced by
A ~ al' Thus, S ~ uAa: implies a1 a2 = ex. So,
S ~ lIAa: ~ uaJa2 = ua
In the second case. (7.26) can be split as
(q, uv, S) P- (q, av, aa) I- (q, v, a)
Also.
1I = u'a for some u' E L. So. (q, !I'av, S) P- (q, av, aa) implies (by
induction hypothesis) S ~ !I'aa =!la. Thus in both the cases we have shown
that S ~!lex. By the principle of induction, (7.25) is true.
244
g
Theory ofComputer Science
Now, we can prove that if >v
E
N(A) then
W
E L(G), As
W
E
N(A),
we have (q, w, S) r- (q, A, A), By taking II = W, V =A, ex =A and applying
(7,25), we get S ::; wA = H', i,e, w E L(G). Thus,
L(G) = N(A)
Theorem 7.4
If A = (Q, L. r, 0, qo. Zoo F) is a pda. then there exists a
context-free grammar G such that L(G) = N(A).
Proof
We first give the construction of G and then prove that N(A) = L(G).
Step 1
(Construction of G). We define G = (Vv, L, P, S), where
Vv = {S} u {[q, Z, q'] Iq. q' E Q, Z E l}
i.e. any element of Vv is either the new symbol S acting as the start symbol
for G or an ordered tliple whose first and third elements are states and the
second element is a pushdown symbol.
The productions in P are induced by moves of pda as follows:
R j: S-productions are given by S ~ [qo, Zo, q] for every q in Q.
R< Each move erasing a pushdown symbol given by (q', A) E O(q, a, Z)
induces the production [q, Z, q'] ~ a.
R,,: Each move not erasing a pushdown symbol given by (qlo ZtZ: ... Zm)
E
O(q, a, Z) induces many productions of the form
[q, Z, q'l
---7 a[qj. Zj, q:1[q:. Z:, q3] ... [qm' Zm' q']
where each of the states q', q2, .. " qll' can be any state in Q. Each move yields
many productions because of R3• We apply this construction to an example
before proving that L(G) = N(A).
EXAMPLE 7.8
Construct a context-free grammar G which accepts N(A), where
A = ({qo, qd, {a. b}, {Zo° Z}, 0, qo, Zo, 0)
and 0 is given by
O(qo, b, Zo) = {(qo, ZZo)}
O(qo, A, Zo) = {(qo, A)}
O(qo, b. Z) = {(qo, ZZ)}
O(qo, a, Z) = {(qj, Z)}
O(qj. b, Z) = {(qj. A)}
O(qj. a, 20) = {(qo, Zo)}
Solution
Let
G = (Vy. {a. b}, P. S)
Chapter 7: Pushdown Automata
g
245
where Vv consists of S, [qo, Zo, qo],
[qo, Zo, qd, [qo· Z. qa], [qo, Z, ql),
[CiI' Zoo qo], [cll, Zoo Cid,
[c11. Z, Cio], [qj, Z. qd·
111e productions are
Pi: S -1 [qo, Zo, qe]
Po: S -1 [qo· Zo° qrJ
01 qo. b, Zo) = {(qe. ZZo)} yields
Po: [qo. Zo, qo] -1 b[qo. Z. qo][qo· ZCh qo]
P4: [qo. Zo, Cio] -1 b[Cio· Z, Lll][qj, Zo, qo]
Ps: [qo, Zoo qd -1 b[qo. Z, qa][qo, Zo, CiI]
P6: [c/o, Zo, q\] -1 b[qo, Z, qI][c/1, Zo, qI]
o( qo. A. Ze) = {(qo, A)} gives
P7: [qo, Zo, qo] -1 A
o(qo. b. Z) = {(CJo. ZZ)} gives
Pg: [qo· Z. Cla] -1 b[c/o· Z. (/0] [qo, Z. qo]
P9: [cIa· Z. qo] -1 b[qo, Z. qd[Clj· Z, qo]
PIO:
[qo· Z. qd -1 b[qo· z. qo][qo, Z. qd
P II : [qe, Z. ql] -1 b[Clo. z. ql][q], Z, LJI]
olqe, a. Z)
{(LJI' Z)} yields
PI~: [cia, Z. LJo] -1 a[q], Z. qo]
P13 : [qo. Z. qd -1 a[qj, Z, q]]
6((/]. b. Z) = {(LJl , A)} gives
P 14: [ql' Z. qd -1 b
6(ql' a. Zo) = {(qe· Zo)} gives
PIS: [qj, Zo, qo] -1 a[qo, Zo, qo]
P 16: [ell' Zo° ql] -1 a[qij, Zo, ql]
P1-P16 give the productions in P.
Using the techniques given in Chapter 6. we can reduce the number of
variables and productions.
Step 2
Proof of the constmction. i.e. N(A) = L(G).
Before proving that ll(A) = L(G), we note that a variable [q. Z. q'] indicates
t::it for the pda the current state is q and the topmost symbol in PDS is Z In
the course of a delivation. a state q' is chosen in such a way that the PDS is
emptied ultimately. This corresponds to applying R~. (Note that the production
given by
R~ replaces a variable by a terminaL)
246
~
Theory ofComputer Science
To prove N(A) = L(G), we need an auxiliary result, i.e.
*
[q, Z, q'] =g w
if and only if
(7.27)
(q,
W, Z) r-- (q', A, A)
(7.28)
We prove the 'if part by induction on the number of steps in (7.28). If
(q, w, Z) r- (q', A, A), then
W is either a in I: or a in A. So, we have
(q', A) E
8(q, w, Z)
By R2 we get a production [q, Z, q'] ~ w. So, [q, Z, q']
~ w. Thus there
is basis for induction.
G
Let us assume the result, namely that (7.28) implies (7.27) when the
former has less than k moves. Consider (q, w, Z) t-L (q', A, A). This can be
split as
(7.29)
where W = aw' and a E I: or a = A, depending on the first move.
Consider the second part of (7.29). This means that the PDS has Z1Z2 ...
ZIl1' initially, and on application of w, the PDS is emptied. Each move of pda
can either erase the topmost symbol on the PDS or replace the topmost symbol
by some non-empty string. So several moves may be required for getting on the
top of PDS. Let WI be the prefix of w such that the PDS has Z2Z3 ... Zm after
the application of WI' We can note that the string Z2Z3 ... Zm is not disturbed
while applying
WI' Lei
Wi be the substring of w such that the PDS has
Zi+I ... Zm on application of Wi' Zi+I ... Zm is not disturbed while applying
Wb W2, .. "'
H'i' The changes in PDS are illustrated in Fig. 7.3.
Z1
Z2 (' Z2
Z2
Zi
Zi+1 ?
Z~1?w
Empty
bd
store
Zm
Zm
Zm
Zm
Zm
Zm
'--r----J
w1
w2
wm
Fig. 7.3
Illustration of changes in pushdown store.
Chapter 7: Pushdown Automata
~
247
In terms of IDs, we have
(qi, Wi, Z) f-2-
(qi+1> A, A) for i = 1, 2, ..., m, qm+l = qt
(7.30)
As each move in (7.30) requires less than k steps, by induction hypothesis we
have
(7.31 )
The first part of (7.29) is given by (% ZjZ2 ... ZII/) E /5(q, a, Z). By R3
we get a production
[q. Z, qt] ::S a[qj, Z1> q2][q2, Z2, q3] ... [qll/' Zm. qt]
From (7.31) and (7.32), we get
[q, Z. qt] ::S aWl w2 ...
Hill/ = W
(7.32)
By the principle of induction. (7.28) implies (7.27).
We prove the 'only if part by induction on the number of steps in the
derivation of (7.27). Suppose [q, Z. q'] ~ w. Then [q. Z, qt] ~ W is a
production in P. This production is obtained by R2. So W =A or W ELand
(q', A) E /5(q. ,V, Z). This gives the move (q, w, Z) r- (q, A, A). Thus there
is basis for induction.
Assume the result for derivations where th~ number of steps is less than k.
Consider [q, Z. qt] b
w. This can be split as
[q. Z. qt] ~ a[qJ. Zl' q2][q2. Z2. q3] ... [qll1' Z""
qt] k;1 W
(7.33)
As G is context-free, we can write W = aWjW2 ... w"', where
[qi, Zi, qi+d ~ Wi
and
qll/+ 1 = qt
C'
By induction hypothesis. we have
for i = 1, 2, ..., m
(qi' Wi, Z) f-2- (qi+l' A, A)i
By applying Results 1 and 2, we get
(qi. Wi, ZiZi+l ... ZII/) f-2- (qi+j, A, Zi+l ... Zm)
(7.34)
(Cli' WiWi+] ... WII/' Zi ... Zm) r- (qi+h
Wi+l ... H'I/l' Zi+l ... Zm) (7.35)
By combining the moves given by (7.35), we get
(q], W]W2 ... WII/' Zl ... ZII/) f-2-
(q', A, A)
(7.36)
The first step in (7.33) is induced by (q]. Z]Z2 ... ZII/) E /5(q, a, Z). The
corresponding move is
(q, a, Z) r- (qj, A, ZjZ2 ... ZIl1)
By applying the Result 1, we get
(q, aWl ... Will' Z) r- (qJ. W] .. , WII/' ZI ... ZII;)
(7.37)
(7.38)
248
~
Theory ofComputer Science
From (7.37) and (7.36). we get (q, tv, Z) pc-
(qt, A, A) . By the principle
of induction, (7.27) implies (7.28).
Thus we have proved the auxiliary result. In particular,
[ql), Zo, ql]
::2>
11
iff (ql).
11', ZI) pc- (qt, /\, A)
Now
11' E L(G)
iff 5 :b
1\'
iff S ~ lqo, ZQ. Q'1 :b
tv (for some q' by R\)
iff (qQ,
W. Zo) pc-
(e/, A. A) by the auxiliary result
iff 11' E N(A)
Thus. N(A) = L(G).
Corollary
If A is a pda, then there exists a context-free grammar G such that
T(A) = L(G).
Proof
By Theorem 7,2 we can find a pda At such that T(A) =N(A'). By
Theorem 7.4 we can construct G such that N(A' ) =LI G). Thus T(A) =L(G). I
EXAMPLE 7.9
Construct a pda accepting {a"blild' \ m.
11
:2': I} by null store. Construct the
corresponding context-free grammar accepting the same set.
Solution
The pda A accepting {(/'b
ll1ail \m. n :2': I} is defined as follows:
A = ({qQ. qd, {a. b}, {a, ZQ},
O. qo. ZI). 0)
where 0 is defined by
R1: O(ql), a, Zo) = {(qo. aZo) }
R-,:
O(qo· a. a) =
{ (qa. aa)}
R3:
O(qo. b. a) = {(q l' a)}
R.:,:
6Cql' b. a) = {(qj, a)}
Rs:
8(ql' a. a) = {(eil· A)}
R6: 8(Q\. A. ZoJ = {(ql, A)}
This is a modification of 8 given in Example 7.2.
We start storing a's until a b occurs (Rules R1 and R:J. When the current
input symbol is b. the state changes, but no change in PDS occurs (Rule R.,).
Once all the b' s in the input string are exhausted (using Rule R4). the
remaining a's are erased (Rule Rs). Using R(). ZI) is erased. So,
(ql). ai/bll'a", Zo) pc- (ql- A. Zo) r- (qlo A. A)
This means that a"blil(/'
E N(A). We can show that
N(A) = {d'b/lla" 1m. n :2': I}
Chapter 7: Pushdown Automata
};l,
249
Define G = (V", {a, b}, P. S). where V" consists of
[qo, 20, qo]. [q1' Zo, qo]· [qo, a, qa], [q), a, qo]
[qo· Zo, q1]' [q), Zo, qI1. [qo· a, qd, [qj, a, qd
The productions in P are constructed as follows:
The S-productions are
PI: 5 -+ [qo, Zo, qa],
8(qo, a. Zo) = {(qo, aZo)} induces
P3: [qo, Zo, qo] -+ a[qo, a, qo][qo. Zo, qo]
P4: [qo, Zo, qo] -+ a[q(j, a. q1][q1, Zo, qo]
P5: [qo· Zo, q!l -+ a[qo, a. qo] [qo, Zoo q1J
P6: [qo, Zo, qj] -+ a[qo, a. ql][q], Zoo ql]
8(qo, a. a) = {(qo. aa)} yields
P7: [qo, a, qo] -+ a[c/o. a. qo][qo, a, qo]
Pg: [qo. a, qoJ -+ a[qo, a. q1][qj, a, qo]
P9: [qo, a. ql] -+ a[qo· a, qo][qo, a. qd
P IO: [qo· a, ql] -+ a[qo, a, q!l[ql, a, q!l
PlJ: [qo, a. qo] -+ b[ql' a. qo]
PI:: [qo· a, qd -+ b[({r, a, q1J
b(qj. b, a) = {(qj, a)} yields
p
. [qj. a, qoJ -+ b[qj. a, qaJ
. 13'
P j4: [q1, a, qd -+ b[qr, a, qd
8(q], a. 0) = {(qj. A)} gIves
P j5 : [qj, a, qd -+ a
8(q;, A. Zo) = {(ql, A)} yields
P 16: [qj, Zo· ql] -+ A
Note:
When the number of states is a large number, it is neither necessary
nor advisable to write all the productions. We construct productions involving
those variables appearing in some sentential form. Using the constructions in
Chapter 6, vve can simplify the grammar further.
Theorem 7.5
Tne intersection of a context-free language L and a regular
language R is a context-free language.
250
Ji,t
Theory ofComputer Science
Proof
Let L be accepted by a pda A = (QA' L, T,
~'i' qo, Zo, FA) by final
state and R by DFA M = (QM' L,
~'11' Po, FM)·
We define a pda M' accepting L () R by final state in such a way that M'
simulates moves of A on input a in L and changes the state of M using ~w. On
input A, M' simulates A without changing the state of M. Let
M' = (QM X QA> L, r, 0, [Po, qo]. Zo, FM x FA)
where 0 is defined as follows:
o([p, q], a, X) contains ([P', q'], y) when
~"J(P, a) = p' and 0A(q, a, X)
contains (q',
y).
o([p, q], A, X) contains ([P, q'],
y) when 0A(q, A. X)
contains (q', y).
To prove T(M') = L () R we need an auxiliary result, i.e.
(7.39)
if and only if
(7.40)
We prove the 'only if part by induction on i (the number of steps). If
i =0, the proof is trivial (In this case, p =Po, q =qo, w =A and y= Zo). Thus
there is basis for induction. Let us assume that (7.39) implies (7.40) when the
former has i-I steps.
Let ([Po, qo]. ,v'a, Zo) ~ ([P, q], A, y). This can be split into ([Po, qo],
w'a, Zo) ~ ([p', q']. a, f3) fM; ([P, q], A, y), where w =w'a and a is in L
or a = A depending on the last move. By induction hypothesis, we have
(qo, w', Zo) ~ (q'. A, f3) and
o'~IP', w') =p'. By definition of 0, ([P', q'],
a, f3) ~ ([P, q], A, y) implies (q', a, f3) ~ (q, A, y) and o,wCp', a) =p.
(Note: p' =p when a = A.) So, o,'i1<Po, w'a) = o!'<IP', a) =p. By combining
the moves of A, we get (qo, w'a, ZQ) ~ (q', a, {3) ~ (q, A, y), i.e. (qo, w,
ZJ) r±- (q, A. y). So the result is true for i steps.
By the principle of induction the 'only if' part is proved.
We prove the 'if' part also by induction on i. It is trivial to see that there
is basis for induction.
Let us assume (7.40) with i-I steps. Assume that (qo, w, ZQ) ~ (q, A, y)
and
~"ho, w') = p. Writing w as w'a and taking oNIPo. w') as p', we get
(qo, }\/a, ZQ) ~ (q', a, {3) ~ (q, A, y). So, (qo, w', 20) iT (q', A, {3).
By induction hypothesis, we get ([Po, qo], w', Zo) ~ ([P', q'], A, {3).
Also, 0fw<P', a) = p and (q', a, {3) ~ (q, A, y) implies ([P', q'], a, {3) ~
([p, qJ, A, y).
Chapter 7: Pushdown Automata
!it
251
Combining the moves. we get ([Po· qo], w. Zo) f-:&- ([p, q], A, y).
Thus the result is true for i steps. By the principle of induction, the 'if'
part is proved.
Note:
In Chapter 8 we \vill prove that the intersection of two context-free
languages need not be context-free (Property 3. Section 8.3).
7.4
PARSING AND PUSHDOWN AUTOMATA
In a natural language, parsing is the process of splitting a sentence into words.
There are two types of parsing, namely the top-down parsing and the bottom-
up parsing. Suppose we want to parse the sentence "Ram ate a mango." If NP,
VP, N. V, ART denote noun predicate, verb predicate. noun. verb and article,
then the top-down parsing can be done as follows:
5 -1 NPVP
-1 Name VP
-1 Ram V NP
-1 Ram ate ART N
-1 Ram ate aN
-1 Ram ate a mango
The bottom-up parsing for the same sentence is
Ram ate a mango -1 Name ate a mango
-1 Name verb a mango
-1 Name V ART N
-1 NP VN P
-1 NP VP
-15
In the case of formal languages. we derive a terminal string in L(G) by
applying the productions of G. If we know that 11' E L* in L(G), then 5 ~ w.
The process of the reconstruction of the derivation of w is called parsing.
Parsing is possible in the case of some context-free languages.
Parsing becomes important in the case of programming languages. If a
statement in a prograffil11ing language is given. only the derivation of the
statement can give the meaning of the statement. (This is termed semantics.)
As mentioned earlier. there are two types of parsing: top-down parsing and
bottom-up parsing.
In top-down parsing. we attempt to construct the derivation (or the
corresponding parse tree) of the input string. starting from the root (with label
51 and ending in the given input string. This is equivalent to finding a leftmost
derivation. On the other hand. in bottom-up parsing we build the derivation
from the given input string to the top (root '.vith label 5).
252
J;!
Theory ofComputer Science
7.4.1
TOP-DOWN PARSING
In this section we present certain techniques for top-down parsing which can
be applied to a certain subclass of context-free languages. We illustrate them
by means of some examples. We discuss LL(l) parsing, LL(k) parsing, left
factOling and the technique to remove left recursion.
EXAMPLE 7.10
Let G = ({S, A, B}, {a, b}, P, S) where P consists of S ~ aAB, S ~ bBA,
A ~ bS, A ~ a, B ~ as, B ~ b. w = abbbab is in L(G) . Let us try to
get a leftmost derivation of w. When we start with S we have two choices:
S ~ aAB and S ~ bBA. By looking at the first symbol of w, we see that
S ~ bBA will not yield w. So we choose S ~ aAB as the production to be
applied in step 1 and we get S => aAB. Now consider the leftmost variable
A in the sentential form aAB. We have to apply an A-production among the
productions A ~ bS and A ~ a. A ~ a will not yield IV subsequently since
the second symbol in IV is b. So, we choose A ~ bS and get S => aAB =>
abSB. Also, the substring ab of w is a substring of the sentential form abSB.
By looking ahead for one symbol, namely the symbol b, we decide to apply
S ~ bBA in the third step. This leads to S => aAB =>abSB => abbBAB. The
leftmost variable in the sentential form abbBAB is B. By looking ahead for
one symbol which is b. we apply the B-production B ~ b in the fourth step.
On similar considerations, we apply A ~ a and B ~ b in the last two steps
to get the leftmost derivation.
S => aAB => abSB => abbBAB => abbbAB => abbbaB => abbbab
Thus in the case of the given grammar. we are able to construct a leftmost
derivation of IV by looking ahead for one symbol in the input string. In order
to do top-down parsing for a general string in L(G). we prepare a table called
the parsing table. The table provides the production to be applied for a given
variable with a particular look ahead for one symbol.
For convenience, we denote the productions S ~ aAB, S ~ bBA, A ~
bS, A ~ a, B ~ as and B ~ b by PI'
P~. .. " P6. Let E denote an error.
It indicates that the given input string is not in L(G). The table for the given
grammar is given in Table 7.1.
TABLE 7.1
Parsing Table for Example 7.10
S
A
B
A
E
E
E
a
b
For example. if A. is the leftmost variable in a sentential form and the first
symbol in unprocessed substring of the given input string is b, then we have to
apply P3'
Chapter 7: Pushdown Automata
);\
253
------~-----------'-----
A grammar possessing this property (by looking ahead for one symbol in
the input string we can decide the production to be applied in the next step)
is called an LL(l) grammar.
EXAMPLE 7.11
Let G be a context-free grammar having the productions S ~ F + S, S ~
F
S, S ~ F and F ~ a. Consider H' = a + a· a. This is a string in L(G).
Let us try to get the top-down parsing for
lV.
Looking ahead for one symbol will not help us. For the string a + a " a,
we can apply F ~ a on seeing a. But if a is followed by + or *, we cannot
apply a. So in this case it is necessary to look ahead for two symbols.
When we stmi with S we have three productions 5 ~ F + S, S ~ F" S
and S ~ F. The first two symbols in a + a * a are a +. This forces us to
apply only 5 ~ F + 5 and not other S-productions. So, 5 ~ F + 5. We can
apply F ~ a now to get 5 :::::} F + S :::::} a + 5. Now the remaining part of
11' is a ;. a. The first two symbols a * suggest that we apply 5 ~ F * 5 in
the third step. So. 5 :b a + 5 :::::} a + F " 5. As the third symbol in 11' is a,
we apply F ~ a, yielding 5 ::; a + F " 5 :::::} a + a " S. The remaining part
of the input string w is a. So. we have to apply 5 ~ F and F ~ a. Thus the
leftmost derivation of a + a ., a is 5
:::::} F + 5 :::::} a + 5 :::::} a + F "
5 :::::} a + a ;. 5 :::::} a + a ;. F :::::} a + a * a.
As in Example 7.10, we can prepare a table (Table 7.2) which enables us
to get a leftmost derivation for any input string. PI, P2, P3 and P4 denote the
productions 5 ~ F + 5, 5 ~ F .. 5. 5 ~ F and F ~ a. E denotes an error.
TABLE 7.2
Parsing Table for Example 7,11
A
a
+
aa
a+
a*
S
E
P3
E
E
E
Pi
P2
F
E
P4
E
E
E
P4
P4
+a
++
+*
*a
*+
,~ *
S
E
E
E
E
E
E
F
E
E
E
E
E
E
For example, if the leftmost variable in a sentential form is F and the next
two symbols to be processed are a ", then we apply P4, i.e. F ~ a. When we
encounter .) a as the next two symbols. an error is indicated in the table and
so the input string is not in L(G).
A grammar G having the property (by looking ahead for k symbols we
derIve a given input stJing in L(G)), is called an LUk) grammar. The grammar
given in Example 7.11 is an LL(2) grammar.
254
~
Theory ofComputer Science
In Examples 7.10 and 7.11 for getting a leftmost derivation, one
production among several choices was obtained by look ahead for k symbols.
This kind of nondeterminism cannot be resolved in some grammars even by
looking ahead.
This is the case when a grammar has two A-productions of the form A ~
a{3 and A ~ ay. By a technique called 'left factoring', we resolve this
nondeterminism. Another troublesome phenomenon in a context-free grammar
which creates a problem is called left recursion. A variable A is called left
recursive if there is an A-production of the form A ~ Aa. Such a production
can cause a top-down parser into an infinite loop. Left factoring and technique
for avoiding left recursion are provided in Theorems 7.6 and 7.7.
Theorem 7.6
Let G be a context-free grammar having two A-productions of
the form A ~ a{3 and A ~ ay. If A ~ a{3 and A ~ ay are replaced by
A ~ aA'. A' ~ {3 and A' ~ Y. where A' is a new variable then the resulting
grammar is equivalent to G.
Proof
The equivalence can be proved by showing that the effect of applying
A ~ a{3 and A ~ ay in a derivation can be realised by applying A ~ aA',
A' ~ {3 and A' ~ Y and vice versa.
Note:
The technique of avoiding nondeterminism using Theorem 7.6 is
called left factoring.
Theorem 7.7
Let
G be a context-free
grammar. Let the set of all
A-productions be {A ~ Aa1, ..., A ~ Aa,;' A ~ {31' .... A ~ {3m}. Then
the grammar G' obtained by introducing a new vmiable A' and replacing all
A-productions in G by A ~ {31A', ..., A ~ {3I1A', A' ~ a]A', ..., A' ~ a;A'
and A' ~ A is equivalent to G.
Proof
Similar to proof of Lemma 6.3.
Theorems 7.6 and 7.7 are useful to construct a top-down parser only for
certain context-free grammars and not for all context-free grammars. We
summarize our discussion as follows:
Construction of Top-Down Parser
Step 1
Eliminate left recursion in G by repeatedly applying Theorem 7.7 to
all left recursive vmiables.
Step 2
Apply Theorem 7.6 to get left factoring wherever necessary.
Step 3
If the resulting grammar is LL(k) for some natural number k, apply
top-down parsing using the techniques explained in Examples 7.10 and 7.11.
EXAMPLE 7.12
Consider the language consisting of all arithmetic expressions involving +, '\
( and) over the variables xl and x2. This language is generated by a grammar
Chapter 7: pushdown Automata
~
255
F --7 xl
F --7 x2
E--7 E + T
E--7 T
T--7 T
:i~ F
T--7 F
G = ({T F, E} L, p, E), where L = {x, L 2, +, *, (,)} and P consists of
F --7 (E)
Let us construct a top-down parser for L(G).
T --7 A
F --7 .12
F --7 (E)
F --7 xl
Step 1
We eliminate left recursion by applying Theorem 7.7 to the left
recursive variables E and T. We replace E --7 E + T and E --7 T by E --7 TE',
E' --7 + TE' and E' --7 A (E' is a new variable). Similarly, T --7 T " F and
T --7 F are replaced by T --7 FT', T'
--7 * FT' and T'
--7 A The resulting
equivalent grammar is
GJ = ({T. F, E, T'. E'L L, P'. E), where P' consists of
E --7 TE'
E' --7 +TE'
E' --7 A
T --7 FT'
T' --7
~ FT'
Step 2
We apply Theorem 7.6 for left factoring to F --7 xl and F --7 x2 to
get new productions F --7 xN --7 N --7 1 and N --7 2.
The resulting equivalent grammar is
G~ = ({T. F. E, T', E1. L, pI!, E) where pI! consists of
PI: E
--7 TE'
P6
T' --7 ;\
Po: E'
--7 +Te
P~
F
--7 (E)
!
Po.: E'
--7 A
Ps
F
--7 xN
P:J,: T
--7 FT'
P9
N
--7 1
Ps: Til
--7
~;: FT'
P lO : N
--7 2
Step 3
The grammar
G~ obtained III step 2 IS an LL(l) grammar. The
parsing table .
.
.
Table 7.3.
IS gIven m
TABLE 7,3
Parsing Table for Example 7.12
.\
x
2
+
';'
E
E
P.
E
E
E
E
P,
E
T
E
P4
E
E
E
E
P4
E
r
E
Ps
E
E
E
E
P7
E
r
Po
E
E
E
E
Po
E
Po
E'
P3
E
E
E
P2
E
E
P3
N
E
E
Pg
PiC
E
E
E
E
256
J;;;l
Theory ofComputer Science
7.4.2
TOP-DOWN
PARSING USING DETERMINISTIC pda's
We have seen that pda's are the accepting devices for context-free languages.
Theorem 7.3 gives us a method of constmcting a pda accepting a given context-
free language by empty store. In certain cases the construction can be modified
in such a way that a leftmost derivation of a given input stling can be obtained
while testing to know whether the given string is accepted by the pda. This
is the case when the given grammar is LL(l). We illustrate this by
constructing
a
(deterministic)
pda
accepting
the
language
given
in
Example 7.10 and a leftmost derivation of a given input string using the pda.
EXAMPLE 7.13
For the grammar given in Example 7.10, construct a deterministic pda
accepting L(G) and a leftmost derivaiton of abbab.
Solution
We construct a pda accepting L(G)$ ($ is a symbol indicating the end of the
input stling). This is done by using Theorem 7.3. The transitions are
8(q. A, A) =
{(q, ex) IA
---ct ex is in P}
8(q, t, t) = {(q, A)}
for every t in L
This
pda
is
not
deterministic
as
we
have
two
S-productions,
two
A-productions. etc. In Example 7.10 we resolved the nondeterminism by
looking ahead for one more symbol in the input string to be processed. In the
constmction of pda this can be achieved by changing the state from q to q"
on reading a. When the pda is in state
Cfa and the current symbol is S we
choose the transition resulting in
(q, aAB). Now the deterministic pda
accepting L(G)S by null store is
A = ({p, q, q", qh}. {a, b, $}, {S, A. B, a, b, ZoL 8, p, Zo, 0)
where 8 is defined by the following rules:
R)
8(p. A. ZD) = (q, s)
Ro
8(q, a. A) = (q", A)
R3
8(q", A. a) = (q, e)
R..
8(q, b. A) = (qb, A)
Rs
8(q(/' A. b) = (q. e)
R6
8(q", A, S) = (qa- aAB)
R7
8(q". A. S) = (q", bBA)
Rs
8(q(/' A, A) = (qu, a)
R9
8(q/" A. A) = (qh, bS)
Chapter 7: Pushdown Automata
~
257
RIO : S(qll' A, B) = (q", as)
R1]
S(q". A, B) = (q", b)
R12
Seq, $. 20) = (q, A)
Here R1 changes the initial ill (p,
11', Z) into (q,
11', SZ). R2 and R4 are for
remembering the next symbol. R6-Rll are simulating the productions. R3 and Rs
are for matching the cunent input symbol and the topmost symbol on PDS and
for erasing it (in PDS). Finally. R12 is a move for erasing Z and making the
PDS empty when the last symbol $ of the input string is read.
To get a leftmost derivation for an input string
lV, apply the unique
transition given by R J to R 12• When we apply R6 to R]J, we are using a
cOlTesponding production. By recording these productions we can test whether
]V E
L(G) and get a leftmost derivation. The parsing for the input string
abbbab is given m Table 7.4.
The last column of Table 7.4 gives us a leftmost derivation of abbbab. It
is S =} aAB =} abSB =} abbBAB =} abbbAB =} abbbaB =} abbbab.
TABLE 7.4
Top-down Parsing for w of Example 7.13
Step
State
Unread input
Pushdown stack
Transition
Production
used
applied
1
p
abbbabS
Zo
2
q
abbbabS
SZo
R1
3
qa
bbbabS
SZo
R2
4
qa
bbbabS
aABZo
R6
S
---7 aAB
5
q
bbbabS
ABZo
R3
6
qb
bbabS
ABZo
R4
7
qo
bbabS
bSBZo
Rg
A
---7 bS
8
q
bbabS
SBZo
Rs
9
qb
babS
SBZo
R4
10
qb
babS
bBABZo
R7
S
---7 bBA
11
q
babS
BABZo
Rs
12
qb
ab$
BABZo
R4
13
qb
abS
bABZo
R11
B
---7 b
14
q
abS
ABZo
Rs
15
qa
b$
ABZo
R2
16
qa
b$
aBZo
Rg
A---7a
17
q
b$
BZo
R3
18
qb
S
BZo
R4
19
qD
$
bZo
R1i
B
---7 b
q
S
Zo
Rs
21
q
A
A
R12
258
Q
Theory ofCompurer Science
7.4.3
BOTTOM-UP PARSING
In bottom-up parsing we build the delivation tree from the given input string
to the top. (the root with label S). For certain classes of grammars, called weak
precedence grammars, we can construct a deterministic pda which acts as a
bottom-up parser. \Ve illustrate the method by constructing the parser for the
grammar given in Example 7.12.
In bOltom-up parsing we have to reverse the productions to get S finally.
This suggests the following moves for a pda acting as bottom-up parser.
(il 8(p. A. (xI) = {(P, A)lthere exists a production A ~ a}
(ii) 8(p, cr, A) = {(P. cr)} for all cr in L.
Using (i) we replace a,r on the basis by A when A ~
(X is a production.
The input symbol a is moved onto the stack using (ii). For acceptability, we
require some moves when the PDS has S or Zo on the top.
As in top-down parsing we construct the pda accepting L(G)$. Here we
will have tvvo types of operations, namely shifting and reducing. By shifting
we mean pushing the input symbol onto the stack (moves given by (ii)). By
reducing \ve mean replacing
(Xl' by A when A ~ a is a production in G
(moves given by (i)l.
At every step we have (i) to decide whether to shift or to reduce (ii) to
choose the prefix of the string on PDS for reducing, once we have decided to
reduce. For (i) \\le use a relation P called a precedence relation. If (a, b) E P
where a is the topmost symbol on PDS and b is the input symbol then we
reduce. Otherwise we shift b onto the stack. Regarding (ii), we choose the
longest prefix of the string on the PDS of the form (Xl' to be reduced to A (when
A ~
(X is a production).
We illustrate the method using the grammar given in Example 7.12.
EXAMPLE 7.14
Construct a bottom-up parser for the language L(G)$. where G is the grammar
given in Example 7.12.
Here the productions are E ~ E + T. E ~ T, T
----7 T * F, T ~ F,
F ~ (E). F ~ xl and F ~ x2. Using these productions, we can construct
the precedence relation P. It is given in Table 7.5. If (a, b) is in p. then we
have a tick mark'.(' in the (a, b) cell of the table. Using Table 7.5 we can
decide the moves. For example, if the stack symbol is F and the next input
symbol is ". then we apply reduction. If the stack symbol is E, then any input
symbol is pushed onto the stack.
Chapter 7: Pushdown Automata
~
259
TABLE 7.5
The Precedence Relation for Example 7.14.
Stack symbol/
2
+
8-
$
Input symbol
Zo
.\
(
)
v'
v'
-/
-/
1
-/
-/
-/
-/
2
v'
-/
-/
-/
+
E
T
-/
v'
-/
F
-/
-/
-/
-/
Using the precedence relation and the moves given at the beginning of this
section we can construct a deterministic pda. As in the construction of top-down
parser. when we look ahead for one symbol we 'remember' it by changing state.
The deterministic pda which acts as a bottom-up parser is
A. = (Q. L
I
• 1. a. p. Zoo 0)
where
r = L U
{$}. Q = {p}
U
{per:
(J E II}
1= {E. T. F}
U 1: U
{Zo, S}
and ais given by the following rules:
R
o(p.
G, A) = (p. A)
Rc
o(Per' A. a) = (p, (Ja) for all (a.
(J) <t P
R,
a(Per. A. T + E) = (Po, E) when (T
(J)
E P
R",
a(Per' A. Ta) = (Per. Ea) when (T,
CJ) E
P and a E 1 -
{+}
R"
a(Per. A. F
'C n = (Per' T) when (F,
CJ)
E
P
Rf
a(Per. A. Fa) = (Per. Ta) when (F.
CJ) E P and a E
1 -
{,,}
R-
a(Per' A. Ee) = (Per' F) when ( ).
CJ) E P
Rei
a(Per' A. lx) = (Pu' F) when (1,
CJ)
E
P
Rq
a(Per' A. 2x) = (Pu. F) when (2.
CJ) E
P
RiO:
a(ps. A, E) = (Ps. A)
R 11
: a(ps. A. Zo) = (Ps. A)
As A. is deterministic. we have dropped parentheses on the R.H.S. of
R 1 - R 11. Using the pda A. we can get a bottom-up parsing for any input string
w. The bottom-up parsing for xl + (x2) is given in Table 7.6.
260
~
Theory ofComputer Science
Table 7.6
Bottom-up Parsing for xl + (x2)
Step
State
Unread
Pushdown
Rule
Production
input
stack
used
applied
1
p
xl + (x2)
Zo
2
p,:
I + (x2)
Zo
Rj
3
p
1 + (x2)
xZo
R2
4
P1
+ (x2)
xZo
R1
5
P
+ (x2)
1.1:Zo
R2
6
P+
+ (x2)
1.>:Zo
R1
7
P+
+ (.>:2)
FZo
Rs
F -7 x1
8
P+
+ (x2)
TZo
R6
T -7 F
9
P+
+ (.12)
EZo
R4
E-7T
10
P
(.>:2)
+EZo
R2
11
p(
.>:2)
+EZo
Rj
12
P
x2)
(+EZo
R2
13
Px
2)
(+EZo
R1
14
P
2)
x(+EZo
R2
15
P2
)
x(+EZo
R1
16
P
)
2x(+EZo
R2
17
P;
$
2x(+EZo
R1
18
P;
$
F(+EZo
Rg
F -7 x2
19
PI
$
T(+EZo
R6
T -7 F
20
P;
$
E(+EZo
R4
E-7T
21
P
$
)E(+EZo
R2
22
Ps
A
)E(+EZo
R1
23
Ps
A
F+EZo
R7
F -7 (E)
24
Ps
A
T+EZo
R6
T -7 F
25
Ps
A
EZo
R3
E-7E+T
26
Ps
A
Zo
R10
27
Ps
A
A
R11
By backtracking the productions that we applied, we get a rightmost
derivation E ~ E + T ~ E + F ~ E + (E) ~ E + (D ~ E + (F) ~ E + (x2)
~ T + (x2) ~ F + (x2) ~ xl + (x2).
In Chapter 8 we will discuss how LR(k) grammars are amenable for
parsing.
7.5
SUPPLEMENTARY EXAMPLES
EXAMPLE 7.15
Construct a pda accepting all palindromes over {a, b}.
Solution
Let L = {w E {a. b}* Iw = VI}}. Before constructing the required pda, note
that L consists of palindromes of odd or even length. If w in L is of odd
(7.41)
(7.42)
(7.43)
(7.44)
(7.45)
(7.46)
(7.47)
(7.48)
(7.49)
(7.50)
(7.51)
(7.52)
Chapter 7: Pushdown Automata
~
261
length, there is a middle symbol which need not be compared with any other
symbol. If w in L is of even length, the rightmost symbol of the first half is
the same as the leftmost symbol of the second half, The key idea of the
construction is to store the symbols in the first half (or the symbols lying to
the left of the middle of the input string) and matching them with the symbols
in the second half (or the symbols to right of the middle of the input string),
If there is no matching. the machine halts.
We define the pda M as follows:
M = ({qo, q], q:c}, {a, b}, {a, b, 2o}, 6, qo, zo, {qJ)
where 6 is defined by
6(qo,
G, Zo) = {(qo, aZo), (qj, Zo)}
6(qo, b, Zo) = {(qQ, bZo), (q], Zo)}
6(qo, a. a) = {(qQ, aa), (ql> a)}
6(qo, b, a) = {(qo, ba), (q], a)}
6(qo. a. b) = {(qQ, ab), (qlo b)}
6(qQ' b. b) = {(qQ, bb), (q], b)}
6(qo, A. Zo)= {(q]. Zu)}
6(qo. A, a) = {(q)o n)}
6(qo, A. b) = {(q]. b)}
6(q], n. a)= {(qj. A)}
6(qlo b, b) = {(qlo A)}
6(q], A. Zo) = {(q:c. 2o)}
Obviously, M is a nondeterministic pda.
(7.41)~(7.46) give us two
choices. The first choice can be used for storing the input symbol without
changing the state. (7.47)-(7.49) are used for indicating that the first half of
the input string is over: there is change of state in this case. The second choice
of (7.41)-(7.46) is used when w is a palindrome of odd length and the middle
symbol of w is reached: in this case. we ignore the middle symbol and make
no change in PDS. (7.50) and (7.51) are used to match symbols of the input
string (second half) and cancel the symbol in PDS when they match. (7.52)
is used to move to the final state q:c after reaching the bottom of PDS.
We can prove that L is accepted by M by final state. The reader can take
an odd palindrome and an even palindrome and check that the final state q:c
is reached.
EXAMPLE 7.16
Construct a deterministic pda accepting L = {w E {a, b}* I the number of a's
in
HI equals the number of b's in w} by final state.
262
iii
Theory ofComputer Science
Solution
We define a pda 1\1 as follows:
lvi = ({Cfo, qd· {a. b}. {a. b. ZoL O. qo, Zo, {qd)
where 0 is defined by
O(q(), a, Zo) = {(qi. Zo)}
O(qo, b. Zo) = {(qo, bZo)}
O(qo, a, b) = {(qo. A)}
O(qo, b, b) = {(qo. bb)}
O(CfI' a, Zo) = {(ql' aZo)}
O(Cfj. h. Zo) = {(qo. aZo)}
O(Cfj, a. a) = {(Cfl, aa)}
O(CfI' b, a) = {(Cfj, A)}
(7.53)
(7.54)
(7.55)
(7.56)
(7.57)
(7.58)
(7.59)
(7.60)
The construction can be explained as follows:
If the pda M is in the final state qj, it means it has seen more a's than
b·s. On seeing the first a. lvi changes state (from qo to ql) «7.53». Afterwards
it stores the a's in PDS without changing state (7.57) and (7.59». It stores
the initial b in PDS ((7.54»
and also the subsequent b's «(7.56». The pda
cancels a in the input string, with the first (topmost) b in PDS «(7.55». If all
b's are matched with stored a's, and M sees the bottom of PDS, M moves
from Cfj to Cfo ((7.58»). The b's in the input string are cancelled on seeing a
in the PDS (7.60»).
Ai is deterministic since 0 is not defined for input A. The reader is advised
to check that Cft is reached on seeing an input string h' in L
EXAMPLE 7.17
Construct a pda M accepting L = {c/hi(J Ii = j or j = k} by final state.
Solution
We define pda M as follows:
I'vi = ({qo. Cfj, ...• Cfd, {a, b, C}. {Z{), X}. 0, Cfo, 20, {Cfj, q3})
where 0 is given by
O(Cfo. A, Zo)= {(Cf], Z{).
(Cf2· L{j). (Cf3, Zo)}
o(qj. c. 20)= {(Cf], Zo)}
0(Cf2' a. Zo) = {(Cf2' XZo)}
0(Q2, a. X) = t(Q2, XX)}
O(q2, b, X) = (q)"
b. X) = {(Cf)"
A)}
O(q),. A. Zo) = {(Cf], Zo)}
(7.61)
(7.62)
(7.63)
(7.64)
(7.65)
(7.66)
Chapter 7: Pushdown Automata
1;\
263
0(q3' a. ZO) = {(q3' Zo)}
0(Q3' b. Zo) = {(q). XZo)}
0«(1). h. X) = {(q). XX)}
O(q).
c, X) = O(qc,. c. X)
= {(qc"
A)}
o(C]c"
A. Zo) = {«(/3. A)}
(7.67)
(7.68)
(7.69)
(7.70)
(7.71)
The states qb q:.
C]~ stand for aObOck, dbick, dbici respectively where
i > 0, j
;:: O. k ;:: O. (7.61) indicates the initial guess. The three choices
correspond to the three cases.
(q(i. c'. 20) = (qo. Ack• Zo) ~ (qlo c'. Zo) 1- (q], A, Zo)
by (7.61) and (7.62). As ql is a final state. ck
E TUvf).
The pda in state q: will not change state and stores a's in the input string
as X's in PDS «(7.63) and (7,64). On seeing the first b after many a·s. M
changes its state to qJ, and cancels X in PDS for subsequent b's «7.64)). If
it reaches the bottom of PDS. M goes back to ql. which is an accepting state
«(7.76)). So Iv! accepts db i• It continuous to be in state q! on seeing c's
subsequently «7.62). So. Ai accepts a'bic'.
For dealing with aibici. Ai makes the initial guess using (7.61) and reaches
state qo. It simply reads a's
without changing state or PDS «7.67».
!vI subsequently replaces b with X and changes to state q). Afterwards M
goes on changing b's to Xs «7.69). On seeing a c, !vI changes state.
Subsequent c's are matched with X's (which correspond to b's read earlier)
and X's in PDS are cancelled. On reaching the bottom of PDS, M reaches q3.
a final state «7.71)).
Thus. aOhOck• (/l/ck, aibici
E
TUYf) for i > O. j
;:: O. k ;:: O. Hence.
T(!vf) = L.
EXAMPLE 7.18
Convert the graJ1lJ11ar 5 -+ aSb! A. A -+ bSa I5 IA to a pda that accepts the
same language by empty stack.
Solution
We construct a pda A as
A = ({q}.
{a. b}. {So A. a. b}. o. q, 5.0)
where 0 is defined by the following rules
o(q. A. 5) = {(q. aSb). (q. A)}
o(q. A. A) = {(q, bSA). (q. S). (q. A)}
O(q. a. a) = {(q, A)}
o(q. b. b) = {(C]. A)}
and A is the required pda.
264
~
Theory ofComputer Science
EXAMPLE 7.19
If A is a pda, then show that there is a one-state pda A, such that
N(A) = N(A 1).
Solution
By Theorem 7.4, we get a context-free grammar G such that L(G) = N(A).
Denote G by (VN' L, P, 5). By Theorem 7.3, we can get a one-state pda A 1
given by
A 1 = ({q}, L, VV U L, 8, q, S,
0)
such that N(A j) = L(G).
SELF-TEST
Choose the correct answer to Questions 1-6.
1. If 8(q, a1' 2 j ) contains (q', /3), then
(a) (q, ala:. 2 12:) I- (q', a:. /32:)
(b) (q, a:a2' 2 12 2) I- (q',
a1 a2, /32:)
(c) (q. ala:, 2:) I- (q', a1' 2 1)
(d) (q. ala:,
21Z~) I- (q'. a:, 2 12:)
2. In a deterministic pda, [8(q, a, Z) I is
(a) equal to 1
(b) less than or equal to 1
(c) greater than 1
(d) greater than or equal to 1
3. In a deterministic pda:
(a) 8(q, a, Z) = 0 =} 8(q, A, Z) ::t 0
(b) 8(q, a, Z)
::t 0 =} 8(q, A, Z) = 0
(c) 8(q, A, Z)::t 0 =} 8(q, a, Z)
::t 0
(d) 8(q. A, Z)::t 0 =} 8(q. a. Z)
= 0
4. {d'b
l1 [n:2: I} is accepted by a pda
(a) by null store and also by final state.
(b) by null store but not by final state.
(c) by final state but not by null store.
(d) by none of these.
5.
{a
l1b
211 [11 :2: I} is accepted by
(a) a finite automaton
(b) a nondeterministic finite automaton
(c) a pda
(d) none of these.
to
Chapter 7: Pushdown Automata
l;!
265
6. The intersection of a context-free language and a regular language is
(a) context-free
(b) regular but not context-free
(c) neither context-free nor regular.
(d) both regular and context-free.
Fill up the blanks:
7. In bottom-up parsing.
we build the deviation from
8. In LR(l) grammar. we can decide the production to be applied in the
next step by _._ .._.
9.
W
E
T(A). where A is a pda if (qo. w. Zo) ~
10.
11' E N(A). where A is a pda if (%. w. 2 0) ~
EXERCISES
7.1 If an initial ill of the pda A in Example 7.2 is (qo. aaeaa. 2 0). what
is the ill after the processing of aacaa? If the input string is (i) abeba.
(ii) abeb. (iii) acba. (iv) abac. (v) abab. will A process the entire
string? If so. what \vill be the final ill?
7.2 What is the ill that the pda A given in Example 7.5 reaches after
processing (i) a3b:. (ii) a:b3. (iii) as. (iv) b5. (v) b3a:. (vi) ababab if
A starts with the initial ill?
7.3 Construct a pda accepting by empty store each of the following
languages.
(a) {a"blllalli m.
11 :2: I}
(b) {allb:" I 11 :2: I}
(c) {allb
lJle" I Tn.
11 2: I}
(d) {a'llbll 1m >
11 :2: I}
7.4 Construct a pda accepting by final state each of the languages given in
Exercise 7.3.
7.5 Construct a context-free grammar generating each of the following
languages. and hence a pda accepting each of them by empty store.
(a) {a" bll In :2: I} u {a'llb:m 1m :2: I}
(b) {d 'b'''all 1m. n :2: l} u
{aile" In
:2: I}
(c) {a"Yllellld" 1m. n :2: l}
7.6 Let L = {d"b!! In < m}. Construct (i) a context-free grammar accepting
L
(ii) a pda accepting L by empty store. and (iii) a pda accepting L
by final state.
266
g
Theory ofComputer Science
7.7 Do Exercise 7.6 by taking L to be the set of all strings over {a, h}
consisting of twice as many a's as b's.
7.8 Construct a pda accepting the set of all even-length palindromes over
{a, b} by empty store.
7.9 Show that the set of all strings over {a, b} consisting of equal number
of a's and b's is accepted by a detemunistic pda.
7.10 Apply the construction given in Theorem 7.4 to the pda M given in
Example 7.1 to get a context-free grammar G accepting N(M).
7.11
Apply the construction given in Theorem 7.4 to the pda obtained by
solving Exercise 7.4.
7.12 Show that {a"b
l1 lll e: I} u
{amb~'" 1m e: I} cannot be accepted by a
deterrninistic pda.
7.13 Show that a regular set accepted by a detemunistic finite automaton
with
11 states is accepted to final state by a deterministic pda with n
states and one pushdown symbol. Deduce that every regular set is a
deterministic context-free language.
(A context-free language is deterministic if it is accepted by a determi-
nistic pda.)
7.14 Show that every regular set accepted by a finite automaton with 11 states
is accepted by a deterministic pda with one state and
11 pushdown
symbols.
7.15 If L is accepted by a detefllljnistic pda A. then show that L is accepted
by deterministic pda A which never adds more than one symbol at a
.
('
'f
5:(
-) -
(
I
.)
h
I
I < )
tlme
l.e.
1
u q, a.
~. -q, y. t en
Y I
_
-).
7.16 If L is accepted by a deterministic pda A, then show that L is accepted
by a deterministic pda A which always (i) removes the topmost symboL
or (ii) does not change the topmost symboL or (iii) pushes a single
symbol above the topmost symboL
--------------
LR(k) Grammars
In this chapter we study LR(k) grammars (a subclass of context-free grammars)
which play an important role in the study of programming languages and the
design of compilers. For example. a typical programming language such as
ALGOL has LR(l) parser.
8.1
LR(k)
GRAMMARS
In Chapters 4 and 6 we were mainly interested in generating strings usmg
productions and in performing the membership test. In the design of
programming languages and compilers, it is essential to develop the parsing
techniques, i.e. techniques for obtaining the 'reverse derivation' of a given
string in a context-free language. In other words, we require techniques to find
a derivation tree for a given sentence w in a context-free language.
To find a derivation tree for a given sentence lV, we can start with lV and
replace a substring, say WI of w, by a variable A if A -7 Wj is a production. We
repeat the process until we get S. But this is more easily said than done, for at
every stage there may be several choices and we have to choose one among
them. If we make a wrong choice, we will not get S, and in this case we have
to backtrack and try some other substring. However, for a certain subclass of
context-free grammars, it is possible to carry out the process, i.e. getting the
derivation in the reverse order for a given string W in a deterrr,inistic way. LR(k)
grammars form one such subclass. Here. LR(k) stands for left-to-light scan of
the input string producing a rightmost derivation using the k symbol look-
ahead on the input string.
Before discussing the LR(k) grammars, we should note that although
parsing gives only the syntactical structure of a string, it is the first step in
understanding the 'meaning' of the sentence.
267
(ii)
(iii)
268
~
Theory ofComputer Science
Consider some sentential form af3H' of a context-free grammar G, where
Ct., 13 E (Y\ U L)* and W E I*, Suppose we are interested in finding the
production applied in the last step of the deivation for af3,v, If A -7 13 is a
productiDn, it is likely that A -7 13 is the production applied in the last step,
but we cannot definitely say that this is the case, If it is possible to assert that
11 -7 13 is the production applied in the last step by looking ahead for k
symbols (i.e. k symbols to the right of 13 in af3w), then G is called an LR(k)
grammar, The production A -7 13 is called a handle production and 13 is called
a handle.
We write a ,,; 13 if 13 is delived from a by a right-most delivation. Before
R
giving the rigorous definition of an LR(kJ grammar, let us consider a grammar
for which parsing is possible by looking ahead for one symbol.
EXAMPLE 8.1
Let G be 5 -7 AB, A -7 aAb. A -7 A, B -7 Bb, B -7 b. It is easy to see
that L(G) = {(Fbi! ill> m
~ I}. Some sentential forms of G obtained by
right-most
derivati~ns are "4B. ABbk. aIllAb
i11bk, a"'b"'+k, where k ~ 1. AB
a;pears as the R.H.S. of 5 -7 AB. So liB may be a handle for AB or ABbk.
If we apply the handle to AB, we get 5 =f' AB. If we apply the handle to ABbk,
we get 5bk
==} ABbk. But Sbk is not a sentential form. So to decide whether
AB can be a handle, we have to scan the symbol to the right of AE. If it is
A, then AB sen'es as a handle. If the next symbol is b, AB cannot be a handle.
So only by looking ahead for one symbol we are able to decide whether AB
is a handle. Let us consider a2b3. As we scan from left to right we see that
the handle production A -7 A may be applied. A can serve as a handle only
when it is taken bet\veen the rightmost a and the leftmost b. In this case we
get a2Ab3 => a2b3, and we are able to decide that A -7 A is a handle
~
R
production only by looking ahead of one symbol (to the right of A). If A is
taken between two a's. we get aAab3 => a2b3. But aAab3 is not a sentential
R
form.
Similarly, \ve can see that the correct handle production can be
determined by looking ahead of one symbol for various sentential forms,
A rigorous definition of an LR(k) grammar is now given.
DefInition 8.1
Let G = (V\". L, P, 5) be a context-free grammar in which
5 .;; 5 only when n = O. G is an LR(k) grammar (k ~ 0) if
(i) 5 ,,; aAH' => af3w, \vhere a,
f3
E
V~,
W
E P,
R
R
S
;i:
,
i.'
I
'f3'
I
h
I
13'
'I
,*
d
. => a Ii H'
=> a
H', were a,
E
V"'.
W
E k;'. an
R
R
the first Iaf3l + k symbols of af3\V and (XI,f3'\v' coincide. Then a =a',
A = A'.
f3 = 13'.
Remarks
1.
If af3H or a'f3'\\' I have less than Iaf3l + k symbols. we add
some 'blank symbols'. say S. on the right and compare.
Chapter 8: LR(k) Grammars
l;!
269
2.
It is easy to see how we can get the derivation tree for a given
terminal string. For getting the derivation tree. we want to get the derivation
"in the reverse order", Suppose a sentential form af3,\' is encountered. We can
get a right-most derivation of f3H' in the fol1mving \vay: If A --+ 13 is a
production. then we have to decide whether A --+ 13 is used in the last step of
a right-most derivative of af3\\', On seeing k symbols beyond 13 in af3w, we
are able to decide that A --+ 13 is the required production in the first step, For,
if a'f3'H.' is another sentential foml satisfying condition (iii), then we can
apply A' --+ 13' in the last step of a light-most derivation of a'{3'w
l
, But by
definition it follows that A = AI, 13 = 13
1 and a = a
l
• So A --+ f3 is the only
possible production we can apply and we are able to decide this after 'seeing'
the k symbols beyond 13, We repeat the process until we get S.
3. If G is an LR(k) grammar, it is an LR(k
l
) grammar for aU k' > k.
EXAMPLE 8.2
Let G be the grammar 5 --+ all., A --+ Abb I b, Show that G is an LR(O)
grammar,
Solution
It is easv to see that anv element in UG) is of the fonn ab~iI+l. The sentential
forms of G. are aA. aAb~iI, ab2n+!. Let us find out the last production applied
in the derivation of ab~n+!. As aA, Ab!?, b are the possible right-hand sides
of productions, only A --+ b can be the last production: we are able to decide
this without looking at any symbol to the right of b, Similarly. the last
productions for aAbc" and aA are A --+ Abb and 5 --+ (lA, respectively. (We
are able to say that A --+ Abb is the last production for any sentential form
aAb2n for all n
;:::: 1.) Thus, G is an LR(O) grammar.
EXAMPLE 8.3
Consider the grammar G given in Example 8.1. Show that G is an LR(l)
grammar. but not an LR(O) grammar. Also. find the delivation tree for a~b4.
Solution
In Example 8.1 we have shown that for sentential forms of G we can determine
the last step of a right-most derivation by looking ahead of one symbol. So G
is LR(l), We have also seen that 5 --+ AB is a handle production for the
sentential form AB, but not for ABbA, In other words, the handle production
cannot be detennined without looking ahead. So G is not LR(O).
To get the derivation tree for aCb4 we scan a~b'+ from left to right. After
scanning a. we look ahead. If the next symbol is a. we continue to scan. If
the next symbol is b. we decide that A --+ A is the required handle production.
Thus the last step of the right-most derivation of a~b4 is
a~Ab4
=>
a~/\.b4
R
270
g
Theory ofComputer Science
To get the last step of a2Ab", we scan a2Ab" from left to right aAb is a
possible handle. We are able to decide that this is the right handle without
looking ahead and so we get
aAbb2
=? a2Ab4
R
Once again using the handle aAb, we obtain
Ab2
=? aAbb2
R
To get the last step of the rightmost derivation of Ab2, we scan Ab2• A possible
handle production is B -'7 b. We also note that this handle production can be
applied to the first b we encounter, but not to the last b. So, we get
ABb
=? Ab2.
R
For ABb. a possible a-handle is Bb. Hence, we get AB =? ABb. Finally,
R
we obtain S =? AB. Thus we have the following derivations:
R
a2Ab4 =? a2Ab4
by looking ahead of one symbol
R
aAbb2 =? a2Ab4
R
Ab2 =? ClAbb2
R
ABb =? Ab2
R
AB =? ABb
R
by not looking ahead of any symbol
by not looking ahead of any symbol
by not looking ahead of any symbol
by not looking ahead of any symbol
by looking ahead of one symbol
S =?AB
R
The derivation tree for a2b4 is as shown in Fig. 8.1.
s
a
a
A
b
b
I
b ab
Fig. 8.1
Derivation tree for a2b4
8.2
PROPERTIES OF LR(k) GRAMMARS
In this section we give some important properties of LR(k) grammars which
are useful for parsing and other applications.
Chapter 8: LR(k) Crammars
~
271
Recall the definition of an ambiguous grammar. A grammar G is
ambiguous if there exists W E L(G) which has two derivation trees. The next
theorem gives the relation between LR(k) grammars and unambiguous
grammars.
Property 1
Every LR(k) grammar G is unambIguouS.
Proof
We have to show that for any x E Ii\ there exists a unique right-most
derivation. Suppose \ve have two rightmost derivations for x, namely
s ~ aAw ~ a[3w = x
R
R
S ~ ajt/w/ ~ a'b'w' = x
R
R
(8.l)
(8.2)
As a[3w = a'[3'w', from the definition it follows that a = a', A = A' and
[3 = [3'. As a[3w = a'[3'w', we get w = w', and so aAw = a'A'w'. Hence the
last step in the derivations (8.l) and (8.2) is the same. Repeating the arguments
for the other sentential forms derived in the course of (8.1) and (8.2), we can
show that (8.l) is the same as (8.2). Therefore, G is unambiguous.
I
We have seen that the detenninistic and the nondeterministic finite
automata behave in the same way in so far as acceptability of languages is
concerned. The same is the case with Turing machines. But the behaviour of
deterministic
and nondeterministic
pushdown
automata is different.
In
Chapter 7 we have proved "that any pushdown automaton accepts a context-
free language and for any context-free language L, we can construct a
pushdown automaton accepting L. The following property gives the relation
between LR(k) grammars and pushdown automata.
Property 2
If G is an LR(k) grammar. there exists a deterministic pushdown
automaton A accepting L(G).
Property 3
If A is a deterministic pushdown automaton A, there exists an
LR(l) grammar G such that L(G) =N(A).
Property 4
If G is an LR(k) grammar, where k > L then there exists an
equivalent grammar G! which is LR(l). In so far as languages are concerned,
it is enough to study the languages generated by LR(O) grammars and LR(l)
grammars.
Defmition 8.2
A context-free language is said to be deterministic if it is
accepted by a detenmnistic pushdO\vn automaton.
Property 5
The class of deterministic languages is a proper subclass of the
class of context-free languages.
The class of deterministic languages can be denoted by
Property 6
ot'dctl is closed under complementation but not under union and
intersection.
The following definition is useful in characterizing the languages accepted
by an LR(O) grammar.
272
];i
Theory ofComputer Science
~~~--~~~~~~~~~~--~~~
Defmition 8.3
A context-free language has prefix property if no proper
prefix of strings of L belongs to L.
Property 7
A context-free language is generated by an LR{O) grammar if
and only if it is accepted by a deterministic pushdown automaton and has
prefix property.
Property 8
There is an algorithm to decide whether a given context-free
grammar is LR(k) for a given natural number k.
8.3
CLOSURE PROPERTIES OF LANGUAGES
'vVe discussed closure properties under union, concatenation, and so on in
Chapter 4. In this section we will discuss closure properties under intersection,
complementation, etc. Recall that
are the families of type 0
languages, context-sensitive languages. context-free languages and regular
languages, respectively.
Property 1
Each of the classes
, i fl is closed under union,
concatenation. closure and transpose operations (Theorems 4.5-4.7).
Property 2
is closed under intersection and complementation (Theorems
5.7 and 5.8).
Property 3
ten is not closed under intersection and complementation.
We establish property 3 by a counter-example. We have already seen that
L 1 = {a"bile" In 2 1. i 2 O} and L~ = {c/b"e" In 2 1, j 2 O} are context-free
languages (Examples 4.8 and 4.9). L) (\ I~ = {d'H'd'ln 2 I}. In Example 6.18.
we have shown that {a"blle"! n 2 I} is not context-free. Thus.
is not
closed under intersection.
Using DeMorgan's law. we can write I) (\
I~ = (LI U L'2),. We have
proved in Chapter 4 that
is closed under union. If
were closed under
complementation. then L) (i L~ turns out to be context-free which is not true.
Hence.
is not closed under complementation.
8.4
SUPPLEMENTARY
EXAMPLES
EXAMPLE 8.4
Show that the grammar 5 ---+ aAc, A ---+ Abb I b is an LR(O) grammar.
Solution
It is easy to see that L(G) = {ab2"+lc In 2 O}. The sentential forms of G are
aAc. aAb2"c,
ab~l1+ic. We consider the last production applied in the derivation
of ab~I1+1c. As aAc. Abb and b are the possible right-hand sides of productions,
Chapter 8: LR(k) Grammars
~
273
only A --+ b can be the last production in the rightmost derivation of aPIl+lc.
(We do not have (lAc and Abb as substrings of ab2n+] c). Similarly the last
productions for aAc and aAb21lc are 5 --+ aAc and A --+ Abb respectively.
Hence G is an LR(O) grammar.
EXAMPLE 8.5
Show that S --+ aAb, A --+ cAc I c is not LR(k) for any natural number k.
Solution
It is easy to see that
L(G) = {ac2n+1b 111 2': O}
Consider aeccb E L(G). The last production is A --+ c. But we can apply this
handle only by knowing the entire string. Tnis can be applied to the middle c
but this is knO\vn only after looking at two symbols beyond the c which
replaces A. Continuing this argument, we can decide the handle of ac21l+1b by
only looking at n + 1 symbols beyond the c which replaces A. So it is not
LR(k) for any k.
EXAMPLE 8.6
Give an example of a language which can be generated by an LR(k) gram..'11ar
for some k and also by a grammar that is not LR(k) for any k.
Solution
Consider {ac21l+1b In :::: O}. This is generated by the grammar 5 --+ aAb.
A --+ cAc Ic which is not LR(k) for any k.
This
language can
also be generated by the grammar S
--+ aAb,
A --+ Acc Ic. This is LR(O). (This grammar is similar to the grammar in
Example 8A.)
SELF-TEST
Choose the correct answer to Questions 1-5:
1. An LR(k) grammar has to be
(a) a type 0 grammar
(b) a type 1 grammar
(c) a type 2 grammar
(d) none of these.
~
An LR(k) gram..'11ar is
(a) ahvays unambiguous
(b) always ambiguous
(c) need not be unambiguous
(d) none of these.
274
g,
Theory ofComputer Science
3. A handle is
(a) a string of variables and terminals
(b) a string of variables
(c) a string of terminals
(d) a production.
4. The automaton corresponding to an LR(k) grammar is
(a) a deterministic finite automaton
(b) a nondeterministic finite automaton
(c) a deterministic PDA
(d) a nondeterministic PDA.
5.
J:dcfl is closed under
(a) union
(b) complementation
(c) intersection
(d) none of these.
EXERCISES
8.1 Show that the grammar 5 ~ aAb. A ~ aAb I a is an LR(l) or is it an
LR(O)?
8.2 Show that the grammar 5 ~ OA2, A ~ L4.1, A ~ 1 is not an LR(O).
8.3 Is 5 ~ AB, 5 ~ aA, A ~ aA, A ~ a, B ~ a an LR(k) for some k?
8.4 Show that {a
lllb
ll1c" IIn, 11 2': I} u {a
lllb"c
l1 IIn, 11 2': 1} cannot be generated
by an LR(k) grammar for any k.
8.5 Are the following statements true? (a) If G is unambiguous, it is LR(k)
for some k. (b) If G is unambiguous. it is LR(k) for every k. Justify your
answer.
8.6 Is 5 ~ C ID, C ~ aC Ib, D ~ aD Ian LR(O)?
8.7 For a production A ~ f3 of a context-free grammar G and w in L*$k
($ is a symbol not in Vv U L), define RkC'rv) to be the set of all strings
of the form
af3~v such that A ~ f3 is a handle for af3ww' for some w'
in L* $* and 5$ :b aA,nv' => af3'vw'. (In other words, a string af3w is
R
R
in Rk(w) if we get a penultimate step of a rightmost derivation of af3ww'
for some w'.) Show that Rk(w) is a regular set.
[Hint: Define G' = (V'v, v.v U L, p', 5'), where
Vv= {[A, w] IA E VN, WE p$k and Iwi = k}
5' = [5, $k]
Chapter 8: LR(k) Grammars
~
275
Also show that each production in P induces a production in p' in the
following manner:
(a) If A ~ x is in P, where x E l:*, then [A w] ~ XH' is included in
p'.
(b) If A ~ XIX~ ... XII/ is in P, Xi E
Vty, Xi+ l ... XII/H' ~ w'w" for
some w', w" in l:*$* with Iw/l = k, then [A
lV] ~ BI ••. Bi- 1
[Bi, H"] is included in V
As the productions in p' have either a terminal string or a terminal string
followed by a vmiable on R.H.S., G' can be reduced to an equivalent
regular grammar. Use the principle of induction to show that
[5, $k] ~ [A, w] if and only if for some w'S$k '? aAww
l
This will establish L(G/) = Rk(w).
8.8 Prove that a context-free grammar G is an LR(k) if and only if the
follO\ving holds: A string y in Rk(v,') corresponding to a production
Al ~ 131 is a substring of some element 8 in Rk(W/) corresponding to a
production A~ ~
f3~ implies y = 8. Al = A~, 131 = 132'
[Hint: The proof follows from the definition of LR(k) grammars.]
8.9 Prove Property 2 of Section 8.2.
Solution
We give an outline of the construction of the required dpda
A A accepts a string w if S$k remains in the stack after the processing
of w. For this purpose. A has to simulate the reverse derivation of w.
This is achieved by finding suitable handles. Rk(w)'s are defined
precisely for this purpose (refer to Exercise 8.7). As Rk(w)'s are regular.
there exist deterministic finite automata Mk(w) corresponding to Rk(w).
For our deterministic pda A to contain the information regarding the
finite automata. the pushdown store of A is required to have an additional
track. In the first track. symbols from Vv u l: are written or erased. In the
additional track, the information regarding Mk(w)'s is stored in the fOlm
of maps. The map Na gives the states of finite automata lvh(w) after the
processing of a string a for all productions in G and strings w in l:*$*
of length k. The existence of a suitable handle is indicated by a final state
of Mk(w) (in the second track).
We can describe the way A acts as follows: A is capable of reading
k + 1 symbols on PDS, where 1 is the length of the longest R.H.S. of
productions of G. (This can be achieved by modifying the finite control.)
A reads the top m symbols on track 1 for some m ~ k + l. The second track
is suitably manipulated. For example. if Xl ... Xm is in track 1, then
track 2 stores the maps NYINxIX2 ... Ny!
..
XII1' If a suitable handle is
found, then a sentential form is obtained by replacing the R.H.S. of the
276
g
Theory ofComputer Science
handle of its L.H.S. in the given string on track 1. As G is LR(k), this
can be done in almost one way.
The above process is repeated until it is no longer possible. If Tn
symbols are not sufficient to carry out the process, more symbols are read
and placed on the stack (track 1).
If stack 1 has S$k at a particular stage, A accepts the corresponding
string. A is the required dpda accepting L(G).
Turing Machines
and Linear Bounded
Automata
In the early 1930s. mathematicians were trying to define effective computation.
Alan Turing in 1936. Alanzo Church in 1933, S.C. Kleene in 1935, Schonfinkel
in 1965 gave various models using the concept of Turing machines, JL-calculus,
combinatory logic, post-systems and p-recursive functions. It is interesting to
note that these were formulated much before the electro-mechanicaVelectronic
computers were devised. Although these formalisms, describing effective
computations. are dissimilar, they tum to be equivalent.
Among these formalisms, the Turing's formulation is accepted as a model
of algorithm or computation. The Church-Turing thesis states that any
algorithmic procedure that can be carried out by human beings/computer can be
carried out by a Turing machine. It has been universally accepted by computer
scientists that the Turing machine provides an ideal theoretical model of a
computer.
Turing machines are useful in several ways. As an automaton, the Turing
machine is the most general model. It accepts type-O languages. It can also be
used for computing functions. It turns out to be a mathematical model of partial
recursive functions. Turing machines are also used for determining the un-
decidability of certain languages and measuring the space and time complexity
of problems. These are the topics of discussion in this chapter and some of the
subsequent chapters.
For fonnalizing computability, Turing assumed that, while computing,
a person writes symbols on a one-dimensional paper (instead of a two-
d;rnensional paper as is usually done) which can be viewed as a tape divided
into cells.
One scans the cells one at a time and usually performs one of the three
simple operations, namely (i) writing a new symbol in the cell being currently
277
278
l;!
Theory of Computer Science
scanned, (ii) moving to the cell left of the present celL and (iii) moving to the
cell light of the present cell. With these observations in mind, Turing proposed
his 'computing machine.'
9.1
TURING MACHINE MODEL
The Turing machine can be thought of as finite control connected to a R/W
(read/write) head. It has one tape which is divided into a number of cells. The
block diagram of the basic model for the Turing machine is given in Fig. 9.1.
RIW head
Finite control
Tape divided into cells
and of infinite length
Fig. 9.1
Turing machine model.
Each cell can store only one symbol. The input to and the output from the finite
state automaton are effected by the R!W head which can examine one cell at
a time. In one move, the machine examines the present symbol under the
R!W head on the tape and the present state of an automaton to determine
(i) a new symbol to be written on the tape in the cell under the RAY head,
(ii) a motion of the RAY head along the tape: either the head moves one
cell left (L). or one cell right (R),
(iii) the next state of the automaton, and
(iv) whether to halt or not.
The above model can be rigorously defined as follows:
DefInition 9.1 A Turing machine M is a 7-tuple, namely (Q, :E, r, 8, qo. b, F),
where
1. Q is a finite nonempty set of states.
')
r is a finite nonempty set of tape symbols,
3. b E r is the blank.
4.
:E is a nonempty set of input symbols and is a subset of rand b E :E.
5. 8 is the transition function mapping (q, x) onto (qt, y, D) where D
denotes the direction of movement of R!W head: D =L or R according
as the movement is to the left or right.
6. qo E Q is the initial state, and
7. F r;;;; Q is the set of final states.
Chapter 9: Turing Machines and Linear Bounded Automata
~
279
Notes:
(1) The acceptability of a string is decided by the reachability from the
initial state to some final state. So the final states are also called the accepting
states.
(2) (5 may not be defined for some elements of Q x r.
9.2
REPRESENTATION OF TURING MACHINES
We can describe a Turing machine employing (i) instantaneous descriptions
using move-relations. (ii) transition table. and (iii) transition diagram (transition
graph).
9.2.1
REPRESENTATION BY INSTANTANEOUS DESCRIPTIONS
.Snapshots' of a Turing machine in action can be used to describe a Turing
machine. These give 'instantaneous descriptions' of a Turing machine. We have
defined instantaneous descriptions of a pda in terms of the cUITent state. the
input string to be processed, and the topmost symbol of the pushdown store.
But the input string to be processed is not sufficient to be defined as the ill of
a Turing machine, for the R1\V head can move to the left as well. So an ill of a
Turing machine is defined in terms of the entire input string and the current
state.
Defmition 9.2
An ill of a Turing machine M is a string af3y, where f3 is the
present state of M, the entire input string is split as (Xl, the first symbol of y is
the current symbol (l under the RJW head and yhas all the subsequent symbols
of the input string, and the string ex is the substring of the input string formed
by all the symbols to the left of a.
EXAMPLE 9.1
A snapshot of Turing machine is shown in Fig. 9.2. Obtain the instantaneous
descliption.
~~
.LI_b----,--I_84----,FGJ;J 821 82~ bib I
~?
dj
IWhead
State
q3
Fig. 9.2
A snapshot of Turing machine.
Solution
The present symbol under the RJW head is al' The present state is Q3' So al
is written to the right of Q3' The nonblank symbols to the left of al form the
string a4(lj(l2(lja2L72, which is written to the left of Q3' The sequence of nonblank
symbols to the right of (ll is (14(12. Thus the ill is as given in Fig. 9.3.
280
J;;i,
Theory of Computer Science
Left sequence
I 8482
•
Right sequence
Present
Symbol under
state
RIW head
Fig. 9.3
Representation of 10.
Notes:
(1) For constructing the ID, we simply insert the current state in the
input string to the left of the symbol under the RIW head.
(2) We observe that the blank symbol may occur as part of the left or right
substring.
Moves in a TM
As in the case of pushdown automata, 8(q, x) induces a change in ID of the
Turing machine. We call this change in ID a move.
Suppose 8(q, Xj) =(P, y, L). The input string to be processed is X1X2 ... Xn,
and the present symbol under the RIW head is Xi' So the ID before processing
Xi is
After processing Xi, the resulting ID is
This change of ID is represented by
If i = 1, the resulting ID is p Y X2 X3 ••. XI/'
If 8(q, xJ = (p, y, R), then the change of ID is represented by
Xj X2'"
'Yi-1q xi'"
.In r- X j X2'"
Xi-lypXi+l'"
x"
If i = 11, the resulting ID is XjX2 ... Xn-l Y P b.
We can denote an ill by Ij for some j. Ij r- 1k defines a relation among IDs.
So the symbol f2- denotes the reflexive-transitive closure of the relation r-'
In particular, Ij f2- Ij . Also, if I] f2- In' then we can split this as II r- 12r-
Io r- ... r- I" for some IDs, 12, ... , 1,,-1'
Note:
The description of moves by IDs is very much useful to represent the
processing of input strings.
9.2.2
REPRESENTATION BY TRANSITION TABLE
We give the definition of 8 in the form of a table called the transition table. If
8(q, a) = (y, a. (3). we write a(3yunder the a-column and in the q-row. So if
Chapter 9: Turing Machines and Linear Bounded Automata
Q
281
we get exf3y in the table, it means that ex is written in the current cell, f3 gives
the movement of the head (L or R) and y denotes the new state into which the
Turing machine enters.
Consider, for example, a Turing machine with five states qj, ..., qs, where
ql is the initial state and qs is the (only) final state. The tape symbols are 0. 1
and b. The transition table given in Table 9.1 describes 8.
TABLE 9.1
Transition Table of a Turing Machine
Present state
Tape symbol
b
0
-'7q1
1Lq2
ORq1
q2
bRq3
OLq2
1Lq2
q3
bRq4
bRq5
q4
ORq5
ORQ4
1RQ4
®
OLQ2
As in Chapter 3. the initial state is marked with ~ and the final state
witho.
EXAMPLE 9.2
Consider the TM description given m Table 9.1. Draw the computation
sequence of the input string 00.
Solution
We describe the computation sequence in terms of the contents of the tape and
the current state. If the string in the tape is al(l2
G;(l;+l ... alii and the TM
in state q is to read aj+ 1, then we write a 1a2
G; q (l;+ 1 ••• all/'
For the input string OOb, we get the following sequence:
qtOOb r- OqtOb r- OOq,b r- Oq201 r- q2001
r- q2bOOl r- bq3001 r- bbq401 r- bboq41 r- bbo1q4b
r- bbOlOqs r- bb01q200 r- bbOq2100 r- bbq20100
r- bq2bOlOO r- bbq30100 r- bbbq4100 r- bbb j q400
r- bbblOq40 r- bbblOOq4b r- bbblOOOqsb
r- bbb100q200 r- bbb lOq2000 r- bbb1q20000
r-bbbq210000 r- bbq2b10000 r- bbbq310000 r- bbbbqsOOOO
9.2.3
REPRESENTATION BY TRANSITION DIAGRAM
We can use the transition systems introduced in Chapter 3 to represent Turing
machines. The states are represented by veltices. Directed edges are used to
282
J;t
Theory of Computer Science
represent transition of states. The labels are triples of the form (0::, [3, y), where
0::. [3. E rand y E {L R}. When there is a directed edge from qi to qj with label
(0::, [3. y), it means that
D(qi' 0::) = (qj, [3. y)
During the processing of an input string, suppose the Turing machine enters
qi and the RJW head scans the (present) symbol 0::. As a result the symbol [3
is written in the cell under the RJW head. The RJW head moves to the left: or
to the right depending on y, and the new state is CJj.
Every edge in the transition system can be represented by as-tuple (qi' 0::,
[3, y, qj)' So each Turing machine can be described by the sequence of 5-tuples
representing all the directed edges. The initial state is indicated by ~ and any
final state is marked with o.
EXAMPLE 9.3
M is a Turing machine represented by the transition system in Fig. 9.4. Obtain
the computation sequence of M for processing the input string 0011.
(b, b, R)
(y, y, R)
(y, y, L)
(y,y, R)
(x, x, R)
(0,0, L)
Fig. 9.4
Transition system for M.
Solution
t
bxOllb
The initial tape input is bOOllb. Let us assume that M is in state qj and the
RJW head scans 0 (the first 0). We can represent this as in Fig. 9.5. The figure
can be represented by
t
bOOllb
qj
From Fig. 9.4 we see that there is a directed edge from qj to q2 with the label
(0. x, R). So the current symbol 0 is replaced by x and the head moves right.
The new state is q2' Thus. we get
Chapter 9: Turing Machines and Linear Bounded Automata
~
283
The change brought about by processing the symbol 0 can be represented as
-t
J-
bOOllb
(O.x.R) > bxOllb
qj
q2
~b
Rrw head
Fig. 9.5
TM processing 0011.
The entire computation sequence reads as follows:
J-
J-
J-
bOOllb
(O.x.R!
bxOllb
(O.O.RI
bxOllb
)
~
ql
q2
q2
J-
J-
J-
(l.,\'.LI ) bxOylb
(O.O.L) )
bxOylb
(x.x.R)
bxOvlb
)
qo,
q4
ql
J-
IO.x.R) 'b
b
--~) xxvI
q2
J-
(\'.\'.R)
,.
) bxxylb
q2
J-
(1.\'.L)
,
) bxxyyb
qj
(".\,LI
J-
(x..t.R) b
J,
(".".R)
J,
"
>bxxyyb
) xxyyb
"
) bxxyyb
qo,
qs
qs
(\,. ,.R)
J,
(b.b.R)
J,
"
) bxxyyb
) bxxyybb
qs
q6
9.3
LANGUAGE ACCEPTABILITY BY TURING
MACHINES
I F't us consider the Turing machine M = (Q. '2:, 1. (5, qo, b. F). A string w in
'2:* is said to be accepted by M if qoVl' r-
(XIP(X2 for some P
E F and (x],
(X:c
E r*.
M does not accept VI' if the machine M either halts in a nonaccepting state
or does not halt.
284
g
Theory of Computer Science
It may be noted that though there are other equivalent definitions of
acceptance by the Turing machine, we will be not discussing them in this text.
EXAMPLE 9.4
Consider the Turing machine M described by the transltlOn table given III
Table 9.2. Describe the processing of (a) OIL (b) 0011, (c) 001 using IDs.
Which of the above stlings are accepted by M?
TABLE 9.2
Transition Table for Example 9.4
Present
state
o
xRq2
ORQ2
OLQ4
OLQ4
Tape symbol
x
y
b
Solution
(a) qjOll f- xq:11 f- q3xy1 f- xqsy1 f- x-.vqs1
As a(qs. 1) is not defined, M halts; so the input string 011 is not accepted.
(b) qjOOll f- xq:011 f- xOq:11 f- xq30y1 f-
q~\:Oyl f- xqjOyl.
f-xxq:y1 f- xX)·q:1 f- xxq3YY f- xq3'W y' f- xxqsYy
f- x.x;yqsY· f- xxyyqsb f- xxY.Vbq6
M halts. As q6 is an accepting state, the input string 0011 is accepted by M.
(c)
Cf jOOl f- xq:01 f- xOq:1 f- .vq30y f- q4xOy
f- xqlOy f- x.\:q:y f- xxyq:
M halts. As q: is not an accepting state, 001 is not accepted by M.
9.4
DESIGN OF TURING MACHINES
We now give the basic guidelines for designing a Turing machine.
(i) The fundamental objective in scanning a symbol by the RJW head is
to 'kno,," \'ihat to do in the future. The machine must remember the
past symbols scanned. The Turing machine can remember this by
going to the next unique state.
(ii) The number of states must be minimized. This can be achieved by
changing the states only when there is a change in the written symbol
or when there is a change in the movement of the RJW head. We shall
explain the design by a simple example.
Chapter 9: Turing Machines and Linear Bounded Automata
g
285
EXAMPLE 9.5
Design a TUling machine to recognize all stlings consisting of an even number
of 1's.
Solution
The construction is made by defining moves in the following manner:
(a) ql is the initial state. M enters the state q2 on scanning 1 and writes b.
(b) If M is in state q2 and scans 1, it enters q, and writes b.
(c) q] is the only accepting state.
So M accepts a stJing if it exhausts all the input symbols and finally is in
state qj. Symbolically,
M = ({qj, q2}, {I. b}, {l, b}, 8, q, b. {qd)
\"here 8 is defined by Table 9.3.
TABLE 9.3
Transition Table for Example 9.5
Present state
Let us obtain the computation sequence of 11. Thus, q j ll f- bq21 f- bbql'
As ql is an accepting state. 11 is accepted. qllil f- bq211 f- bbq]l f- bbbq2'
Af halts and as q2 is not an accepting state, III is not accepted by M.
EXAMPLE 9.6
Design a Turing machine over {I. b} which can compute a concatenation
function over L = {I}. If a pair of words (Wj. 11'2) is the input. the output has
to be W(H'2'
Solution
Let us assume that the two words ,Vj and W2 are written initially on the input
tape separated by the symbol b. For example, if 11'] = 11, W2 = 111. then the
input and output tapes are as shown in Fig. 9.6.
G]1!1=
Fig. 9.6
Input and output tapes.
We observe that the main task is to remove the symbol b. This can be done
in the following manner:
(a) The separating symbol b is found and replaced by 1.
286
~
Theory of Computer Science
(b) The rightmost 1 is found and replaced by a blank b.
(c) The RJW head returns to the starting position.
A computation is illustrated in Table 9.4.
TABLE 9.4
Computation for 11 h111
qo11b111 f- 1qo1b111 f- 11qob111 f- 111q1 111
f- 1111q1 11 f- 11111q1 1 f- 111111q1 b f- 11111q21b
f- 1111 q31 bb f- 111 q311 bb f- 11 q3111 bb f- 1q31111 bb
f- q311111bb f- q3b11111bb f- bqf11111bb
From the above computation sequence for the input string 11b11 L we can
construct the transition table given in Table 9.5.
For the input string Ibl, the computation sequence is given as
qolblI-lqobl 1- llql 11- 11lq j b r- 11q2b r- 1q31bb
r- q311bb r- q3bIlbb r- bqfl1bb.
TABLE 9.5
Transition Table for Example 9.6
Present state
Tape symbof
b
---'fqo
1Rqo
1Rq1
q1
1Rq1
bLq2
q2
bLq3
q3
1Lq3
bRqf
@
EXAMPLE 9.7
Design a TM that accepts
{O"I"ln 2: l}.
Solution
We require the following moves:
(a) If the leftmost symbol in the given input string IV is 0, replace it by x
and move right till we encounter a leftmost 1 in ).i'. Change it to y and
move backwards.
(b) Repeat (a) with the leftmost O. If we move back and forth and no 0 or
1 remains. move to a final state.
(c) For strings not in the form 0"1", the resulting state has to be nonfinal.
where
Chapter 9: Turing Machines and Linear Bounded Automata
I!O!
287
Keeping these ideas in our mind, we construct a TM M as follows:
M = (Q, L, r, 0, qo, b, F)
Q = {qo, qj, q2' q3' qt)
F = {qt}
L = {O, I}
r = {O, 1, x, y, b}
The transition diagram is given in Fig. 9.7. M accepts {0
111
11
1n ;:::: I}. The moves
for 0011 and 010 are given below just to familiarize the moves of M to the
reader.
(0,0, R)
(y, y, R)
(x, x, R)
(y,y, R)
(y, Y, L)
(0,0, L)
Transition diagram for Example 9.7.
+
rt:::\
(b, b, R)
f0.,
(y, Y, R) ~f-----------I'~
Fig. 9.7
qo0011 r- xq j011j- xOq j 11 1- xq20yl
r- q2xOy1 1- xqoOy11- xxqjy11- xxyq j l
r- xxq2..1')' r- xChJ:YY r- xxqoYy r- x·\yq3Y
r- :'oyyq3 = xxyyq3b r- xxyybq.<,b
Hence 0011 is accepted by M.
qoOlO r- xq j lO r- q2·rvO r- xqayO r- xyq30
As 0(Q3' 0) is not defined, M halts. So 010 is not accepted by M.
·-EXAMPLE 9.8
Design a Turing machine M to recognize the language
{1"2"3"ln ;:::: I}.
288
g
Theory of Computer Science
Solution
Before designing the required Turing machine M, let us evolve a procedure for
processing the input stJing 112233. After processing, we require the ID to be
of the form bbbbbbq;. The processing is done by using five steps:
Step 1
qj is the initial state. The RJW head scans the leftmost 1, replaces 1
by b, and moves to the right. M enters q2'
Step 2
On scanning the leftmost 2, the RJW head replaces 2 by b and moves
to the right. M enters q3'
Step 3
On scanning the leftmost 3. the RJW head replaces 3 by b, and moves
to the right. M enters q4'
Step 4
After scanning the rightmost 3, the RJW heads moves to the left until
it finds the leftmost 1. As a result. the leftmost 1. 2 and 3 are replaced by b.
Step 5
Steps 1-4 are repeated until alll's, 2's and 3's are replaced by blanks.
The change of IDs due to processing of 112233 is given as
qj 112233 1- bq212233 1- blq22233 1- blbq3233 1- blb2q333
r- blb2bq..j31- blb2qsb3 1- b1bqs2b3 1- b1qsb2b3 1- bqs1b2b3
r- q6b1b2b31- bq]lb2b31- bbq2b2b3 1- bbbq22b3
r- bbbbq3b3 1- bbbbbq33 1- bbbbbbq..jb r- bbbbbq;bb
Thus.
q\112233 ~ q7bbbbbb
As q7 is an accepting state, the input string 112233 is accepted.
Now we can construct the transition table for M. It is given in Table 9.6.
TABLE 9.6
Transition Table for Example 9.7
Present state
Input tape symbol
2
3
b
-'>q.,
bRq2
q2
1Rq2
bRq3
q3
2Rq3
bRq4
q4
3Lqs
qs
1Lqa
2Lqs
qs
1Lqs
(~
bRq1
bRq2
bRq3
bLq7
bLQs
bRQ1
It can be seen from the table that strings other than those of the form 0"1"2"
are not accepted. It is advisable to compute the computation sequence for
strings like 1223, 1123. 1233 and then see that these strings are rejected by M.
Chapter 9: Turing Machines and Linear Bounded Automata
j;J,
289
9.5
DESCRIPTION OF TURING MACHINES
In the examples discussed so far, the transition function 8 was described as a
partial function (function 8: Q x r ~ Q x r x {L. R} is not defined for all
(q, x» by spelling out the current state, the input symbol, the resulting state, the
tape symbol replacing the input symbol and the movement of R/W head to the
left or right. We can call this a formal description of a TM. Just as we have the
machine language and higher level languages for a computer. we can have a
higher level of description, called the implementation description. In this case
we describe the movement of the head, the symbol stored etc. in English. For
example, a single instruction like 'move to right till the end of the input string'
requires several moves. A single instruction in the implementation description
is equivalent to several moves of a standard TM (Hereafter a standard TM
refers to the TM defined in Definition 9.1). At a higher level we can give
instructions in English language even without specifying the state or transition
function. This is called a high-level description.
In the remaining sections of this chapter and later chapters, we give
implementation description or high-level description.
9.6
TECHNIQUES FOR TM CONSTRUCTION
In this section we give some high-level conceptual tools to make the
construction of TMs easier. The Turing machine defined in Section 9.1 is called
the standard Turing machine.
9.6.1
TURING MACHINE WITH STATIONARY HEAD
In the definition of a TM we defined 8(q, a) as (q', y, D) where D =L or R.
So the head moves to the left or right after reading an input symbol. Suppose,
we want to include the option that the head can continue to be in the same cell
for some input symbol. Then we define 8(q, a) as (q', y, 5). This means that
the TM, on reading the input symbol a, changes the state to q' and writes y in
the current cell in place of a and continues to remain in the same cell. In terms
of IDs,
wqax r- 'wq'yX
Of course, this move can be simulated by the standard TM with two moves.
namely
H'qCV: r- vryq"x r- wq'yx
That is, 8(q, a) = (q', y, 5) is replaced by 8(q, a) = (q", y, R) and 8(q", X) =
(q. y, L) for any tape symbol X.
Thus in this model 8(q. a) = (q', y, D) where D =L. R or S.
290
);l
Theory of Computer Science
9.6.2
STORAGE IN THE STATE
Weare using a state, whether it is of a FA or pda or TM, to 'remember' things.
We can use a state to store a symbol as well. So the state becomes a pair
(q, a) where q is the state (in the usual sense) and a is the tape symbol stored
in (q, a). So the new set of states becomes Q x r.
EXAMPLE 9.9
Construct a TM that accepts the language 0 1* + 1 0*.
Solution
We have to construct a TM that remembers the first symbol and checks that it
does not appear afterwards in the input string. So we require two states, qa, qj.
The tape symbols are 0, 1 and b. So the TM, having the 'storage facility in
state'. is
M = ({qa, qd x {O. L b}, {O, I}, {O, 1, b}, 0, [cIa, b], {[Cf], bJ})
We desClibe 0 by its implementation description.
L In the initial state, M is in qa and has b in its data portion. On seeing
the first symbol of the input sting w, M moves right, enters the state
Cft and the first symbol. say a, it has seen.
2. M is now in [q], a). (i) If its next symbol is b, M enters [cIt- b), an
accepting state. (ii) If the next symbol is a, M halts without reaching
the final state (i.e. 0 is not defined). (iii) If the next symbol is a
(a =°if a = 1 and a = 1 if a =0), M moves right without changing
state.
3. Step 2 is repeated until M reaches [qj, b) or halts (0 is not defined for
an input symbol in vv).
9.6.3
MULTIPLE TRACK TURING MACHINE
In the case of TM defined earlier, a single tape was used. In a multiple track
TM. a single tape is assumed to be divided into several tracks. Now the tape
alphabet is required to consist of k-tuples of tape symbols, k being the number
of tracks. Hence the only difference between the standard TM and the TM with
multiple tracks is the set of tape symbols. In the case of the standard Turing
machine, tape symbols are elements of r; in the case of TM with multiple track,
it is r k. The moves are defined in a similar way.
9.6.4
SUBROUTINES
We know that subroutines are used in computer languages, when some task has
to be done repeatedly. We can implement this facility for TMs as well.
Chapter 9: Turing Machines and Linear Bounded Automata
~
291
First a TM program for the subroutine is written. This will have an initial
state and a 'return' state. After reaching the return state. there is a temporary
halt. For using a subroutine, new states are introduced. When there is a need
for calling the subroutine, moves are effected to enter the initial state for the
subroutine (when the return state of the subroutine is reached) and to return to
the main program of TM.
We use this concept to design a TM for perfonning multiplication of two
positive integers.
EXAMPLE 9.10
Design a TM which can multiply two positive integers.
Solution
The input (m, 11). m.
11 being given, the positive integers are represented by
0
11110". M starts with 0
11110" in its tape. At the end of the computation,
O"ill(mn in unary representation) sUlTounded by b's is obtained as the ouput
The major steps in the construction are as follows:
1.
OIl! 10
111 is placed on the tape (the output will be written after the
rightmost 1).
2. The leftmost °is erased.
3. A block of 11 O's is copied onto the right end.
4. Steps 2 and 3 are repeated
111 times and 10
1"10""
1 is obtained on the
tape.
5. The prefix 101/11 of 101/110
/11
" is erased. leaving the product mn as the
output.
For every 0 in Olil. 0" is added onto the right end. This requires repetition
of step 3. We define a subroutine called COPY for step 3.
For the subroutine COPY. the initial state is qj and the final state is qs. (5
is given by the transition table (see Table 9.7).
TABLE 9.7
Transition Table for Subroutine COpy
State
Tape symbol
°
2
b
q22R
q41L
q20R
q21R
q30L
q30L
q31L
q1 2R
Qs1R
Q40L
The Turing machine M has the initial state qo. The initial ill for M is
CfoO
Ill 10"1. On seeing 0. the following moves take place (q6 is a state of M).
CfrP"10
1I1 t- bq601ll-1101I1 ~ bO
Ill
- 1q6 1O"1 t- bOIll
- 11q j O"1. qj is the initial state
292
~
Theory of Computer Science
of COPY. The TM Ail performs the subroutine COPY. The following moves
take place for M 1: q101711- 2q=:017-11 P- 20n-11q3b f- 20n-1q31O P- 2q j O"-llO.
After exhausting O·s. q1 encounters 1. M 1 moves to state q4' All 2's are
converted back to 0'sand M 1 halts in qs. The TM M picks up the computation
by starting from qs. The qo and q6 are the states of M. Additional states are
created to check whether each °in 0
11I gives rise to 0
11I at the end of the
rightmost 1 in the input string. Once this is over, M erases 10"1 and finds 0"111
in the input tape.
M can be defined by
M = ({qo. qj, .... qd· {O. I}, {O, 1,2, b}, 8, qo, b. {qd)
where 8 is defined by Table 9.8.
TABLE 9.8
Transition Table for Example 9.10
°
2
qo
q6bR
q6
q60R
q, 1R
q5
q70L
q7
qs1L
qs
qgOL
qg
qgOL
q,0
q" bR
q,1
q" bR
q,2bR
b
q1QbR
qobR
Thus M performs multiplication of two numbers in unary representation.
9.7
VARIANTS OF TURING MACHINES
The Turing machine we have introduced has a single tape. 8(q, a) is either a
single triple (p, y, D), where D = R or L, or is not defined. We introduce two
new models of TM:
(i) a TM with more than one tape
(ii) a TM where 8(q. a) = {(PJo YJ, D j ), (P=:, Y=:. D 2), •••• (p,., Yn Dr)}' The
first model is called a multitape TM and the second a nondeterministic
TM.
9.7.1
MULTITAPE TURING MACHINES
A multitape TM has a finite set Q of states. an initial state qo. a subset F of Q
called the set of final states. a set P of tape symbols. a new symbol b. not in
P called the blank symbol. (We assume that :2: ~ rand b
EO :2:.)
Chapter 9: Turing Machines and Linear Bounded Automata
,g
293
There are k tapes. each divided into cells. The first tape holds the input
string w. Initially. all the other tapes hold the blank symbol.
Initially the head of the first tape (input tape) is at the left end of the input
w. All the other heads can be placed at any cell initially.
(5 is a partial function from Q x rk into Q x r k x {L, R, S}k. We use
implementation description to define (5. Figure 9.8 represents a multitape TM.
A move depends on the current state and k tape symbols under k tape heads.
Fig. 9.8
Multitape Turing machine.
In a typical move:
(i) M enters a new state.
(ii) On each tape. a new symbol is written in the cell under the head.
(iii) Each tape head moves to the left or right
or remains stationary. The
heads move independently: some move to the left, some to the right
and the remaining heads do not move.
The initial ill has the initial state Cfo, the input string }v in the first tape
(input tape), empty strings of b's in the remaining k - 1 tapes. An accepting ill
has a final state. some strings in each of the k tapes.
Theorem 9.1
Every language accepted by a multitape TM is acceptable by
some single-tape TM (that is, the standard TM).
Proof
Suppose a language L is accepted by a k-tape TM M. We simulate M
with a single-tape TM with 2k tracks. The second. fourth, ..., (2k)th tracks hold
the contents of the k-tapes. The first. third, ..., (2k - l)th tracks hold a head
marker (a symbol say X) to indicate the position of the respective tape head.
We give an 'implementation description' of the simulation of M with a single-
tape TM MI' We give it for the case k =2. The construction can be extended
to the general case.
Figure 9.9 can be used to visualize the simulation. The symbols A 2 and B5
are the current symbols to be scanned and so the headmarker X is above the two
symbols.
-
--------- ---------
294
x;;I
Theory of Computer Science
I
Finite I
control
/
X
\
A2
t
X
81
8 2
8 3
84
8 5
Fig. 9.9
Simulation of multitape TM.
Initially the contents of tapes 1 and 2 of M are stored in the second and
fourth tracks of MI' The headmarkers of the first and third tracks are at the cells
containing the first symbol.
To simulate a move of fill. the 2k-track TM M1 has to visit the two
headmarkers and store the scanned symbols in its control. Keeping track of the
headmarkers visited and those to be visited is achieved by keeping a count and
storing it in the finite control of MI' Note that the finite control of M1 has also
the infoffilation about the states of M and its moves. After visiting both head
markers. M1 knows the tape symbols being scanned by the two heads of M.
Now /'111 revisits each of the headmarkers:
(il It changes the tape symbol in the cOlTesponding track of M1 based
on the information regarding the move of M corresponding to the state
(of M) and the tape symbol in the corresponding tape M.
(ii) It moves the headmarkers to the left or right.
(iii) M1 changes the state of M in its control.
This is the simulation of a single move of M. At the end of this, M) is ready
to implement its next move based on the revised positions of its headmarkers
and the changed state available in its control.
M) accepts a string \t' if the new state of M, as recorded in its control at
the end of the processing of H'. is a final state of M.
Definition 9.3
Let M be a T~I and tV an input string. The running time of M
on input w. is the number of steps that A! takes before halting. If M does not
halt on an input string w, then the running time of M on 'v is infinite.
Note:
Some TMs may not halt on all inputs of length n. But we are interested
in computing the running time. only when the TM halts.
Definition 9.4
The time complexity of TM 1''11 is the function T(n), n being the
input size, where T(n) is defined as the maximum of the running time of Mover
all inputs w of size n.
Theorem 9.2
If fv1) is the single-tape TA! simulating multitape TM M, then
the time taken by lli![ to simulate n moves of M is
O(n~).
Chapter 9: Turing Machines and Linear Bounded Automata
J;\
295
Proof
Let AI be a k-tape TM. After 11 moves of M, the head markers of M]
will be separated by 211 cells or less. (At the worst. one tape movement can be
to the left by 11 cells and another can be to the right by II cells. In this case the
tape headmarkers are separated by 211 cells. In the other cases, the 'gap'
between them is less). To simulate a move of M, the TM M] must visit all the
k headmarkers. If M starts with the leftmost headmarker, M I will go through all
the headmarkers by moving right by at most 211 cells. To simulate the change
in each tape. M] has to move left by at most 271 cells; to simulate changes in
k tapes, it requires at most two moves in the reverse direction for each tape.
Thus the total number of moves by M 1 for simulating one move of M is
atmost 411 + 2k. (211 moves to light for locating all headmarkers, 211 + 2k moves
to the left for simulating the change in the content of k tapes.) So the number
of moves of M] for simulating n moves of M is 11(411 + 2k). As the constant k
is independent of 11, the time taken by M] is O(n:;).
9.7.2
NONDETERMINISTIC TURING MACHINES
In the case of standard Turing machines (hereafter we refer to this machine as
deterministic TM). 8(q). a) was defined (for some elements of Q x n as an
element of Q x r x {L R}. Now we extend the definition of 8. In a
nondetemlinistic TM. 8(ql, a) is defined as a subset of Q x r x {L R}.
Defmition 9.5
A nondeterministic Turing machine is a 7-tuple (Q, L r. 8, qo.
b. F) where
1. Q is a finite nonempty set of states
2. r is a finite nonempty set of tape symbols
3. b E r is called the blank symbol
4. L is a nonempty subset of 1. called the set of input symbols. We
assume that bEL.
5. qo is the initial state
6. F r;;;; Q is the set of final states
7. 8 is a partial function from Q x r into the power set of Q x r x
{L. R}.
1Vote:
If q
E Q and x
E rand 8(q. x) = {(ql. :\'), D 1). (q:;, .\':;, D:;). ...,
(q",
)'11' Dill) then the NTM can chose anyone of the actions defined by
(qi' )'i, DJ for i = 1. 2..... 11.
We can also express this in terms of f- relation. If 8(q. x) = {(qi, )ii, DJI
i =1. 2...., 11} then the ID zq.nv can change to anyone of the 11 IDs specified
by the l1-element set 8(q. x).
Suppose 8(q, x) = {(q], .\'1, L). (q:;, ":;. R). (Q3, \'3, L)}. Then
or
or
296
~
Theory of Computer Science
So on reading the input symbol, the NTM M whose cun-ent ID is 2]2: ...
ZkqxZk+] ... 2/1 can change to anyone of the three IDs given earlier.
Remark
When o(q, x) = {(qi' Yi, D i)Ii = 1. 2, .... n} then NTM chooses any
one of the n triples totally (that is, it cannot take a state from one triple, another
tape symbol from a second tliple and a third D(L or R) from a third triple, etc.
Definition 9.6
W E L* is accepted by a nondetenninistic TM M if qaw ~
xqfY for some final state qt.
The set of all strings accepted by M is denoted by T(M).
Note: As in the case of NDFA, an ID of the fonn xqy (for some q tt: F) may
be reached as the result of applying the input string w. But 'v is accepted by M
as long as there is some sequence of moves leading to an ID with an accepting
state. It does not matter that there are other sequences of moves leading to an
ID with a nonfinal state or TM halts without processing the entire input stling.
Theorem 9.3
If M is a nondeterministic TM, there is a deterministic TM M j
such that T(M) = TUY!I)'
Proof
We constmct M] as a multitape TM. Each symbol in the input string
leads to a change in ID. M] should be able to reach all IDs and stop when an
ID containing a final state is reached. So the first tape is used to store IDs of
M as a sequence and also the state of M. These IDs are separated by the symbol
* (induded as a tape symbol). The cun-ent ID is known by marking an x along
with the ID-separator * (The symbol * marked with x is a new tape symbol.)
All IDs to the left of the cun-ent one have been explored already and so can be
ignored subsequently. Note that the cun-ent ID is decided by the cun-ent input
symbol of w.
Figure 9.10 illustrates the deterministic TM M j •
Tape 1
Tape 2
x
101 * 102 * 103 * 104 * 105 * 106 * •.•
Fig. 9.10
The deterministic TM simulating M.
To process the current ID. M] perfOlIDs the follO\ving steps.
1. M j examines the state and the scanned symbol of the cun-ent ID. Using
the knowledge of moves of M stored in the finite control of Mjo M]
checks whether the state in the cun-ent ID is an accepting state of M.
In this case M I accepts and stops simulating M.
Chapter 9: Turing Machines and Linear Bounded Automata
J;!
297
2. If the state q say in the current ID xqa), is not an accepting state of M 1
and O(q, a) has k triples, M 1 copies the ID xqay in the second tape and
makes k copies of this ID at the end of the sequence of IDs in tape 2.
3. M j modifies these k IDs in tape 2 according to the k choices given by
O(q, a).
4. M1 returns to the marked current ID. erases the mark x and marks the
next ID-separator * with x (to the * which is to the left of the next ID
to be processed). Then M j goes back to step 1.
M j stops when an accepting state of M is reached in step 1.
Now M 1 accepts an input string IV only when it is able to find that M has
entered an accepting state, after a finite number of moves. This is clear from
the simulated sequence of moves of M j (ending in step 1)
We have to prove that M j will eventually reach an accepting ID (that is,
an ID having an accepting state of M) if M enters an accepting ID after n
moves. Note each move of M is simulated by several moves of M j •
Let m be the maximum number of choices that M has for various (q, a)'s.
(It is possible to find m since we have only finite number of pairs in Q x r.)
So for each initial ID of M. there are at most m IDs that M can reach after one
move. at most m2 IDs that I'v! can reach after two moves. and so on. So
corresponding to n moves of M, there are at most 1 + m + m2 + ... + mil moves
of M 1• Hence the number of IDs to be explored by M 1 is at most nm".
We assume that M] explores these IDs. These IDs have a tree structure
having the initial ID as its root. We can apply breadth-first search of the nodes
of the tree (that is. the nodes at level 1 are searched. then the nodes at level 2,
and so on.) If At reaches an accepting ID after n moves. then M1 has to search
atmost nm/! IDs before reaching an accepting ID. So. if M accepts lV, then M 1
also accepts lV (eventually). Hence T(M) = T(M j ).
9.8
THE MODEL OF LINEAR BOUNDED AUTOMATON
This model is important because (a) the set of context-sensitive languages is
accepted by the model. and (b) the infinite storage is restricted in size but not
in accessibility to the storage in comparison with the Turing machine model. It
is called the linear bounded automaton (LBA) because a linear function is used
to restrict (to bound) the length of the tape.
In this section we define the model of linear bounded automaton and
develop the relation between the linear bounded automata and context-sensitive
languages. It should be noted that the study of context-sensitive languages is
important from practical point of view because many compiler languages lie
between context-sensitive and context-free languages.
A linear bounded automaton is a nondetelministic Turing machine which
has a single tape whose length is not infinite but bounded by a linear function
298
g
Theory of Computer Science
of the length of the input string. The models can be described formally by the
following set format:
M = (Q. L, r. 8, qo, b, ¢ $, F)
All the symbols have the same meaning as in the basic model of Turing
machines with the difference that the input alphabet L contains two special
symbols ¢ and $. ¢ is called the left-end marker which is entered in the left-
most cell of the input tape and prevents the RIW head from getting off the left
end of the tape. $ is called the right-end marker which is entered in the right-
most cell of the input tape and prevents the RIW head from getting off the right
end of the tape. Both the endmarkers should not appear on any other cell within
the input tape, and the RIW head should not print any other symbol over both
the endmarkers.
Let us consider the input string w with II-vi = 11 - 2. The input string w can
be recognized by an LBA if it can also be recognized by a Turing machine
using no more than kn cells of input tape, where k is a constant specified in the
description of LBA. The value of k does not depend on the input string but is
purely a property of the machine. Wbenever we process any string in LBA, we
shall assume that the input string is enclosed within the endmarkers ¢ and $.
The above model ofLBA can be represented by the block diagram of Fig. 9.11.
There are t\\lO tapes: one is called the input tape, and the other, working tape.
On the input tape the head never prints and never moves to the left. On the
working tape the head can modify the contents in any way, without any
restriction.
n cells
cells
IR head moving to the right only
Finite state
RJW
l
control
head
knJ
~\
Working tape
Fig. 9.11
Model of linear bounded automaton.
Input
tape
In the case of LEA, an ID is denoted by (q, w. k), where q E O. w E r
and k is some integer between 1 and n. The transition of IDs is similar except
Chapter 9: Turing Machines and Linear Bounded Automata
~
299
that k changes to k - 1 if the RIW head moves to the left and to k + 1 if the
head moves to the right.
The language accepted by LBA is defined as the set
{w E (l: - {¢, $})*I(qo, ¢,v$, 1) rc-
(q, ex, i)
for some q E F and for some integer i between 1 and n},
Note:
As a null string can be represented either by the absence of input string
or by a completely blank tape, an LBA may accept the null string,
9.8.1
RELATION BETWEEN LBA AND CONTEXT-SENSITIVE
LANGUAGES
The set of strings accepted by nondeterministic LBA is the set of strings
generated by the context-sensitive grammars, excluding the null strings, Now
we give an important result:
If L is a context-sensitive language, then L is accepted by a linear bounded
automaton. The converse is also true.
The construction and the proof are similar to those for Turing machines
with some modifications.
9.9
TURING MACHINES AND TYPE 0 GRAMMARS
In this section we construct a type 0 grammar generating the set accepted by
a given Turing machine M. The productions are constructed in two steps. In
step 1 we construct productions which transform the string [ql¢ w$] into the
string [q2bJ, where qj is the initial state, q2 is an accepting state, ¢ is the left-
endmarker. and $ is the right-endmarker. The grammar obtained by applying
step 1 is called the transfonnational grammar. In step 2 we obtain inverse
production rules by reversing the productions of the transformational grammar
to get the required type 0 grammar G. The construction is in such a way that
11' is accepted by M if and only if w is in L(G).
9.9.1
CONSTRUCTION OF A GRAMMAR CORRESPONDING
TO TM
For understanding the construction. we have to note that a transition of ID
corresponds to a production. We enclose IDs within brackets. So acceptance of
,\ by M corresponds to the transformation of initial ID [ql ¢ W $] into [q2b].
Also, the 'length' of ID may change if the RIW head reaches the left-end or the
right-end, i.e. when the left-hand side or the right-hand side bracket is reached.
So we get productions corresponding to transition of IDs with (i) no change in
length, and (ii) change in length, We assume that the transition table is given,
300
~
Theory of Computer Science
We now describe the construction which involves two steps:
Step 1
(i) No change in length of IDs: (a) Right move.
akRqt corresponding
to qt-row and arcolumn leads to the production
qta) -+ ak-Cft
(b) Left move.
akLqt cOlTesponding to qt-row and arcolumn yields several
productions
for all am E r
(ii) Change in length of IDs: (a) Left-end.
akLqt cOlTesponding to q/-row
and arcolumn gives
[q;a; -+ [cltbak
When b occurs next to the left-bracket, it can be deleted. This is achieved
by including the production [b -+ [.
(b) Right-end.
When b occurs to the left of ], it can be deleted. This is
achieved by the production
a;b] -+ a;J
for all aj E r
When
the RJW head moves
to the right of ], the
length
mcreases.
Corresponding to this \ve have a production
q;] -+ qtb]
for all qt E Q
(iii) lmrodllction ofendmarkers.
For introducing endmarkers for the input
string, the following productions are included:
at -+ [qj ¢ a;
for at E r. at 1= b
for all at E r, at 1= b
For removing the brackets from [q2b], we include the production
[q2b] -+ S
Recall that qj and q2 are the initial and final states, respectively.
Step 2
To get the required grammar, reverse the arrows of the productions
obtained in step 1. The productions we get can be called inverse productions.
The new grammar is called the generative grammar. We illustrate the
construction \'lith an example.
EXAMPLE 9.11
Consider the TM described by the transition table given in Table 9.9. Obtain
the inverse production rules.
Solution
In this example. qj is both initial and final.
Step 1
(i) Prodllctions corresponding to right moves
(9.1)
Chapter 9: Turing Machines and Linear Bounded Automata
~
301
(ii) (a) Productions corresponding to left-end
[b ~ [
(b) Productions corresponding to rig',ot-end
(9.2)
bb] ~ b],
(iii)
1 ~ [qj¢1,
lb] ~ 1],
1 ~ IS],
ql] ~ qlb],
[qlb] ~ S
(9.3)
(9.4)
TABLE 9.9
Transition Table for Example 9.11
Present state
b
Step 2
The inverse productions are obtained by reversing the arrows of the
productions (9.1)-(9.4).
¢ql ~ ql¢
bq~ ~ ql l,
bql ~ q~l
[ ~ [b, b] ~ bb],
1] ~ lb]
qlb ~ ql],
q~b ~
q~].
[cII¢l ~ 1
1$] ~ 1,
S ~ [q1b]
Thus we have shown that there exists a type 0 grammar corresponding to
a Turing machine. The converse is also true (we are not proving this), i.e. given
a type 0 grammar G. there exists a Tming machine accepting L(G). Actually,
the class of recursively enumerable sets, the type 0 languages, and the class of
sets accepted by TM are one and the same. We have shown that there exists
a recursively enumerable set which is not a context-sensitive language (see
Theorem 4.4). As a recursive set is recursively enumerable, Theorem 4.4 gives
a type 0 language which is not type 1. Hence, 4s1 c.~ (d Property 4,
Section 4.3) is established.
9.10
LINEAR BOUNDED AUTOMATA AND LANGUAGES
A linear bounded automaton M accepts a string w if. after starting at the initial
state with RIW head reading the left-endmarker, M halts over the right-end-
marker in a final state. Otherwise, w is rejected.
The production rules for the generative grammar are constructed as in the
case of Turing machines. The following additional productions are needed in
the case of LBA.
Qiqr$ ~ qf$
¢lrS ~ ¢qr,
for all Qi E r
¢qf ~ qt
302
g
Theory of Computer Science
EXAMPLE 9.12
Find the grammar generating the set accepted by a linear bounded automaton
M whose transition table is given in Table 9.10.
TABLE 9.10
Transition Table for Example 9.12
Present state
Tape input symbol
~
$
0
---7q1
~Rq1
1Lq2
ORq2
q2
~Rq4
1Rq3
1Lq1
q3
$Lq1
1Rq3
1Rq3.
®
Halt
OLq4
ORq4
Solution
Step 1
(A) (i) Productiolls corresponding to right moves. The seven right
moves in Table 9.10 give the following productions:
(9.5)
q30 ~ lq3
q3 1 ~ lq3
q.+l ~ Oq.+
qlc): ~ c):qj.
qjl
~ OC):.
q:c): ~ C):C).+,
q:O ~ lq3
(ii) Productions corresponding to left moves. There are four left moves in
Table 9.10. Each left move yields four productions (corresponding to the four
tape symbols). These are:
(a) lLq: corresponding to ql-row and O-column gives
c):qjO ~ q:¢l, SqjO ~ C)2$}, OqlO ~ q201, lq jO ~ q211
(9.6)
(b) lLql corresponding to qj-rmv and I-column yields
¢q:l ~ ql¢l, Sq:l ~ qjSL Oq:l ~ ql0!, lq:l ~ qlll
(9.7)
(c) SLqj corresponding to qrro\V and $-column gives
c):q3$ ~ qj¢$, Sq3$ ~ CJlS$, 0CJ3$ ~ CJjO$, lCJ3$ ~ CJll$
(9.8)
(d) OLq.+ corresponding to CJ.+-row and O-column yields
¢CJ.+O ~ CJ.+¢O, $q.+O ~ q.+$O, Oq.+O ~ q.+OO, lq.+O ~ q410
(9.9)
(B) There are no productions corresponding to change in length.
(C) The productions for introducing the endmarkers are
¢~ [C)l¢¢
$ ~ [CJ1¢$,
°~ [(1J¢0,
1 ~ [ql¢L
[q.+] ~ S
¢ -+ ¢$]
$ ~ $$]
°~ OS]
1 ~ 1$]
(9.10)
(9.11)
Chapter 9: Turing Machines and Linear Bounded Automata
J;!
303
(9.12)
q:q4$ ---7 q:q4
q:q4 ---7 q4
(D) The LBA productions are
q:q.($ ---7 q4$,
$q4$ ---7 q4$,
Oq4$ ---7 q4$,
1q4$ ---7 q4$
Step 2
The productions of the generative grammar are obtained by reversing
the arrows of productions given by (9.5)-(9.12).
9.11
SUPPLEMENTARY EXAMPLES
EXAMPLE 9.13
Design a TM that copies strings of l·s.
Solution
\Ve design a TM so that we have ww after copying W E {I}*. Define M by
M = ({qa. CJI, CJ2' CJ3}' {l}. {L b}, 8. CJa, b, {q3})
where 8 is defined by Table 9.11.
TABLE 9.11
Transition Table for Example 9.13
Present state
Tape symbol
b
a
qo
qoaR
q,bL
q,
q,1L
q3bR
q21R
q2
q21R
q1 1L
q3
Tne procedure is simple.
M replaces every 1 by the symbol a. Then M replaces the lightmost a by
1. It goes to the light end of the string and writes a 1 there. Thus M has added
a 1 for the rightmost 1 in the input string w. This process can be repeated.
M reaches CJI after replacing aU1's by a's and reading the blank at the end
of the input string. After replacing a by 1. M reaches q2' M reaches q3 at the
end of the process and halts. If H' = Iii. than we have 1211 at the end of the
computation. A sample computation is given below.
qaIl r- aqa1 1-- aaqab r- aqja
r- a1qc.b r- aCJ I11 r- qIa11
r- 1qc.11 r- 11CJc.1 r- 111qc.b
r- 11CJc.11 r- 1qI111
r- qI111I r- q 1b1111 r- q31111
304
~
Theory of Computer Science
EXAMPLE 9.14
Construct a TM to accept the set L of all strings over {O,I} ending with 010.
Solution
L is certainly a regular set and hence a deterministic automaton is sufficient to
recognize L. Figure 9.12 gives a DFA accepting L.
°
°
1
Fig. 9.12
DFA for Example 9.14.
Converting this DFA to a TM is simple. In a DFA M, the move is always to
the right. So the TM's move will always be to the right. Also M reads the input
symbol and changes state. So the TM M 1 does the same; it reads an input
symbol. does not change the symbol and changes state. At the end of the
computation. the TM sees the first blank b and changes to its final state. The
initial ill of Mj is qoW. By defining 6(qo, b) =(qj, b, R), M j reaches the initial
state of M. M j can be described by Fig. 9.13.
(1.1.R)
(0,0, R)
(0,0, R)
(1, 1, R)
(1,1,R)
Fig. 9.13
TM for Example 9.14.
Note:
q) is the unique final state of M j • By comparing Figs. 9.12 and 9.13 it
is easy to see that strings of L are accepted by M j •
EXAMPLE 9.15
Design a TM that reads a string in {O, I}* and erases the rightmost symbol.
Solution
The required TM M is given by
M = ({qo, qj, q2, q3, q4}' {O, I}, {O. 1, b}, 6, qo. b, {q4})
Chapter 9: Turing Machines and Linear Bounded Automata
~
305
where 8 is defined by
8(qo, 0) = (e/j, 0, R)
8(q], 0) = (qj, 0, R)
8(q], b) = (q2' b, L)
8(q2, 0) = (q3' b, L)
8(q3' 0) = (q3' 0, L)
8(q3' b) = (q4' b, R)
8(qo, 1) =(qj, 1, R)
8(qj' 1) = (ql' 1, R)
8(q2' 1) =(q3' b, L)
8(q3' 1) = (q3' 1, L)
(R])
(R2)
(R3)
(R4)
(Rs)
(~)
Let w be the input string, By (R]) and (R2), M reads the entire input string
w. At the end, M is in state qj' On seeing the blank to the right of w, M reaches
the state q2 and moves left. The rightmost string in w is erased (by (R4)) and
the state becomes q3' Afterwards M moves to the left until it reaches the left-
end of w, On seeing the blank b to the right of w, M changes its state to q4'
which is the final state of M. From the construction it is clear that the rightmost
symbol of w is erased.
EXAMPLE 9.16
Construct a TM that accepts L = {02
1i I 11 2: O}.
Solution
Let ,v be an input string in {O} *. The TM accepting L functions as follows:
1. It wlites b (blank symbol) on the leftmost 0 of the input string w. This
is done to mark the left-end of w.
2. M reads the symbols of w from left to right and replaces the alternate
O's with x's.
3. If the tape contains a single 0 in step 2, M accepts w.
4. If the tape contains more than one 0 and the number of O's is odd in
step 2, M rejects w.
5. M returns the head to the left-end of the tape (marked by blank b in
step 1).
6. M goes to step 2.
Each iteration of step 2 reduces w to half its size. Also whether the number
of O's seen is even or odd is known after step 2. If that number is odd and
greater than 1, IV cannot be 02
1i (step 4). In this case M rejects w. If the number
of 0's seen is 1 (step 3), M accepts w (In this case 0
211 is reduced to 0 in
successive stages of step 2).
We define M by
M = ({qo, qj, (f2, q3' q4' ql' ql}, {O}, {O, x, b}, 8, qo, b, {qiD
where 8 is defined by Table 9.12.
306
J;I,
Theory of Computer Science
TABLE 9.12
Transition Table for Example 9.16
Present state
Tape symbol
0
b
.r
qo
bRq1
bRqt
xRqt
q1
.rRq2
bRqf
xRq1
q2
ORq3
bRq4
xRq2
q3
xRq2
bRq6
xRq3
q4
OLQ4
bRQ1
xLQ4
Qr
Qt
From the construction, it is apparent that the states are used to know
whether the number of O's read is odd or even.
We can see how M processes 0000.
qoOOOO ~ bCf1000 ~ bJ.(1200 ~ b.xxl30 ~ bX{)XCf2b
~ bxOq.+xb ~ bxq.+Oxb ~ bq4--r:Oxb ~ q4bxOxb
~ bq1xO.-r:b ~ bxq10xb ~ bx.-r:Cf2Xb ~ bxxxq2b
~ bxxq4xb ~ bxq.+xxb ~ bqJ,xxxb ~ qJ,bxxxb
~ bqlxxxb ~ bXqlxxb ~ bxxqjxb ~ bxxxqjb
~ bxxxbCfI'
Hence M accepts \i'.
Also note that M always halts. If M reaches qt, the input stling
11' is
accepted by M. If M reaches qr- }t' is not accepted by M; in this case M halts
in the trap state.
EXAMPLE 9.17
Let M = ({qo, qj, q2}. {O. I}. {O, 1, b}. 8, qo, {q2})
where 8 is given by
8(qo, 0) = (qj, 1, R)
8(qj, 1) = (qo- 0, R)
8(qj. b) = (q2' b, R)
(R j )
(R2)
(R3)
Find T(M),
Solution
Let 11' E T(M), As 8(qo, 1) is not defined, w cannot start with 1. From (Rd
and (R2), we can conclude that M starts from qo and comes back to Cfo after
reaching 01.
So. qo(OI)" f-2- (lO)"qo· Also, qoOb ~ lq]b ~ Ibq2'
Chapter 9: Turing Machines and Linear Bounded Automata
J;!
307
So, (On"O E T(M). Also, (OltO is the only string that makes M move from
qo to
Cf~· Hence, T(M) = {(Olto In;:: O}.
SELF-TEST
Choose the correct answer to Questions 1-10:
1. For the standard TM:
(a) L =r
(b) r
<;:;;; L
(c) L
<;:;;; r
(d) L is a proper subset of r.
2. In a standard TM. D(q. a), q E Q, a E r is
(a) defined for all (q. a) E Q x r
(b) defined for some. not necessarily for all (q, a) E Q x r
(c) defined for no element (q. a) of Q >< r
(d) a set of triples with more than one element.
3. If D(q. .\J = (p. Y. I), then
(a)
XlX~
Xi-lqxi
x" ~ XIX2
xi_~pxi_l.vxi+l ... .en
(b)
XIX~
xi_lqxi
x" ~ Xl.Y2
'Yi-lYi'JXi+I ... x"
(c)
XIX~
xi_IqXi
XI) ~ XI
·'(i-3P.Yi-~Xi-1YXi+l ... XII
(d)
XIX~
xi-lqxi
XII ~ XI
Xi+J1T\'Xi+~ ... Xn
4. If D(q. X;) = (p. y. R). then
(a)
XIX~
'Yi-lqxi
Xii ~ XIX~
Xi-lypXi+l
x"
(b)
X1X~
xi-lqxi
Xli ~ X)X2
XiPXi+l
.1'1/
(e)
X,X2
xi-lq'Yi
X" ~ X)'Y2
Xi-1PXiXi+l
x"
(d)
X!X~
Xi-1CfXi
X1/ ~ .1'1"2
Xi-lypXi+J
X1/
5. If D(q. Xl) = (p, y. I). then
(a)
qXlx~
Xii ~ p.vX~
Xli
(b)
q.YIX~
X" ~ yp.Y~
Xli
(e)
qXJX~
X"
~ pbx]
XI1
(d)
qx(r~
X" ~ pbx~
x"
6. If D(q. xl1 ) = (v.
Y. R). then
(a)
Xl ... x n_lqx"
~ PYX2J:3
Xn
(b)
x • ... X1/-1q·\, p:-
PYX~X3
XII
(c)
Xl
x n_lqxl1
~ XIX~
X"_l)pb
(d)
Xl
X"_lqx,, p:-
XIX~
xn_lypb
7. For the TM given in Example 9.6:
(a) qolbll p:- bqjllbbl
(b) qnlbll i- bqrllbhl
(c) Cj(Jlbll ~ lqoblll
(d) Cjolbll ~ (j3bllbbl
308
g
Theory of Computer Science
8. For the TM given in Example 9.4:
(a) 011 is accepted by M
(b) 001 is accepted by M
(c) 00 is accepted by M
(d) 0011 is accepted by M.
9. For the TM given in Example 9.5:
(a) 1 is accepted by M
(b)
11 is accepted by M
(c) 111 is accepted by M
Cd) 11111 is accepted by M
10. In a standard TM (Q. 2:. r, 8. qQ, b. F) the blank symbol b is
(a) in 2: - r
(b) in r - 2:
(c) r Ii 2:
(d) none of these
EXERCISES
9.1 Draw the transition diagram of the Turing machine given in Table 9.1.
9.2 Represent the transition function of the Turing machine given in
Example 9.2 as a set of quintuples.
9.3 Construct the computation sequence for the input 1b11 for the Turing
machine given in Example 9.5.
9.4 Construct the computation sequence for stlings 1213, 2133. 312 for the
Turing machine given in Example 9.8.
9.5 Explain how a Turing machine can be considered as a computer of integer
functions (i.e. as one that can compute integer functions; we shall discuss
more about this in Chapter 11).
9.6 Design a Turing machine that converts a binary stling into its equivalent
unary string.
9.7 Construct a Turing machine that enumerates {Oil1
11 1/1 2': I}.
9.8 Construct a Turing machine that can accept the set of all even
palindromes over {O, I}.
9.9 Construct a Turing machine that can accept the strings over {O, I}
containing even number of l's.
9.10 Design a Turing machine to recognize the language {a''Y'c
ll1 In. m 2': I}.
9.11 Design a Turing machine that can compute proper subtraction. i.e.
111
-'-
II, where m and n are positive integers. m -'- n is defined as m - n
if In > J7 and 0 if m ::; /1.
Decidability and
Recursively Enumerable
Languages
In this chapter the fonnal definition of an algorithm is given. The problem of
decidability of various class of languages is discussed. The theorem on halting
problem of Turing machine is proved.
10.1
THE DEFINITION OF AN ALGORITHM
In Section 4.4, we gave the definition of an algorithm as a procedure (finite
sequence of instructions ""hich can be mechanically carried out) that tenninates
after a finite number of steps for any input. The earliest algorithm one can think
of is the Euclidean algorithm, for computing the greatest common divisor of
two natural numbers. In 1900, the mathematician David Hilbert, in his famous
address at the International congress of mathematicians in Paris, averred that
every definite mathematical problem must be susceptible for an exact settlement
either in the fonn of an exact answer or by the proof of the impossibility of its
solution. He identified 23 mathematical problems as a challenge for future
mathematicians; only ten of the problems have been solved so far.
Hilbert's tenth problem was to devise 'a process according to which it can
be detennined by a finite number of operations'. whether a polynomial over
Z has an integral root. (He did not use the word 'algorithm' but he meant the
same.) This was not answered until 1970.
The fonnal definition of algorithm emerged after the works of Alan Turing
and Alanzo Church in 1936. The Church-Turing thesis states that any
alEOlithmic procedure that can be carried out by a human or a computer, can
also be carried out by a Turing machine. Thus the Turing machine arose as
an ideal theoretical model for an algorithm. The Turing machine provided a
machinery to mathematicians for attacking the Hilberts' tenth problem, The
problem can be restated as follows: does there exist a TM that can accept a
309
31 0
~
Theory ofComputer Science
polynomial over n variables if it has an integral root and reject the polynomial
if it does not have one,
In 1970, Yuri Matijasevic. after studying the work of Martin Davis, Hilary
Putnam and Julia Robinson showed that no such algorithm (TUling machine)
exists for testing whether a polynomial over n vmiables has integral roots. Now
it is universally accepted by computer scientists that Turing machine is a
mathematical model of an algorithm.
10.2
DECIDABILITY
We are familiar with the recursive definition of a function or a set. We also
have the definitions of recursively enumerable set~ and recursive sets (refer to
Section 4.4). The notion of a recursively enumerable set (or language) and a
recursive set (or language) existed even before the dawn of computers.
Now these terms are also defined using Turing machines. When a Turing
machine reaches a final state. it ·halts.' We can also say that a Turing machine
M halts when Ai reaches a state q and a current symbol a to be scanned so
that O(q. a) is undefined. There are TMs that never halt on some inputs in any
one of these \vays, So we make a distinction between the languages accepted
by a TM that halts on all input strings and a TM that never halts on some input
strings.
DefInition 10.1
A language L ~ 2> is recursively enumerable if there exists
a TM M. such that L = rUvf).
DefInition 10.2
A language L
~ I* is recurslve if there exists some
TM M that satisfies the following two conditions.
(i) If
V\'
E L then M accepts
H' (that is. reaches an accepting state on
processing !-t') and halts.
(ii) If 11' ~ L then Ai eventually halts. without reaching an accepting state.
Note:
Definition 10.2 formalizes the notion of an 'algorithm'. An algorithm,
in the usual sense, is a well-defined sequence of steps that always terminates
and produces an answer. The Conditions (i) and (ii) of Definition 10.2 assure
us that the TM always halts. accepting H' under Condition (i) and not accepting
under Condition (ii). So a TM. defining a recursive language (Definition 10.2)
always halts eventually just as an algorithm eventually terminates.
A problem with only two answers Yes/No can be considered as a language
L. An instance of the problem with the answer 'Yes' can be considered as an
element of the corresponding language L; an instance with ans,ver 'No' is
considered as an element not in L.
DefInition 10.3
A problem with tvvo answers (Yes/No) is decidable if the
corresponding language is recursive. In this case, the language L is also called
decidable.
Chapter 10: Decidability and Recursively Enumerable Languages
I;!
311
Definition 10.4
A problemflanguage is undecidable if it is not decidable.
Note:
A decidable problem is called a solvable problem and an undecidable
problem an unsolvable problem by some authors.
10.3
DECIDABLE LANGUAGES
In this section we consider the decidability of regular and context-free
languages.
First of all. we consider the problem of testing whether a detenninistic
finite automaton accepts a given input string lV,
Definition 10.5
Am; = {(B,
lV) IB accepts the input string w}
Theorem 10.1
ADFA is decidable.
Proof
To prove the theorem. we have to construct a TM that always halts
and also accepts ADf.-\' We describe the TM M using high level description
(refer to Section 9.5). Note that a DFM B always ends in some state of B after
n transitions for an input string of length n.
We defme a TM M as follows:
1. Let B be a DFA and w an input string. (B, w) is an input for the Turing
machine M.
2. Simulate B and input H' in the TM M.
3. If the simulation ends in an accepting state of B. then M accepts w.
If it ends in a nonaccepting state of B, then M rejects w.
We can discuss a few implementation details regarding steps 1. 2 and 3
above. The input (B,
H') for 1\1 is represented by representing the five
components Q, L, 8, qo, f by strings of L* and input string W E L*. M checks
whether (B. w) is a valid input. If not. it rejectes (B, w) and halts. If (B, w)
is a valid input. ['vi writes the initial state qo and the leftmost input symbol of
w. It updates the state using 0 and then reads the next symbol in w. This
explains step 2.
If the simulation ends in an accepting state w' then M accepts (B, w)..
Otherwise, Iv! rejects (B,
IV). This is the description of step 3.
It is evident that M accepts (B,
if and only if H' is accepted by the
DFA B.
I
Definition 10.6
ACFG = {(G, w) i the context-free grammar G accepts the input string w}
Theorem 10.2
ACFG
IS decidable.
Proof
We convert a CFG into Chomsky 110lmal form. Then any derivation
of H' of length k requires 2k -
1 steps if the grammar is in C]\IF (refer
to Example 6.18). So for checking whether the input string ft of length k is
312
g,
Theory ofComputer Science
in L(G), it is enough to check derivations in 2k - 1 steps. We know that there
are only finitely many derivations in 2k - 1 steps. Now we design a TM M
that halts as follows.
1. Let G be a CFG in Chomsky normal form and w an input string.
(G, w) is an input for M.
2. If k = 0, list all the single-step delivations. If k '* 0, list all the
derivations with 2k - 1 steps.
3. If any of the derivations in step 2 generates the given string 'v, M
accepts (G, w). Otherwise M rejects.
The implementation of steps 1-3 is similar to the steps in Theorem 10.1.
(G, w) is represented by representing the four components ViV, L, P, S of G
and input string w. The next step of the derivation is got by the production
to be applied.
M accepts (G, w) if and only if w is accepted by the CFG G.
In Theorem 4.3, we proved that a context-sensitive language is recursive.
The main idea of the proof of Theorem 4.3 was to construct a sequence
{Wo, WI> ..., Wd of subsets of (VV u
L)*, that terminates after a finite
number of iterations. The given string w E L* is in L(G) if and only if w E
WI.' With this idea in mind we can prove the decidability of the context-
sensitive language.
I
Defmition 10.7
ACSG = {(G, ,v) Ithe context-sensitive grammar G accepts
the input string w}.
Theroem 10.3
ACSG is decidable.
Proof
The proof is a modification of the proof of Theorem 10.2. In
Theorem 10.2, we considered derivations with 2k - 1 steps for testing whether
an input string of length k was in L(G). In the case of context-sensitive
grammar we construct Wi ={a E (Vv u L)* IS ~ a in i or fewer steps and
Ia I :; n}. There exists a natural number k such that WI. =Wk+1 =Wk+2 =...
(refer to proof of Theorem 4.3).
So w E L(G) if and only if W E
Wk' The construction of WI. is the key
idea used in the construction of a TM accepting AcsG. Now we can design a
Turing machine M as follows:
1. Let G be a context-sensitive grammar and w an input string of length
n. Then (G, w) is an input for TM.
2. Construct Wo = {S}. W'+l = W,
U
{{3 E (Vv u
L)* Ithere exists
ai E Wi such that a=>{3 and I{3! :; n}. Continue until WI. = Wk+1
for some k. (This is possible by Theorem 4.3.)
3. If W E
WI., 'v
E L(G) and M accepts (G, w); otherwise M rejects
(G, w).
I
Note:
If cid denotes the class of all decidable languages over L, then
Chapter 10: Decidahility and Recursively Enumerahle Languages
l;l
313
10.4
UNDECIDABLE LANGUAGES
In this section we prove the existence of languages that are not recursively
enumerable
and
address
the
undecidability
of recursively
enumerable
languages.
Theorem 10.4
There exists a language over 2: that is not recursively
enumerable.
Proof
A language L is recursively enumerable if there exists a TM M such
that L = T(M). As L is finite, 2:* is countable (that is, there exists a one-to-
one correspondence between 2:* and N).
As a Turing machine M is a 7-tuple (Q. 2:, f',
8, qo. b, F) and each
member of the 7-tuple is a finite set M can be encoded as a string. So the
set I of all TMs is countable.
Let J: be the set of all languages over 2:. Then a member of J: is a subset
of P
(Note that P
is infinite even though I
is finite), We show that ;i is
uncountable (that is, an infinite set not in one-to correspondence with N).
We prove this by contradiction. If
;L
were countable then J: can be
written as a sequence {L[, L2• L3, ... }. We \\Tite 2:* as a sequence {11']. W2'
We, . ... }. So L i can be represented as an infinite binary sequence XnXi2Xi3' ..
where
r1
ihv}
E L;
lO
otherwise
Using this representation we write L; as an infinite binary sequence.
L]
XI]X12X 13
x]i
L,
x2I x ::x :3
x:}
L i
Xil Xi2Xi3
xi)
Fig. 10.1
Representation of
T
We define a subset L of 2:* by the binary sequence ."].":."3 ... where Y; =
1 - Xii' If Xii =0, Yi = 1 and if Xii = I, ."; =O. Thus according to our assumption
the subset L of I* represented by the infinite binary sequence YIY:Y3 ...
should be Lk for some natural number k. But L =f. Lt. since Wk E L if and only
if Hk It:
Lk~ This contradicts our assumption that ci is countable. Therefore 1.
is uncountable. As J is countable.
;L should have some members not
corresponding to any TM in 1. This proves the existence of a language over
2: tnat is not recursively enumerable.
I
Defmition 10.8
ATM = {(M, w) IThe TM M accepts w}.
D(D»
314
l;i
Theory of Computer Science
Theorem 10.5
An! is undecidable.
Proof
We can prove that i'inl is recursively enumerable. Construct a TM U
as follows:
(M, 11') is an input to U. Simulate M on w. If M enters an accepting state,
L! accepts (lyl, wL Hence ADl is recursively enumerable. We prove that AT1I1
is undecidable by contradiction. We assume that Anl is decidable by a TM H
that eventually halts on all inputs. Then
{
accept
if M accepts 1'1i
H(M, 1V) =
reject
if M does not accept Hi
We construct a new TM D with H as subroutine. D calls H to determine
what M does when it receives the input (M;, the encoded description of M as
a string. Based on the received information on (M, (I"1» , D rejects M if M
accepts (M) and accepts 1vl if 1v1 rejects (IVI). D is described as follows:
1, (A1) is an input to D, where (M) is the encoded string representing M.
2, D calls H to run on (M, (A1)
3. D rejects (M) if H accepts (M, (M»
and accepts (M) if H rejects
(lvI, (AI».
Now step 3 can be described as follows:
raccept
if M does not accept (M)
D(UvI») =
~
'"
lreject
if M accepts (M)
Let us look at the action of D on the input (D). According to the
construction of D,
raccept
if D does not accept (D)
==
~lreject
if D accepts (0)
This means D
accepts (D) if D does not accept (D),
which is a
contradiction. Hence ATM is undecidable.
The Turing machine U used in the proof of Theorem 10.5 is called the
universal Turing machine. U is called universal since it is simulating any other
TUling machine.
10.5
HALTING PROBLEM OF TURING
MACHINE
In this section \ve introduce the reduction technique. This technique is used to
prove the undecidability of halting problem of Turing machine.
We say that problem A is reducible to problem B if a solution to problem
B can be used to solve problem A.
For example, if A is the problem.of finding some root of x4 - 3xc + 2 = 0
and B is the problem of finding some root of XC - 2 = 0, then A is reducible
to B. As XC - 2 is a factor of x-+ - 3xc + 2. a root of XC - 2 = 0 is also a root
of x4 -
3.\-" + :2 = O.
Chapter 10: Decidability and Recursively Enumerable Languages
~
315
Note:
If A is reducible to Band B is decidable then A is decidable. If A is
reducible to B and A is undecidable. then B is undecidable.
Theorem 10.6
HALTrM = {(M, w) IThe Turing machine M halts on input
11'} is undecidable.
Proof
We assume that HALTTM is decidable, and get a contradiction. Let M j
be the TM such that T(MI ) = HALTrM and let MI halt eventually on aU
(M, w). We construct a TM M2 as follows:
1. For M2, (M, w) is an input.
2. The TM MI acts on (M, w).
3. If MI rejects (M, w) then M 2 rejects (M, ,v).
4. If MI accepts (M, w), simulate the TM M on the input string w until
M halts.
5. If M has accepted w, M2 accepts (M, w); otherwise M 2 rejects (M, w).
When MI accepts (M,
iV) (in step 4), the Turing machine M halts on w.
In this case either an accepting state q or a state q' such that D(q', a) is
undefined tiU some symbol a in w is reached. In the first case (the first
alternative of step 5) M2 accepts (M.
w). In the second case (the second
alternative of step 5) M2 rejects (M, w).
It follows from the definition of M 2 that M 2 halts eventually.
Also,
T(M2) = {(M, vv) IThe Turing machine accepts w}
= ATM
This is a contradiction since An.1 is undecidable.
10.6
THE POST CORRESPONDENCE PROBLEM
The Post Correspondence Problem (PCP) was first introduced by Emil Post
in 1946. Later, the problem was found to have many applications in the theory
of formal languages. The problem over an alphabet 2: belongs to a class of
yes/no problems and is stated as foUows: Consider the two lists x =(Xl' .. Xn),
Y = (YI ... Yn) of nonempty strings over an alphabet 2: = {O, 1}. The PCP
is to determine whether or not there exist i lo ..•, im,
where 1 S ii S n, such
that
Note:
The indices ij ' s need not be distinct and m may be greater than n.
Also, if there exists a solution to PCP, there exist infinitely many solutions.
EXAMPLE 10.1
Does the PCP with two lists x = (b, bab3, ba) and v = (b3, ba, a) have a
solution?
316
!!O!
Theory ofComputer Science
Solution
We have to determine whether or not there exists a sequence of substrings of
x such that the string formed by this sequence and the string formed by the
sequence of corresponding substrings of yare identical. The required sequence
is given by i1 = 2, i2 = 1, i3 = 1, i4 = 3, i.e. (2, 1, 1,3), and m = 4. The
corresponding strings are
=
Thus the PCP has a solution.
EXAMPLE 10.2
Y2
Yl
Yl
Y3
Prove that PCP with two lists x = (01, 1, 1), Y = (01 2, 10, 11) has no solution.
Solution
For each substring Xi E
X and Yi E
)', we have IXi I < IYi I for all i. Hence
the string generated by a sequence of substrings of X is shorter than the string
generated by the sequence of corresponding substrings of y. Therefore, the PCP
has no solution.
Note:
If the first substring used in PCP is always Xl and Yb then the PCP
is known as the Modified Post Correspondence Problem.
EXAMPLE 10.3
Explain how a Post Correspondence Problem can be treated as a game of
dominoes.
Solution
The PCP may be thought of as a game of dominoes in the following way: Let
each domino contain some
Xi in the upper-half, and the corresponding
substring of Y in the lower-half. A typical domino is shown as
o
upper-half
~
lower-half
The PCP is equivalent to placing the dominoes one after another as a
sequence (of course repetitions are allowed). To win the game, the same string
should appear in the upper-half and in the lower-half. So winning the game
is equivalent to a solution of the PCP.
I
Chapter 10: Decidability and Recursively Enumerable Languages
Q
317
We state the following theorem by Emil Post without proof.
Theorem 10.7
The PCP over 2: for 12:1 ;::: 2 is unsolvable.
It is possible to reduce the PCP to many classes of two outputs
(yes/no) problems in formal language theory. The following results can be
proved by the reduction technique applied to PCP.
1. If L 1 and L2 are any two context-free languages (type 2) over an
alphabet 2: and 12:1 ;::: 2, there is no algorithm to determine whether or
not
(a)
L] (l L2 = 0,
(b)
L 1 (l L2 is a context-free language,
(c)
L] k L2, and
(d)
L1 = L2•
2. If G is a context-sensitive grammar (type 1), there is no algorithm to
determine whether or not
(a)
L(G) = 0,
(b)
L(G) is infinite, and
(c)
Xo E L(G) for a fixed string Xcr
3. If G is a type 0 grammar, there is no algorithm to determine whether
or not any string x
E 2:* is in L(G).
10.7
SUPPLEMENTARY
EXAMPLES
EXAMPLE 10.4
If L is a recursive language over 2:, show that I (I is defined as 2:* - L) is
also recursive.
Solution
As L is recursive, there is a Turing machine M that halts and T(M) =L. We
have to construct a TM M 1, such that T(M1) = [
and M 1 eventually halts.
M] is obtained by modifying M as follows:
1. Accepting states of M are made nonaccepting states of MI'
2. Let M 1 have a new state qf After reaching qfi M] does not move in
further transitions.
3. If q is a nonaccepting state of M and 6(q, x) is not defined, add a
transition from q to qf for lvh
As M halts, M 1 also halts. (If M reaches an accepting state on w, then M]
d.~es not accept wand halts and conversely.)
Also M] accepts w if and only if M does not accept w. So I is recursive.
318
);;,]
Theory ofComputer Science
EXAMPLE 10.5
If Land L are both recursively enumerable. show that Land L are recursive.
Solution
Let M 1 and M 2 be two TMs such that L = T(M1) and L =T(M2). We construct
a new two-tape TM M that simulates M] on one tape and M 2 on the otheL
If the input string w of M is in L, then M1 accepts wand we declare that
M accepts w. If w E [, then M2 accepts wand we declare that M halts without
accepting. Thus in both cases, M eventually halts. By the construction of M
it is clear that T(lvl) = T(M]) = L Hence L is recursive. We can show that
[ is recursive, either by applying Example lOA or by interchanging the roles
of M) and M 2 in defining acceptance by M.
EXAMPLE 10.6
Show that ATM is not recursively enumerable.
Solution
We have already seen that Anv! is recursively enumerable (by Theorem 10.5).
If it TIY! were also recursively enumerable, then ATM is recursive (by
Example 10.5). This
~ a contradiction since ATM is not recursive by
Theorem 10.5. Hence A TM is not recursively enumerable,
EXAMPLE 10.7
Show that the union of two recursively enumerable languages is recursively
enumerable and the union of two recursive languages is recursive.
Solution
Let L 1 and L2 be two recursive languages and M 1, M 2 be the corresponding
TMs that halt. We design a Th1 M as a two-tape TM as follows:
1.
w is an input string to M.
2. M copies ,von its second tape.
3. M simulates M) on the first tape. If w is accepted by M10 then M
accepts ,v.
4. M simulates /'112 on the second tape. If w is accepted by M2, then M
accepts w.
M always halts for any input w.
Tnus L J U
L2 = T(M) and hence L J U
L 2 is recursive.
If L) and L2 are recursively enumerable. then the same conclusion gives
a proof for L)
U L2 to be recursively enumerable. As M 1 and M 2 need not
halt, M need not halt.
Chapter 10: Decidability and Recursively Enumerable Languages
Q
319
SELF-TEST
1. What is the difference between a recursive language and a recursively
enumerable language?
2. The DFA M is given by
M = ({Cia, %
Q2, Ci3}, to, 1}, 0, qo, {Qo})
where {) is defined by the transition Table 10.1.
TABLE 10.1
Transition Table for Self-Test 2
State
0
-,>@
q2
q1
q1
q3
qo
q2
qo
q3
q3
q1
q2
Answer the following:
(a) Is (A1, 001101) in AuA?
(b) Is (M, 01010101) in ADL;.?
(c) Does M
E ADFA?
(d) Find w such that (lvI, w) E ADFA .
3. What do you mean by saying that the halting problem of TM is
undecidable?
4. Describe ADFA, ACFG , AcsG, ATM , and HALTTlv1'
5. Give one language from each of ;i rl, ;i ell, ot c,I'
6. Give a language
(a) which is in ;f csl but not in ;r: rl
(b) which is in ;f ell but not in
J: c51
(c) which is in ;i ell but not in
;irl'
EXERCISES
10.1 Describe the Euclid's algorithm for finding the greatest common
divisor of two natural numbers.
10.2 Show that A NDFA = {(B,
w) IB is an NDFA and B accepts w}
IS
decidable.
10.3 Show that EDFA = {M IM is a D FA and T(M) = 0} is decidable.
10.4 Show that EQDFA = {(A, B) IA and Bare DFAs and T(A) = T(B)}
IS
decidable
10.5 Show that ECFG is decidable (ECFG is defined in a way similar to that
of EDFA).
320
Q,
Theory ofComputer Science
10.6 Give an example of a language that is not recursive but recursively
enumerable.
10.7 Do there exist languages that are not recursively enumerable?
10.8 Let L be a language over L. Show that only one of the following are
possible for Land r.
(a) Both Land r are recursive.
(b) Neither L nor r is recursive.
(c) L is recursively enumerable but L is not.
(d) r is recursively enumerable but L is not.
10.9 What is the difference between ATM and HALTTM?
10.10 Show that the set of all real numbers between 0 and 1 is uncountable.
(A set S is uncountable if S is infinite and there is no one-to-one
correspondence between S and the set of all natural numbers.)
10.11 Why should one study undecidability?
10.12 Prove that the recursiveness problem of type 0 grammar is unsolvable.
10.13 Prove that there exists a Turing machine M for which the halting
problem is unsolvable.
10.14 Show that there exists a Turing machine Mover {O, I} and a state qm
such that there is no algorithm to determine whether or not M will enter
the state ql11 when it begins with a given ill.
10.15 Prove that the problem of determining whether or not a TM over {O,1}
will ever print the symbol 1, with a given tape configuration, is
unsolvable.
10.16 (a) Show that {x I x is a set and x
~ x} is not a set. (Note that this
seems to be well-defined. This is one version of Russell's paradox.)
(b) A village barber shaves those who do not shave themselves but no
others. Can he achieve his goal? For example, who is to shave the
barber? (This is a popular version of Russell's paradox.)
Hints: (a) Let S = {x I x be a set and x
~ x}. If S were a set, then S E S or
S ~ S. If S ~ S by the 'definition' of S, then S E S. On the other
hand,
if S E S by the 'definition' of S, then S ~ S. Thus we can
neither assert that S ~ S nor S E S. (This is Russell's paradox.)
Therefore, S is not a set.
(b) Let S = {x Ix be a person and x does not shave himself}. Let b
denote the barber. Examine whether b
E
S. (The argument is
similar to that given for (a).) It will be instructive to read the proof
of HP of Turing machines and this example, in order to grasp the
similarity.
10.17 Comment on the following: "We have developed an algorithm so
complicated that no Turing machine can be constructed to execute the
algorithm no matter how much (tape) space and time is allowed."
Chapter 10: Decidability and Recursively Enumerable Languages
g
321
10.18 Prove that PCP is solvable if Il: I = l.
10.19 Let x =(Xl' .. X,,) and Y =(YI ... y,,) be two lists of nonempty strings
over l: and Il: I 2: 2. (i) Is PCP solvable for n = I? (ii) Is PCP solvable
for n = 2?
10.20 Prove that the PCP with {(01, 011), (1, 10), (I,ll)} has no solution.
(Here,
Xl = 01, X2 = 1, X3 = 1, YI = 011, 1'2 = 10, Y3 = 11.)
10.21 Show that the PCP with S = {(O, 10), (120, 03), (021, IOn has no
solution. [Hint: No pair has common nonempty initial substring.]
10.22 Does the PCP with X =(b3, ab2) and Y =(b3, bab3) have a solution?
10.23 Find at least three solutions to PCP defined by the dominoes:
1
10m
I
10
10.24 (a) Can
you
simulate
a Turing machine on
a general-purpose
computer? Explain.
(b) Can you simulate a general-purpose computer on a Turing
machine'? Explain.
Computability
In this chapter we shall discuss the class of primitive recursive functions-a
subclass of partial recursive functions. The Turing machine is viewed as a
mathematical model of a partial recursive function.
11.1
INTRODUCTION AND BASIC CONCEPTS
In Chapters 5, 7 and 9, we considered automata as the accepting devices. In
this chapter we will study automata as the computing machines. The problem
of finding out whether a given problem is 'solvable' by automata reduces to
the evaluation of functions on the set of natural numbers or a given alphabet
by mechanical means.
We start with the definition of partial and total functions.
A partial function f from X to Y is a rule which assigns to every element
of X at most one element of Y.
A total function from X to Y is a rule which assigns to every element of
X a unique element of Y. For example. if R denotes the set of all real numbers,
the rule f from R to itself given by fer) = +J; is a partial function since fer)
is not defined as a real number when r is negative. But g(r) = 21' is a total
function from R to itself. (Note that all the functions considered in the earlier
chapters were total functions.)
In this chapter we consider total functions from Xk to X,
where
X = {O, 1, 2, 3, ... } or X = {a, b}*. Throughout this chapter we denote
(0, 1, 2, ...) by N and (a, b) by L. (Recall that Xk is the set of all k-tuples
of elements of X) For example, f(m, 17) = m -
11 defines a partial function
from N to itself as f(m, 11) is not defined when m - n < 0; gem, 17) = m + 11
defines a total function from N to itself.
322
Chapter 11: Computahility
g
323
Remark
A partial or total function f from Xk to X is also called a function
of k variables and denoted by f(x), Xc, .. "
Xk)' For example. f(x)-
X2) =
2Xl + X2 is a function of two variables: f(l, 2) = 4, 1 and 2 are called
arguments and 4 is called a value.
g(H'j_ W2) = 11'JIV2 is a function of two
variables (H)'1/2 E L*); g(ab, aa) = abaa, ab, aa are called arguments and
abaa is a value.
11.2
PRIMITIVE RECURSIVE FUNCTIONS
In this section we construct primitive recursive functions over Nand L. We
define some initial functions and declare them as plimitive recursive functions.
By applying certain operations on the primitive recursive functions obtained so
far. we get the class of primitive recursive functions.
11.2.1
INITIAL FUNCTIONS
The initial functions over N are given in Table 11.1. In particular,
5(4) = 5.
Z(7) = 0
U](2, 4, 7) = 4,
U?(2, 4, 7) = 2,
utO, 4, 7)
_
'7
-I
TABLE 11,1
Initial Functions Over tv
Zero function Z defined by Z(x) = 0,
Successor function S defined by S(x) = x + 1
Projection function U/" defined by Ur"(.'1
""
xn) = x"
Note:
As Ul(x) = x for every x in N. ui is simply the identity function. So
Uf' is also termed a generalized identity function.
The initial functions over L are given in Table 11.2. In pmticular,
nil (abab) = A
cons a(abab) = aabab
cons b(abab) = babab
Note:
We note that cons a(x) and cons b(x) simply denote the concatenation
of the 'constant' string a and X and the concatenation of the constant string
band x.
TABLE 11.2
Initial Functions Over {a, b}
nil (x) = A
cons
a(x) = ax
cons
b(x) = bx
324
~
Theory of Computer Science
In the follO\ving definition, we introduce an operation on functions over X.
Definition 11.1
If fl. f2, ..., fk are partial functions of n variables and g is
a partial function of k variables, then the composition of g with f1, 12, .. .Jk
is a partial function of 11 variables defined by
g(j1(XI,
Xl> •.. , x ll), h(x1o X2, ..., XII)' ..., fk(.>::l, X2, ..., XII))
If. for example, f1, f2 and f, are partial functions of two variables and g
is a partial function of three variables, then the composition of g with f1, 12,
f, is given by g(jl(Xl, X2), h(x)o X2), f,(X1, X2))'
EXAMPLE 11.1
Let f1(X, y) =X + y, f2(x, y) = 2x,h(x, y) =.tT and g(.>::, y, z) =X + Y + z be
functions over N. Then
g(jI(X, y), f2(X, y),h(x, y)) = g(x + y, 2x, xy)
=x+y+2x+xy
Thus the composition of g with f1, .h. 13 is given by a function h:
hex, y) = x + y + 2x + xy
Note:
Definition 11.1 generalizes the composition of two functions. The
concept is useful where a number of outputs become the inputs for a subsequent
step of a program.
The composition of g with!J. .. ·,fll is total when g,fj, f2, .. ·,fn are total.
The function given in Example 11.1 is total as !J. f2, 13 and g are total.
EXAMPLE 11.2
Let f1 (x, y) =X - y, f2(X, y) =Y - X and g(x, y) =x + y be functions over
N. The function fl is defined only when x ~ y and 12 is defined only when
y ~ x. So f1 and 12 are defined only when x = y. Hence when X = y,
g(jl(x. y), f2(X, y)) = g(x - x, x - x) = g(O, 0) = 0
Thus the composition of g with fl and 12 is defined only for (x, x), where
x E N.
EXAMPLE 11.3
Let fl (:>:1' X2) = X1X2, h(x)o X2) = A, 13(x)o X2) = Xl> and g(x)o X2, x3) = X2X3
be functions over L. Then
g(jI(Xlo X2), h(xI, X2), 13(x1, X2)) = g(XIX2, A, x[) = Ax1 = Xl
So the composition of g with !J. 12, 13 is given by a function h, where
h(x)o x:) =Xl'
Chapter 11: Computahility
J;;;!
325
The next definition gives a mechanical process of computing a function.
Definition 11.2
A function f(x) over N is defined by recursion if there exists
a constant k (a natural number) and a function hex, y) such that
flO) = k,
fen + 1) = hen, fin))
(11.1)
By induction on n, we can define fen) for all n. As f (0) = k, there is basis
for induction. Oncef(n) is known,f(n + 1) can be evaluated by using (11.1).
E;XAMPLE 11.4
Define n! by recursion.
Solution
flO) = 1 and fen + 1) = hen, fen)), where hex, y) = Sex) * y.
The above definition can be generalized for f(x]o X2, ..., Xll' X,,+I)' We
fix n variables in f(XI, X2, ..., XII+I), say. XI, X2, ..., XII" We apply Definition
11.2 to f(x], X2, ..., XII' y). In place of k we get a function g(Xlo X2, ..., XII)
and in place of hex, y), we obtain hex], X2, ..., X", y, f(x], ..., Xll' y)).
Definition 11.3
A function f of n + 1 variables is defined by recursion if
there exists a function g of n variables, and a function lz of n + 2 variables,
and f is defined as follows:
fCx:], X2• ..., XII' 0) = g(x]. X2, .... x,,)
(11.2)
f(x] ... XII' Y + 1) = h(x]o X2, ..., X'I' y, f(XI, X2, ..., Xll' y))
(11.3)
We may note that f can be evaluated for all arguments (x]o X2, ..., x"' y)
by induction on y for fixed Xj. X2, ..., X/1" The process is repeated for every
Xl' X2, .... XII"
Now \ve can define the primitive recursive functions over N.
11.2.2
PRIMITIVE RECURSIVE FUNCTIONS OVER N
Definition 11.4
A total function f over N is called primitive recursive
(i) if it is anyone of the three initial functions, or (ii) if it can be obtained
by applying composition and recursion a finite number of times to the set of
initial functions.
EXAMPLE 11.5
Show that the function fl(X, y) = X + Y is ptimitive recursive.
Solution
fl is a function of two variables. If we want f] to be defined by recursion, we
need a function g of a single variable and a function h of three variables.
fleX, 0) = X + 0 = x
326
~
Theory ofComputer Science
By comparing fleX, 0) with L.H.S. of (11.2), we see that g can be defined by
g(x) = x = UlC"K)
Also. flex, :v + 1) = x + (:v + 1) = (x + y) + 1 = flex, y) + 1
By comparing fi(x, Y + 1) with L.R.S. of (11.3), we have
hex. y, flex, y) = fi(x, y) + 1 = Sifl(X, y)) = s(uiex, y, fleX, y)))
Define hex, :v, z) = s(U!(x, y, z)). As g = ul, it is an initial function. The
function lz is obtained from the initial functions ui and S by composition, and
by recursion using g and h. Thus fl is obtained by applying composition and
recursion a finite number of times to initial functions ui, Uj' and S. So fl is
primitive recursive.
Note:
A total function is primitive recursive if it can be obtained by applying
composition and recursion a finite number of times to primitive recursive
functions fl' f2, ..., f;,,· This is clear as each fi is obtained by applying
composition and recursion a finite number of times to initial functions.
EXAMPLE 11.6
The function f2(x. y) = x * Y is primitive recursive.
Solution
As multiplication of two natural numbers is simply repeated addition, f2 has
to be primitive recursive. We prove this as follows:
f2(x, 0) = 0,
hex, y + 1) =x * (:v + 1) =hex, y) + x
i.e. hex, y + 1) =flCf2(."K, y), x). Comparing these with (11.2) and (11.3), we
can write
hex. 0) = Z(x) andf2(x. y + 1) = fl(uj(x, Y,h(x, y)), u((x. y, f2(X, y))))
By taking g = Z and h defined by
hex, y. z) = fl (U!(x, y, z), U((x, y, z))
we see that f2 is defined by recursion. As g and h are primitive recursive, 12
is primitive recursive (by the above note).
EXAMPLE 11.7
Show that f(x, y) = x' is a primitive recursive function.
Solution
We define
f(x, 0) = 1
f(x, Y + 1) = x * f(x. y)
= U((x. y. f(x, y)) * uj(x. y. f(x, y))
Therefore. f(x. y) is primitive recursive.
Chapter 11: Computability
);l
327
EXAMPLE 11.8
Show that the following functions are primitive recursive:
(a) The predecessor function p(x) defined by
p(x) =x-I
if x :f:- 0,
p(x) = 0
if x = O.
(b) The proper subtraction function
..:.. defined by
x
..:.. y = X
-
Y
if x
~ v
and
x..:.. y = 0
if x < y.
(c) The absolute value function I I given by
Ix I = x
if x
~ 0,
Ix I = -x
if x < O.
(d) min (x, y), i.e. minimum of x and y.
Solution
(a) p(O) = 0 and p(y + 1) = Uf(Y, p(y))
(b) x
..:.. 0 = x and x
..:..
(y + 1) = p(x
y)
(c) Ix - y I = (x
..:.. y) + (y
..:.. x)
(d) win(x, y) = x
..:..
(:r
..:.. y)
The first function is defined by recursion using an initial function. So it is
primitive recursive.
The second function is defined by recursion using the primitive recursive
function p and so it is primitive recursive. Similarly. the last two functions are
primitive recursive.
11.2.3
PRIMITIVE RECURSIVE FUNCTIONS OVER {a, b}
For constructing the primitive recursive function over {a, b}, the process is
similar to that of function over N except for some minor modifications. It
should be noted that A plays the role of 0 in (11.2) and ax or bx plays the role
of y + 1 in (11.3). Recall that 2: denotes {a, b}.
DefInition 11.5
A function f(x) over 2: is defined by recursion if there exists
a 'constant" string 11' E 2:* and functions hl(x, y) and h2(x. y) such that
f(A) = w
f(ax) = hl(x, f(x))
f(bx) = h2(x, f(x))
(,k and h: may be functions in one variable.)
(11.4)
(11.5)
(11.6)
(11.7)
328
~
Theory of Computer Science
Defmition 11.6
A function f(x) , x:> ..., x ll ) over 1: is defined by recursion
if there exist functions g(x), ..., xll_)), 11)(x), ..., Xll+l), h2(Xlo ..., x ll+)),
such that
f(A, X2' ..., xll) = g(X2' ..., XII)
f(ax), X2'
XII) = 111(.:1:). X2'
, Xil • f(Xlo X2'
, XIl ))
f(bx),
X2, .... xll) =
112(x), X2'
, XIl ' fix), X2,
, xll))
(hI and h2 may be functions of m variables, where m < n + 1.)
Now we can define the class of primitive recursive functions over 1:.
Definition 11.7
A total function f is primitive recursive (i) if it is anyone
of the three initial functions (given in Table 11.2), or (ii) if it can be obtained
by applying composition and recursion a finite number of times to the initial
functions.
In Example 11.9 we give some primitive recursive functions over 1:.
Note:
As in the case of functions over N. a total function over 1: is primitive
recursive if it is obtained by applying composition and recursion a finite number
of times to primitive recursive function flo f2, ..., fm'
EXAMPLE 11.9
Show that the following functions are primitive recursive:
(a) Constant functions a and b (i.e. a(x) = a, b(x) = b)
(b) Identity function
(c) Concatenation
(d) Transpose
(e) Head function (i.e. head (a)a2 ..., all) =a))
(f) Tail function (i.e. tail (a)a2 ... all) = a2 ..., an)
(g) The conditional function "if x)
:;t A. then X2 else X3'"
Solution
(a) As a(x) =cons a (nil (x)). the function a(x) is the composition of the
initial function cons a with the initial function nil and is hence
primitive recursive.
(b) Let us denote the identity function by id. Then,
ideA) = A
id(ax) = cons a(x)
id(bx) = cons b(x)
So id is defined by recursion using cons a and cons b. Therefore, the
identity function is primitive recursive.
(c) The concatenation function can be defined by
concat(x). X2) = X)X2
concat(A, X2) = id(x2)
Chapter 11: Computability
~
329
concat(ax], X2) = cons a (concat(xb X2))
concat(bx], X2) = cons b (concat(x], X2))
SO concat is defined by recursion using id, cons a and cons b.
Therefore, concat is primitive recursive.
(d) The transpose function can be defined by trans(x) = xT. Then
trans(A) = A
trans(ax) = concat(trans(x), a(x))
trans(bx) = concat(trans(x), b(x))
Therefore, trans(x) is primitive recursive.
(e) The head function head(x) satisfies
head(A) = A
head(ax) = a(x)
head(bx) = b(x)
Therefore, head(x) is primitive recursive.
(f) The tail function tail(x) satisfies
tail(A) = A
tail(ax) = id(x)
tail(bx) = id(x)
Therefore, tailex-) is pnm]t]ve recursive.
(g) The conditional function can be defined by
cond(x], X2, X3) = "if x] "* A then X:; else X3"
Then,
cond(A, Xb x3) = id(x3)
cond(ax], X:;, X3) = id(x:;)
cond(bx], X2, X3) = id(x:;)
Therefore, id(x], x2. :\"3) is primitive recursive.
11.3
RECURSIVE
FUNCTIONS
By introducing one more operation on functions, we define the class of
recursive functions, which includes the class of primitive recursive functions.
Def"mition 11.8
Let g(x], X2' ..., X/1' y) be a total function over N. g is a
regular function if there exists some natural number Yo such that g(x], X2, .. ",
XII' Yo) = 0 for all values X], X2, ..., X/1 in N.
330
~
Theory ofComputer Science
For instance, g(x, y) = min(x, y) is a regular function since g(x, 0) = 0
for all x in N. But lex, y) = I x-\' 1 is not regular since lex, y) = 0 only when
x = y, and so we cannot find a fixed y such that lex, y) = 0 for all x in N.
DefInition 11.9
A function !(X1' x2' ..., XII) over N is defined from a total
function g(Xl'
X2' ... , XII' y) by minimization if
(a) f{XI' X2, .... Xl/) is the least value of all y's such that g(xj, X2, ..., Xll' y) = 0
if it exists. The least value is denoted by J.1,(g(xj, X2' ... , XII' y) = 0).
(b) fi."'Cj, X2, .... Xii) is undefined if there is no y such that g(xj, X2 ... Xll' y) = O.
Note:
In general. ! is partial. But, if g is regular then! is total.
DefInition 11.10
A function is recursive if it can be obtained from the initial
functions by a finite number of applications of composition, recursion and
minimization over regular functions.
DefInition 11.11
A function is partial recursive if it can be obtained from
the initial functions by a finite number of applications of composition,
recursion and minimization.
EXAMPLE 1.1.10
fix) = x/2 is a partial recursive function over N.
Solution
Let g(x, y) = 12y - x I. where 2y - x = 0 for some y only when X is even.
LetNx) = ,uJI2y - xl = 0). Thenf!(x) is defined only for even values of X
and is equal to x/2. When x is odd, !1(x) is not defined'!1 is partial recursive.
As lex) = x/2 =!1(x), ! is a partial recursive function.
The following example gives a recursive function which is not primitive
recursive.
EXAMPLE 11.11
The Ackermann's function is defined by
A(O, y) = y + 1
A(x + 1. 0) = A(x, 1)
A(x + 1, y + 1) = A(x, A(x + 1. y))
(11.8)
(11.9)
(11.10)
A(x, y) can be computed for every (x, y), and hence A(x, y) is total.
The Ackermann's function is not primitive recursive but recursive.
Chapter 11: Computability
~
331
EXAMPLE 11.12
Compute A(1. 1). A(2, 1). A(l, 2), A(2, 2).
Solution
by (11.8)
by (11.8)
by (11.10)
by (11.10)
by (11.10)
by (11.8)
by (11.10)
by (l1.10)
by (11.9)
by (11.10)
by (11.9)
by 01.8)
A(1, 1)= A(O + 1, 0 + 1)
= A(O, A(1. 0»
= A(O, A(O, 1»
= A(O, 2)
= 3
A(l,
= A(O + 1, 1 + 1)
=A(O, A(1, 1»
= A(O, 3)
= 4
A(2, 1) = A(l + 1. 0 + 1)
= .4(1, .4(2, 0»
=A(1. A(1.
1»
= A(l. 3)
=A(O + 1. 2 + 1)
= A(O. A(l, 2»
=A(O. 4)
= 5
A(2, 2) = A(l + 1. 1 + 1)
= A(1, A(2, 1»
= A(1, 5)
A(1. 5) = A(O + 1. 4 + 1)
= A(O. A(1, 4)
= 1 + A(l. 4)
= 1 + A(O + 1. 3 + 1)
= 1 + A(O. A(1. 3»
= 1 + 1 + A(1. 3)
= 1 + 1 + 1 + A(1. 2) = 1 + 1 + 1 + 4
= 7
As
A(L 2) = A( 1. 5). we have A(2, 2) = 7
332
.\;l
Theory ofComputer Science
So far we have dealt with recursive and partial recursive functions over
N. We can define partial recursive functions over L using the primitive
recursive predicates and the minimization process. As the process is similar,
we \vi11 discuss it here.
The concept of recursion occurs in some programming languages when a
procedure has a call to the same procedure for a different parameter. Such a
procedure is called a recursive procedure. Certain programming languages like
C, C++ allow recursive procedures.
11.4
PARTIAL RECURSIVE FUNCTIONS AND TURING
MACHINES
In this section we prove that partial recursive functions introduced in the earlier
sections are Turing-computable.
11.4.1
COMPUTABILITY
In mid 1930s. mathematicians and logicians were trying to rigorously define
computability and algOlithms. In 1934 Kurt GOdel pointed out that primitive
recursive functions can be computed by a finite procedure (i.e. an algorithm).
He also hypothesized that any fL1nction computable by a finite procedure can
be specified by a recursive function. Around 1936, Tming and Church
independently designed a 'computing machine' (later termed Turing machine)
\vhich can carry out a finite procedure.
For formalizing computability, Turing assumed that. while computing, a
person wlites symbols on a one-dimensional paper (instead of a two-
dimensional paper as is usually done) which can be viewed as a tape divided
into cells. He scans the cells one at a time and usually peliorms one of the three
simple operations. namely (i) \vriting a new symbol in the cell he is scanning,
(ii) moving to the cell left of the present cell, and (iii) moving to the cell right
of the present cell. These observations led Turing to propose a computing
machine. The Turing machine model we have introduced in Chapter 9 is based
on these three simple operations but with slight variations. In order to introduce
computability, \ve consider the Turing machine model due to Post. In the
present model the transition function is represented by a set of quadruples (i.e.
4-tuples), whereas the transition function of the model we have introduced in
Chapter 9 can be represented by a set of quintuples (5-tuples). For example,
6(qj. a) = (qj, a, {3) is represented by the quintuple qjaa{3CJj. Using the model
specifying the transition function in terms of quadruples. we define Turing-
computable functions and prove that partially recursive functions are Turing-
computable.
Chapter 11: Computability
l;;!
333
11.4.2
A TURING MODEL FOR COMPUTATION
As in the model introduced in Chapter 9, Q, qo and r denote the set of states.
the initial state, and the set of tape symbols, respectively. The blank symbol b
is in r. The only difference is in the transition function. In the present model
the transition function represents only one of the following three basic
operations:
(i) Writing a new symbol in the cell scanned
(ii) Moving to the left cell
(iii) Moving to the right cell
Each operation is followed by a change of state. Suppose the Turing machine
M is in state q and scans ai' If ai is written and M enters q', then this basic
operation is represemed by the quadruple qaiad. Similarly. the other two
operations are represented by the quadruples qaiLq' and qaiRq'. Thus the
transition function can be specified by a set P of quadruples. As in Chapter 9.
we can define instantaneous descriptions, i.e. IDs.
Each quadruple induces a change of IDs. For example, qa;ajq' induces
P.
'
,
[3
CI.qail-' I
aq ai
The quadruple qaiLq' induces
and qaiRq' induces
When we require M to perform some computation, we 'feed' the input by
initial tape expression denoted by X. So qaX is the initial ID for the given
input. For computing with the given input X. the Turing machine processes
X using appropriate quadruples in P. As a result. we have qoX =IDi r- ID2
r- .... When an ID. say IDil' is reached. which cannot be changed using any
quadruple in P, M halts. In this case, ID" is called a terminal ill. Actually,
aqj a[3 is a terminal ID if there is no quadruple starting with qi{l. The terminal
ID is called the result of X and denoted by Res(X). The computed value
cOlTesponding to input X can be obtained by deleting the state appeming in it
as also some more symbols from Res(X).
11.4.3
TURING-COMPUTABLE FUNCTIONS
Before developing the concept of Turing-computable functions. let us recall
Example 9.6. The TM developed in Example 9.6 concatenates two strings a
aej [3. Initially, a and [3 appear on the input tape separated by a blank b.
Finally, the concatenated string a[3 appears on the input tape. The same
method can be adopted with slight modifications for computing I(x] , ..., x,,')'
Suppose we want to construct a TM which can compute I(xl' ..., XII,) over
334
~
Theory ofComputer Science
N for given arguments a], .... am' Initially, the input OJ, a2' ..., am appears
on the input tape separated by markers
Xj, ... ,
Xlii' The computed value
f(a] , ..., am)' say, c appears on the input tape, once the computation is over.
To locate c ,ve need another marker. say y. The value c appears to the right
of X m and to the left of v. To make the construction simpler, we use the tally
notation to represent the elements of N. In the tally notation, 0 is represented
by a string of b's. A positive integer n is represented by a string consisting
of II 1's. So the initial tape expression takes the form 1"lx,1{/2x: ... 1{/mxmby.
As a resulr of computation, the initial ID qOFtxll"2X2 ... l{/lI/xlII by is changed
to a terminal ID of the form 1{/lXl1{/2X: ... 11lmx",1'q'y for some q' E Q. In
fact, the position of q' in a tenninal ID is immaterial and it can appear
anywhere in Res(X). The computed value is found between
XIII and y.
Sometimes we may have to omit the leading b's.
We say that a function f(x] . ..., x",) is Turing-computable for arguments
aj, ....
(1m if there exists a Turing machine for which
where IDII is a terminal ID containing f(al' ..., alii) to the left of y.
Our ultimate aim is to prove that partial recursive functions are Turing-
computable. For this purpose. first of all we prove that the three initial primitive
recursive functions are Turing-computable.
11.4.4
CONSTRUCTION OF THE TURING MACHINE THAT
CAN COMPUTE THE ZERO FUNCTION Z
The zero function Z is defined as Zeal) = 0 for all al :::: O. So the initial tape
expression can be taken as X = 11l'xlby. As we require the computed value
Zeal)' namely O. to appear to the left of y, we require the machine to halt
without changing the input. (Note that 0
IS represented by b in the tally
notation.)
Thus we define a TM by taking Q = {qo, qd, r = {b.
LXI_ Y},
X = l"jx,by. P consists of qobRqo, qolRqo.
q(~llx1ql' qobRqo and qolRqo are
used to move to the right until Xl is encountered.
q~llx1ql enables the TM to
enter the state ql' M enters qj without altering the tape symbol. In terms of
change of IDs. we have
a l"lx by f.2-
l"la-l i bv L- l"I('IX by
10
I.;
1(} ..
I
1.
,
.
As there is no quadruple starting with ql' M halts and Res(X) = 11Ij(11X1by.
By deleting ql in Res(X), we get l"lxlby (which is the same as X) yielding 0
(given by b).
Note:
We can also represent the quadruples in a tabular form which is
similar to the transition table obtained in Chapter 9. In this case we have to
specify (i) the new symbol written. or (ii) the movement to the left (denoted
by L/. or (iii) the movement to the right (denoted by R). So we get
Table 11.3.
Chapter 11: Computability
g
335
TABLE 11.3
Representation of Quadruples
State
b
y
r
= {h.
1. Xj. y},
11.4.5
CONSTRUCTION OF THE TURING MACHiNE FOR
COMPUTING-THE SUCCESSOR FUNCTION
The successor function S is defined by Sea)) = aj + 1 for all aj 2': O. So the
initial tape expression can be taken as X = 1({lX1by (as in the case of the zero
function). At the end of the computation. we require 1([1+1to appear to the left
of Y. Hence we define a TM by taking
Q = {qo. .. " q9},
where P consists of
(i) qobRqo. qolbql' q(}"t!Rqc;
(ii) q1bRqj. qjlRqj. qlxjRqj.
qj\'lq~
(iii)
q~ lRq~.
C]~byq3'
(iy) q3bLq3' q31Lq3' q3yLq3. q,X1Lq.+
(v) qJ,lLqJ,. C]J,blq).
(vi) q)lRqo.
(vii) qc;bRq6' qc;lRq6' C]fY"tjRq6' q6yu 17
(viii) q71Lq7. q7b1qs.
(ix) qsbLqs. qslLqs· qsyLqs. q8x IX jq9'
The corresponding operations can be explained as follows:
(i) If :'v.f starts from the initial lD. the head replaces the first 1 it
encounters by b. Afterwards the head moves to the right until it
encounters
Y (as a result of q()lbqj. qjbRql' qllRql. qlxlRql)'
(ii) y is replaced by 1 and M enters
q~. Once the end of the input tape is
reached. y is added to the next cell. M enters q3
(qjylq~.
q~lRq~.
q~byq3)'
(iii) Then the head moves to the left and the state is not changed until Xl
is encountered (q3)'Lq3. q3yLq3' q3bLq3)'
(iv) On encountering Xlo the head moves to the left and 1\1 enters qJ,. Once
again the head moves to the left till the left end of the input string is
reached (q3xjLqJ,. q)LqJ,).
(v) The leftmost blank (written in point (i)) is replaced by 1 and M enters
q) ((].+blq)).
Thus at the end of operations (i)-(v). the input part remains unaffected but
the first 1 is added to the left of y.
336
!;!
Theory ofComputer Science
(vi) Then the head scans the second 1 of the input string and moves right,
and M enters qo (qs1Rqo).
Operations (i)-(vi) are repeated until all the 1's of the input part
(i.e. in 1ii1 ) are exhausted and 11 ... 1 (al times) appear to the left
of y. Now the present state is qo, and the current symbol is Xj.
(vii) M in state qo scans Xl, moves right. and enters CJ6' It continues to move
to the right until it encounters y (CJoxlRCJ6, q6bRCJ6, CJ61RCJ6, CJfrYIRCJ6)'
(viii) On encountering y. the head moves to the left and M enters CJ7, after
which the head moves to the left until it encounters b appearing to the
left of 1"1 of the output part. This b is changed to 1, and M enters
CJs (CJ6yLCJ7. ql lLq7, q7b jCJS)'
(ix) Once M is in qs, the head continues to move to the left and on scanning
Xl. M enters CJ9' As there is no quadruple starting with q9, M halts
(qsbLqs·
CJsILqs, qsxIXICJ9)'
The machine halts, and the terminal ill is 1Ulq9Xjl"I+1y. For example, let
us compute S(I). In this case the initial ill is CJolx] by. As a result of the
computation, we have the following moves:
CJOlxlby 1- CJjbxjby r- bCJ]xjby
f- bXICJ/J)'I- bx1bqlY r- bxjbCJ:1
r- bxlblCJ2b r- bxlblq3Y r- bxl bCJ31y
r- bCJ,xlb1y r- CJ4bx l b1y r- qslx 1b1y
r-
1CJ6-y j bh r-
1xjb1CJ6Y r- l.ylbq71y
r- LrjCJ7b1y r- lxjCJsllv r- 1Qsxj lly
r-
lCJ9x j 11y
Thus. M halts and SO) = 2 (given by 11 to the left of y).
11.4.6
CONSTRUCTION OF THE TURING MACHINE FOR
COMPUTING THE PROJECTION Ur
Recall Ut'(al' .... a/l1) = ai' The initial tape expression can be taken as
We define a Turing machine by taking Q = {qo, ..., qs}
r = {b. L Xl> ....
XII!' y}. P consists of
qozRqo
qoxiLqj.
CJ:;:RCJ:
CJ::vlCJ,·
for all ;: E r - {x;}
qlbbqg'
qlbq:
for all ;: E r - {y}
q3 1Rq3'
q3bYCJ4
Chapter 11: Computability
~
337
qJ.zLqJ.
for all z E 1-
{x;}
qJ.xiLqS,
qsILqs,
qsblq6'
q6ILq7,
q7 Ibq2
q7zRqS
for all z E f- {I}
The operations of M are as follows:
(i) M starts from the initial ill and the head moves to the right until it
encounters Xi (qozRqo)·
(ii) On seeing Xi, the head moves to the left (qoXiLql)'
(iii) The head replaces 1 (the rightmost 1 in l a;) by b (q1lbq2)'
(iv) The head moves to the right until it encounters y and replaces y by 1
(q2zRq2' Z E f
-
{y} and
q~vlq3)'
(v) On reaching the right end, the head scans b and replaces this b by
Y (q3byqJ.).
(vi) The head moves to the left until it scans the symbol b. This b is
replaced by 1 (qJ.zLqJ., Z E f
-
{x;}, qJ.xiLqS' qsblq6)'
(vii) The head moves to the left and one of the l's in 1{/i is replaced by
b. M reaches q2 (q6ILq,. qilbq2)'
As a result of (i)-(vii), one of the 1's in l a; is replaced by band
1 is added to the left of y. Steps (iv)-(vii) are repeated for all l's
in I"i.
(viii) On scanning Xi-I'
the head moves
to the right and M
enters
qs (q,xi-lRqS)'
As there are no quadruples starting with qs, the Turing machine M halts.
When i :j:. 1 and ai
:j:. O. the telminal ill is I"lXI ... Xi-lq81";xi ... xl1bl aiy.
For example, let us compute Ul(l. 2. 1):
qol.tlIl.c2Ix3by
~ Ix 1 llqox21.\:3by
r- Lt1lq 1Ix2h 3by
r- Ixllq2bx2Ix3by
~ IXllbx2Ix3bq2Y
r- Ix[lbx2Ix)7q31
r- lxIlbx2Ix3blq3b r- lXllbx2lx}blq4Y
~ Ix 1lbq4x2Ix}bly r- Ix! Iqsbx21x}bly
r- Ixllq6Ix2Ix3blv r- lXlq711x2Ix3bly
r- b:lq2blx2Ix3blv
From the above derivation. we see that
l.Yjlq2bx2Ix3by ~
l.Ylq2bl.t21x3bIy
Repeating the above steps, we get
Ixlq2blx2lx}bl.v P-
LCjqslh2Ix3blly
It should be noted that this construction is similar to that for the successor
function. While computing U/", the head skips the portion of the input
corresponding to ai. j
:j:. i. For every 1 in l"i. 1 is added to the left of y.
338
~
Theory ofComputer Science
Thus we have shown that the three initial primitive recursive functions are
Turing-computable. Next we construct Turing machines that can perform
composition, recursion. and minimization.
11 .4.7
CONSTRUCTION OF THE TURING MACHINE THAT
CAN PERFORM
COMPOSITION
Let fIC-'l' X.2' ..., .\,,)•...• fiJxI, .... XIII) be Turing-computable functions. Let
g(YI' .. " YI.) be Turing-computable. Let h("I, ..., XIII) = g(fl(XI
...• xm) .•••
.MXi' . , ., XIII»' We construct a Turing machine that can compute h(aj, ...• am)
for given arguments aj, .... am' This involves the following steps:
Step 1 Construct Turing machines All' .... Mk which can compute fl' ...,.I;.
respectively. For the TMs Mj, .... lvII.:' let T' = {I, b, Xl. X.:;• .... xn , y} and
X = 1°1,]
10m
Xin by. But the number of states for these TMs will vary.
Let 111 + 1.
11k + 1 be the number of states for /\.11, .... A'h. respectively.
As usuaL the initial state is (jo and the states for M i are qQ, ..., q,,;, As in the
earlier constructions. the set Pi of quadruples for Mi is constructed in such a
way that there is no quadruple starting with
ql1;'
Step 2
Let f;(ai' ..., am) = bi for i = 1. 2, ..., k. At the end of step 1,
we have M;'s and the computed values bi's. As g is Turing-computable. we
can construct a TM A'h+l which can compute g(b] • .... bk)· For Mk+l •
X, -
I hl .'
10m
'
l .
-
X \ •••
X III 7)
(\Ve use different markers for Mk+ 1 so that the TM computing h to be
constructed need not scan the inputs a] . ..., am') Let I1k+\ + 1 be the number
of states of M k+]. As in the earlier constructions, M k+ 1 has no quadruples
starting with qk+ I'
Step 3
At the end of step 2. we have TMs M I , ..., M b lvh+1 which give
bl ..... bin and g(b] . .... bJ = c (say). respectively. So we are able to
compute h(al' .... am) using k + 1 Turing machines. Our objective is to
construct a single TM kh+.2 which can compute heal' ..., an,). We outline the
construction of M without giving the complete details of the encoding
mechanism. For M. let
T' = {1. b, Xi' "
.. X"I' x'\.
(1) In the beginning, lv! simulates M j • As a result. the value bj
=
fi(al' ..., am) is obtained as output. Thus we get the tape expression
1°\xI 1!!2x.:; ... 1(1mxml bly which is the same as that obtained by M 1
while halting. lv! does not halt but cbanges y to x'] and adds by to the
right of X'I' The head moves to the left to reach the beginning of X.
Chapter 11: Computability
J;;t
339
(ii) The tape expression obtained at the end of (i) is
The construction given in (i) is repeated. i.e. M simulates M 2• .•.• M k,
changes y to x;, and adds by to the right of x;. After simulating Mk>
the tape expression is
X' = I"IXl ... I"mxm1btx'l ... 1bk-lXk_tlbkx'k by
Then the head moves to the left until it is positioned at the cell having
1 just to the right of xm.
(iii) M simulates Mk+1• Mk+1 with initial tape expression X' halts with the
tape expression Iblx'i ... Ibkx;" 1'y. As a result, the corresponding tape
expression for M is obtained as
·1"1.- I"2x
1"",.- 1b1 ,J
Ibh-' Ii'v
.,1
2' . .
"",
AI'"
., k
~
(iv) The required value is obtained to the left of y. but Ibtx'l ... Ihkx'k also
appears to the left of c. M erases all these symbols and moves Iiy just
to the right of XIII' The head moves to the cell having x", and M halts.
The final tape expression is 1"lxJI"2x2 ... 1"mxm I(y.
11.4.8
CONSTRUCTION OF THE TURING MACHINE THAT
CAN
PERFORM
RECURSION
Let g(XI' .... x",). h(Yl' 1'2, .... Ym.d be Turing-computable. Let f(x[o ...,
Xm+1) be defined by recursion as follows:
f(XI' ....
Xm • 0) = g(x] ...
XII,)
f(x) . ..., x"',
I' + 1) = h(xio ..., xm, y, f(xJ' ..., Xm' Y»
For the Turing machine Ai, computing f(a1o .... am, c), (say k), X
IS
taken as
As
the
construction is
similar to
the
construction
for
computing
composition. \ve outline below the steps of the construction.
Step 1
Let l'vf simulate the Turing machine M' which computes g(at, ..., am)'
The computed value, namely g(a)
, all,). is placed to the left of y. If
c = O. then the computed yalue g(a]
, a,/i) is f(a]- .... am, 0). The head
is placed to the right of Xl/! and M halts.
Step 2
If c is not equal to zero, lito the left of Xm+1 is replaced by bi. The
marker Y is changed to Xm+2 and bv is added to the right of X",+2' The head
moves to the left of 1"1.
Step 3
h is computable. M is allowed to compute h for the arguments at •...,
a",_ O. g(at . ..., am) \vhich appear to the left of XJ' ... , xm'
Xm+1o
XII/+2,
340
'I
Theory of Computer Science
respectively. The computed value is f(a J•••• ,
0.1/1' 1). And f(a], ..., 0.1/1' 2)
... f(a], .. " am. c) are computed successively by replacing the rightmost b
and computing h for the respective arguments.
The computation stops with a terminal ill. namely
bI"!>-I'"
q··I'·y
Ik,.
k
f(
(C')
-'I
-...
.t
AI/+]
."
=.
a], ... ,
[m'
11.4.9
CONSTRUCTION OF THE TURING MACHINE THAT
CAN PERFORM MINIMIZATION
When f(x], ..., xm) is defined from g(x] . .. "
X"1' y) by minimization,
f(xj . .... XI/) is the least of all k's such that g(xlo .... Xm' k) = O. So the
problem reduces
to computing
g(aj . ....
0. 111'
k)
for
given
arguments
a), ..., am and for values of k starting from O. f(aj, ..., am) is the first k
for which g(a], ....
0.111' k) = O. Hence as soon as the computed value of
g(a] . .... am' y) is zero, the required Turing machine M has to halt. Of
course, when no such y exists. M never halts, andf(a), ...• am) is not defined.
Thus the construction of M is in such a way that it simulates the TM that
computes g(a] . .... alii' k) for successive values of k. Once the computed value
g(al' .... am' k) =a for the first time. M erases by and changes xlll+I to y. The
head moves to the left of :r,n and M halts.
As partial recursive functions are obtained from the initial functions by a
finite number of applications of composition. recursion and minimization
(Definition 11.11) by the various constructions we have made in this section.
the partial recursive functions become Turing-computable.
Using Godel numbering \vhich converts operations of Turing machines
into numeric quantities. it can be proved that Turing-computable functions are
partial recursive. (For proof. refer Mendelson (1964).)
11.5
SUPPLEMENTARY EXAMPLES
EXAMPLE 11.1 3
Shmv that the function f(x),
x~, ..., Xli) = 4
IS primitive recursive.
Solution
4 = 5"+(0)
= 5"\Z('YtJ)
= 5"+(Z(Uj'(x\.
x~. ..., XII»)
I.e.
f(x].
X~, .... XII) = S"+(Z(U{'(Xj,
X~, ..., xl/)))'
As f is the composition of initial functions, f is primitive recursive.
Chapter 11: Computability
~
341
EXAMPLE 11.14
If I(X1' x2) is primitive recursIve, show that g(XI'
x~, x"
X4) = I(XI' X4)
IS
primitive recursive.
Solution
g(xj, X2' x"
X4)
= I(xl' X4)
=I(UI
4(X\l X2, x"
X4)'
U}(Xl' X2'
X3' X4))
U j-+ and uj are initial functions and hence primitive recursive. I is primitive
recursive. As the function g is obtained by applying composition to primitive
recursive functions, g is primitive recursive (by the Note appearing at the end
of Example 11.5).
EXAMPLE 11.15
If I(x, y) is primitive recursIve. show that g(x, y) =1(4. y) is primitive
recurSIve.
Solution
Let hex, y) = 4. h IS primitive recursive by Example 11.13.
g(x, y)
=1(4, 1')
=I(h(x, y). vl(x, .\'))
As I and g are primitive recursive and Co' IS an initial function, g IS
primitive recursive.
EXAMPLE 11.16
Show that I(x, y) =l\4 + 7-1.,,3 + 4y5 is primitive recursive.
Solution
As 11 (x, y) =x + y is primitive recursive (Example 9.5), it is enough to prove
that each summand of I(x, y) IS primitive recursive.
But.
As multiplication is primitive recursive, g(x, y) = x 2l
is primitive recursive.
As hex, 1') = ,n' is primitive recursive, 7xy3 = .\}" + ' .. + xy3 is primitive
recursive. Similarly, 41'5 is primitive recursive.
342
~
Theory ofComputer Science
SELF-TEST
Choose the correct answer to Questions 1-10.
1. 5(Z(6»
is equal to
(a) U1
3(L 2. 3)
(b) Ui'(L 2. 3)
(c) ufo, 2, 3)
(d) none of these.
2. Cons a(y) is equal to
(a) /\
(b) ya
(c) ay
(d) a
3. min(x, y) is equal to
(a) x -=-(x -=-y)
(b) y
-=- (y
-=- x)
(c) x - Y
(d) y - x
4. A(L 2) is equal to
(a) 3
(b) 4
(c) 5
(d) 6
5. f(x) = x/3 over N is
(a) total
(b) partial
(c) not partial
(d) total but not partial.
6.
1fI{4}(3) is equal to
(a) 0
(b) 3
(c) 4
(d) none of these.
7. sgn(x) takes the value 1 if
(a) x < 0
(b) x ::::; 0
(c) x > 0
(d) x 2': 0
8.
IfIA + IfIB = 1f!,4uB if
(a) A u
B = A
(b) A u
B = B
(c) A II B = A
(d) A II B = 0
Chapter 11: Computability
~
343
9. U2'+C5(4), 5(5), 5(6), Z(7))
IS
(a) 6
(b) 5
(c) 4
(d) 0
10. If g(x, y) = min(x, y) and hex, y) = Ix - y I. then:
(a) Both functions are regular functions.
(b) The first function is regular and the second is not regular.
(c) Neither of the functions is regular.
(d) The second function is not regular.
State whether the Statements 11-15 are true or false.
11. f(x, y) = x + y is primitive recursive.
12. 3
-=- 4 = O.
13. The transpose function is not primitive recursive.
14. The Ackermann's function is recursive but not primitive recursive.
15. A(2, 2) = 7.
EXERCISES
11.1 Test which of the following functions are total. If a function is not
total. specify the arguments for which the function is defined.
(a) f(x) = x/3 over N
(b) fex) = 1/(x -
1) over N
(c) f(x) = _~ - 4 over N
(d) f(x) = x + lover N
(e) f(x) =rover N
11.2 Show that the following functions are primitive recursive:
r1
if x =0
(a) X{Oj(x) =
~lO
if x "# 0
(b) f(x) = r
(c) f(x, y) = maximum of x and y
{
X/2
when x is even
(d) f(x) =
(x - 1)/2
when x is odd
(e) The sign function defined by
sgn(O) = 0,
sgn(x) = 1
if
x> O.
344
i;1.
Theory ofComputer Science
g
if x> v
(f) L(x. ,) =
if x :S y
(1
if x = v
(g) Eer, v) = to
if x *- v
11.3 Compute A(3, 2). A(2, 3), A(3, 3).
11.4 Show that the following functions are primitive recursive:
(a) q(x. y) = the quotient obtained when x is divided by y
(b) rex, y) = the remainder obtained when x is divided by y
{
2X
if x is a perfect square
(c) f(x) =
2x + 1
otherwise
11.5 Show that f(x) = integral part of j;
is patti'll recursive.
11.6 Show that the Fibonacci numbers are generated by a primitive recursive
function.
11.7 Let f(O) = 1, f(l) = 2, f(2) = 3 and f(x + 3) = f(x) + f(x + 1)1 +
fi.'( + 2)3. Shovv that f(:r) is primitive recursive.
11.8 The characteristic function XA of a given set A is defined as
ra
if a ~ A
x\(a) =
~II
if a E A
If A, B are subsets of Nand XAo XB are recursive, show that X/, XAuB,
Xv.. B are also recursive.
11.9 Show that the characteristic function of the set of all even numbers is
recursive. Prove that the characteristic function of the set of all odd
integers is recursive.
11.10 Show that the function lex. .1') = x - v is partial recursive.
11.11 Show that a constant function over N. i.e. fen) =k for all n in N where
k is a fixed number. is primitive recursive.
11.12 Show that the characteristic function of a finite subset of N is primitive
recurSIve.
11.13 Show
that
the
addition
function fl (x,
y)
is
Turing-computable.
(Represent x and v in tally notation and use concatenation.)
11.14 Show that the Tming machine 1'v1 in the Post notation (i.e. the transition
function specified by quadruples) can be simulated by a Turing
machine Iv! (as defined in Chapter 9).
[Hint: The transition given by a quadruple can be simulated by two
quintuples of i'v1' by adding new states to M~]
Chapter 11: Computability
l;;l
345
11.15 Compute Z(4) using the TUling machine constructed for computing the
zero function.
11.16 Compute 5(3) using the Turing machine which computes 5.
11.17 Compute U?(2.
1. 1). Ui'(L 2, 1). U3\L 2, 1) using the Turing
machines which can compute the projection functions.
11.18 Construct a Turing machine which can compute lex) =x + 2.
11.19 Construct a Turing machine which can compute f{.ylo xJ = XI + 2 for
the arguments 1, 2 (i.e. Xl = 1, X2 = 2).
11.20 Construct a Turing machine which can compute l(xl' X2) =XI + Xo for
the arguments 2. 3 (i.e. Xl = 2. X2 = 3).
12
Complexity
When a problem/language is decidable, it simply means that the problem is
computationally solvable in principle, It may not be solvable in practice in the
sense that it may require enormous amount of computation time and memory,
In this chapter we discuss the computational complexity of a problem, The
proofs of decidability/undecidability are quite rigorous, since they depend
solely on the definition of a Turing machine and rigorous mathematical
techniques. But the proof and the discussion in complexity theory rests on the
assumption that P -:;t NP. The computer scientists and mathematicians strongly
believe that P
-:;t "Nt>. but this is still open.
This problem is one of the challenging problems of the 21st century. This
problem carries a prize money of $lM. P stands for the class of problems that
can be solved by a deterministic algorithm (i.e. by a Turing machine that
halts) in polynomial time: "Nt> stands for the class of problems that can be
solved by a nondeterministic algorithm (that is, by a nondeterministic TM) in
polynomial time; P stands for polynomial and
~TJ> for nondeterminisitc
polynomial. Another important class is the class of NP-complete problems
which is a subclass of "Nt>.
In this chapter these concepts are formalized and Cook's theorem on the
NP-completeness of SAT problem is proved.
12.1
GROWTH RATE OF FUNCTIONS
\Vhen we have two algorithms for the same problem, we may require a
comparison between the running time of these two algorithms. With this in
mind.
we study the grO\vth rate of functions defined on the set of natural
numbers.
In this section. lv' denotes the set of natural numbers.
346
Chapter 12: Complexity
,l;l,
347
Definition 12.1
Let ,j; g : N -7 R+ (R+ being the set of all positive real
numbers), We say that fen) = O(g(n»
if there exist positive integers C and
No such that
f(n) S Cg(n)
for all n
~ No,
In this case we say f is of the order of g (or f is 'big oh' of g)
Note:
f(n) = O(g(n» is not an equation. It expresses a relation between two
functions f and g.
EXAMPLE 12.1
Let f(n) = 4n3 + 5112 + 7n + 3. Prove that f(n) = 0(n3).
Solution
In order to prove that f(n) = 0(n3), take C = 5 and No = 10. Then
f(n) = 4n3 + 5n2 + 7n + 3 S 5n3
for n ~ 10
\Vhen n = 10. 5112 + 7n + 3 = 573 < 103. For
11 > 10, 5n2 + 7n + 3 < n3•
Then, f(n) = 0(11\
Theorem 12.1
If pen) = Gk1/ + Gk_lnk-I + ... + ([In + Go is a polynomial
of degree k over Z and az, > 0, then pen) = O(nk).
Proof
pen) = Qk1/ + aZ_lnk- 1 + ... + Gin + Go. As
Qk is an integer and
positive,
(lk
~ 1.
As {[i-i' aZ-2' ..., (Ii' ao and k are fixed integers, choose No such that for
all
11
~
each of the numbers
Hence,
n
lak-21
la11
lao I
1
-~2-' .. ,,~.
k
is less than
n
11
n
k
(*)
Also.
for all n 2 No
So,
S az + 1
pen) S 0/.
by (*)
where
Hence.
p(ll) = O(nk).
348
];I
Theory of Computer Science
Corollary
The order of a polynomial is detennined by its degree.
Defmition 12.2
An exponential function is a function q : N -'7 N defined by
q(n) = a"
for some fixed a > 1.
When n increases, each of n, n". 2" increases. But a comparison of these
functions for specific values of 11 will indicate the vast difference between the
growth rate of these functions.
TABLE 12.1
Growth Rate of Polynomial and Exponential Functions
n
fen) = n2
g(n) = n2 + 3n + 9
q(n) = 2"
1
1
13
2
5
25
49
32
10
100
139
1024
50
2500
2659
(113)1015
100
10000
10309
(1.27)1030
1000
1000000
1003009
(1.07)10301
From Table 12.1. it is easy to see that the function q(ll) grows at a very fast
rate when compared to fen) or g(ll). In particular the exponential function
grows at a very fast rate when compared to any polynomial of large degree.
We prove a precise statement comparing the growth rate of polynomials and
exponential function.
Deftnition 12.3
We say g '* O(j), if for any constant C and No, there exists
n
:2: No such that g(l1) > Cf(n).
Definition 12.4
If f and g are two functions and f = O(g), but g '* O(f),
we say that the growth rate of g is greater than that of.f (In this case
g(n)/f(n) becomes unbounded as
11 increases to 00.)
Theorem 12.2
The growth rate of any exponential function is greater than
that of any polynomial.
Proof
Let pen) = ae/ + ak-lnk-1 + . , . + a1n + ao and q(n) = a" for some
a > 1.
As the growth rate of any polynomial is determined by its term with the
highest power, it is enough to prove that Ilk = O(a") and a" '* O(ll), By
L'Hospital's rule. log 11 tends to 0 as n -'7 00. (Here log n = 10gel1.) If
n
then.
I
(~(n»" = le
(log n 'I
As
11 gets large, k ~-'-l-) tends to 0 and hence
~(Il) tends to O.
is unbounded for large
Chapter 12: Complexity
&;!
349
So we can choose No such that ::;(n) ::; a for all n 2: No. Hence n' =
::;(n)" ::; all, proving It' = Oed').
To prove a" i= O(n'), it is enough to show that a"hl is unbounded for
large n. But we have proved that n' ::; a" for large n and any positive integer
a"
k and hence for k + 1. SO ,/+J ::; d' or t:+l:::: 1.
n
Multiplying by n, n (~)
2: n, which means a~
nk+l
n
values of 11.
I
Note:
The function n1og " lies between any polynomial function and d
1 for
any constant a. As log n 2: k for a given constant k and large values of n,
nJog " ;:: 11' for large values of n. Hence nJog 11 dominates any polynomial. But
100
Joo ,
1.
1 1
I'
(log x)2
B
L"H
'1'
n ,,11= (eJog ,,)
"I =e(Jog"f.Letuscacuate
1m
.
y
. ospnas
x~O)
ex
.
(logx)2
I'
21
lIx
I'
210gx
l'
2
0
rule,
11m
= 1m( ogx)- = 1m --- = 1m -
= .
x~o:
ex
x~o:
e
X~O:
ex
X~O: ex
So (log n)2 grows more slowly than en. Hence I1Jog " = e{]og 11)2 grows more
slowly than 2'''. The same holds good when logarithm is taken over base 2
since logell and lOg211 differ by a constant factor.
Hence there exist functions lying between polynomials and exponential
functions.
12.2
THE CLASSES P AND NP
In this section we introduce the classes P and l'Ii"'P of languages.
Definition 12.5
A Turing machine M is said to be of time complexity T(n)
if the following holds: Given an input 11' of length n. M halts after making at
most T(n) moves.
Note:
In this case. lH eventually halts. Recall that the standard TM is called
a deterministic TM.
Definition 12.6
A language L is in class P if there exists some polynomial
T(n) such that L = TUI1) for some deterministic TM M of time complexity
T(n).
EXAMPLE 12.2
Construct the time complexity T(n) for the Turing machine M gIven in
Example 9,7.
350
~
Theory ofComputer Science
Solution
In Example 9.7. the step (i) consists of going through the input string (0"1")
forward and backward and replacing the leftmost 0 by x and the leftmost 1
by
Y. SO we require at most 2n moves to match a 0 with a 1. Step (ii) is
repetition of step (i)
11 times. Hence the number of moves for accepting a"Yl
is at most (2n)(nl. For strings not of the form ailb", TM halts with less than
2n~ steps. Hence T(Al) =
O(n~).
We can also define the complexity of algorithms. In the case of
algorithms. nn) denotes the running time for solving a problem \vith an input
of size n. using this algorithm.
In Example 12.2. we use the notation f- which is used in expressing
algorithm. For example. a f- b means replacing a by b.
iac denotes the smallest integer greater than or equal to a. This is called
the ceiling junction.
EXAMPLE 12.3
Find the running time for the Euclidean algorithm for evaluating gcd(a. b)
where a and 17 are positive integers expressed in binary representation.
Solution
The Euclidean algOlithm has the following steps:
1. The input is (a.
b)
')
Repeat until 17 = 0
3. Assign a f- a mod 17
-1-. Exchange a and b
5. Output a.
Step 3 replaces a by a mod b. If a/2 2 b, then a mod 17 < b :::; al2. If
a/2 < 17, then a < 217. Wlite a =
17 + r for some r < b. Then a mod b =
r < 17 < a/2. Hence ([ mod b :::; a/2. So a is reduced by at least half in size on
the application of step 3. Hence one iteration of step 3 and step 4 reduces a
and b by at least half in size. So the maximum number of times the steps 3
and -1- are executed is min {Dog~a1. 'log~bT If n denotes the maximum of the
number of digits of a and b. that is max{ilog~al.
!log~bl} then the number of
iterations of steps 3 and 4 is O(ll). We have to perform step 2 at most
min {ilog~aI.
ilog~bl} times or n times. Hence T(n) = nO(n) = O(n\
Note:
The Euclidean algorithm is a polynomial algOlithm.
DefInition 12.7
A language L is in class NP if there is a nondeterministic
TIvl M and a polynomial time complexity T(n) such that L = T(lv1) and Ai
executes at most nn) moves for every input
1\' of length n.
Chapter 12: Complexity
f;;!
351
We have seen that a deterministic TM i'vJI simulating a nondetenninistic
TM At exists (refer to Theorem 9.3). If T(n) is the complexity of M, then the
complexity of the equivalent deterministic TM M I is
2°!TIII)). This can be
justified as follows. The processing of an input string w of length n by M is
equivalent to a ·tree' of computations by M j • Let k be the maximum of the
number of choices forced by the nondeterministic transition function. (It is
maxlo(q, .1.')1, the maximum taken over all states q and all tape symbol K)
Every branch of the computation tree has a length T(n) or less. Hence the total
number of leaves is atmost kT(n). Hence the complexity of M I is at most
20ITII/I)
It is not known whether the complexity of M] is less than 2°([(11)). Once
again an answer to this question will prove or disprove P 1= NP. But there do
exist algorithms where T(n) lies between a polynomial and an exponential
function (refer to Section 12.1).
12.3
POLYNOMIAL TIME REDUCTION AND
NP-COMPLETENESS
If P j and
P~ are t\vo problems and
P~
EO P, then we can decide whether
Pi
EO P by relating the t\VO problems P j and
P~. If there is an algorithm for
obtaining an instance of P~ given any instance of Pj, then we can decide about
the problem P j' Intuitively if this algOlithm is a polynomial one, then the
problem PI can be decided in polynomial time.
DefInition 12.8
Let PI and P~ be two problems. A reduction from PI to P~
is an algorithm which converts an instance of PI to an instance of P~. If the
time taken by the algOlithm is a polynomial pen), n being the length of the
input of Pj. then the reduction is called a polynomial reduction PI to
P~.
Theorem 12.3
If there is a polynomial time reduction from P j to P: and if
P: is in P then P j is in P.
Proof
Let In denote the size of the input of PI' As there is a polynomial-
time reduction of P j to P:. the corresponding instance of
P~ can be got in
polynomial-time. Let it be O(;1'zi), So the size of the resulting input of P: is
atmost Cln! for some constant c. As P~ is in P. the time taken for deciding the
membership in P: is O(ni:} n being the size of the input of P:. So the total
time taken for deciding the membership of m-size input of P I is the sum of
the time taken for conversion into an instance of p, and the time for decision
of the corresponding input in
P~. This is O[mi + (cmjll which is the same
as o(mfk). So PI is in P.
Definition 12.9
Let L be a language or problem in NP. Then L is NP-
complete if
1. L is in NP
352
J;!
Theory ofComputer Science
2. For every language L' in ~'P there exists a polynomial-time reduction
of L' to L.
Note: . The class of NP-complete languages is a subclass of I\TP.
The next theorem can be used to enlarge the class of NP-complete
problems provided we have some knO\vn NP-complete problems.
Theorem 12.4
If Pi
is NP-complete, and there is a polynomial-time
reduction of Pi to P2, then P2 is NP-complete.
Proof
If L is any language in NP, we show that there is a polynomial-time
reduction of L to P2. As P1 is NP-complete, there is a polynomial-time
reduction of L to PI' SO the time taken for converting an n-size input string
11' in L to a string x in PI is at most Pl (/1) for some polynomial Pl' As there
is a polynomial-time reduction of PI to P:c. there exists a polynomial P2 such
that the input x to Pi is transferred into input y to P2 in at most P2(n) time.
So the time taken for transfomling w to y is at most PI (n) + P2(Pl (n)). As
Pl(n) + p:c(PI(n)) is a polynomial. we get a polynomial-time reduction of
L to P2. Hence P2 is NP-complete.
I
Theorem 12.5
If some NP-complete problem is in P, then P = NP.
Proof
Let P be an NP-complete problem and PEP. Let L be any
NP-complete problem. By definition, there is a polynomial-time reduction of
L to P. As P is in P, L is also in P by Theorem 12.3. Hence NP = P.
12.4
IMPORTANCE OF NP-COMPLETE PROBLEMS
In Section 12.3, we proved theorems regarding the properties of NP-complete
problems. At the beginning of this chapter we noted that the computer
scientists and mathematicians strongly believe that P 7: NP. At the same time,
no problem in .N'P is proved to be in P. The entire complexity theory rests
on the strong belief that P 7: NP.
Theorem 12.4 enables us to extend the class of NP-complete problems,
while Theorem 12.5 asselts that the existence of one NP-complete problem
admitting a polynomial-time algorithm will prove P = NP. More than 2500
NP-complete problems in various fields have been found so far.
We will prove the existence of an NP-complete problem in Section 12.5.
We will give a list of NP-complete problems in Section 12.6. Thousands of
NP-complete problems in various branches such as Operations Research,
Logic, Graph Theory, Combinatorics. etc. have been constructed so far. A
polynomial-time algorithm for anyone of there problems will yield a proof
of P = NP. But such multitude of NP-complete problems only strengthens the
belief of the computer scientists that P 7:- 1'01'. We will discuss more about this
in Section 12.7.
Chapter 12: Complexity
J;i
353
12.5
SAT IS NP-COMPLETE
In this section, we prove that the satisfiability problem for boolean expressions
(whether a boolean expression is satisfiable) is NP-complete. This is the first
problem to be proved NP-complete. Cook proved this theorem in 1971.
12.5.1
BOOLEAN
EXPRESSIONS
In Section 1.1.2, we defined a well-formed formula involving propositional
variables. A boolean expression is a well-formed formula involving boolean
variables x, y, z replacing propositions P, Q, R and connectives v, 1\ and -,.
The truth value of a boolean expression in x, y, z is determined from the truth
values of x, y, z and the truth tables for v, 1\ and -,. For example, -, x 1\ -,
(y V
~) is a boolean expression. The expression -, x 1\ -, (y V z) is true when
x is false, y is false and
~ is false.
Defmition 12.10
(a) A truth assignment t for a boolean expression E is the
assignment of truth values T or F to each of the variables in E. For example,
t = (F, F, F) is a truth assignment for (x, y,
~) where .Y, y, Z are the variables
in a boolean expression E(x, y,
~) =-,
X
1\ -, (y
V
~).
The value E(t) of the boolean expression E given a truth assignment t is
the truth value of the expression of E, if the truth values give by t are assigned
to the respective vmiables.
If t = (F, F, F) then the truth values of -, x and -, (y v z) are T and T.
Hence the value of E = -,
X /\ -, (v V z) is T. So E(t) = T.
Definition 12.11
A truth assignment t satisfies a boolean expression E if the
truth value of E(l) is T. In other words, the truth assignment t makes the
expression E true.
Defmition 12.12
A boolean expression E is satisfiable if there exists at least
one truth assignment t that satisfies E (that is E(t) = T). For example, E =
-, X
1\ -, (y
V
~) is satisfiable since E(t) = T when t = (F, F, F).
12.5.2
CODING
A
BOOLEAN
EXPRESSION
The symbols in a boolean expression are the variables
.Y, y, z, etc. the
connectives v. /\, -,. and parantheses ( and ). Thus a boolean expression in
three variables will have eight distinct symbols. The variables are written as
Xl'
x~, X3- etc. Also we use X" only after using XI,
x~, ..., X,,_I for variables.
We encode a boolean expression as follows:
1. The variables Xl,
x~, x3' ... are written as xl, xlO, ;d 1. .. _etc. (The
binary representation of the subscript is written after x.)
2. The connectives v, /\, -', (, and ) are retained in the encoded
expresslOn.
354
~
Theory ofComputer Science
For example, -, x 1\ -, (1' V z) is encoded as -, x11\ -, (xlO v xlI), (where
x, y, z are represented by Xl'
x~, x:<).
Note:
Any boolean expression is encoded as a stling over L = {x, 0, 1, v,
1\. -,.
C (,
)}
Consider a boolean expression having
III occurrences of variables,
connectives and parantheses. The variable
XIII can be represented using
1 + log~ III symbols (x together \vith the digits in the binary representation of
m). The other occurrences require less symbols. So any occurrence of a
variable. connective or a parenthesis requires at most 1 + log~ 111 symbols over
L. So the length of the encoded expression is at most Oem log 111).
As our interest is only in deciding whether a problem can be solved in
polynomial-time. we need not distinguish between the length of the coded
expression and the number of occurrences of variables etc. in a boolean
expression.
12.5.3
COOK'S THEOREM
In this section we define the SAT problem and prove the Cook's theorem that
SAT is NP-complete.
Definition 12.13
The satisfiability problem (SAT) is the problem:
Given a boolean expression. is it satisfiable?
Note:
The SAT problem can also be formulated as a language. We can
define SAT as the set of all coded boolean expressions that are satisfiable. So
the problem is to decide whether a given coded boolean expression is in SAT.
Theorem 12.6
(Cook's theorem) SAT is NP-complete.
Proof
PART I: SAT E
~T.
If the encoded expression E is of length n, then the number of variables is
lnlr Hence. for guessing a truth assignment t we can use multitape TM for
E. The time taken by a multitape NTM M is O(n). Then M evaluates the
value of E for a truth assignment t. This is done in O(n~) time. An equivalent
single-tape TM takes 0(n4) time. Once an accepting truth assignment is found,
M accepts E and I'v! and halts. Thus we have found a polynomial time
NTM for SAT. Hence SAT E NP.
PART II: POLYNOMIAL-TL\1E REDUCTION OF ANY L IN NP TO SAT.
1. Construction of NTM for L
Let L be any language in ~T. Then there exists a single-tape NTM M and a
polynomial pen) such that the time taken by M for an input of length n is at
most pen) along any branch. We can further assume that this M never writes
a blank on any move and never moves its head to the left of its initial tape
position (refer to Example 12.6).
Chapter 12: Complexity
J;;l
355
If M accepts an input 11' and I}v I= n. then there exists a sequence of moves
of M such that
1.
O'{! is the initial ID of M with input w.
2.
0"0 ~ at ~ ... ~
(XI;, k ::; pen).
3. al; is an ID with an accepting state.
4. Each
(Xi is a string of nonblanks, its leftmost symbol being the
leftmost symbol of w (the only exception occurs when the processing
of w is complete, in which case the ID is qb).
2. Representation of Sequence of Moves of M
As the maximum number of steps on w is pen) we need not bother about the
contents beyond pen) cells. We can write ai as a sequence of pen) + 1 symbols
(one symbol for the state and the remaining symbols for the tape symbols).
So
Gi = XiOXil ... Xi, p(Il)'
By assuming Q n r = 0, we can locate the state in ai and hence the
position of the tape head. The length of some ID may be less than pen). In
this case we pad the ID on the right with blank symbols. so that all IDs are
of the same length pen) + 1. Also the acceptance may happen earlier. If alll
is an accepting ID in the course of processing 11', then we write O'{! 1- ... f-
a;;! f-
all! ... f-
am = ai.pU/I·
Thus all IDs have pen) + 1 symbols and any computation has pen) moves.
TABLE 12.2
Array of IDs
ID
0
Ct.o
.'1'00
.'1'01
Ct.,
.'1'10
.'1',1
Ct.,
.'1"0
X"
Ui+l
X", 0
Xi+1 1
Cf.,D(ni
j - 1
X,j-1
Xj+1,j-1
j
j + 1
pen)
So we can represent any computation as an (p(n) +1) X (p(n) + 1) alTay
as in Table 12.2.
3. Representation of IDs in Terms of Boolean Variables
We define a boolean variable
conesponding to ii, j)th entry in the ith ID.
The variable YiiA represents the proposition that
=A. where A is a state or
tape symbol and 0 ::; i, j :; pen).
We simulate the sequence of IDs leading to the acceptance of an input
string w by a boolean expression. This is done in such a way that M accepts
1\' if an only if the simulated boolean expression El1.\! is satisfiable.
356
l;!
Theory of Computer Science
4. Polynomial Reduction of M to SAT
In order to check that the reduction of M to SAT is correct. we have to ensure
the correctness of
(a) the initial ID.
eb) the accepting ID. and
(c) the intermediate moves between successive IDs,
(al Simulation of initial ID
Xoo must start with the initial state qo of M followed by the symbols of
H' = ala:
' ..
all of length n and ending with b's (blank symbol). The
cOlTesponding boolean expression S is defined as
S = )'00'10 i\ )'Ola; /\ )'O]a, /\ ... /\ )'Olla" /\ YO,II+1-I) /\
. ..
/\ YO'P(III,b
Thus given an encoding of M and 1V, we can write S in a tape of a multiple
TM M t, This takes O(p(n)) time,
(b) Simulation of accepting ID
a')II" is the accepting ID. If Pi, P:. . , ., P, are the accepting states of M, then
~Ji'" contains one of Pi' s. 1 :S i :S k in any place j. If al'ill! contains an accepting
state Pi in jth position. then
is the accepting state Pi' The corresponding
boolean expression covering all the cases (0 :S j :S pen), 1 :S i :S k) is given
by
F = Fo V F] v .. , V Fpinl
where
F· -,-
v
1', V ... v
Each Fi has k variables and hence has constant number of symbols
depending on M but not on n. The number of F;'s in F is pen). Thus given
an encoding of M and H, F can be \V11tten in O(p(n)) time on the multiple
TMMI ,
(c) Simulation of intermediate moves
We have to simulate valid moves
a i r
ai+], i = 0,
1, 2, '"
pen).
COlTesponding to each move. \ve have to define a boolean variable Ni. Hence
the entire sequence of IDs leading to acceptance of w is
N = No /\ Nt /\ ... /\ NpiliH
First of all note that the symbol
X i+Lj can be determined from Xi,j-lo
Xij,
Xi.)+] by the move (if there is one changing ai to a different ai+i)' For every
position (i, j). we have t\\70 cases:
Case 1
The state of ai is at position j.
Case 2
The state of ai is not in any of the (j - l)th, jth and (j + l)th
positions.
Chapter 12: Complexity
~
357
Case 1 is taken care of by a variable Ai} and Case 2 by a variable Bu.
The variable Ni will be designed in such a way that it gurantees that ID
(X,+] is one of the IDs that follows the ID (Xi'
X i+l,j can be determined from
(i) the three symbols Xi,j_], Xi).
Xi,I+] above it
(ii) the move chosen by the nondeterministic TM M when one of the three
symbols (in (i» is a state.
If the state of (Xi is not Xij, X i.j- 1 or X i.j+l • then Xi+],j
= XU, This is taken
care of by the variable BU'
If Xu is the state of (Xi, then X i.I+1 is being scanned by the state Xu' The
move corresponding to the state-tape symbol pair (Xu' X i.i +]) will determine
the sequence X i+1•i - 1 Xi+l. j Xi+1,j+!' This is taken care of by the variable Au'
We write Ni = Ai (Aii v BIi), where A is taken over all is, 0 :::: j :::: pen).
(i) Formulation of Bij When the state of (Xi is none of Xi.i- b Xli' Xi.j +].
then the transition corresponding to
(Xi r-- ai+l will not affect X i.i +l • In this
case Xi+l,i = Xij
Denote the tape symbols by ZI' Z:, ..., Z,. Then Xi,i-I' Xi,i and Xi,i+]
are the only tape symbols. So we ""Tite Bii as
Bij =(Yi.i-1. Zl
V
Yi.i-1. Z2
V
."
V
Yi.}-1. z)
A
V
Yi.j+1. z)
A
C\'i,i, Zl
A
Zj)
V
Z2
A
Yi+l.j,Z2
V
'"
V
(Yi,j.z,.
A Yi+J,j,Z)
This first line of Bij says that
Xi. H
is one of the tape symbols
ZI_ Z:, . , ., Zj" The second and third lines are regarding X i.i and Xi.i+]. The
fourth line says that Xli and Xi,j+1 are the same and the common value is any
one of Zj_ Z:. ..., Zj"
Recall that the head of M never moves to the left of O-cell and does not
have to move to the right of the p(n)-cell. So Bio will not have the first line
and Bi. jJlil
will not have the third line.
(ii) Formulation of Ai} This step corresponds to the correctness of
the 2 x 3 array (see Table 12.3).
TABLE 12.3
Valid Computation
'I-----,--------,.------1
I
Xi )-l
Xfj,
x i )+' I
~---
----i---1
! X,r+1,j-1
Xi+1,.)
xi+1,j+11
358
J;;i
Theory ofComputer Science
The expression B U takes care of the case when the state of (Xi is not at the
position X i.j - I , X i.j or X i.j +!. The AU cOlTesponds to the case when the state
of (Xi is at the position Xi)' In this case we have to assign boolean variables
to six positions given in Table 12.3 so that the transition conesponding to
(Xi ~
(Xi+1 is described by the variables in the box correctly.
We say that that an assignment of symbols to the six variables in the box
IS valid if
1.
Xu is a state but X i.j _1 and X i.j +! are tape symbols.
2. Exactly one of X i+ l .j- b
Xi+l,j, Xi+l. j +i is a state.
3. There is a move which explains how (Xi.j _l , XU, Xi,j+l) changes to
(Xi+1.j-l,
Xi+l,j, X i+1.j+l) in
(Xi ~
(Xi+!'
There are only a finite number of valid assignments and AU is obtained
by applying OR (that is v) to these valid assignments. A valid assignment
conesponds to one of the fol1owing four cases:
Case A
Case B
Case C
Case D
(p,
C, L)
E
O(q, A)
(p. C, R)
E
O(q, A)
(Xi = (Xi+! (when
(Xi and
(Xi"-l contain an accepting state)
j = 0 and j = pen)
Case A
Let D be some tape symbol of Ai. Then Xi.j-1XijXi,j+1 = DqA and
X i+1. i-IXi+ Li X i+Li+ 1 = pDC. This can be expressed by the boolean variable.
Yi.j-I, D ;\ Yi.j. '/ /\ Yi,j+L\
/\ Yi+J.j-l.p /\
Yi+l.j. D /\
Yi+Lj+L C
Case B
As in case A, let D be any tape symbol. In this case Xi j-1XUXi,j+ I
= DqA
and
Xi-\. i-lXi+1 ,jXi+1.j+l = DCp.
The
corresponding
boolean
expression is
D /\
q
/\ )'i/+1.-1
/\ "i+J.!-J.D /\ Yi+J.j,C /\ Yi+l,j+J.p
Case C In this case Xij-1XUXi]+1 = Xi"-Li-lXi+l.jXi+l.j+I'
In this case the same tape symbol say D appears in Xi,j-J and Xi+Lj- i ; some
other tape symbol say
D ' in X i.j +1 and
X i+1.j+I' Xi,j and Xi+!,j contain the
same state. One typical boolean expression is
)'i,j-LZ,.
/\ "i.j.'!
/\ "i,]+J.Zi /\ Yi+l.j-LZk /\ Yi+Lj,'! /\ Yi+Lj+I'zi
Case D
\Vhen j = 0, we have only X!C0il and Xi+l. 0 Xi+J. I' This is a special
case of Case B. j = pen) conesponds to a special case of Case A.
So. Aii is defined as the OR of all valid telTTIS obtained in Case A to
Case D.
(iii) Definition of N i and N
We define Ni and N by
N i = (A iO v
B iO) /\ (Ail v
B il )
/\ ... /\
(Ai. Pill)
V
B i,
N =
/\ N I /\ No /\ , .. /\ NpIllJ-1
Chapter 12: Complexity
~
359
(iv) Time taken for writing N
The time taken to write Bii is a constant
depending on the number Ir I of tape symbols. (Actually the number of
variables in BU is SI r I). The time taken to write AU depends only on the
number of moves of M. As N; is obtained by applying OR to AU /\ Bij,
o ::; i ::; pen) - L 0 ::; j ::; pen) - 1, the time taken to write on Ni is O(p(n».
As N is obtained by applying /\ to No, Nj, .. "
Nl;ll1l~j' the time taken to write
N is p(n)O(p(n»
=
O(p~(n).
.
5. Completion of Proof
Let EM. H = S /\ N /\ F.
We have seen that the time taken to write Sand Fare O(p(n»
and the
time taken for N is O(p~(n», Hence the time taken to write Elf." is O(p~(n».
Also M accepts
]V if and only if EAt.\! is satisfiable.
Hence the deterministic multitape TM Mj can convert w to a boolean
expression EM. II in
O(p~(n»
time~ An equivalent single tape TM takes
01/'(11» time. This proves the Part II of the Cook's theorem. thus completing
the proof of this theorem.
I
12.6
OTHER NP-COMPLETE PROBLEMS
In the last section, we proved the NP-completeness of SAT. Actually it is
difficult to prove the NP-completeness of any problem. But after getting one
NP-complete problem such as SAT. \ve can prove the NP-completeness of
problem P' by obtaining a polynomial reduction
of SAT to P'.
The
polynomial reduction of SAT to P' is relatively easy. In this section we give
a list of NP-complete problems without proving their NP-completeness. Many
of the NP-complete problems are of practical interest.
1. CSAT-Given a boolean expression in Cp,r (conjunctive normal
form-Definition 1.10), is it satisfiable?
We can prove that CSAT is NP-complete by proving that CSAT is
in NP and getting a polynomial reduction from SAT to CSAT,
'1
Hamiltonian circuit problem--Does G have a Hamiltonian circuit (i.e.
a circuit passing through each edge of G exactly once)?
3. Travelling salesman problem (TSP)-Given n cities, the distance
between them and a number D, does there exist a tour programme for
a salesman to visit all the cities exactly once so that the distance
travelled is at most D?
4. Vertex cover problem-Given a graph G and a natural number k, does
there exist a vertex cover for G with k vertices? (A subsets C of
vertices of G is a veltex cover for G if each edge of G has an odd
vertex in C.)
360
g
Theory of Computer Science
5. Knapsack problem-Given a set A = {al' a2, ..., an} of nonnegative
integers. and an integer K, does there exist a subset B of A such that
~ b i = K?
!J,ER
'
This list of NP-complete problems can be expanded by having a
polynomial reduction of known NP-complete problems to the problems which
are in I\it> and which are suspected to be NP-complete.
12.7
USE OF NP-COMPLETENESS
One practical use in discovering that problem is NP-complete is that it
prevents us from wasting our time and energy over finding polynomial or easy
algorithms for that problem.
Also \ve may not need the full generality of an NP-complete problem.
Particular cases may be useful and they may admit polynomial algOlithms.
Also there may exist polynomial algorithms for getting an approximate
optimal solution to a given NP-complete problem.
For example, the travelling salesman problem satisfying the triangular
inequality for distances between cities (i.e. dij ::; dik + dki for all i, j, k) has
approximate polynomial algorithm such that the ratio of the error to the
optimal values of total distance travelled is less than or equal to 1/2.
12.8
QUANTUM COMPUTATION
In the earlier sections we discussed the complexity of algorithm and the dead
end was the open problem P = I\it>. Also the class of NP-complete problems
provided us with a class of problems. If we get a polynomial algorithm for
solving one NP-complete problem we can get a polynomial algorithm for any
other NP-complete problem.
In 1982. Richard Feynmann, a Nobel laurate in physics suggested that
scientists should start thinking of building computers based on the principles
of quantum mechanics. The subject of physics studies elementary objects and
simple systems and the study becomes more intersting when things are larger
and more complicated. Quantum computation and information based on the
principles of Quantum Mechanics will provide tools to fill up the gulf between
the small and the relatively complex systems in physics. In this section we
provide a brief survey of quantum computation and information and its impact
on complexity theory.
Quantum mechanics arose in the early 1920s, when classical physics could
not explain everything even after adding ad hoc hypotheses. The rules of
quantum mechanics were simple but looked counterintuitive, and even Albert
Einstein reconciled himself with quantum mechanics only \vith a pinch of salt.
Quantum i'vfechanics is real black magic calculus.
-A. Einstein
Chapter 12: Complexity
);!
361
12.8.1
QUANTUM COMPUTERS
We know that a bit (a 0 or a 1) is the fundamental concept of classical
computation and information. Also a classical computer is built from an
electronic circuit containing wires and logical gates. Let us study quantum bits
and quantum circuits which are analogous to bits and (classical) circuits.
A quantum bit, or simply qubit can be described mathematically as
Ilf/) = alO; + 1310)
The qubit can be explained as follows. A classical bit has two states, a °and
a 1. Two possible states for a qubit are the states 10; and 11). (The notation
I-l is due to Dirac.) Unlike a classical bit, a qubit can be in infinite number
of states other than 10) and 11). It can be in a state IVJ> = alO) + 1310), where
a and 13 are complex numbers such that lal2 + 11312 = 1. The 0 and 1are called
the computational basis states and IVJ> is called a superposition. We can call
Ilf/) = alO) + 1310) a quantum state.
In the classical case, we can observe it as a 0 or a 1. But it is not possible
to determine the quantum state on observation. When we measure/observe a
qubit we get either the state 10; with probability lal2 or the state 11) with
probability 11312.
This is difficult to visualize. using our 'classical thinking' but this is the
source of power of the quantum computation.
Multiple qubits can be defined in a similar way. For example. a two-qubit
system has four computational basis states, 100), 1°1), 110; and Ill) and
quantum states Ilf/) = O'{)oIOO) + aodOl) + aiOlI0) + a11I11) with 10'{)o12 + lo:od2
+ la1012 + lal11
2 = l.
Now we define the qubit gates. The classical NOT gate interchanges 0
and 1. In the case of the qubit the NOT gate, a 10) + 1311), is changed to
al1) + 1310;.
The action of the qubit NOT gate is linear on two-dimensional complex
vector spaces. So the qubit NOT gate can be described by
lal~[O 1]la]=[13]
L13J
1 ° l13
a
The matrix [~
~] is a unitary matrix. (A matrix A is unitary if A adjA = I.)
We have seen earlier that {NOR} is functionally complete (refer to
Exercises of Chapter 1). The qubit gate conesponding to NOR is the
cLntrolled-NOT or CNOT gate. It can be described by
IA. B) -7 IA. B EB A)
362
g
Theorv of Computer Science
where EB denotes addition modulo 2. The action on computational basis is
100) ~ 100). 101) ~ 101). 110) ~ Ill), Ill) ~ 110). It can be described by
the following 4 x 4 unitary matrix:
1 ° °
~I
°
1 °
u cx ° ° °
° °
1 OJ
Now. we are in a position to define a quantum computer:
A quantum computer is a system built from quantllm circuits, contaznmg
wires and elementary quantum gates, to carry out manipulation of quantum
il(foJ7natiol/.
12.8.2
CHURCH-TURING THESIS
Since 1970s many techniques for controHing the single quantum systems have
been developed but with only modest success. But an experimental prototype
for performing quantum cryptography. even at the initial level may be useful
for some real-world applications.
Recall the Church-Turing thesis which asserts that any algorithm that can
be performed on any computing machine can be performed on a Turing
machine as well.
NIiniaturization of chips has increased the power of the computer. The
grmvth of computer power is now described by Moore's law. which states that
the computer power will double for constant cost once in every two years.
Now it is felt that a limit to this doubling power will be reached in two or
three decades. since the quantum effects will begin to interfere in the
functioning of electronic devices as they are made smaller and smaller. So
efforts are
on to provide a theory of quantum computation \vhich wi]]
compensate for the possible failure of the Moore's law.
As an algorithm requiring polynomial time was considered as an efficient
algorithm. a strengthened version of the Church-TUling thesis was enunciated.
Any algorithmic process can be simulated efficiently by a Turing machine.
But a challenge
to the strong Cburch-Turing thesis arose from analog
computation. Certain types of analog computers solved some problems
efficiently whereas these problems had no efficient solution on a TUling
machine. But when the presence of noise was taken into account, the power
of the analog computers disappeared.
In mid-1970s. Robert Soiovay and Volker Strassen gave a randomized
algorithm for testing the primality of a number. (A deterministic polynomial
algorithm was given by Manindra Agrawal. Neeraj Kayal and Nitein Saxena
of IIT Kanpur in 2003.) This led to the modification of the Church thesis.
Chapter 12: Complexity
~
363
Strong Church-Turing Thesis
An\' algorithmic process can be simulated efficiently llsing a nondetenninistic
Turing machine.
In 1985, David Deutsch tried to build computing devices using quantum
mechanics.
Computers are ph}'sical o~jects, and computations are physical processes.
What computers can or callnot compute is determined by the lmv of
physics alone, and not by pure mathematics
-David Deutsch
But it is not known whether Deutsch's notion of universal quantum
computer will efficiently simulate any physical process. In 1994, Peter Shor
proved that finding the prime factors of a composite number and the discrete
logarithm problem (i.e. finding the positive value of s such that b =as for the
given positive integers a and b) could be solved efficiently by a quantum
computer. This may be a pointer to proving that quantum computers are more
efficient than Turing machines (and classical computers).
12.8.3
POWER OF QUANTUM COMPUTATION
In classical complexity theory, the classes P and NP play a major role, but
there are other classes of interest. Some of them are given below:
L -
The class of all decision problems which may be decided by a TM
running in logarithmic space.
PSPACE-The class of decision problems which may be decided on a Turing
machine using a polynomial number of working bits, with no limitation on the
amount of time that may be used by the machine.
EXP -
The class of all decision problems which may be decided by a TM in
exponential time, that is, O(2"
k), k being a constant.
The hierarchy of these classes is given by
L
<;;; P
<;;; NP <;;; PSPACE
<;;; EXP
The inclusions are strongly believed to be strict but none of them has
been
proved so far in classical complexity theory.
We also have two more classes.
BPP- The class of problems that can be solved using the randomized
algorithm in polynomial time, if a bounded probability of error (say 1110) is
allowed in the solution of the problem.
Bnp-The class of all computational problems which can be solved
efficiently (in polynomial time) on a quantum computer where a bounded
probability of error is allowed. It is easy to see that BPP <;;; BQP. The class
BQP lies somewhere benveen P and PSPACE, but where exactly it lies with
respect to P, NP and PSPACE is not known,
l
364
~
Theory of Computer Science
It is easy to give non-constructive proofs that many problems are in EXP,
but it seems very hard to prove that a particular class of problems is in EXP
(the possibility of a polynomial algorithm of these problems cannot be ruled
out).
As far as quantum computation is concemed. two important classes are
considered. One is BQP. which is analogous to BPP. The other is NPI
(Nt> intermediate) defined by
NPI -
The class of problems which are neither in P nor NP-complete
Once again. no problem is shown to be in
~t>I. In that case P "* NP is
established.
Two problems are likely to be in NPI, one being the factoring problem
(i .e. given a composite number 11 to find its prime factors) and the other being
the graph isomorphism problems (i.e. to find whether the given undirected
graphs with the same set of vertices are isomorphic).
A quantum algorithm for factoring has been discovered. Peter Shor
announced a quantum order-finding algorithm and proved that factoring could
be reduced to order-finding. This has motivated a search for a fast quantum
algOlithm for other problems suspected to be in
~t>l.
Grover developed an algorithm called the quantum search algorithm. A
loose formulation of this means that a quantum computer can search a
pm1icular item in a list of N items in O(fN) time and no further improvement
is possible. If it were OOog N). then a quantum computer can solve an NP-
complete problem in an efficient way. Based on this, some researchers feel that
the class BQP cannot contain the class of NP-complete problems.
If it is possible to find some structure in the class of NP-complete
problems then a more efficient algorithm may become possible. This may
result in finding efficient algorithms for NP-complete problems. If it is
possible to prove that quantum computers are strictly more powerful than
classical computers, then it \vill follow that P is properly contained in
PSPACE. Once again. there is no proof so far for P c
PSPACE.
;=
12.8.4
CONCLUSION
Deutsch proposed the first blueprint of a quantum computer. As a single qubit
can store two states 0 and 1 in quantum superposition, adding more qubits to
the memory register will increase the storage capacity exponentially. When
this happens. exponential complexity will reduce to polynomial complexity.
Peter Shor' s algorithm led to the hope that quantum computer may work
efficiently on problems of exponential complexity.
But problems arise at the implementation stage. 'When more interacting
qubits are involved in a circuit, the surrounding environment is affected by
those interactions. It is difficult to prevent them. Also quantum computation
will spread outside the computational unit and will irreversibly dissipate useful
Chapter 12: Complexity
);!
365
information to the environment. This process is called decoherence. The
problem is to make qubits interact with themselves but not with the
environment. Some physicists are pessimistic and conclude that the efforts
cannot go beyond a few simple experiments involving only a few qubits.
But some researchers are optimistic and believe that efforts to control
decoherence will bear fruit in a few years rather than decades.
It remains a fact that optimism, however overstretched, makes things
happen. The proof of Fermat's last theorem and the four colour problem are
examples of these. Thomas Watson. the Chairman of IBM. predicted in 1943,
"1 think there is a world market for maybe five computers". But the growth
of computers has very much surpassed his prediction.
Charles Babbage (1791-1871) conceived of most of the essential elements
of a modem computer in his analytical engine. But there was not sufficient
technology available to implement his ideas. In 1930s, Alan Turing and
John von Neumann thought of a theoretical model. These developments in
'Software'
were matched by
'Hardware'
support. resulting in the first
computer in the early 1950s. Then. the microprocessors in 1970s led to the
design of smaller computers with more capacity and memory.
But computer scientists realized that hard\vare development will improve
the power of a computer only by a multiplicative constant factor. The study
of P and NP led to developing approximate polynomial algorithms to
NP-complete problems. Once again the importance of software arose. Now the
quantum computers may provide the impetus to the development of computers
from the hardware side.
The problem of developing quantum computers seems to be very hard but
the history of sciences indicates that quantum computers may rule the universe
in a few decades.
12.9
SUPPLEMENTARY
EXAMPLES
EXAMPLE 12.4
Suppose that there is an NP-complete problem P that has a deterministic
solution taking 0(n1og 11) time (here log n denotes log2n). What can you say
about the running time of any other NP-complete problem Q?
Solution
As Q E NP. there exists a polynomial pen) such that the time for reduction
of Q to P is atmost pen). So the running time for Q is O(p(n) + p(n)logPIIII).
"'s p(n)iOgp[il( dominates pen), we can omit pen) in pen) + p(n)10gplil). If the
degree of pen) is k, then p(n), = O(ll). So we can replace pen) by It So
p(n)logpilll =0((n'f k1!"') = O(nHoglI). Hence the running time of Q is Olrlogll)
for some constant c.
366
~
Theory ofComputer Science
EXAMPLE 12.5
Show that P is closed under (a) union, (b) concatenation, and (c) comple-
mentation.
Solution
Let L j and L2 be two languages in P. Let w be an input of length n.
(a) To test whether W E L j
U L2, we test whether W E L j • This takes
polynomial time pen). If W eo L b test another W
E
L2. This takes
polynomial time q(n). The total time taken for testing whether
W E L j U L 2 is pen) + q(n), which is also a polynomial in n. Hence
L j
U L 2 E P.
(b) Let w =xIX2 •.. x//, For each k, 1 ::; k ::; n - 1, test whether XjX2 •.• Xk
E L j and Xk+jXk+2 ... X/1 E L2• If this happens, W E L jL 2. If the test
fails for all k,
W eo L jL2• The time taken for this test for a particular
k is pen) + q(n), where pen) and q(n) are polynomials in n. Hence the
total time for testing for all k's is at most n times the polynomial
pen) + q(n). As n(pn) + q(n) is a polynomial, L]L2 E P.
(c) Let M be the polynomial time TM for Lt. We construct a new TM
AI j as follows:
1. Each accepting state of M is a nonaccepting state of Mj from
which there are no further moves. So if M accepts w, M] on
reading
W will halt without accepting.
2. Let qf be a ne\v state. which is the accepting state of M j • If
O(q, a) is not defined in M, define 0,\1
1(q. a) = (qji a, R). So,
W e:
L if and only if M accepts wand halts. Also M j is a
polynomial-time TM. Hence L{ E P.
EXAMPLE 12.6
Show that every language accepted by a standard TM M is also accepted by
a TM lvl] with the following conditions:
1. M j ' s head never moves to the left of its initial position.
2. M j will never write a blank.
Solution
It is easy to implement Condition 2 on the new machine. For the new TM,
create a new blank b'. If the blank is written by M, the new Turing machine
writes b'. The move of this new TM on seeing b' is the same as the move of
M for b. The new TM satisfies the Condition 2. Denote the modified TM by
M itself. Define the modified M by
M = (Q. 2:. r, 0, q2, b, F)
Define a new TM M j as
M j = (Qj. 2: x {b}, r i • OJ,
qQ, [b, bJ, F j )
Chapter 12: Complexity
!;I,
367
where
QI = {qo, qd u (Q x {U. L})
Ij = (I x n u
{[x, *] Ix E n
qo and ql are used to initiate the initial move of M. The two-way infinite tape
of M is divided into two tracks as in Table 12.4. Here * is the marker for the
leftmost cell of the lower track. The state [q, U] denotes that M 1 simulates M
on the upper track. [q, L] dentoes that M j simulates M on the lower track. If
M moves to the left of the cell with *, M 1 moves to the right of the lower
track.
TABLE 12.4
Folded Two-way Tape
Xo
X1
X2
II
*
K..1
K..2
K.:J
i
I
We can define F 1 of M j by
F1 = F x {U, L}
We can describe D as follows:
1.
Dj(qo, [a,
bJ) = (qj,
[a. *], R)
D(qj. [X, bJ) = ([q:, U],
[X, b], L)
By Rule 1, M j marks the leftmost cell in the lower track with * and
initiates the initial move of M.
2.
If D(q, X) = (p,
Y, D) and 2
E r. then:
(i)
D1([q, [;1,
[X, 2]) = ([P, [n
[Y, 2], D) and
(ii)
Dj([q, L], [2, X]) = ([P,
L],
[2,
Y], 15)
where 15 = L if D = Rand 15 = R if D = L.
By Rule 2. M 1 simulates the moves of M on the appropriate track. In
(i) the action is on the upper track and 2 on the lower track is not
changed. In (ii) the action is on the lower track and hence the
movement is in the opposite direction 15; the symbol in the upper
track is not changed.
3. If D(q. X) = (p,
Y, R) then
Dj([q. L], [X, *J) = D1([q, [;1, [X, *J) = ([P, [;1,
[Y, ,,], R)
When M1 see * in the lower track. M moves right and simulates M
on the upper track.
4. If D(q, X) = (p.
Y, L), then
D1([q, L], [X,
*J) = D1([q, [;1, [X, "J) = ([P. L]. [1', *]. R)
When M j sees' in the lower track and M's movement is to the left
of the cell of the two-way tape corresponding to the ;, cell in the
lower track. the M's movement is to X_I and the M1's movement is
also to X_I but towards the right. As the tape of M is folded on the
368
~
Theory of Computer Science
cell with *. the movement of M to the left of the *' cell is equivalent
to the movement of M I to the right.
M reaches q in F if and only if M] reaches [q, L) or [q, R). Hence
T(M) = T(ivI I ).
EXAMPLE 12.7
We can define the 2SAT problem as the satisfiability problem for boolean
expressions written as /\ of clauses having two or fewer variables. Show that
2SAT is in P.
Solution
Let the boolean expression E be an instance of the 2SAT problem having
11
variables.
Step 1
Let E have clauses consisting of a single variable (xi or x;). If (x;)
appears as a clause in E, then Xi has to be assigned the truth value T in order
to make E satisfiable. Assign the truth value T to Xi' Once Xi has the truth value
T, then (Xi V)) has the truth value T irrespective of the truth value of 'Yi (Note
that Xi can also be x). So (.I:i V x)
or (Xi v x) can be deleted from E. If
E contains (x i V
as a clause, then xi should be assigned the truth value T
in order to make E satisfiable. Hence we replace (Xi v Xi)
by xi in E so that
.,> should be assigned the truth value T is order to make E satisfiable. Hence
we replace (x i V x) by ."li in E so that xi can be assigned the truth value T
later. If we repeat this process of eliminating clauses with a single variable (or
its negation). we end up in t\vo cases.
Case 1
We end up with (Xi) /\ (xJ In this case E is not satisfiable for any
assignment of truth values. We stop.
Case 2
In this case all clauses of E have two variables. (A typical clause is
xi v
Xi or Xi v
X
Step 2
We have the apply step 2 only in Case 2 of step 1. We have already
assigned truth values for variables not appealing in the reduced expression E.
Choose one of the remaining variables appearing in E. If we have chosen Xi,
assign the truth value T to Xi' Delete ''Ci v
Xj or Xi v xi from E. If xi
V
Xj
appears in E, delete xi to get (Yi)' Repeat step 1 for clauses consisting of a
single variable. If Case 1 occurs. assign the truth value F for Xi and proceed
with E that \ve had before applying step 1.
Proceeding with these iterations, we end up either in unsatisfiability of E
or satisfiability of E.
Step 2 consists of repetition of step 1 at most 11 times and step 1 requires
0(11) basic steps.
Chapter 12: Complexity
);!
369
Let /1 be the number of clauses in E. Step 1 consists of deleting (x; v xi)
from E or deleting x i from (x; v xi)' This is done at most
11 times for each
clause. In step 2, step 1 is applied at most two times. one for Xi and the second
for
Xi' As the number of variables appearing in E is less than or equal to n,
we delete
(Xi v
Xj) or delete
Xi from (x; V),) at most O(n) times while
applying steps 1 and 2 repeatedly. Hence 2SAT is in P.
SELF-TEST
Choose the correct answer to Questions 1-7:
1. If f(l1) = 2n3 + 3 and g(n) = 10000n2 + 1000, then:
(a) the growth rate of g is greater than that of f
(b) the growth rate of f is greater than that of g.
(c) the growth rate of f is equal to that of g.
(d) none of these.
2. If fen) = n3 + 4/1 + 7 and g(n) = 1000n2 + 10000. then f(n) + g(n) is
(a) 0(/12)
(b)
0(11)
(c) 0(n3)
(d) 0(/15)
3. If f(n) = O(lh and g(n) = O(lh then f(n)g(/1) is
(a) max{k, l}
(b) k + 1
(c) kl
(d) none of these.
4. The gcd of (1024. 28) is
(al 2
(bl 4
(c) 7
(d)
14
S. 110.7: + '9.9l is equal to
(a) 19
(b) 20
(c) 18
(d) none of these.
6. log21024 is equal to
(a) 8
(b) 9
(c) 10
(d) none of these.
370
J;;I.
Theory ofcomputer Science
7. The truth value of f(x,
Y, .::) = (x v -,y) 1\ (-, X V y) 1\ .:: is T if x, y. z
have the truth values
(a) T. T. T
(b) F. F. F
(c)
T, F. F
(d) F. T. F
State whether the following Statements 8-15 are true or false.
8. If the truth values of x, y. .:: are T. F. F respectively. then the truth value
of f(x. y, .::) = x
1\ -,(v v .::) is T.
9. The complexity of a k-tape TM and an equivalent standard TM are the
same.
10. If the time complexity of a standard TM is polynomial, then the time
complexity of an equivalent k-tape TM is exponential.
11. If the time complexity of a standard TM is polynomial. then the time
complexity of an equivalent 1'.TTM is exponential.
12. fix. y, .::) = (x v y v :::) 1\ (-, X 1\ -, Y 1\ -,.::) is satisfiable.
13. f(x.
Y.
.::) = (x v y)
1\ (-,.Y 1\ -, v) is satisfiable.
14. If f and g are satisfiable expressions, then f v g is satisfiable.
15. If f and g are satisfiable expressions. then f
1\ g is satisfiable.
EXERCISES
12.1 If fen) = O(ll) and g(n) = O(lh then show that fen) + g(n) = O(nt)
where t = max{k, l} and f(n)g(n) = O(nk+I).
12.2 Evaluate the growth rates of (i) fin) = 2n2. (ii) g(n) = 1On2 + 7n log n +
log 11. (iii) hen) = n210g n + 211 log n + 7n + 3 and compare them.
12.3 Use the O-notation to estimate (i) the sum of squares of first n natural
numbers. (ii) the sum of cubes of first n natural numbers, (iii) the sum
of the first n terms of a geometric progression whose first term is a and
the common ratio is r, and (iv) the sum of the first n terms of the
arithmetic progression whose first term is a and the common difference
is d.
12.4 Show that fen) = 3112log2 11 + 411 log3 11 + 5 log2log211 + log 11 + 100
dominates
112 but is dominated by 11'.
12.5 Find the gcd (294. 15) using the Euclid's algoritr,m.
12.6 Show that there are five truth assignments for (P, Q, R) satisfying
p v (-, P /\ -, Q 1\ R).
Chapter 12: Complexity
J;J,
371
12.7 Find whether (P /\ Q /\ R) /\ -, Q is satisfiable.
12.8 Is f(x, y, z, w) = (x v y v z) /\ (x V y v z) satisfiable?
12.9 The set of all languages whose complements are in NP is called
CO-NP. Prove that NP = CO-NP if and only if there is some
NP-complete problem whose complement is in NP.
12.10 Prove that a boolean expression E is a tautology if and only if -, E is
unsatisfiable (refer to Chapter 1 for the definition of tautology).
Answers to Self-Tests
Chapter 1
1.
(d)
2.
(a)
3.
(c)
4.
(c)
5.
(b)
6.
F
7.
F,T,F;F,T,T
8.
T
Chapter 2
1.
(b)
2.
(c)
3.
(a)
4.
(b)
5.
(c)
6.
(b)
7.
(c)
8.
(d)
9.
(d)
10.
(d)
Chapter 3
1.
(d)
2.
(a)
3.
(d)
4.
(a)
5.
(d)
6.
T
7.
F
8.
T
9.
T
10.
F
11.
T
12.
T
13.
F
14.
T
15.
F
Chapter 4
1.
(b)
2.
(d)
3.
(c)
4.
(a)
5.
(b)
6.
(a)
7.
(d)
8.
(a)
9.
(a)
10.
(b)
11.
(c)
12.
(b)
13.
F
14.
T
15.
F
16.
T
17.
F
18.
T
19.
F
20.
F
Chapter 5
1
(a)
2.
(d)
3.
(a)
4.
(d)
5.
(a)
6.
(d)
7.
(b)
8.
(b)
9.
(d)
10.
(a)
11.
F
12.
F
13.
F
14.
T
15.
F
16.
T
17.
F
373
374
};l
AnswerstoSelf-Tests
Chapter 6
1.
(a)
A
(b)
Yes
(e)
Yes
(e)
Lables for nodes 1-14 are A, b, A, A, A, b, a, a, A, A, b, A,
a, a.
2.
(a)
F
(b)
T
(e)
T
(e)
T
3.
(a)
T
(b)
T
(e)
T
(e)
F
(f)
T
(g)
F
(d)
F
(d)
F
(h)
T
Chapter 7
1.
(a)
2.
(b)
3.
(d)
4.
(a)
5.
(e)
6.
(a)
7.
input string to S
8.
looking ahead by one symbol
9.
(qfi
1\, a) for some qr E F and a
E r*
10.
(q,
1\, 1\) for some state q.
Chapter 8
1.
(e)
5.
(b)
Chapter 9
1.
(d)
5.
(a)
9.
(b)
Chapter 11
1.
(a)
5.
(b)
9.
(a)
13.
F
Chapter 12
1.
(b)
5.
(a)
9.
F
13.
T
2.
(a)
2.
(b)
6.
(e)
10.
(b)
2.
(e)
6.
(a)
10.
(b)
14.
T
2.
(e)
6.
(e)
10.
F
14.
T
3.
(a)
3.
(a)
7.
(b)
3.
(a)
7.
(e)
11.
T
15.
T
3.
(b)
7.
(a)
11.
T
15.
F
4.
(e)
4.
(d)
8.
(d)
4.
(b)
8.
(d)
12.
T
4.
(b)
8.
T
12.
F
Solutions (or Hints) to
Chapter-end Exercises
Chapter 1
1.1
All the sentences except (g) are propositions.
1.2
Let L, E, and G denote a < b, a = b and a > b respectively. Then
the sentence can be written as
(E 1\ ,
G 1\ ,
L) v (G 1\ ,
E 1\ ,
L) v (L 1\ ,
E 1\ ,
G).
1.3
(i) David gets a first class or he does not get a first class. Using the
truth table given in Table A1.I, v is associative since the columns
corresponding to (P v Q) v Rand P v EQ v R) coincide.
TABLE A1.1
Truth Table for Exercise 1.3
P
Q
R
P v Q
Q v R
(P v Q) v R
P v (Q v R)
T
T
T
F
F
T
T
T
T
F
F
T
F
F
T
F
T
T
T
F
F
T
F
F
T
F
T
T
F
T
T
T
F
F
F
F
T
F
T
T
T
T
F
F
T
F
T
T
T
F
F
F
F
T
F
F
The commutative and distributive properties of Exclusive OR can be
proved similarly.
1.4
Using v and "
all other connectives can be described. P 1\ Q and
P :::::> Q can be expressed as , (, P v ,
Q) and, P v Q respectively.
1.5 ,P =pip
P 1\ Q = (P i
Q) i
(P i
Q)
P v Q = (P i
P) i
(Q i
Q)
Verify these three equations using the truth tables.
375
376
j;J,
Solutions (or Hints) to Chapter-end Exercises
1.6 -,p=pJ--p
p v Q = (P J-- Q) J-- (P J-- Q)
P 1\ Q = (P J-- P) J-- (Q J-- Q)
1.7
(a) The truth table is given in Table Al.2.
TABLE A1.2
Truth Table for Exercise 1.7(a)
P
Q
R
PvQ
PvR
RvQ
PvR=:,RvQ
P v Q =:, ((P v R) =:, (R v Q))
T
T
T
T
T
T
T
T
T
T
F
T
T
T
T
T
T
F
T
T
T
T
T
T
T
F
F
T
T
F
F
F
F
T
T
T
T
T
T
T
F
T
F
T
F
T
T
T
F
F
T
F
T
T
T
T
F
F
F
F
F
F
T
T
1.8
(a) -, P ~ (-, P 1\ Q) == -, (-, P) v (-, P 1\ Q)
by 112
== P V (-, P 1\ Q)
by 17
== (P V -, P) 1\ (P V Q)
by 14
== T 1\ (P V Q)
by Is
==PvQ
by 19
-,P~(-,P~ (-,P 1\ Q))== -,(-,P) V (P V Q)
by 112
== P V (P V Q)
by 17
==PvQ
by h and Ii
1.9
We prove Is and 16 using the truth table.
TABLE A1.3
Truth Table for Exercise 1.9
P
Q
PI\Q
-,P
-,Q
P v (P
1\ Q)
-, (P
1\ Q)
-,Pv-,Q
T
T
T
F
F
T
F
F
T
F
F
F
T
T
T
T
F
T
F
T
F
F
T
T
F
F
F
T
T
F
T
T
P v
(P
1\ Q)
== P since the columns corresponding to P and
P v (P 1\ Q) are identical; -, (P 1\ Q) =-, P v -, Q is true since the
columns corresponding to -, (P 1\ Q) and -, P v -, Q are identical.
1.11
We construct the truth table for (P ~ -, P) ~ -, P.
TABLE A1.4
Truth Table for Exercise 1.11
P
T
F
-,P
F
T
P =:, -,P
F
T
(P =:, -,P)
=:, -,P
T
T
As the column corresponding to (P ~ -, P) ~ -, P has T for all
combinations, (P ~ -, P) ~ -, P is a tautology.
Solutions (or Hints) to Chapter-end Exercises
~
377
1.12
P /\ (P ==> -, Q) == P /\ (-, P V -, Q) == (P /\ -, P)
V (P /\ -, Q) ==
F v (P /\ -, Q) == P /\ -, Q
So (P /\ (P ==> -, Q)) v (Q ==> -, Q) == (P /\ -, Q) V (-, Q v -, Q) ==
(P A -, Q) v -, Q == -, Q. Hence
((P /\ (P ==> -, Q)) v (Q ==> -, Q)) ==> -, Q == (--, Q ==> -, Q) == -, (--, Q)
v-,Q==Qv-,Q=T
1.13
ex == (Q /\ -,R /\ -,5) v (R /\ 5)
== (Q /\ -, R /\ -, 5) v (R /\ 5 /\ Q) v (R /\ S /\ -, Q)
== (Q 1\ -,R 1\ -,5) v (Q /\ R /\ 5) v (-,Q 1\ R 1\ S)
1.14
Let the literals be P, Q, R. Then
ex == 110 v 100 v 010 v 000
== ((P 1\ Q /\ -, R) v (P 1\ -, Q 1\ -, R)) v (010 v 000)
== (P 1\ -, R) v (-, P 1\ Q 1\ -, R) v (-, P 1\ -, Q /\ -, R))
== (P 1\ -, R) v (--, P 1\ -, R)
== -, R
1.15
(a) The given premises are: (i) P ==> Q, (ii) R ==> -, Q. To derive
P ==> -, R, we assume (iii) P as an additional premise and deduce -, R.
1. P
Premise (iii)
2. P ==> Q
Premise (i)
3. Q
RI..
4. -, (--, Q)
17
5. R ==> -, Q
Premise (ii)
6. -, R
RIs
7. P ==> -, R
Lines 1 and 6
Hence the argument is valid.
(b) Valid
(c) Let the given premises be (i)
P,
(ii)
Q, (iii) -, Q
==>
R,
(iv) Q ==> -, R.
Then
1. Q
Premise (ii)
2. -, R
Premise (iv)
Hence the given argument is valid.
(d) Let the given premises be (i) 5, (ii) P, (iii) P
==> Q 1\ R,
(iv) Q v S ==> T. Then
1. P
Premise (ii)
2. Q /\ R
Premise (iii)
3. Q
RI3
4. Q v 5
Rl j
5. T
Premise (iv)
Hence the argument is valid. Note that in (c) and (d) the conclusions
are obtained without using some of the given premises.
378
J;;l
Solutions (or Hints) to Chapter-end Exercises
1.16
We name the propositions in the following way:
R denotes 'Ram is clever'.
P denotes 'Prem is well-behaved'.
J denotes 'Joe is good'.
S denotes 'Sam is bad'.
L denotes 'Lal is educated'.
The given premises are (i) R ~ P, (ii) J ~ S !\ -. P, (iii) L ~
J v R. We have to derive L !\ -. P ~ S. Assume (iv) L !\ -. P as an
additional premise.
1. L !\ -. P
Premise (iv)
2. L
R/3
3. -. P
R/3
4. J v R
Premise (iii)
5. R ~ P
Premise (i)
6. -. R
Line 3, Premise (i) and R/s
7. J
Lines 5 and 4 and Rh
8. S !\ -. P
Premise (ii)
9. S
R/3
Hence, L !\ -. P ~ S.
1.17
The candidate should be a graduate and know Visual Basic, JAVA and
c++.
1.18
{5, 6, 7, ... }.
1.19
Let the universe of discourse be the set of all complex numbers. Let
P(x) denote 'x is a root of ? + at + b =0'. Let a and b be nonzero
real numbers and b
::j:. 1. Let P(x) denote 'x is a root of ? + at + b
=0'. Let Q(x) denote 'x is a root of b? + at + 1 =0'. If x is a root
of ? + at + b = 0 then lIx is a root of b? + at + 1= O. But x is a
root of t2 + at + b = 0 as well as that of b? + at + 1 = 0 only when
x = ±1. This is not possible since b
::j:. 1. So, :3x (P(x) ~ Q(x)) q
(:3x P(x) ~ :3x Q(x)) is not valid.
1.20
Similar to Example 1.22.
1.21
Let the universe of discourse be the set of all persons. Let P(x) denote
'x is a philosopher'. Let Q(x) denote 'x is not money-minded'. Let
R(x) denote 'x is not clever'. Then the given sentence is
(Vx (P(x) ~ Q(x))) !\ (:3x(-. Q(x) !\ R(x))) ~ (:3x(-. P(x) !\ R(x)))
1. -. Q(e) !\ R(e)
Rl 14
2. -. Q(e)
R/3
3. -. pee)
R/s
4. R(e)
Line I and Rh
5. -. Q(e) !\ R(e)
Lines 2 and 4
Hence the given sentence is true.
1.22
Similar to Exercise 1.21.
Solutions (or Hints) to Chapter-end Exercises
J;!
379
Chapter 2
2.1
(a)
The set of all strings in {a, b, c}*.
(b)
A II B = {b}. Hence (A II B)* = {b" In:::: OJ.
(c)
The set of all strings in {a, b, c}* which are in {a, b}* or in
{b, c}*.
(d)
A* II B* = W' In:::: O}.
(e)
A - B = {a}. Hence (A - B)* = {d1ln :::: OJ.
(f)
(B - A)* = {c"ln :::: OJ.
2.2
(a)
Yes
(b)
Yes
(c)
Yes. The identity element is A.
(d)
0 is not commutative since x
0 y
:;t. Y 0 x when x = ab and
y = ba; in this case x 0 y = abba and y 0 x = baab.
2.3
(a) 0 is commutative and associative. (b) 0 is the identity element
with respect to o. (c) A u B = A u C does not imply that B = C.
For example, take A = {a, b}, B = {b, c} and C = {c}. Then A u
B =A u
C = {a, b, c}. Obviously, B
:;t. C.
2.4
(a)
True. 1 is the identity element.
(b)
False. 0 does not have an inverse.
(c)
True. 0 is the identity element.
(d)
True. 0 is the identity element. The inverse of A is A'.
2.5
(c)
Obviously, mRm. If mRn, then m - n = 3a. So, n - m = 3(-a).
Hence nRm. If mRn and nRp, then m - n = 3a and n - p =3b.
m - p = 3(a + b), i.e. mRp.
2.6
(a)
R is not reflexive.
(b)
R is neither reflexive nor transitive.
(c)
R is not symmetric since 2R4, whereas 4R'2.
(d)
R is not reflexive since lR'l (1 + 1 :;t. 10).
2.7
An equivalence class is the set of all strings of the same length. There
is an equivalence class corresponding to each non-negative number.
For a non-negative number n, the corresponding equivalence class is
the set of all strings of length n.
2.8
R is not an equivalence relation since it is not symmetric, for example,
abRaba, whereas abaR'abo
2.9
R = {(1, 2), (2, 3), (1, 4), (4, 2), (3, 4)}
R2 = {(I, 3), (2, 4), (1, 2), (4, 3), (3, 2)}
R3 = {(I, 4), (2, 2), (1, 3), (4, 4), (3, 3)}
R4 = {(1, 2), (2, 3), (1, 4), (4, 2), (3, 4)} = R
Hence
R+=RuR2 uR3
R* =R+
U
{(1, I)}
2.11
R+ = R* = R, Since R2 = R (an equivalence relation is transitive).
j
I,
I
380
;;!.
Solutions (or Hints) to Chapter-end Exercises
2.12
Suppose f(x) :::: fey). Then eLY :::: ay. So, x :::: y. Therefore, f is one-one.
f is not onto as any string with b as the first symbol cannot be written
as f(x) for any x
E
{a, b}*.
2.14
(a) Tree given in Fig. 2.9.
2.15
(a) Yes.
(b) 4. 5. 6 and 8
(c) L 2, 3 and 7
(d) 3 (The longest path is 1 -7 3 -7 7 -7 8)
(e) 4-5-6-8
(f) 2
(g) 6 and 7
2.16
Form a graph G whose vertices are persons. There is an edge
connecting A and B if A knows B. Apply Theorem 2.3 to graph G.
2.17
Proof is by induction on IXI. When IXI :::: L Xis a singleton. Then
2x :::: {0. X}. There is basis for induction. Assume !2XI :::: 21X1 when
X has 11 -
1 ·elements. Let Y :::: {aj.
a~, ..., an}
Y:::: X U
{a"L where X:::: {al'
a~, ., .. an-d. Then X has 11 -
1
elements. As X has 11 - 1 elements. !2X
j
:::: 2[X] by induction hypothesis.
Take any subset Y1 of Y. Either YI is a subset of X or YI -
{an}
is a subset of X. So each subset of Y gives rise to two subsets of X.
Thus. !2 Y\ :::: 2!2xl. But 12X\ :::: i X . Hence !2 Y\ :::: 2[Yi. By induction the
result is true for all sets X.
2.18
Ca) When /1 :::: L 1~::::
1(1 + 1)(1 + 2)
:::: L Thus there is basis for
6
induction. Assume the result for 11 - 1. Then
n-l
:::: I
k~
-+- 11~
k=1
::::
(11 - 1)(/1 - 1 + 1)(211 - 1)
~
6
+11
[by induction hypothesis]
11(11 + 1)(2/1 + 2)
....
::::
6
on slmphflCatlOn.
Thus the result is true for 11.
(cl When
11
:::: 2,
lo~n -
1 :::: 9999 which is divisible by 11. Thus
there is basis for induction. Assume
lO~(n-l) -
1 is divisible by 11.
Then,
lO~n - 1 ::::
lO~lO~(n - I) -
1 ::::
1O~[10~(n - 1) -
1] +
10~ - L As
1O~' 11-1, -
1 and
10~ - 1 are divisible by 11,
1O~" - 1 is divisible by
11. Thus the result is true for 11.
Solutions (or Hints) to Chapter-end Exercises
);1
381
2.19
(b) 22 > 2 [Basis for induction]. Assume 2" - ] > n - 1 for n > 2.
Then. 2" = 2 . 2" -
I > 2(n -
1), i.e. 2" > n + n - 2 > n (since
n > 2). The result is true for n. By the principle of induction, the
result is true for all n > 1.
2.20
(a) When n = 1, F(2n + 1) = F(3) = F(l) + F(2) = F(O) + F(2). So,
I
F(2n + 1) = L F(2k).
k=O
Thus there is basis for induction. Assume
,,-1
F(2n -
1) = L F(2k)
k=O
F(2n + 1) = F(2n -
1) + F(2n)
By induction hypothesis,
[by induction hypothesis]
[by definition]
n--l
F(2n + 1) =L F(2k) + F(2n) =
k=O
So the result is true for n.
"L F(2k)
k=O
2.21
In a simple graph, any edge connects two distinct nodes. The
number of ways of choosing two nodes out of n given nodes is
"e = nell -1) . So the maximum number of edges in a simple graph
-
2
n(n-l)
IS
2
2.22
2.23
We prove by induction on Iwl. When w= A, we have abA = Aab.
Clearly, IAI = 0, which is even. Thus there is basis for induction.
Assume the result for all wwith Iwl < n. Let wbe of length nand
abw = wab. As abw = wab, w = abw] for some w in {a, b}*. So
ababw] = abw]ab and hence abw] = w]ab. By induction hypothesis,
IwI! is even. As Iwl = Iwd + 2, Iwl is also even. Hence by the principle
of induction, the result is true for all w.
Let pen) be the 'open the nth envelope'. As the person opens the first
envelope, P(l) is true. Assume Pen -
1) is true. Then the person
follows the instruction contained therein. So the nth envelope is
opened, i.e. pen) is true. By induction, Pen) is true for all n.
Cnapter 3
3.1
101101 and 000000 are accepted by M. 11111 is not accepted hy M.
3.2
{qo, q], q4}' Now. 8(qo, 010) = {qo, qJ and so 010 is not accepted
by M.
382
.\;!,
Solutions (or Hints) to Chapter-end Exercises
3.3
Both the strings are not accepted by M.
3.4
As b(q!, a) = b(q[, a), R is reflexive. Obviously it is symmetric.
If q!Rq2, then b(q[, a) = b(q2, a).
If q2Rq3 then b(q2' a) = b(q3, a). Thus b(q[, a) = b(q3' a), implying
that q!Rq3' So R is an equivalence relation.
3.5
The state table of NDFA accepting {ab, ba} is defined by Table A3.1.
TABLE A3.1
State Table for Exercise 3.5
StatelI.
a
b
The state table of the corresponding DFA is defined by Table A3.2.
TABLE A3.2
State Table of DFA for Exercise 3.5
StatelI.
[qo]
[q1]
[q2]
~
o
a
b
3.6
The NDFA accepting the given set of strings is described by
Fig. A3.1. The corresponding state table is defined by Table A3.3.
a, b
Fig. A3.1
NDFA for Exercise 3.6.
TABLE A3.3
State Table for Exercise 3.6
State/I.
a
b
qo
qo- q1
qo
q1
q2
q2
q3
q3
The DFA accepting the given set is defined by Table A3.4.
Solutions (or Hints) to Chapter-end Exercises
J,;iJ,
383
TABLE A3.4
State Table of DFA for Exercise 3.6
State/I.
a
b
[qQ]
[qQ, q1]
[q1]
[qQ, q1]
[qQ, q1]
[qQ, q2]
[qQ, q2]
[qQ, q1, q3]
[qoJ
[qQ, q1, q3]
[qQ, q1]
[qQ, q2]
3.7
The state table for the required DFA is defined by Table A3.5.
TABLEA3.5
State Table for Exercise 3.7
State
[qQ]
[q4]
[q1' q4]
[q2, q3]
o
o
2
[q2, q3]
o
o
[q2' q3]
o
3.9
The state table for the required DFA is defined by Table A3.6.
TABLE A3.6
State Table for Exercise 3.9
State
[q1]
[q2. q3]
[q1' q2]
[q1. q2, q3]
o
[q2, q3]
[q1. q2]
[q1, q2, q3]
[q1, q2, q3]
[q1]
[q1, q2]
[q1]
[q1, q2]
3.10
The required transition system is given in Fig. A3.2. Let L denote
{a, b, c, ..., z}. * denotes any symbol in L - {c, r}. ** denotes any
symbol in L -
{c, a, r}. *** denotes any symbol in L.
c
Fig. A3.2
Transition system for Exercise 3.10
3.11
The corresponding Mealy machine is defined by Table A3.7.
384
~
Solutions (or Hints) to Chapter-end Exercises
TABLE A3.7
Mealy Machine of Exercise 3.11
Present state
Next
state
a = 0
a = 1
state
output
state
output
Q1
0
q2
1
q3
1
q2
1
q2
1
q1
0
qo
1
q3
1
3.12
qj is associated with 1 and q2 is associated with 0 and 1. Similarly, q3
is associated with 0 and 1, whereas q4 is associated with 1. The state
table with new states qj, Q20, Q2j, q30, Q3l and q4 is defined by Table A3.8.
TABLE A3.8
State Table for Exercise 3.12
Present stare
Next
state
a = 0
a = 1
state
output
state
output
q1
0
q20
0
q4
1
q4
1
q4
q4
1
q21
q31
1
q21
1
q31
1
Q30
0
q1
1
The revised state table is defined by Table A3.9.
TABLE A3.9
Revised State Table for Exercise 3.12
Present state
Next state
a = 0
a = 1
output
-7
qo
q1
Q20
0
q1
q1
q20
1
q20
q4
q4
0
q21
q4
q4
1
q30
q21
q31
0
q31
q21
q31
1
q4
q30
q1
1
3.13
The Mealy machine is described by Fig. A3.3.
a,Even
a,Odd
H
1,Odd H
~
1, Even
Fig. A3.3
Mealy machine of Exercise 3.13.
Solutions (or Hints) to Chapter-end Exercises
i;;
385
3.14
Jr/s are given below:
JrQ = {{q6}, {qQ, q), q2' q3' q4. q5}}
Jr, = {{q6}, {qQ, qlo q2, q3, q5}, {q4}}
Teo = {{qd, {q4}, {qQ, ql, q3}, {q2. q5}}
Jr3 = {{q6}, {q4}, {qd, {qd, {Q3}, {q2' q5}}
Jr4 = {{qd,
{Q4}, {qQ}, {qd,
{q3}, (q2}, {q5}}
Here Jr = Q. The minimum state automaton is simply the gIven
automaton.
3.16
Chapter 4
4.1
(a) 5 ~ 0
1151
11 ~ O"O
Il1A1
1111", n 2:: 0, m 2:: 1.
A ~ lkA
=:} lk+l, k 2:: 0.
0
1111" E L(G) when n > 111 2:: 1. So L(G) = {Oil/I" : n > m 2:: I}
(b) L(G) = {0
1l11"
1 Tn -:,t n and at least one of 111 and n 2:: I}. Clearly,
0
111 E L(G) and I" E L(G), where Tn, n 2:: 1.
For
Tn > n, 5 ~ OIlSIIl
=:} OIlOAl" ~ 0"00"1-11-11" = 01i/1/. Thus
0
1111" E
L(G).
(c)
L(G)
=
{Oil1
110
11
1 n
2::
I}.
The
proof is
similar to
that of
Example 4.10.
(d) L(G) = {O"1
1110
111III
1 Tn, n 2:: I}.
For Tn, n 2:: 1,
5 ~ 01l-IS1,,-1
=:} 011-'OA11 11- 1 =:} 01l111l-1AOI1l- 111l- 1 =:} 01l1 1110lil11l
So,
{Oil1
1110
1111
11 I111, n 2:: I}
<;;;; L(G).
It is easy to prove the other inclusion.
(e) L(G) = {x E to, It I x does not contain two consecutive O's}
4.2
(a) G = ({S, A, B}, to, I}, P, S), where P consists of 5 ---'t OB IIA,
A
---'t 0!05!lAA, B
---'t 111510BB.
Prove by induction on
1wi, W E L*, that
(i) 5 ~ W if and only if w consists of an equal number of O's and l's
Oi) A ~ w if and only if lV has one more °than it has l' s.
(iii) B ~ lV if and only if w has one more 1 than it has D's.
A =:} 0, B =:} 1 and 5 does not derive any terminal string of length one.
386
~
Solutions (or Hints) to Chapter-end Exercises
Thus there is basis for induction. Assume (i), (ii) and (iii) are true
for strings of length k -
1. Let W be a string in L* with Iwi = k.
Suppose 5 ~ w. The first step in this derivation is either 5 ~ OB or
5 ~ 1A. In the first case W = OWl' B b
WI and IWII = k -
1. By
induction hypothesis, W has one more 1 than it has 0's. Hence W has
an equal number of 0'sand l's. To prove the converse part, assume
}v has an equal number of 0'sand l's and IW I = k. If W starts with
0, then W = OWl where IwtI = k -
1. WI has one more 1 than it has
O's. By induction hypothesis, B b WI' Hence 5 ~ OB ~ OWl =w.
Thus (i) is proved for all strings w. The proofs for (ii) and (iii) are
similar.
(b) The required grammar G has the following productions:
5 ~ 051, 5 ~ OA1, A ~ lAO, A ~ 10.
Obviously, L(G) ~ {0"1
I110
Il11" Im, n ;:: I}. For getting any terminal
string, the first production is 5
~ 051 or 5
~ OA1, the last
production is A ~ 10. By applying 5 ~ 051 (n - 1) times, 5 ~ OA1
(once), A ~ lAO (m - 1) times, and A ~ 10 (once) we get 0"1
1110
1111".
So, {0"1
I110
Il11" I m, n ;:: I}
~ L(G).
(c)
The required productions are 5 ~ 05111 OIl.
(d)
The required productions are 5 ~ OA111BO, A ~ OA11 A, B ~
1BO IA.
(e)
Modify the constructions given in Example 4.7 to get the
required grammar.
4.3
For the derivation of 001100, 001010 or 01010, the fIrst production
cannot be 5 ~ 051. The other possible productions are 5 ~ OA and
5 ~ lB. In these cases the resulting terminal strings are Oil or I". So
none of the given strings are in the language generated by the
grammar given in Exercise 4.1(b).
4.4
It is easy to see that any derivation should start with 5 ~ OAB ~
OA5A ~ OAOABA or 5 ~ OAB ~ OA01
~ 050Bl. If we apply
5 ~ OAB, we get A in the sentential. form. If we try to eliminate A
using AO ---7 50B or Al ---7 5Bl, we get 5 in the sentential form. So
one of the two variables, namely A or S, can never be eliminated.
4.5
The language generated by the given grammar is {01
1112"31 m, n;:: I}.
This language can also be generated by a regular grammar (refer to
Chapter 5).
4.6
(a) False. Refer to Remark 2 on page 123.
(b) True. If L = {Wlo W:, ..., W,,}, then G = ({S}, L, P, 5), where
P consists of 5 ~ wtlw:1 ... IWll"
(c)
True. By Theorem 4.5 it is enough to show that {w} is regular
where W E L*. Let W = ala: ... all" Then the grammar whose
Solutions (or Hints) to Chapter-end Exercises
J;;t,
387
productions are 5 -7 alAI, Al -7 azAz, ... A",_I -7 am generate
{w}.
4.7
We prove (a) 5 ~ A {'Az'A4, (b) A;'A2'A4 ::; d,2Az'A4, (c) Az'A{'A4 ~
az,,+I.
We first prove (a). 5 ~ A:04 ~ A IA:0zA4 :b Aj'-IA:0:£,-IA4 ~
A(,-IAIAzA:£,-jA4 (We are applying A 3 -7 A jA:0z(n - 2) times and
A3 -7 AjAz once.) Hence (a). To prove (b) start with A('A:!'A4. Then
A{,-IA jA zA:£,-IA4 ~
A{'-laAzA IA:£'-IA4 ~
A{'-zaA IAzA jA:£,-jA4 ~
A ,,-zdAA A A~,-IA
~A'l-zdAoA aAoA A~l-zA :;A,,-zdAoA AoA A~l-zA
~
j
_II~
4
I
_j~j~
4
I
_I~I_
4
A{'-Za3AzaAzAIAIA:£,-zA4 ::; a4A {,-zAlA rA:!,-zA4.
.',-
}
Proceeding in
a similar way,
we
get
Al'A:£'A4 =:,
a"-A:£'A I"A4·
Hence (b).
Finally, Az'Aj'A4 ~ A2'A{'-IA4a ::; A2'A4a"
~ A:!,-IAsa',+1 ::; Asa2"
~ az,,+I. (We apply A IA4 -7 A4a, A zA4 -7 Asa, AzAs -7 Asa and
finally As -7 a).
Using (a), (b) and (c), we get 5 ~ a(,,+1)2.
4.8
The productions for (i) are 5 -7 a5 1B, a5 -7 aa, B -7 a. For (ii) the
productions are 5 -7 A51 a, A -7 a. For (iii) the productions are
5 -7 a51 a.
4.9
The required grammar G = ({5, 51, A, B}, 1:, P, 5), where 1: =
{O, 1, 2, ..., 9} and P consists of
5 -7 °1214161 8, 5 -7 A51, A -7 1 12 I
19
51 -7 °121 416 I 8, 5 -7 AB5j, B -7 1 I2I
19
5 -7 °I 2 14 I 6 I 8 generate even integers with one digit.
5 -7 A51 and A-productions and 51-productions generate all even
numbers with two digits. The remaining productions can be used to
generate all even integers with three digits.
4.10
(a) G = ({5, A, B}, {O, I}, P, 5), where P consists of 5 -7 0511 OA
lIB 1°11, A -7 OA I0, B -7 IB 11. Using 5 -7 051, 5 -7 0,
5 -7 1, we can get 0"'1", where m and differ by 1. To get more O's
(than I's) in the string we have to apply A -7 OA 10, 5 -7 OA
repeatedly. To get more l's, apply 5 -7 IB, B -7 IB 11 repeatedly.
(b) The required productions are 5 -7 a51, 51 -7 b5lc, 51 -7 bc,
5 -7 a5zc, 5z -7 a5zc, 5z -7 b, 5 -7 53c, 53 -7 a53b, 53 -7 abo The
first three productions derive ab"c". 5 -7 a5zc and the 5r productions
generate a"bc". The remaining productions generate a"b"c.
(c) The required productions are 5 -7 051, 5 -7 01, 5 -7 OAI, A -7
lA, A -7 1.
(d) The required productions are 5 -7 a5c, 5 -7 ac, 5 -7 bc, 5 -7
b5lc, 51 -7 b5lc, 51 -7 bc. A typical string in the given language can
be written in the form alb'''cl/lcl where 1, m 2': 0. 5 -7 a5c, 5 -7 bc
388
~
Solutions (or Hints) to Chapter-end Exercises
generate a1c1for 1 2:: 1. 5 -'t b51c, 51 -'t b51c, 51 -'t bc generate b'''d''
for m 2:: 1. For getting a1b/lld"c1, we have to apply 5 -'t a5c I times;
5 -'t bc, 5 -'t b51c, 51 -'t b51c and 51 -'t bc are to be applied. For
m = 1, 5 -'t bc has to be applied. For m > 1, we have to apply
5 -'t b5 i c, 51 -'t b5 1c and 51 -'t bc repeatedly. The terminal c is added
whenever the terminal a or b is added in the course of the derivation.
This takes care of the condition 1 + m = n.
(e)
Let
G be
a context-free
grammar whose
productions
are
5 ---1 5051505, 5 -'t 5050515, 5 -'t 5150505, 5 -'t A. It is easy to see
that elements in L(G) are in L. Let W E L. We prove that W E L(G)
by induction on Iwi. Note that every string in L is of length 3n,
n 2:: 1. When Iwi = 3, W has to be one of 010, 001 or 100. These
strings can be derived by applying 5 -'t 5051505, 5 -'t 5050515 and
5 -'t 5150505 and then 5 -'t A. Thus there is basis for induction.
Assume the result for all strings of length 3n - 3. Let W ELand let
\wi =3n,w should contain one of 010, 001 or 100 as a substring. Call
the substring W1' Write w = W2WiW3' Then IW2W31 = 3n - 3 and by
induction hypothesis 5 ~ W2W3' Note that all the productions (except
S -'t A) yield a sentential form staning and ending with 5 and having
the symbol 5 between every pair of terminals. Without loss of
generality, we can assume that the last step in the derivation
5 ~ w2w3 is of the form w25w3 => W2W3'
SO, 5 => W25w3' But
WI ELand so 5:b WI' Thus, 5:b W2WiW3' In other words, W E L(G).
By the principle of induction, L = L(G).
4.11
The required productions are:
(a) 5 -'t a51, 5) -'t a5, 5 -'t a52, 52 -'t a
(b) 5 -'t a5, 5 -'t b5, 5 -'t a
(c) 5 -'t a51, 51 -'t a51, 51 -'t bS\o 51 -'t a, 5) -'t b
(d) 5 -'t a5), 51
-'t a51, 51 -'t b52, 52 -'t b52, 53 -'t c53, 53
---1
C
(e) 5 -'t a51, 51 -'t b5, 5 -'t a52, 52 -'t b.
4.12
:b is not symmetric and so the relation is not an equivalence relation
(Refer to Note (ii), page 109)
4.13
It is clear that L(G)) = {a"b" I 11 2:: 1}. In G2, 5 => AC => A5B :b
A"-) 5B"-). Also, 5 => AB :b abo Hence 5 :b a"b" for all 11 2:: 1. Tnis
means L(G2) = {a"b" I 11 2:: 1} = L(G)).
4.14
L(G) = 0, for we get a variable on the application of each production
and so no terminal string results.
4.15
The required productions are 5 -'t a5), 5) -'t b52, 52 -'t C, 5 ---1 b53,
53 -'t c54, 54 -'t a, 5 -'t c55, 55 -'t a56, S6 -'t b.
4.16
The required productions are 5 ---1 5\0 51 -'t ab5), 51 -'t ab, S -'t 52,
52 -'t ba52, 52 -'t ba.
Solutions (or Hints) to Chapter-end Exercises
&,;l,
389
4.17
Let the given granm1ar be G j . The production A ~ xB where
x = ala2 ... all is replaced by A ~ alAj, Al ~ a2A2, ..., An-I ~
allB. The production A ~ y, where y = blb2 ... bm is replaced by
A ~ bjB], B j ~ b2B2, •••, B/1/-1
~ b/1/' The grammar G2 whose
productions are the new productions is regular and equivalent to G j .
Chapter 5
5.1
(a)
0 + 1 + 2
(b)
1(11)*
(c)
w in the given set has only one a which can occur anywhere in
w. So w =xay, where x and y consist of some b's (or none).
Hence the given set is represented by b* ab*.
(d)
Here we have three cases: w contains no a, one a or two a's.
Arguing as in (c), the required regular expression is
b* + b* ab* + b* ab* ab*
(e)
aa(aaa)*
(0
(aa)* + (aaa)* + aaaaa
(g) a(a + b)* a
5.2
(a)
The strings of length at most 4 in (ab + a)* are A, a, ab, aa,
aab, aba, abab, a:;, aaba, aaab and d+. The strings in aa + bare
aa and b. Concatenating (i) strings of length at most 3 from the
first set and aa and (ii) strings of length 4 and b, we get the
required strings. They are aa, aaa, abaa, aaaa, aabaa, abaaa,
as, ababb, aabab, aaabb, and aaaab.
(c)
The strings in (ab + a) + (ab + a)2 are a, ab, aa, abab, aab and
aha. The strings of length 5 or less in (ab + a)3 are a3, abaa,
aaab, ababa. The strings of length 5 or less in (ab + a)4 are a4,
a3ab, aba3• In (ab + a)5, as is the only string of length 5 or less.
The strings in a* are in (ab + a)* as well. Hence the required
strings are A, a, ab, a2, abab, aab, aba, a3, abaa, aaab, ababa,
a4, aaaab, abaaa, and as.
5.3
(a) The set of all strings starting with a and ending in abo
(b) The strings are either strings of a's followed by one b or strings
of b's followed by one a.
(c)
The set of all strings of the form vw where a's occur in pairs in
v and b's occur in pairs in w.
5.5
The transition system equivalent to (ab + a)*(aa + b) (5.2(a»
is
given in Fig. AS.l.
390
~
Solutions (or Hints) to Chapter-end Exercises
(ab + a)*(aa + b)
r:\
f-------_+{~
Fig. A5.1
Transition system for Exercise 5.5.
5.6
The transition system equivalent to a(a + b)*ab (5.3)(a)) is given in
Fig. A5.2.
a(a + b)*ab
r:\
f-----------+l~
a, b
-GY0-
q4
Fig. A5.2
Transition ,system for Exercise 5.6.
Solutions (or Hints) to Chapter-end Exercises
~
391
5.7
(a) 0; (b) (a + b)*; (c) the set of all strings over {a, b} containing
two successive a's or two successive b's; (d) the set of all strings
containing even number of a's and even number of b's.
5.8
We get the following equations:
qo = A
q1 = qol + q1} + q31
q2 = q10 + q20 + Q30
q3 = q21
Therefore,
q2 = q10 + q20 + q210 = q10 + q2(0 + 10)
Applying Theorem 5.1, we get
q2 = q10(0 + 10)*
Now,
q1 =1 + q1} + q211 =1 + q1} + q10(0 + 10)*11
= 1 + q,(l + 0(0 + 10)* 11)
By Theorem 5.1, we get
q1 = 1(1 + 0(0 + 10)*11)*
q3 = q21 =1(1 + 0(0 + 10)11)* 0(0 + 10)*1
As q3 is the only final state, the regular expression corresponding to
the given diagram is 1(1 + 0(0 + 10)* 11)* 0 (0 + 10)* 1.
5.9
The transition system corresponding to (ab + c*)*b is given in Fig.
A5.3.
5.10
(a) The required regular expression
(1 + 01)* + (1 + 01)* 00(1 + 01)* + (0 + 10)* + (0 + 10)* 11(0
+ 10)* (1 + 01)* represents the set of all strings containing no pair
of 0's. (1 + 01)* 00 (1 + 01)* represents the set of all strings
containing exactly one pair of O's. The remaining two expressions
correspond to a pair of l's.
(c) Let W be in the given set L. If w has n a's then it is in a set
represented by (b)*. If w has only one a then it is in a set represented
by b*ab*. If w has more than one a, write w = WjaW2, where W does
not contain any a. Then Wj is in a set represented by (b + abb)*. So
the given set is represented by the regular expression b* + (b + abb)*
ab*. (Note that the regular set corresponding to b* is a subset of the
set corresponding to (b + abb)*
(d) (0 + 1)* 000 (0 + 1)*
(e) 00(0 + 1)*
(f) 1(0 + 1)* 00.
392
g
Solutions (or Hints) to Chapter-end Exercises
ab + c*
Fig. A5.3
Transition system for Exercise 5.9.
5.12
The corresponding regular expression is (0 + 1)* (010 + 0010). We
can construct the transition system with A-moves. Eliminating
A-moves. we get the NDFA accepting the given set of strings. The
NDFA is described by Fig. A5.4.
0, 1
°
o
°
o
Fig. A5.4
NDFA for Exercise 5.12.
The equivalent DFA is defined by Table A5.L
Solutions (or Hints) to Chapter-end Exercises
J;i
393
TABLE A5.1
State Table of DFA for Exercise 5.12
StatelI
0
[qol
[qo, q"
q3]
[qo, q2]
[qo, q"
Q3, q4]
[qo, q"
q3, q,]
[qo, q2, qs]
[qo, q"
q3J
[qo, q"
q3, q4]
[qo, q"
q3, or]
[qo, q"
q3, q4]
[qo, q"
q3, q4]
[qo, q"
q3, qr]
[qo]
[qo, q2]
[qo]
[qo, q2, as]
[qo, q2]
lao]
5.13
Similar to Exercise 3.10.
5.14
The state table for the NDFA accepting (a + b)* abb is defined by
Table A5.2.
TABLE A5.2
State Table for Exercise 5.14
StatelI
a
b
The corresponding DFA is defined by Table A5.3.
TABLE A5.3
State Table of DFA for Exercise 5.14
StatelI
a
b
lao]
[qo, q,]
[qo]
[ao, q,]
[qo, q,]
[qo, a2]
lao, q2]
[00, q,]
[qo, qr]
[qo, qr]
[qo, q,]
[qo, qr]
5.15
Let L be the set of all palindromes over {a, b}. Suppose it is accepted
by a finite automaton M = (Q, L, 8, q, F). {8(qo, a") i n 2: I} is a
subset of Q and hence finite. So 8(qo, a") = 8(qo, alll) for some
m and n, m < n. As a"b2"a" E L, 8(qo, a"b2"d') E F. But 8(qo, alll)
= 8(Qo, a"). Hence 8(qo, alllb2"a") = 8(qo, a
l b2"a/), which means
alllb2lla" E L. This is a contradiction since alllb2"a" is not a palindrome
(remember m < n). Hence L is not accepted by a finite automaton.
5.16
The proof is by contradiction. Let L = {d'b" In > O}. {8(qo, a") I
n 2 O} is a subset of Q and hence finite. So 8(qo, a") = 8(qo, a1/})
for some m and n, m i= n. So 8(qo, alllb") = 8(8(qo, alll), b") =
8(8(qo, a"), b") = 8(qo, a"b/). As a"b" E L, 8(qo, a"b/) is a final state
and so is 8(qo, a'"b"). This means alllb" E L with m i= n. which is a
contradiction. Hence L is not regular
394
g
Solutions (or Hints) to Chapter-end Exercises
5.17
(a) Let L = {a"b2/ In > O}.
We prove
L is not regular by
contradiction. If L is regular, we can apply the pumping lemma. Let
17 be the number of states. Let w = a"b21 . By pumping lemma,
w = xyz with Ixy I ~ 17, Iy I > 0 and xz E L. As Ixy I ~ 17, xy = am and
y = a 1 where 0 < 1 ~ n. So xz = a"-1b2"
E L, a contradiction since
17 -
1 :;i= n. Thus L is not regular.
(b) Let L = {d'b
ll1 I 0 < 17 < m}. We show that L is not regular. Let
17 be the number of states and w = a"b
ll1
, where m > n. As in (a),
y = ai, where 0 < 1 ~ n. By pumping lemma xykZ E L for k ~ O. So
a
l
- la1kb'" E L for all k ~ O. For sufficiently large k,
17 -
1 + lk > m.
This is a contradiction. Hence L is not regular.
5.19
Let M = (Q,
~, 8, qo, F) be a DFA accepting a nonempty language.
Then there exists w =ala2 ... ap accepted by M. If p < 17, the result
is true. Suppose p > n. Let 8(qo, ala2 ... a;) = qi for i = L 2, ..., p.
As p > 17, the sequence of states {qj, q2, ..., qp} must have a pair
of repeated states. Take the first pair (qj, qk) (Note % = qk)' Then
8(qo, aja2 ..., aj) = %' 8(%, ai+l
ak) = % and 8(qj' ak+l ...,
ap) E F. So 8(qo, ala2 ... ajak+l
ap) = 8(%, aja2 ..., ap) E F.
Thus we have found a string in ~* whose length is less than p (and
differs from Iwl by k - J). Repeating the process, we get a string of
length m, where m < n.
5.20
Let M =({qo, qIo qj}, {a, b}, 8, qo, {qj}), where qo and qj correspond
to S and A respectively.
Then the NDFA accepting L(G) is defined by Table A5.4.
Table A5.4
Table for Exercise 5.20
SlalelI.
a
b
O(Q2, b) = Ql
D(q4, a) = qj
5.21
The transitions are:
8(qj, a) = q4,
D(qj, b) = q2,
8(q2' a) = q3
8(q3, a) = q2
8(q3' b) = q4
O(q4' b) = q3
Let Aj, A2, A3, A4 correspond to qj,
q2,
Q3,
Q4' The induced
productions are Al ~ aA4, Al ~ bA2, A2 ~ aA3, A3 ~ aA2, A3 ~
Solutions (or Hints) to Chapter-end Exercises
J;\
395
bA4, A4 ~ bA3 (corresponding to the first six transitions) and A2 ~
bAj, A2 ~ b, A4 ~ aAJ, A4 ~ a (corresponding to the last two
transitions). So G = ({Aj, A2, A 3, A4 }, {a, b}, P, A j), where P
consists of the induced productions given above.
5.22
The required productions are:
S ~ ASjlBSjl .,. IZSJ,
SI ~ ASI!BSjl '"
IZSj,
SI ~ OSlllSI! ... 19S1
Sj ~ AIBI ... IZ and Sj ~ 0[11 '"
19
5.24
The given grammar is equivalent to G = ({S, A, B}, {a, b}, P, S),
where P consists of S ~ as IbS IaA, A ~ bB, B ~ a(B ~ aC and
C ~ A is replaced by B ~ a). Let qo, qj and q2 correspond to S, A
and B. qf is the only final state. The transition system M accepting
I(G) is given as
M = ({qo, qj, q2' qf}, {a, b}, 8, qo, {qr})
where qo, qj, and q2 correspond to S, A and Band qf is the (only)
final state. S ~ as, S ~ bS, S ~ aA and A ~ bB induce transitions
from qo to qo with, labels b and a, from qo ~ qj with label a and
from qj to q2 with label b. B ~ a induces a transition from q2 to qf
with label a. M is defined by Fig. AS.5.
a, b
Fig AS.S
Transition system for Exercise 5.24.
The equivalent DFA is given in Table AS.S.
TABLE AS.S
State Table of DFA for Exercise 5.24
State/I.
a
b
[qQ]
[qQ, q1]
[qQl
[qQ, q1]
[qQ, q1]
[qQ, q2]
[qQ, q2]
[qQ. q1. qf]
[qQ]
[qQ, q1, qf]
[qQ. q1]
[qQ. q2]
396
~
Solutions (or Hints) to Chapter-end Exercises
Chapter 6
6.1.
The derivation tree is given in Fig. A6.1.
s
b
s
+
s
Ob
Fig. A6.1
Derivation tree for Exercise 6.1.
s
a
6.2.
5 -'> 050, 5 -'> 151 and 5 -'> A give the derivation 5 :::S wAw, where
w E
{O, l}+, A -'> 2B3 and B -'> 2B3 give A :::S 2
111B3
111
• Finally,
B -'> 3 gives B => 3. Hence 5 :::S wAw :::S w2
111B3
111w => w2
1113
111 + lw.
Thus, L(G) s; {w2
1113
111+lw I w
E
{O, I}* and m ~ I}. The reverse
inclusion can be proved similarly.
6.3
(a) Xb X3, X5; (b) X2, Xl; (c) As Xl = 5, X4X2 and X2X~4X2 are
sentential forms.
6.4
(i)
5 => SbS => abS => abSbS => ababS => ababSbS => abababS =>
abababa.
(ii)
5 => SbS => SbSbS => SbSbSbS => SbSbSba => SbSbaba =>
Sbababa => abababa
(iii)
5 => SbS => abS => abSbS => abSbSbS => ababSbS => abababS
=> abababa
6.5
(a)
5 => aB => aaBB => aaaBBB => aaabBB => aaabbB =>
aaabbaBB => aaabbabB => aaabbabbS => aaabbabbbA
=>
aaabbabbba
(b)
5 => aB => aaBB => aaBbS => aaBbbA. => aaBbba => aaaBBbba
=> aaabBbba => aaabbSbba => aaabbaBbba => aaabbabbba.
6.6
abab has two different derivations 5 => abSb => abab (using
5 -'> abSb and 5 -'> a) 5 => aAb => abSb => abab (using 5 -'> aAb,
A --'> bS and 5 -'> a).
6.7
ab has two different derivations.
5 => ab (using 5 -'> ab)
5 => aB => ab (using 5 -'> aB and B -'> b).
Solutions (or Hints) to Chapter-end Exercises
l;\
397
6.8
Consider G = ({S, A, B},
{a,
b},
P,
S), where P consists of
S ~ AB Iab and B ~ b.
Step.1 When we apply Theorem 6.4, we get
WI = {S}, W2 = {S} u
{A, B, a, b} = W3
Hence
G j = G.
Step 2 When we apply Theorem 6.3, we obtain
Wj = {S, B}, W2 = {S, B} u
0 = W3
So, G2 = ({S, B}, {a, b}, {S ~ ab, B ~ b}, S).
Obviously, G2 is not a reduced grammar since B ~ b does not appear
in the course of derivation of any terminal string.
6.9
Step 1 Applying Theorem 6.3, we have
Wj = {B}, W2 = {B} u
{C, A}, W3 = {A, B, C} u
{S} = VN
Hence Gj = G.
Step 2 Applying Theorem 6.4, we obtain
Wj = {S}, W2 = {S} u
{A, a}, W3 = {S, A, a} u
{B, b}
w. = {S, A, B, a, b} u 0
Hence, G2 =({S, A, B}, {a, b}, P, S), where P consists of S ~ aAa,
A ~ bBB and B ~ abo
6.10
The given grammar has no null productions. So we have to eliminate
unit productions. This has already been done in Example 6.10. The
resulting equivalent grammar is G = ({S, A, B, C, D, E}, {a, b}, P,
S), where P consists of S ~ AB, A ~ a, B ~ b I a, C ~ a, D ~ a
and E ~ a. Apply step 1 of Theorem 6.5. As every variable derives
some terminal string, the resulting grammar is G itself.
Now apply step 2 of Theorem 6.5. Then
W j = {S}, W2 = {S} u
{A, B} = {S, A, B}, W3 = {S, A, B} u
{a, b} = {So A, B, a, b} and W4 = W3.
Hence the reduced grammar is G' = ({S, A, B}, {a, b}, P~ S), where
P' = {5 ~ AB, A ~ a, B ~ b, B ~ a}.
6.11
We prove that by eliminating redundant symbols (using Theorem 6.3
and Theorem 6.4) and then Unit productions, we may not get an
equivalent grammar in the most simplified form.
Consider the
grammar G whose productions are 5 ~ AB, A ~ a, B ~ C, B ~ b,
C ~ D, D ~ E and E ~ a.
Step 1
Using Theorem 6.3, we get
Wj = {A, B, E}, W2 = {A, B, E} u
{S, D},
W3 = {5, A, B, D, E} u
{C} = Vv.
Hence G j = G.
Step 2 Using Theorem 6.4, we obtain
Wj = {5}, W2 = {S} u
{A, B}, W3 = {S, A, B} u
{a, c, b},
w. = {S, A, B,
C. a, b} u
{D},
Ws = {S, A, B. C, D, a, b} u
{E} = VN u L.
Hence G2 = G1 = G.
398
g
Solutions (or Hints) to Chapter-end Exercises
Step 3 We eliminate unit productions. We then have
W(S) = {S} W(A) = {A], WeE) = {E} Wo(B) = {B},
W](B) = {B} u
{C}, W2{B) = {B, C} u
{D},
W3(B) = {B, C, D, E} = W(B) , W(e) = {C, D, E}, WeD) = {D, E}.
The productions in the new grammar G3 are S ~ AB, A
~ a,
B ~ b, B ~ a and E ~ a. G3 contains the redundant symbol E. So
G3 is not the equivalent grammar in the most simplified form.
6.12
(a) As there are no null productions or unit productions, we can
proceed to step 2.
Step 2
Let GI = (V;y,
{O, I}, Pj, S), where PI and V;v are
constructed as follows:
(i) A ~ 0, B ~ 1 are included in Pj.
(ii) S ~ lA, B ~ IS give rise to S ~ CIA, B ~ CIS and CI ~ 1.
(iii) S ~ OB, A ~ OS give rise to S ~ CoB, A ~ CaS and Co ~ 0.
(iv) A ~ lAA, B ~ OBB give rise to A ~ ClAA and B ~ CoBB.
V~y = {S, A, B, Co' Cd.
Step 3
G2 = (Vil, [0. I}, P2, 5), where P2 and v,~, are constructed
as follows:
(i) A ~ 0, B ~ 1, S ~ CIA, B ~ CjS, Cj ~ 1, S ~ CoB,
A ~ CaS, Co ~ °are included in P2•
(ii) A ~ ClAA and B ~ CoBB are replaced by A ~ CIDj, D 1 ~
AA, B ~ CoD2' D2 ~ BB.
Thus, G2 = ({S, A, B, Co, Cj, Dj. D 2 }, {O, I}, P2, S) is in CNF and
equivalent to the given grammar where P2 consists of S ~ CjAICoB,
A ~ O!CoS!CIDj, B ~ 1ICISICoD2, CI ~ 1, Co ~ 0, DI ~ AA and
D2 ~ BB.
(b) Step 2
GI = (V'x,
{a, b, C}, Pj, S), where PI and V'N are
defined as follows:
(i) S ~ a, S ~ b are included in PI
(ii) S ~ cSS is replaced by S ~ CSS, C ~ c, V",v = {S, C}
Step 3
G2 = (V,<:, {a, b, C}, P2, S). where P2 is defined as follows:
(i) S ~ a, S ~ b, C ~
C are included in P2•
(ii) S ~ CSS is replaced by S ~ CD and D ~ SS.
Thus, the equivalent grammar in CNF is G2 = ({S, C, D}, {a, b, c},
P2, S). where P2 consists of S ~ albl CD, C ---;} c, D
---;} SS.
6.13
Consider G =({S}, {a. b, +, *}, P, S), where P consists of S ---;} S + S,
S ---;} S * S. S ~ a, S ~ b.
Step 2
GI = (V(, {a, b, +, 8}, P lo S), where P j is constructed as
follows:
(i) S ~ a, S ~ b are included in PI,
(ii) S ~ S + Sand S ~ S * S are replaced by S ~ SAS, S ~ SBS,
A ~ +, B ~ *
V~· = {S, A, B}
Solutions (or Hints) to Chapter-end Exercises
g
399
Step 3
G: = (~<:, {a, b, +, *}. P:, S), where P2 is constructed as
follows:
(i) S ~ a, S ~ b, A ~ + and B ~ * are included in P2•
(ii) S ~ SAS and S ~ SBS give rise to S ~ SA b A j ~ AS,
S ~ SBj. Bj
~ BS.
The required grammar in CNF is
G2 = ({S, A, B, Aj, Bd, {a, b, +, *}. Pb
S)
where P2 consists of
S ~ alblSAdSBj, A ~ +, B ~ *, A j ~ AS and Bj ~ BS
6.14
(a) Rename S as Aj, By Remark following Theorem 6.9, it is enough
to replace terminals by new variables to get an equivalent grammar
Gj. Now, G j is defined as
G j = ({A j, A2, A3}, {O, I}, Pb A j)
where P j consists of
A j ~ AjAIiA2AjA3IA2A3' A2 ~ 0 and A3 ~ 1
This completes step 1.
Step 2
All productions of Gj except A j ~ A jA j are in proper form.
Applying Lemma 6.2 to A j ~ AjA]> we get a new variable Zj and
new productions A j ~ A2A jA3Z j l A2A3Zj, Zj ~ Aj, Zj ~ AjZj • The
new grammar IS
G2 = ({A b A2, A3, Zd, {a, b}, P2, A j)
where P: consists of
A j ~ A2AjA31A2A31A2AtA3ZjlA2A3Zj
Zj ~ AjZj, Zj ~ A j, A2 ~ 0 and A j
~ 1
Step 3 As A3-productions and Arproductions are in proper form we
have to modify only the A j-productions using Lemma 6.1. So the
modified A j-productions are
A j ~ OA jA3 1 OA310AjA3ZjlOA3Zj
Step 4 The productions Zj ~ A j and Zj ~ A jZj are modified using
Lemma 6.1. They are:
Zj
~ OA jA3 1 OA3 1 OA jA3Z j
l OA3Z j
Zj
~ OA jA3Z j I OA3Zj I OA jA3Z jZ j IOA3ZjZj
Thus the required equivalent grammar in GNF is
G3 = ({A j• A:> A3• Zd. {O, I}. P3, A j). where Pj consists of
A j
~ OA jA3 1 OA31 OA jA3Zj l OA3Z j
A, ~ 0, A3 ~ 1
Zj
~ OA jA3 1 OA31OA jA3Z j I OA3Z j
Zj
~ OA jA3Z j IOA3Z j IOA jA3Z jZ j IOA3ZjZj
400
g
Solutions (or Hints) to Chapter-end Exercises
(b) Step 1 Replace B ~ aSb by B ~
aSC and C ~ b. Rename S,
A, Band C by Aj, A2, A3 and A.j. The resulting grammar is
G[ = ({A j • A2• A3, A.j}, {a, b}, P lo Ai)
where Pi consists of A[ ~ A2A3• A2 ~ A:03
A2 ~ A3A[A3, A2 ~ b, A3 ~ aA2A4• A3 ~ a and A4 ~ b.
Steps
2, 3 and 4
Step 2 construction is not necessary for G j •
The only A4-production. A4 ~ b, is in proper form. So we go to step
4. The modified A2-productions are:
A2 ~ aA2A,A3 IaA31aA2A4A 1A3 1aA[A31b.
The modified A[-productions are:
A j
~
aA2A4A~31 aA:03\ aA2A.jA jA3A3 1 aA[A~31 bA3·
Step 5 is not necessary since there is no new variable in the form in
Zi' So an equivalent grammar in GNP is
G2 = ({A j , A2, A3• A.j}, {a, b}, P2, A[)
where P2 consists of
A[
~
aA2A.jA~3 1aA,A3 I aA2A4AiA3A3 I aAiA~31 bA3
A2 ~ aA2A4A3IaA3
1 aA2A.+AIA3
1 aA[A3 1 b
A3 ~ aA2A4 1a, A.j ~ b.
6.15
The grammar given in Exercise 6.7 has the following productions:
S ~ aB. S ~ ab, A ~ aAB. A ~ a. B ~ ABb and B ~ b. Of these,
S ~ aBo A ~ aAB. A ~ a and B ~ b are in the required form. So
we replace the terminals which appear in the second and subsequent
places of the R.H.S. of S ~ ab and B ~ ABb by a new variable C
and add a production C ~ b. Renaming S, B. A and C as Aj, A2, A3
and A4, the modified productions tum out to be A j ~ aA2• A j ~ aA4,
A3 ~
aA~2' A3 ~ a. A2 ~ A:02A.j, A2 ~ band A4 ~ b.
This completes the first three steps:
Step 4
A2 ~ A~2A4 is replaced by A2 ~ aA~2A2A41 aA~4' The
other productions are in the proper form. The resulting grammar in
GNP has the productions
A j ~ aA21aA4. A2 ~ aA:02A2A4IaA2A4Ib, A3 ~ aA~2Ia, A4 ~ b.
The grammar in Exercise 6.10 has unit productions. Eliminating unit
productions, we get an equivalent grammar G, where
G = ({S, A. B. C, D. E}, {a. b}, P. S)
where P consists of 5 ~ AB. A ~ a, B ~ a Ib, C ~ a, D ~ a
and E ~ a. Rename S. A, B. C, D and E as A j , A2, A3, A4, As and
A 6•
Solutions (or Hints) to Chapter-end Exercises
~
401
Thus P consists of A j ~ A 2A 3, A 2 ~ a, A 3 ~ a Ib, A4 ~ a, As ~ a
and A6 ~ a.
We have to modify only A j ~ A2A3 using Lemma 6.1.
Thus an equivalent grammar in GNP has the following productions:
A j ~ aA3, A 2 ~ a. A 3 ~ a Ib, A4 ~ a, As ~ a and A6 ~ a.
6.16.
(a) The given language is generated by a grammar whose productions
are S ~ aSalbSblc.
Step 2
(i) S ~ c is in P j
(ii) S ~ aSa and S ~ bSb give rise to
S ~ ASA, S ~ BSB, A ~ a. B ~ b in Pj
Thus G j
= ({S, A, B}, {a, b, c}, Pj, S), where P j consists of
S ~ ASA IBSB Ic, A ~ a and B ~ b.
Step 3
The equivalent grammar G2 in CNP is defined by
G2 = ({S, A, B. A j , Bd, {a, b, c}, P2, S), where P2 consists of
S ~ AA j, A j ~ SA, S ~ BB b B j ~ SB. A ~ a, B ~ b, S ~ c.
(b) The grammar generating the given set is having the productions
S ~ bAlaB. A ~ bAAlaSla, B ~ aBBlbSlb.
Step 2 The productions obtained in this step are:
S ~ BlA, B j ~ b, S ~ AjB, A j ~ a, A
~ BjAA, A
~ AjS,
A ~ a, B ~ AjBB, B ~ BjS, B ~ b.
Step 3 The equivalent grammar in CNP is given by
G2 =
(V~0, (a, b}. P2, S), where P2 consists of
(i) S ~ BjA, B j ~ b, S ~ AlB, A j ~ a, A
~ AjS, A
~ a,
B ~ BjS, B ~ b
(ii) A ~ BjCj, C j ~ AA, B ~ A j C2, C2 ~ BB (corresponding to
A ~ BjAA and B ~ AjBB).
6.17
(a) The grammar generating the given language has the productions
S ~ aSa, S ~ bSb, S ~ c. The first two productions will be in GNP
if the last symbols on R.H.S. are variables. Hence S ~ aSa, S ~ bSb
can be replaced by S ~ aSA, A
~ a, S ~ bSB, B ~ b. Hence
G' = ({S, A, B}, {a, b}, r, S), where P' consists of S ~ aSA IbSB,
S ~ c, A
~ a, B ~ b is in GNP and is generating the given
language.
(b) The given language is generated by
G = ({ S, A. B}, {a, b}, P, S), where P consists of
S ~ bA IaB, A ~ bAA Ias Ia. B ~ aBB IbS Ib. This itself is in GNP.
(c) The given language is generated by a grammar whose productions
are S ~ aAb. S ~ aA, A ~ aA, A ~ a, S ~ a, S ~ bB, B ~ b
and S ~ b. Of these productions we have to modify only one
production namely, S
~
aSb.
This is done by replacing this
402
&;l,
Solutions (or Hints) to Chapter-end Exercises
production by S ---+ aSB j , B[ ---+ b. (Note: In this problem we can also
replace S ---+ aSb by S ---+ aSB alone. B ---+ b is already in the grammar
and there are no other B-productions.)
(d) The given language is generated by a grammar whose productions
are S ---+ aSb, S ---+ cS. S ---+ c. The equivalent grammar in GNP is
G = ({S, B}, {a, b, c}, P, S),
where P consists of S ---+ aSB, B ---+ b, S ---+ cS, S ---+ c.
6.18
(i) Let W E L(G) and Iwl = k. In the Chomsky normal form, each
production yields one terminal or two variables but nothing else. For
getting the terminals in w, we have to apply production of the form
A ---+ a (k times). The corresponding string of variables, which is of
length k, can be obtained by k - 1 steps. (Each production A ---+ Be
increases the number of variables by one.) So the total number of
steps is 2k - 1. (The reader is advised to prove this result by induction
on Iwl.)
(ii) When G is in GNP, the number of steps in the derivation of w
is k(k = Iwl). The number of terminals increases by 1 for each
application of a production to a sentential form. Hence the number of
steps in the derivation of l-V is k.
6.19
Step 1 Let
11 be the natural number obtained by applying pumping
lemma.
Step 2 Let .:: = a"
2
• Write.:: = UVl-V.xy where 1 ~ Ivxl
~
11. (This is
possible since Ivwxl
~ n by (ii) of pumping lemma.) Let Ivxl = m,
m
~ n. By pumping lemma, uv2wx2y is in L. As luv2wx2yl > n2,
IUV2HX2yl = k, where k ;:: n + 1. But luv2wx2yl = n2 + m < n2 +
2n + 1. So luv2w~yI strictly lies between n2 and (n + 1)2 which means
uv2wx2y E L. a contradiction. Hence {a"2: n;:: I} is not context-free.
6.20
(a) Take.:: = d'b"c" in L(G). Write z = UVw,xy, where 1 ~ Ivxl
~ n.
So vx cannot contain all the three symbols a, band c. So uv2wx2y
contain additional occurrences of two symbols (found in vx) and the
number of occurrences of the third symbol remains the same. This
means the number of occurrences of the three symbols in UV 2WX2y are
not the same and so uv2w~y E L. This is a co~tradiction. Henc~ the
language is not context-free.
(b) As usual, n is the integer obtained from pumping lemma.
Let z = d'b"c21 . Then z =
uV],VX)', where 1 ~ Ivxl
~ n. So vx cannot
contain all the three symbols a, band c. If vx contains only a's and
b's then we can choose
i such that uviwxiy has more than 2n
occurrences of a (or b) and exactly 2n occurrences of c. This means
uviwxiy E
L, a contradiction. In other cases too, we can get a
contradiction by proper choice of i. Thus the given language is not
context-free.
Solutions (or Hints)to Chapter-end Exercises
~
403
6.21
(a) Suppose G = (VN, L, P, S) is right-linear. A production of the
form A ~ a]a2 ... amB, m
~ 2 can be replaced by A
~ alA],
A]
~ a2A2 ..., Am_]
~ amB. A ~ b]b2 ... bm, m
~ 2, can be
replaced by A ~ bIB], B] ~ b2Bb ..., Bm-2 ~ blll_]Bm_], Bm_] ~
bm. The required equivalent regular grammar G' is defined by the new
productions constructed above.
If G = (VN, L, P, S) is left-linear, then an equivalent right-linear
grammar can be defined as G) =(V'N, L, Ph S), where PI consists of
(i) S ~ W when S ~ W is in P and W E L*,
(ii) S ~ wA when A ~ W is in P and W
E L*,
(iii) A ~ wB when B ~ Aw is in P and w E L*,
(iv) A ~ w when S ~ Aw is in P and w E L*.
Let w E L(G). If S => w then S ~ w is in P. Therefore, S ~ w is
in PI (by (i».
Assume S => Alwl => A2W2W] => ... Am_1wm_] ... WI => WmWm_1
... WI = W is a derivation in G. Then the productions applied in the
derivation are S ~ Alw], A] ~ A2W2, ..., A IIl- 1 ~ Wm. The induced
productions in G] are
A] ~ WI, A2 ~ W2A h A3 ~ W02, ., ., S ~ wlI,Am-]
(by (ii), (iii) and (iv)' in the construction of PI)'
Taking the productions in the reverse order we get a derivation of G]
as follows:
Thus L(G) ~ L(G'). The other inclusion can be proved in a similar
way. So G is equivalent to a right-linear grammar G] which is
equivalent to a regular grammar.
(b) Let G = ({S, A}, {a, b, e}, P, S). where P consists of S ~
Sc IAc, A ~ aAb lab. G is linear (by the presence of A ~ aAb).
L(G) = {allbl/ellli m, n ~ I}
Using pumping lemma we prove that L(G) is not regular. Let n be the
number of states in a finite automaton accepting L(G).
Let w =a'W'c". By pumping lemma w =xyz, where Ixyl ::; nand
Iy I> O. If y =J< then xz =a"-kYle". This is not in L(G). By pumping
lemma. xz E L(G) a contradiction.
6.22
L =L(G), where G is a regular grammar. For every variable A in G.
A :b ex implies ex =uB, where u E L* and B E V, Thus Gis nonself-
embedding. To prove the sufficiency part, assume that G is a nonself-
embedding, context-free grammar. If G' is reduced, in Greibach
normal form and equivalent to G, then G' is also nonself-embedding.
(This can be proved.) Let ILI =nand m be the maximum of the
lengths of right-hand sides of productions in G'. Let ex be any
404
g
Solutions (or Hints) to Chapter-end Exercises
sentential form. By considering leftmost derivations, we can show that
the number of variables in a is ::; mn. (Use the fact that G/ is in GNF).
Define:
Gl = (Vf:,
~, PI, 5) where
Vf: = {[a]llal ::; mn and a E Vn
51 = [51]
PI = {[Af3] ~ b[a 13] IA ~ ba is in P, 13 E
Vf: and laf3l ::; mn}
GI is regular. It can be verified that L(G I ) = L(G').
Chapter 7
7.1
(qo, aacaa, Zo) t- (qo, acaa, aZo) r- (qo, caa, aaZo) r- (qj, a, aaZo)
r- (qjo a, aZo) r- (qj, A, Zo) r- (qj' A, Zo)·
(i) Yes, the final ill is (qr, A, Zo)·
(ii) Yes, the final ill is (qjo A, aZo)·
(iii) No, the pda halts at (ql' ba, aZo)·
(iv) Yes, the final ill is (ql' A, abaZo)
(v) Yes, the final ill is (qo, A, babaZo)·
7.2
(i) (qj, A, aZo).
(ii) Halts at (q), b. A).
(iii) (qo, A, a5Zo)·
(iv) Does not move.
(v) Does not move.
(vi) Halts at (qjo ab, Zo).
7.3
(a) Example 7.9.
(b) The required pda A is defined as follows:
A = ({qo, ql' q:;}, {a, b}, {a, Zo}, 6, qo, Zo, 0). 6 is defined by
6(qo, a, Zo) = {(ql' aZo)} ,
6(ql' a, a) = {(qlo aa)}
6(qlo b, a) = {(q:;, a)},
6(q:;, b, a) = {(qj, A)}
6(q\, A, Zo) = {(qjo A)}.
(c) A = ({qo, qd, {a, b, C}, {Zo, Zd, 6, qo, Zo, 0)
6 is defined by
6(qo, a, Zo) = {(qo, ZIZa)},
6(qo, a, ZI) = {(qo, ZIZ1)}
6(qo, b, Z\) = {(qj, A)},
6(ql' b, ZI) = {(qlo A)}
6(qlo c, Zo) = {(ql, Za)},
6(ql' A, Zo) = {(ql' A)}
Note that on reading a, we add ZI; on reading b we remove ZI and
the state is changed. If the input is completely read and the stack
symbol is Zo, then it is removed by a A-move.
7.4
(a) Example 7.9 gives a pda accepting {a"b
llla" 1m, n ~ I} by null
store. Using Theorem 7.1, a pda B accepting the given language by
final state is constructed.
Solutions (or Hints) to Chapter-end Exercises
~
405
8 is defined by
8(qo, A, Zo) = {(qo, ZoZ'o)}
8(qo, A, Z'o) = {(l1r, A)} = 8(qo, A, Z'o)
8(qj, A, Zo) = {(qj' A)} = 8(q). A, Z'o)
8(qo, a, Zo) = {(qo, aZo)},
8(qo, a, a) = {(qo, aa)}
8(qo, b, a) = {(qj, a)},
8(q), b, a) = {(q), a)}
8(qj, a, a) = {(qr, A)},
8(qj, A, Zo) = {(qb A)}
7.5
(a) G = ({S, Sj, S2},
{a, b}, P, S) generates the given language,
where P consists of S --? Sj, S --? S2' SI --? aS1b, SI --? ab, S2 --?
aS2bb, S2 --? abb. The pda accepting L(G) by null store is
A = ({q}, {a, b}, {S, Sj, S2' a, b}, 8, q, S, 0)
where 8 is defined by the following rules:
8(q, A, S) = {(q, SI), (q, S2)}
8(q, A. SI) = {(q. aSj, b), (q, ab)}
8(q, A, S2) = {(q, aS2bb), (q, abb)}
8(q, a, a) = 8(q, b, b) = {(q, A)}
7.6
(i) G = ({S}, {a, b}, P, S), where P consists of S --? aSb, S --? as,
S --? a, generates
{alllY'1 n < m}. For S =? alllSblll. m 2:: 0.
S =? a'z, n 2:: 1, and hence S =? a'"a"b"', m 2:: 0, n 2:: 1,
So L(G) k
{a'" b" In < m}. The other inclusion can be proved
similarly.
(ii) The pda A accepting UG) by null store is given by
A = ({q}, {a, b}, {S, a, b}, 8, q, S, 0)
where 8 is defined by the following rules:
8(q, A, S) = {(q, aSb), (q, as), (q, a)}
8(q, a, a) = 8(q, b, b) = {(q, A)}
(iii) Define B = (Q', L,
r~ 8B, qo, Z'o, F), where
Q' = {qo, qO, qj},
r' = (S, a, b, Z'o)
F = {qr}' (We apply Theorem 7.1 to (ii)).
8B is given by .
8B(q'o, A. Zo) = {(q, ZoZo)}
8B(q, A, S) = {(q, aSb), (q, as), (q, a)}
8B(q, a, a) = {(q, A)} = 8B(q, b, b)
8B(q, A, Z6) = {(q" A)}
(Note: 8B(q, a, S) = 8(q, a, S) = 0 and 8B(q, b, S) = 8(q, b, S) =0)
406
~
Solutions (or Hints) to Chapter-end Exercises
7.7
(i) Define G = ({S}, {a, b}, P, 5). where P consists of
S ~ SaSbSaS, S ~ SaSaSbS, S ~ SbSaSaS. S ~ A
(Refer to Exercise 4.1O(e).) G is the required grammar. (ii) Apply
Theorem 7.3. (iii) Apply Theorem 7.1 to the pda obtained in (ii).
7.8
Let A = ({qo, qd, {a. b}, {Zo. a, b}, 8, qo, Zo, 0). where 8 is given
by
8(qo. a. Zo) = {(qo. aZo)}'
8(qo, b, Zo) = {(qo, bZo)}
8(qo. a. h) = {(qo. ab)},
8(qo, b. a) = {(qo. ba)}
8(qo, a, a) = {(qo. aa). (q)o A)}
8(qo, b. b) = {(qo. bb), (q, A)}
8(qo, A, Zo) = {(qlo A)}
8(qj. a. a) = {(qjo A)},
8(qlo b, b) = {(qlo A)}
8(ql. A. Zo) = {(qlo A)}
A makes a guess whether it has reached the centre of the string. A
reaches the centre only when the input symbol and the topmost
symbol on PDS
are the same. This explains the definition of
8(qo, a. a) and 8(qo. b. b). A accepts the given set by null store.
7.9
Example 7.6 gives a pda A accepting the given set by empty store.
The only problem is that it is not deterministic. We have 8(q. a, Zo)
= {(q. aZo)} and 8(q. A, Zo) = {(q, A)}. So A is not deterministic
(refer to Definition 7.5). But the construction can be modified as
follows:
Al = ({q. q]}, {a, b}, {Zo. a, b}, 8, q. Zo. 0)
where 8 is defined by the following rules:
8(q. a, Zo) = {(qto aZo)}.
8(q. b. Zo) = {(ql. bZo)}
8(q], a. a) = {(ql, aa)},
8(ql. b, b) = {(qlo bb)}
8(q]. a, b) = {(qlo A)},
8(qlo b. a) = {(q]o A)}
8(q]o A, Zo) = {(% A)}
A] is deterministic and accepts the given set by empty store.
7.10
The S-productions are
S ~ [qo. Zo. qo] I [qo, Zo, qJJ
8(q]. b, a) = {(ql. A)},
8(q]o A, Zo) = {(qlo A)}
and
8(qo, b. a) = {(q]o A)}
Now these induce [qlo a. qJJ ~ b. [q], Zo. ql] ~ A and [qo, a. qd
~ b. respectively. 8(qo, a, Zo) = {(qo, aZo)} induces
Solutions (or Hints) to Chapter-end Exercises
);\
407
[qo, 7-{), qo] ~ a[qo, a, %][qo, Zo, qo]
[qo, Zo, qo] ~ a[qo, a, qJJ[qj, Zo, qo]
[qo, Zo, ql] ~ a[qo, a, qo][qo, Zo, qd
[qo, Zo, qd ~ a[qo, a, qd(qlo Zo, qd
8(qo, a, a) = {(qo, aa)} induces
[qo, a, qo] ~ a[qo, a, qo][qo, a, qo]
[qo,a, qo] ~ a[qo, a, qd(qIo a, qo]
[qo, a, qJJ ~ a[qo, a, qo][qo, a, ql]
[qo, a, qI] ~ a[qo, a, ql][qlo a, qd
7.13
Let M = (Q, L, 8, qo, F) be a DFA accepting a given regular set.
Define a pda A by A = (Q, L, {Zo}. 8Io qo, ~, F). 8 is given by the
following rule:
81(q, a, Zo) = {(qQ, Zo)}
if 8(q, a) = q'
It is easy to see that T(M) = T(A). Let w E
n~. Then 8(qo, w) =
q' E F.
8(qo, w, Zo) = {(q', Zo)}· So W
E
T(A), i.e., T(M) C T(A). The
proof that T(A) C T(M) is similar.
7.15 If 8(q, a, z) contains (q', 2 12 2 ... 2 11), n ~ 3. we introduce new states
qj, q2, ..., qn-2' We define new transitions involving new states as
follows:
(i) (% 211_ 1211 ) is included in (Kq, a, Z)
(ii) 8(qi, A, ZIl_i) = {(qi+Io 2 1l- i- 1 2,,_)} for i = 1, 2, ..., n - 3
(iii)
8(q"_2' A, 2 2) = {(q', 2 122)}
This construction is repeated for every transition given by (q', y) E
8(q, a, 2), IYI
~ 3. Deleting such transitions and adding the new
transitions induced by them we get a pda which never adds more than
one symbol at a time.
Chapter 8
8.1
For a sentential form such as a"+lbll
, A ~ a is the production applied
in the last step only when a is followed by abo So A ~ a is a handle
production if and only if the symbol to the right of a is scanned and
found to be b. Similarly, A ~ aAb is a handle production if and only
if the symbol to the right of aAb is b. Also, S ~ aAb is a handle
production if and only if the symbol to the right of aAb is A.
Therefore, the grammar is LR(l), but not LR(O).
8.2
We can actually show that the given grammar is not LR(k) for any
k
~ O. Suppose it is LR(k) for some k. Consider the rightmost
408
Q
Solutions (or Hints) to Chapter-end Exercises
derivations of 01 2k+12 and 01 2k+32 given by:
S ~ 01kAlk2 ~ 01 2k+12 = af3w
R
R
where a = 01\ f3 = a, W = 1k2.
S ~ 01 k+1A1k+12 ~ OI 2k+3 2 :::} a'{3'w'
R
R
(A8.I)
(A8.2)
where a' = 01k+l, f3' = a, w' = l'k+12. As the strings formed by the
first 2k + 1 symbols (note laf31 + k = 2k + 1) of af3w and a'{3'w' are
the same. a =at, i.e. Olk =01k+1, which is a contradiction. Thus the
given grammar is not LR(k) for any k.
8.3
The given grammar is ambiguous and hence is not LR(k) for any k.
For example, there are two derivation trees for abo
8.4
As a"b"e" appears in both the sets, it admits two different derivation
trees. So the set cannot be generated by an unambiguous grammar.
Chapter 9
9.2
The set of quintuples representing the TM consists of q 1bILq2,
qIOORql, q:bbRq3' q:OOLq2, q211Lq2, q30bR% q3 1bRqs· q4bORqs,
q400Rq4, q4IIRq4. QSbOLq2'
9.3
The computation for the first symbol 1 is qjllbll f-c- bq2bIl.
Afterwards it halts.
9.4
The computation sequence for the substring 12 of 1213 is
q j I213 f-c- bq2213 f-c- bbq3 13.
As 8(q3' 1) is not defined, the TM halts. For 2133 and 312 the TM
does not start.
9.6
Modify the construction given in Example 9.7.
9.8
We
have
the
following
steps
for
processing
the
even-length
palindromes:
(a) The Turing machine M scans the first symbol of the input tape
(0 or 1), erases it and changes state (qj or q2)'
(b) M scans the remaining part without changing the tape symbol
until it encounters b.
(c) The RJW head moves to the left. If the rightmost symbol tallies
with the leftmost symbol (which can be erased but remembered),
the rightmost symbol is erased. Otherwise M halts.
(d) The R/W head moves to the left until b is encountered.
Steps (a), (b). (c), (d) are repeated after changing the states suitably.
The transition table is defined by Table A9.1.
Solutions (or Hints) to Chapter-end Exercises
!;t
409
TABLE A9.1
Transition Table for Exercise 9.8
Present state
--7 qa
q1
q2
q3
q4
q5
q6
®
Input symbol
0
1
b
bRqj
bRq2
bRq7
ORq1
1Rq1
bLq3
ORq1
1Rq2
bLq4
bLq5
bLq6
OLq5
1Lq5
bRqa
OLq6
1Lq6
bRqa
9.9 We have three states qQ, qj, qj, where qQ is the initial state used to
remember that even number of l's have been encountered so far. qj is
used to remember that odd number of l's have been encountered so far.
qr is the final state. The transition table is defined by Table A9.2.
TABLE A9.2
Transition Table for Exercise 9.9
Present state
o
b
9.10 The construction given in Example 9.7 can be modified. As the number
of occurrences of c is independent of that of a or b, after scanning the
rightmost c, the RJW head can move to the left and erase c.
9.11 Assume that the input tape has 0
11l1O" where m
--'- n is required. We
have the following steps:
(a) The leftmost 0 is replaced by b and the RJW head moves to the
right.
(b) The RIW head replaces the first 0 after 1 by 1 and moves to the
left. On reaching the blank at the left end the cycle is repeated.
(c) Once the 0's to the left of l's are exhausted, M replaces all 0'sand
l's by b's. a
--'- b is the number of 0's left over in the input tape
and equal to O.
(d) Once the O's to the right of l's are exhausted, nO's have been
changed to l's and n + 1 of m O's have been changed to b. M
replaces l's (there are n + II's) by one 0 and n b's. The number
of O's remaining gives the values of a
--'- b. The transition table is
defined by Table A9.3.
410
!!!!
Solutions (or Hints) to Chapter-end Exercises
TABLE A9.3
Transition Table for Exercise 9.11
Present state
Input symbol
0
1
b
qo
bRq,
bRqs
q,
ORq,
1Rq2
q2
1Lq3
1Rq2
bLq4
q3
OLQ3
1Lq3
bRqo
q4
OLq4
bLq4
ORQ6
Qs
bRQs
bRQs
bRQ6
®
Chapter 10
10.2
1. (B, w) is an input to M.
2. Convert B to an equivalent DFA A.
3. Run the Turing machine M j for AOFA on input (A, w)
4. If M j accepts, M accepts; otherwise M rejects
10.3 Construct a TM M as follows:
1. (A) is an input to M.
2. Mark the initial state of A (qo marked as q'6, a new symbol).
3. Repeat until no new states are marked: a new state is marked if
there is a transition from a state already marked to the new state.
4. If a final state is marked, M accepts (A); otherwise it rejects.
10.4 Let L = (T(A l ) - T(A 2)) U (T(A 2) - T(A l )). L is regular and L = T(A').
Apply EOFA to (A').
10.8 Use Examples 10.4 and 10.5.
10.9 ATM is regarding a given Turing machine accepting an input, that is,
reaching an accepting state after scanning wand halting HALTTM is
regarding a given TM halting on an input (or M need not accept w in
this case).
10.10 Represent a number between 0 and 1 as 0 . Qj Q2 .•. where Qj, Q2, •••
are
binary
digits.
Assume
the
set
to
be
a
sequence,
apply
diagonalization process and get a contradiction.
10.11 When a problem is undecidable, we can modify or take a particular case
of the problem and try for algorithms. Studying undecidable problems
may kindle an imagination to get better ideas on computation.
10.12 Suppose the problem is solvable. Then there is an algorithm to decide
whether a given terminal string w is in L. Let M be a TM. Then there
is a grammar G such that L(G) is the same as the set accepted by M.
Then w E L(G) if and only if M halts on w. This means that the halting
problem of TM is solvable, which is a contradiction. Hence the
recursiveness of a type 0 grammar is unsolvable.
Solutions (or Hints) to Chapter-end Exercises
Q
411
10.14 Suppose there exists a Turing Machine Mover {O, l} and a state qlll
such that the problem of determining whether or not M will enter qm
is unsolvable. Define a new Turing machine M' which simulates M and
has the additional transition given by 6(qm' A) = (qm' 1, R). Then M
enters qlll when it starts with a given tape configuration if and only if
M' prints 1 when it starts with a given tape configuration. Hence the
given problem is unsolvable.
10.17 According to Church's thesis we can construct a Turing Machine which
can execute any given algorithm. Hence the given statement is false.
(Of course, the Church's thesis is not proved. But there is enough
evidence to justify its acceptance.)
10.18 Let L = {a}. Let x = (x], X2, ..., xn) and Y = (YI, Y2, ..., Yn), where
. -
k;
• _ 1 2
d
}. -
Ii
. - 12Th
()Il(
)IZ
X, - a , I -
,
, ..., n an
)j - a·, ] -
,
, ..., n.
en
Xl
X2
.. .(xjlJ = (Yli1(Y2)kZ... (Y,ilJ • Both are equal to aLkil;. Hence PCP is
solvable when IL I = 1.
10.20 Xl = 01, Y1 = OIl, X2 = 1, Y2 = 10, x3 = 1, Y3 = 1. Hence IXi I < IYi I
for
i = 1, 2, 3. So XilXi2 ... xim :;t. YiIYiz ... Yim for no choice of i's.
Note:
IxilXiz ... xlm 1< 1Yi1Yiz ... Yim I· Hence the PCP with the given
lists has no solution.
10.21
Xl =0, Y1 = 10, X2 = 110, Y2 = 000, X3 =001, Y3 = 10. Here no pair
(Xlo YI), (x2' Y2) or (x3' Y3) has common nonempty initial substring. So
XilXiz ... xim :;t. YilYiz ... Yim for no choice of i/s. Hence the PCP with
the given lists has solution.
10.22 As Xl = Y1, the PCP with the given lists has a solution.
10.23 In this problem, Xl = 1, X2 = 10, X3 = 1011, Y1 = 111, Y2 = 0,
Y3 = 10. Then, XJX1X1X2 =Y3Y1Y1Y2 = 101111110. Hence the PCP with
the given lists has a solution. Repeating the sequence 3, 1, 1, 2, we can
get more solutions.
10.24 Both (a) and (b) are possible. One of them is possible by Church's
thesis. Find out which one?
Chapter 11
11.1 (a) The function is defined for all natural numbers divisible by 3.
(b) x = 2
(c) x ;:: 2
(d) all natural numbers
(e) all natural numbers
11.2 (a) X(Oj(O) = 1, X(Oj(x + 1)
-'- X(Ojsgn(p(x»)
(b) f(x + 1) = x2 + 2x + 1
So, f(x + 1) = f(x) + S(S(Z(x»)) * ui(x) + S(Z(x»
412
J!!1,
Solutions (or Hints) to Chapter-end Exercises
Hence f is obtained by recursion and addition of primitive recursive
functions
(c) .l(x, y) = y + (x
-'- y)
(d) Define parity function P,.(y) by
Pr(o) = P,.(2) = ... = 0,
Pll) = Pr(3) = ... = 1
P,. is primitive recursive since Pr(o) = 0, Pr(x + 1) = x{o}(Ul(x),
Pr(x)). Define f by .1(0) = 0, f(x + 1) =f(x) + P,.(x).
(e) sgn(O) = Z(O), sgn(x + 1) = s(z(ul (x, sgn (x))))
(f) L(x, y) = sgn(x
-'- y)
(g) E(x, y) = X{Oj«x
-'- y) + (y
-'- x))
All the functions (a)-(g) are obtained by applying composition and
recursion to known primitive functions and hence primitive recursive
functions.
11.3 A(I, y) = A(1 + 0, Y - 1 + 1) = A(O, A(1, y - 1)) using (11.10) of
Example 11.11. Using (11.8), we get
A(1, y) = 1 + A(I, Y -
1).
Repeating the argument, we have
A(I, y) =y -
1 + A(I, 1) =y + 2 (By Example 11.12, A(I, 1) = 3).
This result is used in evaluating A(3, 1).
A(2, 3) = A(l + 1, 2 + 1) = A(I, A(2, 2)) = A(1, 7) using
Example 11.12. Using A(I, y) = y + 2, we get A(2, 3) = 2 + 7 = 9.
Then using (11.10), A(3, 1) = A(2 + 1, °+ 1) = A(2, A(3, 0»)
By Example 11.12, A(2, 1) = 5. Also, A(3, 0) = A(2, 1) by (11.9).
Hence A(3, 1) = A(2, 5) = A(l + 1, 4 + 1) = A(I, A(2, 4». Since
A(l, y) = Y + 2, A(3, 1) = 2 + A(2, 4). Applying (11.10), we have
A(2, 4) = A(I, A(2, 3)) = 2 + A(2, 3) = 2 + 9 = 11.
Hence, A(3, 1) = 2 + 11 = 13.
A(3, 2) =A(2, A(3, 1)) = A(2, 13) = A(I, A(2, 12)) = 2 + A(2, 12)
= 2 + A(l, A(I, 11) = 2 + A(1, 13) = 2 + 2 + 13 = 17.
To evaluate A(3, 3), we prove A(2, y + 1) = 2y + A(2, 1). Now,
A(2, y + 1) = A(l + 1, y + 1) =A(1, A(2, y) = 2 + A(2, y).
Repeating this argument, A(2, y + 1) = 2y + A(2, 1). Now,
A(3, 3) = A(2 + 1, 2 + 1) = A(2, A(3, 2» = A(2, 17) = 2(16) +
A(2, 1) = 32 + 5 = 37.
Solutions (or Hints) to Chapter-end Exercises
~
413
11.4 (b) It is clear that rex, 0) = O. Also, rex, y) increases by 1 when y is
increased by 1 and rex, y) =0 when y =x. Using these observations we
see that rex, y + 1) = S(r (x,y)) * sgn(x
-'-
S(r (x, y))). Hence
rex, y) is 1 defined by
rex, 0) = 0
rex, y + 1) = S(r(x, y)) * sgn(x -'- S(r(x, y)))
11.5 I(x) is the smallest value of y for which (y + 1)2 > x. Therefore,
fix) =.uyCX[O}((y + 1)2 -'- x)), I is partial recursive since it is obtained
from primitive recursive functions by application of minimization.
11.8 The constant function I(x) = 1 is primitive recursive for 1(0) = 1 and
fix + 1) = Ul(x, fix)). Now XAc, XAnB and XA u B are recursive for
XAc = 1 -'-
XA' XAnB = XA * XB and XAuB = XA + XB
-'-
XAnB
(Addition and proper subtraction are primitive recursive functions and
the given functions are obtained from recursive functions using
composition.)
11.9 Let E denote the set of all even numbers. XE(O) = 0, XECn + 1) =
1 -'- sgn(U}(n,
XE(n)).
The sign function and proper subtraction
function are primitive recursive. Thus E is obtained from primitive
recursive functions using recursion. Hence XE is primitive recursive and
hence recursive. To prove the other part use Exercise 11.8.
11.11 Define I by flO) = k, fin + 1) = U}(n, j(n)). Hence I is primitive
recurSIve.
11.12 X{{lj.a2'" .. all }
=
X{ad + X[a2} + ... + X{a ll }' As
X{ad is primitive
recursive. (Refer
to Exercise 11.2(a)) and the sum of primitive
recursive functions is primitive recursive, X{ar, a2, ..., all} is primitive
recursive.
11.13 Represent x and y in tally notation. Using Example 9.6 we can compute
concatenation of strings representing x and y which is precisely x + y
in tally notation.
11.14 Let M in the Post notation have {qj, q2,
, qll} and {at> a2, ..., am}
as Q and L respectively. Let Q' = {qj,
, qll' qll+j, ..., q21l}, where
qll+l ..., q211 are new states. Let a quadruple of the form qiajRqk induce
the quintuple qiapkRqk' Let a quadruple of the form qia/-LJk induce the
quintuple
q;apjLqk'
Finally,
let
qiapkq,
induce
qiapkRqll+i'
We
introduce quintuples qll+iafltLqi for i = 1, 2, ..., nand t = 1, 2, 3,
..., m. The required TM has Q' as the set of states and the set of
quintuples represent 8.
414
j;\
Solutions (or Hints) to Chapter-end Exercises
11.15 qo1ll1xlby ~ llllqoXlby ~ llllqlxby. As b lies between Xl and y,
Z(4) = 0 (given by b).
11.16 In Section 11.4.5 we obtained q01xlby
~
qs1xbbly. Similarly,
qoll1xlby
~
qs1llxlb1y
~
1qollxlbly.
Proceeding
further,
1qollxlb1y ~ 1qsllxlblly ~ llqolxlblly f-"'- lllqc;Xllllly (as in
Section 11.4.5). Hence S(3) = 4.
11.18 Represent the argument x in tally notation. j(x) = S(S(x». Using the
construction given in Section 11.4.7, we can construct a TM which
gives the value S(S(x».
11.19 f(x]> X2) = S(S(U?(x]> X2»)' Use the construction in Section 11.4.7.
11.20 Represent
(Xl>
X2) by P 1bl'2. By taking the input as $1'Ib1'2($ is
representing the left-end) and suitably modifying the TM given in
Example 9.6, we get the value of Xl + X2 to the right of $.
Chapter 12
12.1 Denote j(n)
k
= L aini and g(n) =
1=0
I
L bini, where
aj, bi' are positive
./=0
<
k
integers. Assume k :2 t. Then fen) + g(n) = L (ai + bJni, where bi = 0
1=0
for i > l. f(n) + g(n) is a polynomial of degree k. Hence j(n)g(n) =
O(nk+/).
12.2 As n2 dominates n log nand n2 10g n dominates n2, the growth rate of
hen) > growth rate of g(n). Note f(n) = g(n) = 0(n2).
Jl
II
11
12.3 As
L i =n(n + 1)/2,
L P=n(n + 1)(211 + 1)/6 and
L P=(n(n + 1)12)2,
1=0
1=0
1=0
the answers for (i) and (ii) are 0(n3) and O(n\ (iii) a(1 - r")/l - r =
n
O(l'lr) = 0(1'-1). (iv) '2 [2a + (n-1)d] = 0(n2).
12.4 As log2 n, 10g3 n, loge n, differ by a constant factor, j(n) = O(r/ log n)
12.5 gcd = 3.
12.6 The principal disjunctive normal form of the boolean expression has
5 terms (refer to Example 1.13). P ;\ Q ;\ R is one such term. So
(T, T, T) satisfies the given expression. Similar assignments for the
other four terms.
12.7 No.
12.8 (T. T, F, F) makes the given expression satisfiable.
12.9 Only if: Take an NP-complete problem L. Then r is in CO-NP = NP.
Solutions (or Hints) to Chapter-end Exercises
J!O!
415
if: Let P be IVP-complete and P E NP. Let L be any language in NP.
We get a polynomial reduction ¢ of L to P and hence a polynomial
reduction If! of [
to p. We prove NP c CO-NP. Combine If! and
nondeterministic
polynomial-time
algorithms
for
p
to
get
a
nondeterministic polynomial-time algorithm for [. So [
E NP or L
E CO-NP. This proves NP c CO-NP. The other inclusion is similar.
Further Reading
Chandrasekaran, N., Automata and Computers, Proceedings of the KMA
National Seminar on Discrete Mathematics and Applications, St. Thomas
College, Kozhencherry, January, 9-11, 2003.
Davis, M.D. and E.J. Weyuker, Computability, Complexity and Languages.
Fundamentals
of Theoretical
Computer
Science,
Academic
Press,
New York, 1983.
Deo, N., Graph Theory ,vith Applications to Engineering and Computer
Science, Prentice-Hall of India, New Delhi, 2001.
Ginsburg,
S.,
The
Mathematical
Theory
of Context-Free
Languages,
McGraw-HilL New York. 1966.
Glorioso. R.M., Engineering Cybernetics, Prentice-Hall. Englewood Cliffs,
New Jersey. 1975.
Gries, D., The Science ofProgramming, Narosa Publishing House, New Delhi,
1981.
Hanison, M.A., Introduction to Fonnal Language Them}', Addison-Wesley,
Reading (Mass.). 1978.
Hein. J.L., Discrete Structures, Logic and Computability, Narosa Publishing
House, New Delhi, 2004.
Hopcroft. J.E. and J.D. Ullman, Fanl1al Languages and Their Relation to
Automata, Addison-Welsey, Reading (Mass.). 1969.
Hopcroft J.E., and J.D. Ullman, Introduction to Automata Theory, Languages
and Computation, Narosa Publishing House. New Delhi, 1987.
Hopcroft. J.E..
.T. Motwani. and J.D. Ullman, Introduction to Automata
Theory, Languages and Computation, Pearson Education, Asia, 2002.
417
-
418
~
Further Reading
Kain, R.Y., Automata Theon: Machines and Languages, McGraw-HilI,
New York. 1972.
Kohavi, ZVI. Switching and Finite Automata Theory', Tata McGraw-Hil!.
New Delhi, 1986.
Korfhage,
RR..
Discrete
Computational
Structures,
Academic
Press
New York, 1984.
Krishnamurthy, E.V., Introductory Theory of Computer Science, Affiliated
East-West Press. New Delhi. 1984.
Levy,
L.S..
Discrete
Structures
of" Computer
Science,
Wiley Eastern,
New Delhi. 1988.
Lewis, H.R and c.L. Papadimitrou, Elements of" the Theory of Computation,
2nd ed., Prentice-Hall of India, New Delhi. 2003.
Linz. P.. An Introduction to Formal Languages and Automata, Narosa
Publishing House. New Delhi. 1997.
Mandelson, E., Introduction
to Mathematical Logic, D. Van Nostrand,
New York, 1964.
Manna, Z.. Mathematical Theory of Computation, McGraw-Hill Kogakusha,
Tokyo. 1974.
Martin, J.H.. Introduction to Languages and the Theory of Computation,
McGraw-Hill International Edition. New York. 1991.
Minsky, M., Computation:
Finite and Int"inite Machines,
Prentice-Hall,
Englewood Cliffs, New Jersey, 1967.
Nelson, R.J., Introduction to Automata, Wiley, New York, 1968.
Preparata,
F.P.
and
RT.
Yeh,
Introduction
to
Discrete
Structures,
Addison-Wesley, Reading (Mass.), 1973.
Rani Siromoney. Fonnal Languages and Automata, The Chiristian Literature
Society, Madras, 1979.
Revesz. G.E., Introduction to Fomwl Languages, McGraw-HilL New York,
1986.
Sahni, D.F. and D.F. McAllister. Discrete Mathematics in Computer Science,
Prentice-Hall, Englewood Cliffs. New Jersey. 1977.
Sipser, M., Introduction to the Theory of Computation, Brooks/Cole, Thomson
Learning, London. 2003.
Tremblay. J.P. and R. Monohar. Discrete Mathematical Structures with
Applications to Computer Science, McGraw-Hill, New York, 1975.
Ullman.
J.D.,
Fundamental
Concepts
of
Programming
Systems,
Addison-Wesley, Reading (Mass.), 1976.
Index
Abelian group. 38
Acceptance
by finite automaton. 77
by NDFA, 79
by pda by final state, 233-240
by pda by nun store, 234-240
by Turing machine, 284
Accepting state (see Final state)
Ackermann's function, 330
Algebraic system, 39
Algorith.'Il, 124, 309, 310
Alphabet. 54
Ambiguous grammar, 188
Ancestor of a vertex, 51
Arden's theorem. 139
Associativity. 38
A-tree. 183
Automaton. 71. 72
minimization of. 91-97
Bijection, 45
Binary operation, 38
Binary trees. 51
Boolean expressions, 353
Bottom-up parsing, 258
Chain production (see Unit production)
Characteristic function, 344
Chomsky classification. 220
Chomsky normal form (see Normal form)
Church-Turing thesis, 362
Circuit. 49
Closure, 37
properties, 126-128, 165-167, 272
of relations, 43
Commutativity, 38, 39
Comparison method, 152
Complexity, 346-371
Composition of functions. 324
Concatenation, 54
Congruence relation, 41
Conjunction (AND), 2
Conjunctive normal form, 14
Connectives, 2-6
Construction of reduced grammar, 190-196
Context-free
grammar
(language),
122,
180-218
decision algorithms for, 217
normal forms for, 201-213
and pda, 240-251
simplification of, 189-201
Context-sensitive grammar (language), 120,
299
Contradiction, 8
Cook's theorem. 354
CSAT. 359
Decidability. 310
Decidable languages, 311
419
-
420
~
Index
Decision algorithms (see Context-free grammar)
Derivation, 110, 111
leftmost, 187
rightmost, 187
Derivation tree (parse tree), 181. 184
definition of, 181
subtree of, 182
yield of, 182
Descendant, 51
Deterministic
finite automaton, 73
pda, 236
Directed graph (or digraph), 47
Dirichlet drawer principle, 46
Disjunction (OR), 3
Disjunctive nonnal fonn, 11
Distributivity, 39
Elementary product, 11
Equivalence
class, 42
of DFA and NDFA, 80--84
of finite automata, 157, 158
of regular expressions, 160
relation, 41
of states, 91
of well-fonned fonnulas, 9
Euclidean algorithm, 349
Fibonacci numbers, 69
Field, 39
Final state, 73, 74, 77
Finite automaton
deterrrtinistic, 73
minimization of, 91-97
nondeterministic, 78
and regular expression, 153
Function (or map), 45
by minimization, 330
partial, 322
by recursion, 328
total, 322
GOdel, Kurt, 332
Grammar, 109
monotonic, 121
self-embedding, 226
Graph,47
connected, 49
representation, 47
Greibach nonnal fonn (see Nonnal fonn)
Group, 38
Growth rate of functions, 346
Halting
problem
of
Turing
machine,
314-315
Hamiltonian circuit problem, 359
Handle production, 268
Hierarchy of languages, 120--122
ID (Instantaneous description)
of pushdown automaton, 229
of Turing machine, 279
Identities
logical, 10
for regular expressions, 138
Identity element, 38, 55
If and only if, 4
Implication (IF,,,THEN,,,), 4
Inclusion relation, 123
Initial function, 323
Induction, 57, 58, 60
Initial state, 73-74, 78, 228, 278
Internal vertex, 50
Inverse, 38
Kleene's theorem, 142
A-move, 140
elimination of, 141
Language(s)
and automaton, 128
classification of, 120
generated by a grammar, 110
Leaf of a tree, 50
Length
of path, 51
of string, 55
Levi's theorem, 56
Linear bounded automaton (LBA), 297-299,
301-303
and languages, 299-301
Logical connectives (see Connectives)
LR(k) graJlh'TIar, 267
Map, 45, 46
bijection (one-to-one correspondence), 45
one-to-one (injective), 45
onto (surjective), 45
Maxterm. 14
Mealy machine, 84
transformation
into
Moore
machine,
85-87
Minimization of automata. 91-97
Minterm, 12
Modus ponens, 16
Modus tollens, 16
Monoid. 38
Moore machine, 84
transformation into Mealy machine, 87-
89
"\love relation
in pda, 230
in Turing machine, 280
.'{AND, 33
Negation (NOT), 2
Nondeterministic finite automaton, 78
conversion to DFA, 80
Nondeterministic Turing machine, 295-297
NOR. 33
Normal form
Chomsky, 201-203
Greibach. 206--213
of well-formed formulas, II
NP-complete problem, 352
importance of, 352
Null productions, 196
elimination of. 196--199
One-to-oniC
correspondence
(see
also
Bijection),45
Operations on languages. 126--128
Ordered directed tree. 50
Palindrome. 55, 113
Parse tree (see Derivation tree)
Parsing and pda, 251-260
Letial recursive function. 330
and Turing machine. 332-340
Partition, 37
Index
l;i
421
Path,48
acceptance by, 231-240
pda, 230
Phrase structure grammar (see Grammar)
Pigeonhole principle, 46
Post
correspondence
problem
(PCP),
315-317
Power set 37
Predecessor, 48
Predicate. 19
Prefix of a string, 55
Primitive recursive function, 323-329
Principal conjunctive normal form, 15
construction to obtain, 15
Principle of induction, 57
Production (or production rule), 109
Proof
by contradiction, 61
by induction, 57
by modified method, 58
by simultaneous induction, 60
Proposition (or Statement), I
Propositional variable, 6
Pumping lemma
for context-free languages and appli-
cations, 213, 216
for regular sets and applications, 162-163
Push-down automaton, 227-251, 254
and context-free languages, 240-251
Quantum computation, 360
Quantum computers, 361
Quantum bit (qubiO, 361
Quantifier
existential, 20
universal, 20
Recursion, 37
Recursive definition of a set, 37
Recursive function, 329
Recursive set, 124
Recursively enumerable set, 124, 310
Reduction technique, 351
Reflexive-transitive closure, 43
Regular expressions, 136
finite automata and, 140
identities for. 138
rl
422
);!
Index
Regular grammar, 122
Regular sets, 137
closure properties of, 165-167
and regular grammar, 167
Relations
reflexive, 41
symmetric, 41
transitive, 41
Right-linear grammar, 226
Ring, 39
Root, 50
Russels paradox, 320
SAT problem (satisfiability problem), 353
Self-embedding grammar, 226
Semigroup, 38
Sentence, 110
Sentential form, 110
Sets, 36, 37, 38, 39, 40
complement of, 37
intersection of, 37
union of, 37
Simple graph, 70
Start symbol, 109
Statement (see Proposition)
String
empty, 54
length of, 55
operations on, 54
prefix of, 55
suffix of, 55
Strong Church-Turing thesis, 363
Subroutines, 290
Successor, 48
Symmetric difference, 68
Tautology, 8
Time complexity, 294, 349
Top-down parsing, 252
Top-down parsing, using deterministic pda's
256
Transition function, 73, 78, 228
properties of, 75-76
Transition system, 74
containing A-moves, 140
and regular grammar, 169
Transitive closure, 43
Transpose, 55
Travelling salesman problem, 359
Tree. 49
height of, 51
properties of, 49-50
Turing-computable functions, 333
Turing machine, 277
construction of, to compute the projection
function. 336
construction of, to compute the successor
function, 335
construction of, to compute the zero
function, 334
construction of, to pertorm composition,
338
construction of, to perform minimization.
340
construction of, to perform recursion, 339
description of, 289
design of, 284
multiple track, 290
multitape, 292
nondetenninistic, 295
representation by 10, 279
representation by transition diagram, 281
representation by transition table, 280
and type 0 grammar, 299-301
Type 0 grammar (see Grammar)
Type
1 grarmnar
(see Context-sensitive
grammar)
Type 2 grammar (see Context-free grammar)
Type 3 grammar (see Regular grammar)
Unambiguous grammar, 271
Undecidable language, 313
Unit production, 199
elimination of, 199-201
Valid
argument, 15
predicate formula, 22
Variable. 109
Vertex, 47
ancestor of. 51
degree of, 48
descendant of, 51
son of. 51
Well-formed formula, 6
or predicate calculus, 21
